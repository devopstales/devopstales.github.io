<?xml version="1.0" encoding="utf-8"?> 
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <generator uri="https://gohugo.io/" version="0.145.0">Hugo</generator><title type="html"><![CDATA[Virtualizations on DevOpsTales]]></title>
    
        <subtitle type="html"><![CDATA[Kubernetes, Cloud & Infrastructure Guides]]></subtitle>
    
    
    
            <link href="https://devopstales.github.io/virtualization/" rel="alternate" type="text/html" title="html" />
            <link href="https://devopstales.github.io/virtualization/index.xml" rel="alternate" type="application/rss+xml" title="rss" />
            <link href="https://devopstales.github.io/virtualization/atom.xml" rel="self" type="application/atom+xml" title="atom" />
    <updated>2026-03-02T10:00:21+00:00</updated>
    
    
    
    
        <id>https://devopstales.github.io/virtualization/</id>
    
        
        <entry>
            <title type="html"><![CDATA[veeam backup: I can not create a backup job to my AWS S3 repository]]></title>
            <link href="https://devopstales.github.io/virtualization/veeam-cant-create-job-to-s3/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/virtualization/veeam-cant-add-repo/?utm_source=atom_feed" rel="related" type="text/html" title="veeam backup: Cant Add Repository to a Scale Out Backup Repository?" />
                <link href="https://devopstales.github.io/virtualization/install-vmware-in-proxmox/?utm_source=atom_feed" rel="related" type="text/html" title="How to install ESXi nested inside ProxMox VE" />
                <link href="https://devopstales.github.io/kubernetes/k8s-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure Kubernetes In-Tree vSphere Cloud Provider" />
                <link href="https://devopstales.github.io/kubernetes/openshift-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure Openshift In-Tree vSphere Cloud Provider" />
                <link href="https://devopstales.github.io/kubernetes/openshift-restrict-access/?utm_source=atom_feed" rel="related" type="text/html" title="Restrict access to OpenShift routes by IP address" />
            
                <id>https://devopstales.github.io/virtualization/veeam-cant-create-job-to-s3/</id>
            
            
            <published>2022-07-24T00:00:00+00:00</published>
            <updated>2022-07-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this pos I will show you how you can backup to AWS S3 with Veeam Backup.</p>
<h3 id="the-issue">The Issue</h3>
<p>I already created the repository under &lsquo;backup infrastructure&quot; &gt; &ldquo;backup repositories&rsquo; however I can&rsquo;t select it as a storage location when creating a new backup job.</p>
<p><img src="/img/include/veeam-201.png" alt="veem-s3"  loading="lazy" decoding="async" class="zoomable" /></p>
<p>Choose &lsquo;Object storage&rsquo;</p>
<p><img src="/img/include/veeam-202.png" alt="veem-s3"  loading="lazy" decoding="async" class="zoomable" /></p>
<p>Choose &lsquo;Amazon S3&rsquo;</p>
<p><img src="/img/include/veeam-203.png" alt="veem-s3"  loading="lazy" decoding="async" class="zoomable" /></p>
<p>Fill in the name and description and click &lsquo;Next&rsquo;</p>
<p><img src="/img/include/veeam-204.png" alt="veem-s3"  loading="lazy" decoding="async" class="zoomable" /></p>
<p>Configure the region and the bucket.</p>
<p><img src="/img/include/veeam-205.png" alt="veem-s3"  loading="lazy" decoding="async" class="zoomable" /></p>
<h3 id="the-solution">The Solution</h3>
<p>Veeam uses object storage as a &ldquo;Capacity Tier&rdquo; of Scale-Out Backup Repository. So you will have to create a Scale Out Backup Repository.</p>
<p>Go &lsquo;Backup Infrastructure &gt; Add a Scale Out Backup Repository&rsquo;.</p>
<p><img src="/img/include/veeam-101.png" alt="Scale-out repository"  loading="lazy" decoding="async" class="zoomable" /></p>
<p><img src="/img/include/veeam-102.png" alt="Scale-out repository"  loading="lazy" decoding="async" class="zoomable" /></p>
<p>In its settings point Veeam to a local repository of any type.</p>
<p><img src="/img/include/veeam-103.png" alt="Scale-out repository"  loading="lazy" decoding="async" class="zoomable" /></p>
<p>Also point to your Capacity Tier account for archivals to an AWS S3 Bucket.</p>
<p><img src="/img/include/veeam-104.png" alt="Scale-out repository"  loading="lazy" decoding="async" class="zoomable" /></p>
<p>Here you can select to copy the bacup after creation or move backups to S3 after some time.</p>
<p><img src="/img/include/veeam-105.png" alt="Scale-out repository"  loading="lazy" decoding="async" class="zoomable" /></p>
<p>Point your backup job to that Scale-Out Backup repository.</p>
<p>Veeam will perform local backup first, and than, according to Scale-Out Backup Repository&rsquo;s Capacity Tier settings, will automatically offload backups to the cloud tier.</p>]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="virtualization" term="virtualization" label="Virtualization" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="virtualization" term="virtualization" label="Virtualization" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="vmware" term="vmware" label="vmware" />
                             
                                <category scheme="vsphere" term="vsphere" label="vSphere" />
                             
                                <category scheme="veeam-backup" term="veeam-backup" label="veeam backup" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[veeam backup: Cant Add Repository to a Scale Out Backup Repository?]]></title>
            <link href="https://devopstales.github.io/virtualization/veeam-cant-add-repo/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/virtualization/install-vmware-in-proxmox/?utm_source=atom_feed" rel="related" type="text/html" title="How to install ESXi nested inside ProxMox VE" />
                <link href="https://devopstales.github.io/kubernetes/k8s-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure Kubernetes In-Tree vSphere Cloud Provider" />
                <link href="https://devopstales.github.io/kubernetes/openshift-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure Openshift In-Tree vSphere Cloud Provider" />
                <link href="https://devopstales.github.io/kubernetes/openshift-restrict-access/?utm_source=atom_feed" rel="related" type="text/html" title="Restrict access to OpenShift routes by IP address" />
            
                <id>https://devopstales.github.io/virtualization/veeam-cant-add-repo/</id>
            
            
            <published>2022-07-22T00:00:00+00:00</published>
            <updated>2022-07-22T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>When adding repositories to a Veeam Scale Out Backup Repository you may see this error:  Cant Add Repository to a Scale Out Backup Repository? In this Pos I will show you how you can fix is issue.</p>
<h3 id="the-issue">The Issue</h3>
<p>When I try to add the defult repo in veeam bakup to the Scale Out Backup Repository I get the fallowin error:</p>
<blockquote>
<p>Unable to add extent {Repository-Name} because it serves as the target for one or more job types which are not supported by a scale-out backup repository.</p></blockquote>
<p>Selecting &lsquo;Show jobs&rsquo; shows:</p>
<p><img src="/img/include/veeam-001.png" alt="Show jobs"  loading="lazy" decoding="async" class="zoomable" /></p>
<h3 id="the-solution">The Solution</h3>
<p>Veeam Backup create a &ldquo;Job&rdquo; to vackup its internal database in case you you wanted to reinstall or migrate Veeam to another server. It gets created automatically, to the default repository.
To Fix: Simply create a new repository just for the Configuration-Backup-Job. I typically name the repository &lsquo;Config-Backup-Repository&rsquo; to avoid confusion in the future.</p>
<p><img src="/img/include/veeam-002.png" alt="Repoosirory List"  loading="lazy" decoding="async" class="zoomable" /></p>
<p>Now simply change the job to use the new repository, this is NOT done where all the other jobs are configured!
Select &lsquo;Options &gt; Configuration Backup &gt; Change the Repository&rsquo;  (I manually run it, by clicking Backup-Now) at this point, just to make sure all is well.</p>
<p><img src="/img/include/veeam-003.png" alt="Change Job"  loading="lazy" decoding="async" class="zoomable" /></p>
<p>You should now be able to create your Scale Out Backup Repository without an error.</p>]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="virtualization" term="virtualization" label="Virtualization" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="virtualization" term="virtualization" label="Virtualization" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="vmware" term="vmware" label="vmware" />
                             
                                <category scheme="vsphere" term="vsphere" label="vSphere" />
                             
                                <category scheme="veeam-backup" term="veeam-backup" label="veeam backup" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to install ESXi nested inside ProxMox VE]]></title>
            <link href="https://devopstales.github.io/virtualization/install-vmware-in-proxmox/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure Kubernetes In-Tree vSphere Cloud Provider" />
                <link href="https://devopstales.github.io/kubernetes/openshift-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure Openshift In-Tree vSphere Cloud Provider" />
                <link href="https://devopstales.github.io/kubernetes/openshift-restrict-access/?utm_source=atom_feed" rel="related" type="text/html" title="Restrict access to OpenShift routes by IP address" />
            
                <id>https://devopstales.github.io/virtualization/install-vmware-in-proxmox/</id>
            
            
            <published>2022-07-20T00:00:00+00:00</published>
            <updated>2022-07-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Int his post I will show you how you can install an vmware ESX host in Proxmox VE.</p>
<h3 id="the-issue">The Issue</h3>
<p>I need o test a vmware environment so I will install vmware ESX 7.0 as a VM in Proxmox VE 7.2. The problem is that, with wrong selection of virtual devices, vSphere will not be able to see the virtual hard drive, and will not be star the install.</p>
<h3 id="enable-nested-virtualization-on-proxmox">Enable nested virtualization on Proxmox:</h3>
<p>First we need to make sure nested virtualization is enabled on PVE host:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>cat /sys/module/kvm_intel/parameters/nested
</span></span></code></pre></div><p>If it returns <code>N</code>, that means it&rsquo;s disabled:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Intel</span>
</span></span><span style="display:flex;"><span>echo <span style="color:#e6db74">&#34;options kvm-intel nested=Y&#34;</span> &gt; /etc/modprobe.d/kvm-intel.conf
</span></span><span style="display:flex;"><span><span style="color:#75715e"># AMD</span>
</span></span><span style="display:flex;"><span>echo <span style="color:#e6db74">&#34;options kvm-amd nested=1&#34;</span> &gt; /etc/modprobe.d/kvm-amd.conf
</span></span></code></pre></div><p>Then you need to reenable the kvm kernel module. Fot this you need to stop all the virtual machines on the host or reboot the host after run the commands:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>modprobe -r kvm_intel
</span></span><span style="display:flex;"><span>modprobe kvm_intel
</span></span><span style="display:flex;"><span><span style="color:#75715e"># AMD</span>
</span></span><span style="display:flex;"><span>modprobe -r kvm_amd
</span></span><span style="display:flex;"><span>modprobe kvm_amd
</span></span></code></pre></div><p>Check again:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Intel</span>
</span></span><span style="display:flex;"><span>echo <span style="color:#e6db74">&#34;options kvm-intel nested=Y&#34;</span> &gt; /etc/modprobe.d/kvm-intel.conf
</span></span><span style="display:flex;"><span><span style="color:#75715e"># AMD</span>
</span></span><span style="display:flex;"><span>echo <span style="color:#e6db74">&#34;options kvm-amd nested=1&#34;</span> &gt; /etc/modprobe.d/kvm-amd.conf
</span></span></code></pre></div><h3 id="creating-the-virtual-machine">Creating the Virtual Machine</h3>
<p>This step is quite straightforward: you just need to create the new virtual machine, but you need to pay attention to the options:</p>
<ul>
<li>&lsquo;OS Tab&rsquo;
<ul>
<li>&lsquo;Type:&rsquo; Linux</li>
<li>&lsquo;Version:&rsquo; 5.x – 2.6 Kernel</li>
</ul>
</li>
<li>&lsquo;System Tab&rsquo;
<ul>
<li>&lsquo;Graphic card:&rsquo; Default</li>
<li>&lsquo;SCSI Controller:&rsquo; VMware PVSCSI</li>
<li>&lsquo;BIOS:&rsquo; SeaBIOS</li>
<li>&lsquo;Machine:&rsquo; q35</li>
</ul>
</li>
<li>&lsquo;Hard Disk Tab&rsquo;
<ul>
<li>&lsquo;Bus/Device:&rsquo; SATA</li>
<li>&lsquo;SSD emulation:&rsquo; Check</li>
<li>&lsquo;Discard:&rsquo; Check</li>
</ul>
</li>
<li>&lsquo;CPU Tab&rsquo;
<ul>
<li>&lsquo;Cores:&rsquo; 4</li>
<li>&lsquo;Sockets:&rsquo; 2</li>
<li>&lsquo;Type:&rsquo; host</li>
<li>&lsquo;Enable NUMA:&rsquo; Check if our system supports</li>
</ul>
</li>
<li>&lsquo;Memory Tab&rsquo;
<ul>
<li>&lsquo;Memory (MiB):&rsquo; At least 4096 but to run vcenter 16384</li>
<li>&lsquo;Ballooning Device:&rsquo; Uncheck</li>
</ul>
</li>
<li>&lsquo;Network Tab&rsquo;
<ul>
<li>&lsquo;Model:&rsquo; VMware vmxnet3</li>
</ul>
</li>
</ul>
<p>Start the vSphere VM</p>]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="virtualization" term="virtualization" label="Virtualization" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="virtualization" term="virtualization" label="Virtualization" />
                            
                        
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="centos" term="centos" label="CentOS" />
                             
                                <category scheme="proxmox" term="proxmox" label="Proxmox" />
                             
                                <category scheme="vmware" term="vmware" label="vmware" />
                             
                                <category scheme="vsphere" term="vsphere" label="vSphere" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to: Enable Serial Console for guest virtual machine (VM) on Proxmox VE (PVE)]]></title>
            <link href="https://devopstales.github.io/virtualization/proxmox-xtermjs-enable/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/virtualization/proxmox-backup-error/?utm_source=atom_feed" rel="related" type="text/html" title="Solution for: Proxmox backup error due to iothread" />
                <link href="https://devopstales.github.io/virtualization/proxmox-backup-pve-zsync/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with pve-zsync" />
                <link href="https://devopstales.github.io/virtualization/proxmox-backup-sanoid/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with sanoid" />
                <link href="https://devopstales.github.io/virtualization/proxmox-backup-znapzend/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with znapzend" />
                <link href="https://devopstales.github.io/linux/proxmox-node-remove/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox node removal" />
            
                <id>https://devopstales.github.io/virtualization/proxmox-xtermjs-enable/</id>
            
            
            <published>2022-04-19T00:00:00+00:00</published>
            <updated>2022-04-19T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>This article explains how to redirect messages to a serial console in on Debian and use Serial Console on Proxmox VE.</p>
<p>We need to add serial0 to the virtual machine. Use following command to add the serial0 port</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>qm set <span style="color:#f92672">[</span>vmid<span style="color:#f92672">]</span> -serial0 socket
</span></span></code></pre></div><p>Now we need to configure the Debian:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>nano /etc/default/grub
</span></span><span style="display:flex;"><span>GRUB_CMDLINE_LINUX_DEFAULT<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;console=ttyS0,115200n8 console=tty1&#34;</span>
</span></span><span style="display:flex;"><span>GRUB_CMDLINE_LINUX<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;&#34;</span>
</span></span><span style="display:flex;"><span>GRUB_TERMINAL<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;serial console&#34;</span>
</span></span><span style="display:flex;"><span>GRUB_SERIAL_COMMAND<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;serial --speed=115200 --unit=0 --word=8 --parity=no --stop=1&#34;</span>
</span></span></code></pre></div><p>Now we need to configure the REHEL:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>echo <span style="color:#e6db74">&#39;GRUB_CMDLINE_LINUX=&#34;quiet console=tty0 console=ttyS0,115200&#34;&#39;</span> &gt;&gt; /tmp/grub
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>nano /etc/default/grub
</span></span></code></pre></div><p>Add <code>console=tty0 console=ttyS0,115200</code> to the end of the line <code>GRUB_CMDLINE_LINUX</code></p>
<p>Now we need to update the grub by using following command</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Debian/Ubuntu etc.</span>
</span></span><span style="display:flex;"><span>update-grub
</span></span><span style="display:flex;"><span><span style="color:#75715e"># RHEL/CentOS/Fedora</span>
</span></span><span style="display:flex;"><span>grub2-mkconfig --output<span style="color:#f92672">=</span>/boot/grub2/grub.cfg
</span></span></code></pre></div><p>Set to autostart the serial consol service.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>mkdir -p /etc/systemd/system/serial-getty@ttyS0.service.d/
</span></span><span style="display:flex;"><span>nano /etc/systemd/system/serial-getty@ttyS0.service.d/override.conf
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>Service<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>ExecStart<span style="color:#f92672">=</span>
</span></span><span style="display:flex;"><span>ExecStart<span style="color:#f92672">=</span>-/sbin/agetty -o <span style="color:#e6db74">&#39;-p -- \\u&#39;</span> <span style="color:#ae81ff">115200</span> %I $TERM
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>systemctl daemon-reload
</span></span><span style="display:flex;"><span>systemctl restart serial-getty@ttyS0.service
</span></span><span style="display:flex;"><span>systemctl enable serial-getty@ttyS0.service
</span></span></code></pre></div><p>Reboot and test the autostart of the Serial Console</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>init <span style="color:#ae81ff">6</span>
</span></span><span style="display:flex;"><span>ps -ef | grep ttyS0
</span></span><span style="display:flex;"><span>systemctl status serial-getty@ttyS0.service
</span></span></code></pre></div>]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="proxmox" term="proxmox" label="Proxmox" />
                            
                        
                    
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="proxmox" term="proxmox" label="Proxmox" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Enable Proxmox PCIe Passthrough]]></title>
            <link href="https://devopstales.github.io/virtualization/proxmox-pci-passthrough/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/virtualization/proxmox-pci-passthrough/</id>
            
            
            <published>2022-03-08T00:00:00+00:00</published>
            <updated>2022-03-08T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Proxmox VE allows the passthrough of PCIe devices to individual virtual machines. In this blog post I will show you how you can configure it.</p>
<h3 id="grub-configuration">Grub Configuration</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>nano /etc/default/grub
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Example for an Intel system:</span>
</span></span><span style="display:flex;"><span>GRUB_CMDLINE_LINUX_DEFAULT<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;quiet intel_iommu=on iommu=pt&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Example for an AMD system:</span>
</span></span><span style="display:flex;"><span>GRUB_CMDLINE_LINUX_DEFAULT<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;quiet amd_iommu=on iommu=pt&#34;</span>
</span></span></code></pre></div><p>Then update the grub:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>update-grub
</span></span></code></pre></div><h3 id="add-kernel-modules">Add kernel modules</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>nano /etc/modules
</span></span><span style="display:flex;"><span>vfio
</span></span><span style="display:flex;"><span>vfio_iommu_type1
</span></span><span style="display:flex;"><span>vfio_pci
</span></span><span style="display:flex;"><span>vfio_virqfd
</span></span></code></pre></div><p>Save the file and update the initramfs:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>update-initramfs -u -k all
</span></span></code></pre></div><h3 id="perform-restart">Perform restart</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>init <span style="color:#ae81ff">6</span>
</span></span></code></pre></div><h3 id="check-function">Check function</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>cat /proc/cmdline
</span></span><span style="display:flex;"><span>BOOT_IMAGE<span style="color:#f92672">=</span>/boot/vmlinuz-5.4.128-1-pve root<span style="color:#f92672">=</span>/dev/mapper/pve-root ro quiet intel_iommu<span style="color:#f92672">=</span>on iommu<span style="color:#f92672">=</span>pt
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>dmesg |grep -e DMAR -e IOMMU -e AMD-Vi
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>...<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>    0.273374<span style="color:#f92672">]</span> DMAR: IOMMU enabled
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>...<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>    1.722014<span style="color:#f92672">]</span> DMAR: Intel<span style="color:#f92672">(</span>R<span style="color:#f92672">)</span> Virtualization Technology <span style="color:#66d9ef">for</span> Directed I/O
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>lsmod | grep vfio
</span></span><span style="display:flex;"><span>vfio_pci               <span style="color:#ae81ff">49152</span>  <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>vfio_virqfd            <span style="color:#ae81ff">16384</span>  <span style="color:#ae81ff">1</span> vfio_pci
</span></span><span style="display:flex;"><span>irqbypass              <span style="color:#ae81ff">16384</span>  <span style="color:#ae81ff">2</span> vfio_pci,kvm
</span></span><span style="display:flex;"><span>vfio_iommu_type1       <span style="color:#ae81ff">32768</span>  <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>vfio                   <span style="color:#ae81ff">32768</span>  <span style="color:#ae81ff">2</span> vfio_iommu_type1,vfio_pci
</span></span></code></pre></div><h3 id="configuration-ethernet-network-card-passthrough">Configuration Ethernet network card passthrough</h3>
<p>Show PCI devices:</p>
<p>Identify network card:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ll /sys/class/net/eno1
</span></span><span style="display:flex;"><span>lrwxrwxrwx <span style="color:#ae81ff">1</span> root root <span style="color:#ae81ff">0</span> Aug  <span style="color:#ae81ff">6</span> 12:09 /sys/class/net/eno1 -&gt; ../../devices/pci0000:00/0000:00:01.1/0000:09:00.0/net/eno1
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>lspci | grep -iE --color <span style="color:#e6db74">&#39;network|ethernet&#39;</span>
</span></span><span style="display:flex;"><span>05:00.0 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection <span style="color:#f92672">(</span>rev 01<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>05:00.1 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection <span style="color:#f92672">(</span>rev 01<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>09:00.0 Ethernet controller: Intel Corporation I350 Gigabit Network Connection <span style="color:#f92672">(</span>rev 01<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>09:00.1 Ethernet controller: Intel Corporation I350 Gigabit Network Connection <span style="color:#f92672">(</span>rev 01<span style="color:#f92672">)</span>
</span></span></code></pre></div><h3 id="add-device-to-vm">Add device to VM</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>nano /etc/pve/qemu-server/108.conf
</span></span><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>hostpci0: 0000:09:00.0
</span></span><span style="display:flex;"><span><span style="color:#75715e"># for multiple ones</span>
</span></span><span style="display:flex;"><span>hostpci0: 0000:09:00.0;0000:09:00.1
</span></span></code></pre></div>]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="proxmox" term="proxmox" label="Proxmox" />
                            
                        
                    
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="proxmox" term="proxmox" label="Proxmox" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Solution for: Proxmox backup error due to iothread]]></title>
            <link href="https://devopstales.github.io/virtualization/proxmox-backup-error/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/virtualization/proxmox-backup-pve-zsync/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with pve-zsync" />
                <link href="https://devopstales.github.io/virtualization/proxmox-backup-sanoid/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with sanoid" />
                <link href="https://devopstales.github.io/virtualization/proxmox-backup-znapzend/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with znapzend" />
                <link href="https://devopstales.github.io/linux/proxmox-node-remove/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox node removal" />
            
                <id>https://devopstales.github.io/virtualization/proxmox-backup-error/</id>
            
            
            <published>2019-11-18T00:00:00+00:00</published>
            <updated>2019-11-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>If you see the following error when trying to backup a KVM VM image on Proxmox:</p>
<pre tabindex="0"><code>ERROR: Backup of VM 100 failed – disk ‘scsi0’ ‘zfsvols:vm-100-disk-1’ (iothread=on) can’t use backup feature currently. Please set backup=no for this drive at /usr/share/perl5/PVE/VZDump/QemuServer.pm line 77. INFO: Backup job finished with errors TASK ERROR: job errors
</code></pre><p>Posted on September 9, 2017 by Daniel Mettler
Solution for: Proxmox backup error due to iothread=1</p>
<p>If you see the following error when trying to backup a KVM VM image on Proxmox:</p>
<p>ERROR: Backup of VM 100 failed – disk ‘scsi0’ ‘zfsvols:vm-100-disk-1’ (iothread=on) can’t use backup feature currently. Please set backup=no for this drive at /usr/share/perl5/PVE/VZDump/QemuServer.pm line 77. INFO: Backup job finished with errors TASK ERROR: job errors</p>
<p>edit /etc/pve/qemu-server/100.conf, look for a line similar to</p>
<pre tabindex="0"><code>scsi0: zfsvols:vm-100-disk-1,iothread=1,size=70G
</code></pre><p>and change it to</p>
<pre tabindex="0"><code>scsi0: zfsvols:vm-100-disk-1,iothread=0,size=70G
# OR
scsi0: zfsvols:vm-100-disk-1,size=70G
</code></pre><p>After this you can backup the VM. This Problem was solvd in the proxmox 6 (pve-manager 6.0-11)</p>]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="proxmox" term="proxmox" label="Proxmox" />
                            
                        
                    
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="proxmox" term="proxmox" label="Proxmox" />
                             
                                <category scheme="zfs" term="zfs" label="ZFS" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install CEHP Radosgateway on Proxmox]]></title>
            <link href="https://devopstales.github.io/virtualization/proxmox-ceph-radosgw/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/virtualization/proxmox-ceph-radosgw/</id>
            
            
            <published>2019-06-14T00:00:00+00:00</published>
            <updated>2019-06-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>RADOS Gateway  is an object storage interface in Ceph. It provides interfaces compatible with OpenStack Swift and Amazon S3.</p>
<p>First create a keyring than generated the keys and added them to the keyring:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>root@pve1:~# ceph-authtool --create-keyring /etc/ceph/ceph.client.radosgw.keyring
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>root@pve1:~# ceph-authtool /etc/ceph/ceph.client.radosgw.keyring -n client.radosgw.pve1 --gen-key
</span></span><span style="display:flex;"><span>root@pve1:~# ceph-authtool /etc/ceph/ceph.client.radosgw.keyring -n client.radosgw.pve2 --gen-key
</span></span><span style="display:flex;"><span>root@pve1:~# ceph-authtool /etc/ceph/ceph.client.radosgw.keyring -n client.radosgw.pve3 --gen-key
</span></span></code></pre></div><p>And then I added the proper capabilities and add the keys to the cluster:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>root@pve1:~# ceph-authtool -n client.radosgw.pve1 --cap osd <span style="color:#e6db74">&#39;allow rwx&#39;</span> --cap mon <span style="color:#e6db74">&#39;allow rwx&#39;</span> /etc/ceph/ceph.client.radosgw.keyring
</span></span><span style="display:flex;"><span>root@pve1:~# ceph-authtool -n client.radosgw.pve2 --cap osd <span style="color:#e6db74">&#39;allow rwx&#39;</span> --cap mon <span style="color:#e6db74">&#39;allow rwx&#39;</span> /etc/ceph/ceph.client.radosgw.keyring
</span></span><span style="display:flex;"><span>root@pve1:~# ceph-authtool -n client.radosgw.pve3 --cap osd <span style="color:#e6db74">&#39;allow rwx&#39;</span> --cap mon <span style="color:#e6db74">&#39;allow rwx&#39;</span> /etc/ceph/ceph.client.radosgw.keyring
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>scp /etc/ceph/ceph.client.admin.keyring /etc/ceph/ceph.client.radosgw.keyring pve2:/etc/ceph/
</span></span><span style="display:flex;"><span>scp /etc/ceph/ceph.client.admin.keyring /etc/ceph/ceph.client.radosgw.keyring pve3:/etc/ceph/
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>root@pve1:~# ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.radosgw.pve1 -i /etc/ceph/ceph.client.radosgw.keyring
</span></span><span style="display:flex;"><span>root@pve1:~# ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.radosgw.pve2 -i /etc/ceph/ceph.client.radosgw.keyring
</span></span><span style="display:flex;"><span>root@pve1:~# ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.radosgw.pve3 -i /etc/ceph/ceph.client.radosgw.keyring
</span></span></code></pre></div><p>If you get the fallofing error: <code>handle_auth_bad_method server allowed_methods [2] but i only support [2]</code></p>
<p>You Have a problem with your <code>/etc/ceph/ceph.client.admin.keyring</code> file:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo ceph --cluster ceph auth get-key client.admin
</span></span><span style="display:flex;"><span>AQDxnppkhI2ZOBAAJ1VFYV6FvRi8vZyuUYzwZQ<span style="color:#f92672">==</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>nano /etc/ceph/ceph.client.admin.keyring
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>client.admin<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>	key <span style="color:#f92672">=</span> AQDxnppkhI2ZOBAAJ1VFYV6FvRi8vZyuUYzwZQ<span style="color:#f92672">==</span>
</span></span><span style="display:flex;"><span>	caps mds <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;allow *&#34;</span>
</span></span><span style="display:flex;"><span>	caps mgr <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;allow *&#34;</span>
</span></span><span style="display:flex;"><span>	caps mon <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;allow *&#34;</span>
</span></span><span style="display:flex;"><span>	caps osd <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;allow *&#34;</span>
</span></span></code></pre></div><p>Copy to the other nodes:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>scp /etc/ceph/ceph.client.admin.keyring /etc/ceph/ceph.client.radosgw.keyring pve2:/etc/ceph/
</span></span><span style="display:flex;"><span>scp /etc/ceph/ceph.client.admin.keyring /etc/ceph/ceph.client.radosgw.keyring pve3:/etc/ceph/
</span></span></code></pre></div><p>Copy the rings to the proxmox ClusterFS</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>root@pve1:~# cp /etc/ceph/ceph.client.radosgw.keyring /etc/pve/priv
</span></span></code></pre></div><p>Add the following lines to <code>/etc/ceph/ceph.conf</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#f92672">[</span>client.radosgw.pve1<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>        host <span style="color:#f92672">=</span> pve1
</span></span><span style="display:flex;"><span>        keyring <span style="color:#f92672">=</span> /etc/pve/priv/ceph.client.radosgw.keyring
</span></span><span style="display:flex;"><span>        log file <span style="color:#f92672">=</span> /var/log/ceph/client.radosgw.$host.log
</span></span><span style="display:flex;"><span>        rgw_dns_name <span style="color:#f92672">=</span> s3.devopstales.intra
</span></span><span style="display:flex;"><span>        rgw_frontends <span style="color:#f92672">=</span> civetweb port<span style="color:#f92672">=</span>10.83.110.1:7480
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>client.radosgw.pve2<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>        host <span style="color:#f92672">=</span> pve2
</span></span><span style="display:flex;"><span>        keyring <span style="color:#f92672">=</span> /etc/pve/priv/ceph.client.radosgw.keyring
</span></span><span style="display:flex;"><span>        log file <span style="color:#f92672">=</span> /var/log/ceph/client.radosgw.$host.log
</span></span><span style="display:flex;"><span>        rgw_dns_name <span style="color:#f92672">=</span> s3.devopstales.intra
</span></span><span style="display:flex;"><span>        rgw_frontends <span style="color:#f92672">=</span> civetweb port<span style="color:#f92672">=</span>10.83.110.2:7480
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>client.radosgw.pve3<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>        host <span style="color:#f92672">=</span> pve3
</span></span><span style="display:flex;"><span>        keyring <span style="color:#f92672">=</span> /etc/pve/priv/ceph.client.radosgw.keyring
</span></span><span style="display:flex;"><span>        log file <span style="color:#f92672">=</span> /var/log/ceph/client.rados.$host.log
</span></span><span style="display:flex;"><span>        rgw_dns_name <span style="color:#f92672">=</span> s3.devopstales.intra
</span></span><span style="display:flex;"><span>        rgw_frontends <span style="color:#f92672">=</span> civetweb port<span style="color:#f92672">=</span>10.83.110.3:7480
</span></span></code></pre></div><p>Install the pcakages and start the service. If all goes well, RADOSGW will create some default pools for you.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>root@pve1:~# apt install radosgw
</span></span><span style="display:flex;"><span>root@pve1:~# service radosgw start
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>root@pve1:~# tail -f /var/log/ceph/client.rados.pve1.log
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>root@pve1:~# ceph osd pool application enable .rgw.root rgw
</span></span><span style="display:flex;"><span>root@pve1:~# ceph osd pool application enable default.rgw.control rgw
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>root@pve1:~# ceph osd pool application enable default.rgw.data.root rgw
</span></span><span style="display:flex;"><span>root@pve1:~# ceph osd pool application enable default.rgw.gc rgw
</span></span><span style="display:flex;"><span>root@pve1:~# ceph osd pool application enable default.rgw.log rgw
</span></span><span style="display:flex;"><span>root@pve1:~# ceph osd pool application enable default.rgw.users.uid rgw
</span></span><span style="display:flex;"><span>root@pve1:~# ceph osd pool application enable default.rgw.users.email rgw
</span></span><span style="display:flex;"><span>root@pve1:~# ceph osd pool application enable default.rgw.users.keys rgw
</span></span><span style="display:flex;"><span>root@pve1:~# ceph osd pool application enable default.rgw.buckets.index rgw
</span></span><span style="display:flex;"><span>root@pve1:~# ceph osd pool application enable default.rgw.buckets.data rgw
</span></span><span style="display:flex;"><span>root@pve1:~# ceph osd pool application enable default.rgw.lc rgw
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>root@pve1:~#  ssh pve2 <span style="color:#e6db74">&#39;apt install radosgw &amp;&amp; service radosgw start&#39;</span>
</span></span><span style="display:flex;"><span>root@pve1:~#  ssh pve3 <span style="color:#e6db74">&#39;apt install radosgw &amp;&amp; service radosgw start&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>root@pve1:~#  ceph osd pool ls
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>root@pve1:~# radosgw-admin user create --uid<span style="color:#f92672">=</span>devopstales --display-name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;devopstales&#34;</span> --email<span style="color:#f92672">=</span>devopstales@devopstales.intra
</span></span><span style="display:flex;"><span>root@pve1:~# radosgw-admin user info devopstales
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>root@pve1:~# ceph osd pool application enable default.rgw.buckets.index rgw
</span></span><span style="display:flex;"><span>root@pve1:~# ceph osd pool application enable default.rgw.buckets.data rgw
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#for minio cli to create bucketceph osd pool create default.rgw.buckets.data 32</span>
</span></span><span style="display:flex;"><span>ceph osd pool create default.rgw.buckets.index <span style="color:#ae81ff">8</span>
</span></span><span style="display:flex;"><span>ceph osd pool set default.rgw.buckets.index pgp_num <span style="color:#ae81ff">8</span>
</span></span><span style="display:flex;"><span>ceph osd pool set default.rgw.buckets.index size <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>ceph osd pool application enable default.rgw.buckets.index rgw
</span></span><span style="display:flex;"><span> 
</span></span><span style="display:flex;"><span>ceph osd pool create default.rgw.buckets.data <span style="color:#ae81ff">32</span>
</span></span><span style="display:flex;"><span>ceph osd pool set default.rgw.buckets.data pgp_num <span style="color:#ae81ff">32</span>
</span></span><span style="display:flex;"><span>ceph osd pool set default.rgw.buckets.data size <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>ceph osd pool application enable default.rgw.buckets.data rgw
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>root@pve1:~# apt-get install s3cmd
</span></span><span style="display:flex;"><span>root@pve1:~# s3cmd --configure
</span></span><span style="display:flex;"><span>Access Key: xxxxxxxxxxxxxxxxxxxxxx
</span></span><span style="display:flex;"><span>Secret Key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>root@pve1:~#  s3cmd mb s3://devopstales
</span></span><span style="display:flex;"><span>Bucket <span style="color:#e6db74">&#39;s3://devopstales/&#39;</span> created
</span></span></code></pre></div>]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="proxmox" term="proxmox" label="Proxmox" />
                             
                                <category scheme="ceph" term="ceph" label="Ceph" />
                            
                        
                    
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="proxmox" term="proxmox" label="Proxmox" />
                             
                                <category scheme="ceph" term="ceph" label="Ceph" />
                             
                                <category scheme="s3" term="s3" label="S3" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Proxmox backup with pve-zsync]]></title>
            <link href="https://devopstales.github.io/virtualization/proxmox-backup-pve-zsync/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/virtualization/proxmox-backup-sanoid/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with sanoid" />
                <link href="https://devopstales.github.io/virtualization/proxmox-backup-znapzend/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with znapzend" />
                <link href="https://devopstales.github.io/linux/proxmox-node-remove/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox node removal" />
            
                <id>https://devopstales.github.io/virtualization/proxmox-backup-pve-zsync/</id>
            
            
            <published>2019-03-11T00:00:00+00:00</published>
            <updated>2019-03-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Backup your machines on Pfsense with pve-zsync tool.</p>
<h3 id="the-servers">The servers</h3>
<pre tabindex="0"><code>proxmox-node1 192.168.10.20
omv-nas1 192.168.10.50
</code></pre><h3 id="install-pve-zsync-on-servers">Install pve-zsync on servers</h3>
<pre tabindex="0"><code>apt-get install pve-zsync
</code></pre><h3 id="configure-pve-zsync">Configure pve-zsync</h3>
<pre tabindex="0"><code>pve-zsync create --source 107 --dest 192.168.10.50:tank --verbose --maxsnap 14 --name Backup_ZFS_srv_107

# test the config
cat /etc/cron.d/pve-zsync
* 8 * * * root pve-zsync sync --source 107 --dest 192.168.10.50:tank --name Backup_ZFS_srv_107 --maxsnap 14 --method ssh

# send diff
pve-zsync sync --source 107 --dest 192.168.10.50:tank --verbose --maxsnap 14 --name Backup_ZFS_srv_107

# the tool send the vm config to the /var/lib/pve-zsync/
</code></pre>]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="proxmox" term="proxmox" label="Proxmox" />
                            
                        
                    
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="proxmox" term="proxmox" label="Proxmox" />
                             
                                <category scheme="zfs" term="zfs" label="ZFS" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Proxmox backup with sanoid]]></title>
            <link href="https://devopstales.github.io/virtualization/proxmox-backup-sanoid/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/virtualization/proxmox-backup-pve-zsync/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with pve-zsync" />
                <link href="https://devopstales.github.io/virtualization/proxmox-backup-znapzend/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with znapzend" />
                <link href="https://devopstales.github.io/linux/proxmox-node-remove/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox node removal" />
            
                <id>https://devopstales.github.io/virtualization/proxmox-backup-sanoid/</id>
            
            
            <published>2019-03-11T00:00:00+00:00</published>
            <updated>2019-03-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Backup your machines on Pfsense with sanoid tool.</p>
<h3 id="the-servers">The servers</h3>
<pre tabindex="0"><code>proxmox-node1 192.168.10.20
omv-nas1 192.168.10.50
</code></pre><h3 id="build-sanoid-on-servers">Build sanoid on servers</h3>
<pre tabindex="0"><code>apt-get install libcapture-tiny-perl libconfig-inifiles-perl git

cd /opt
git clone https://github.com/jimsalterjrs/sanoid

ln /opt/sanoid/sanoid /usr/sbin/
</code></pre><p>Or you can build deb package:</p>
<h3 id="build-and-install-sanoid-deb-package">Build and install sanoid deb package</h3>
<pre tabindex="0"><code>sudo apt-get install devscripts debhelper dh-systemd
git clone https://github.com/jimsalterjrs/sanoid.git
cd sanoid
debuild -us -uc

cd ..
sudo apt-get install libconfig-inifiles-perl
sudo dpkg -i sanoid_2.0.1_all.deb
</code></pre><h3 id="configure-sanoid">Configure sanoid</h3>
<pre tabindex="0"><code>mkdir -p /etc/sanoid
cp /opt/sanoid/sanoid.conf /etc/sanoid/sanoid.conf
cp /opt/sanoid/sanoid.defaults.conf /etc/sanoid/sanoid.defaults.conf

nano /etc/crontab
* 2 * * * root /usr/sbin/sanoid --cron
* 3 * * * root /usr/sbin/syncoid --recursive tank root@192.168.10.50:tank
</code></pre><pre tabindex="0"><code>    ####################
    # sanoid.conf file #
    ####################
    [local-zfs]
            use_template = production
    #############################
    # templates below this line #
    #############################
    [template_production]
            # store hourly snapshots 36h
            # hourly = 36
            # store 14 days of daily snaps
            daily = 14
            # store back 6 months of monthly
            # monthly = 6
            # store back 3 yearly (remove manually if to large)
            # yearly = 3
            # create new snapshots
            autosnap = yes
            # clean old snapshot
            autoprune = yes
</code></pre>]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="proxmox" term="proxmox" label="Proxmox" />
                            
                        
                    
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="proxmox" term="proxmox" label="Proxmox" />
                             
                                <category scheme="zfs" term="zfs" label="ZFS" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Proxmox backup with znapzend]]></title>
            <link href="https://devopstales.github.io/virtualization/proxmox-backup-znapzend/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/virtualization/proxmox-backup-pve-zsync/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with pve-zsync" />
                <link href="https://devopstales.github.io/virtualization/proxmox-backup-sanoid/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with sanoid" />
                <link href="https://devopstales.github.io/linux/proxmox-node-remove/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox node removal" />
            
                <id>https://devopstales.github.io/virtualization/proxmox-backup-znapzend/</id>
            
            
            <published>2019-03-11T00:00:00+00:00</published>
            <updated>2019-03-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Backup your machines on Pfsense with znapzend tool.</p>
<h3 id="the-servers">The servers</h3>
<pre tabindex="0"><code>proxmox-node1 192.168.10.20
omv-nas1 192.168.10.50
</code></pre><h3 id="create-sub-volume">Create sub volume</h3>
<p>Note that while recursive configurations are well supported to set up backup and retention policies for a whole dataset subtree under the dataset to which you have applied explicit configuration, at this time pruning of such trees (&ldquo;I want every dataset under var except var/tmp&rdquo;) is not supported.</p>
<pre tabindex="0"><code>zfs create local-zfs/vm-data
pvesm add zfspool local-zfs --pool local-zfs/vm-data
</code></pre><h3 id="build-znapzend-on-servers">Build znapzend on servers</h3>
<pre tabindex="0"><code>apt-get install perl unzip git mbuffer build-essential git

cd /root
git clone https://github.com/oetiker/znapzend
cd /root/znapzend
./configure --prefix=/opt/znapzend

make
make install

ln -s /opt/znapzend/bin/znapzend /usr/local/bin/znapzend
ln -s /opt/znapzend/bin/znapzendzetup /usr/local/bin/znapzendzetup
ln -s /opt/znapzend/bin/znapzendztatz /usr/local/bin/znapzendztatz

znapzend --version
</code></pre><p>Or you can download a the deb package from here:
<a href="https://github.com/devopstales/znapzend-debian/releases">https://github.com/devopstales/znapzend-debian/releases</a></p>
<h3 id="install-znapzend-on-servers">Install znapzend on servers</h3>
<pre tabindex="0"><code>dpkg -i znapzend_0.19.1_amd64_stretch.deb
</code></pre><h3 id="configure-znapzend">Configure znapzend</h3>
<pre tabindex="0"><code>znapzendzetup create --recursive\
--mbuffer=/usr/bin/mbuffer \
--mbuffersize=1G \
SRC &#39;2d=&gt;1d&#39; local-zfs/vmdata \
DST:a &#39;14d=&gt;1d&#39; root@192.168.10.50:tank

# test
znapzend --debug --noaction --runonce=local-zfs
znapzendzetup list
</code></pre><h3 id="create-znapzend-service">Create znapzend service</h3>
<pre tabindex="0"><code>nano /etc/default/znapzend
ZNAPZENDOPTIONS=&#34;--logto=/var/log/znapzend.log&#34;

systemctl enable znapzend.service
systemctl restart znapzend.service
systemctl status znapzend.service
</code></pre>]]></content>
            
                 
                    
                         
                        
                            
                             
                                <category scheme="proxmox" term="proxmox" label="Proxmox" />
                            
                        
                    
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="proxmox" term="proxmox" label="Proxmox" />
                             
                                <category scheme="zfs" term="zfs" label="ZFS" />
                            
                        
                    
                
            
        </entry>
    
</feed>
