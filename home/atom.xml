<?xml version="1.0" encoding="utf-8"?> 
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <generator uri="https://gohugo.io/" version="0.89.2">Hugo</generator><title type="html"><![CDATA[Homes on devopstales]]></title>
    
        <subtitle type="html"><![CDATA[Blog about dev and ops stuff]]></subtitle>
    
    
    
            <link href="https://devopstales.github.io/home/" rel="alternate" type="text/html" title="HTML" />
            <link href="https://devopstales.github.io/home/index.xml" rel="alternate" type="application/rss+xml" title="RSS" />
            <link href="https://devopstales.github.io/home/atom.xml" rel="self" type="application/atom+xml" title="Atom" />
    <updated>2022-03-16T10:54:58+00:00</updated>
    
    
    <author>
            <name>Blaiserman</name>
            </author>
    
        <id>https://devopstales.github.io/home/</id>
    
        
        <entry>
            <title type="html"><![CDATA[Flagger NGINX Canary Deployments]]></title>
            <link href="https://devopstales.github.io/home/flagger-nginx-canary-deployments/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/gitops-flux2-sops/?utm_source=atom_feed" rel="related" type="text/html" title="Flux2 and Mozilla SOPS to encrypt secrets" />
                <link href="https://devopstales.github.io/home/gitops-flux2/?utm_source=atom_feed" rel="related" type="text/html" title="Flux2 Install and Usage" />
                <link href="https://devopstales.github.io/home/gitops-flux2-kubeseal/?utm_source=atom_feed" rel="related" type="text/html" title="Flux2 and kubeseal to encrypt secrets" />
                <link href="https://devopstales.github.io/home/argocd-image-updater/?utm_source=atom_feed" rel="related" type="text/html" title="Argo CD Image Updater for automate image update" />
                <link href="https://devopstales.github.io/home/argocd-kubeseal/?utm_source=atom_feed" rel="related" type="text/html" title="ArgoCD and kubeseal to encrypt secrets" />
            
                <id>https://devopstales.github.io/home/flagger-nginx-canary-deployments/</id>
            
            
            <published>2022-03-15T00:00:00+00:00</published>
            <updated>2022-03-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this blog post, I will show you how you can install Flagger and use it to set up progressive delivery for the <code>podinfo</code> app to your Kubernetes cluster.</p>
<H3>Parst of the K8S Gitops series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-gitops/">GitOps solutions for Kubernetes</a></li>
     <li>Part2: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part3: <a href="../../kubernetes/argocd-image-updater/">Argo CD Image Updater for automate image update</a></li>
<!-- ArgoCD notify
https://blog.argoproj.io/notifications-for-argo-bb7338231604
-->
<!----------------------------------------------->
     <li>Part4: <a href="../../kubernetes/gitops-flux2/">Flux2 Install and Usage</a></li>
     <li>Part5: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part6: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- Flux CD auto image update
https://particule.io/en/blog/flux-auto-image-update/
-->
<!----------------------------------------------->
<!-- Fleet -->
<!----------------------------------------------->
<!-- Canary Deployment:
Canary Basics - https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#canary-deployments
Canary with helm:
  https://itnext.io/safe-and-automation-friendly-canary-deployments-with-helm-669394d2c48a
  https://github.com/stoehdoi/canary-demo/tree/master/deploy
ArgoCD + Argo Rollouts for canary deploy:
-->
     <li>Part7: <a href="../../kubernetes/flagger-nginx-canary-deployments/">Flagger NGINX Canary Deployments</a></li>
</ul>



<h3 id="what-is-flagger">What is Flagger</h3>
<p>Flagger is a progressive delivery operator for Kubernetes that resolves the outlined problem by gradually shifting traffic to the new release while monitoring configured metrics. It can perform automated analysis and testing on the new release, deciding whether to propagate it to the whole cluster or stop if issues are found. Flagger slowly increases the load on the new release while keeping the old one available, ensuring minimal downtime. It can send notifications to Slack, Microsoft Teams, and other platforms to notify you and your team of transpired events.</p>
<h3 id="prerequisites">Prerequisites</h3>
<ul>
<li>nginx ingress controller</li>
<li>flagger</li>
<li>flagger-loadtester</li>
<li>prometheus</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">helm repo add flagger https://flagger.app

helm upgrade -i flagger flagger/flagger <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--namespace ingress-nginx <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set prometheus.install<span style="color:#f92672">=</span>true <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set meshProvider<span style="color:#f92672">=</span>nginx
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
kubectl create ns ingress-nginx
helm upgrade -i ingress-nginx ingress-nginx/ingress-nginx <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--namespace ingress-nginx <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set controller.metrics.enabled<span style="color:#f92672">=</span>true <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set controller.podAnnotations.<span style="color:#e6db74">&#34;prometheus\.io/scrape&#34;</span><span style="color:#f92672">=</span>true <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set controller.podAnnotations.<span style="color:#e6db74">&#34;prometheus\.io/port&#34;</span><span style="color:#f92672">=</span><span style="color:#ae81ff">10254</span>
</code></pre></div><h3 id="deploying-an-app">Deploying an App</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl create ns test

kubectl apply -k https://github.com/fluxcd/flagger//kustomize/podinfo?ref<span style="color:#f92672">=</span>main
</code></pre></div><pre tabindex="0"><code>kubectl get pods -n test
#Output
NAME                       READY   STATUS    RESTARTS   AGE
podinfo-78fd6c49bf-jsjm5   1/1     Running   0          18s
podinfo-78fd6c49bf-k2nh4   0/1     Running   0          3s
</code></pre><p>Now that the pods are running, you’ll create an Ingress to expose the app at your domain. Open a file called <code>podinfo-ingress.yaml</code> for editing:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano podinfo-ingress.yaml
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: podinfo
  namespace: test
  labels:
    app: podinfo
spec:
  ingressClassName: nginx
  rules:
    - host: <span style="color:#e6db74">&#34;app.k8s.mydomain.intra&#34;</span>
      http:
        paths:
          - pathType: Prefix
            path: <span style="color:#e6db74">&#34;/&#34;</span>
            backend:
              service:
                name: podinfo
                port:
                  number: <span style="color:#ae81ff">80</span>
</code></pre></div><p>Before you create the canary, you’ll need to deploy Flagger’s load tester, which allows canary resources to test releases by sending HTTP requests.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f podinfo-ingress.yaml

helm install flagger-loadtester flagger/loadtester -n test
</code></pre></div><p>Note that the <code>podinfo</code> service does not yet exist in your cluster. It will be created later by Flagger automatically as part of the canary.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano podinfo-canary.yaml
---
apiVersion: flagger.app/v1beta1
kind: Canary
metadata:
  name: podinfo
  namespace: test
spec:
  provider: nginx
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: podinfo
  ingressRef:
    apiVersion: networking.k8s.io/v1
    kind: Ingress
    name: podinfo
  progressDeadlineSeconds: <span style="color:#ae81ff">60</span>
  service:
    port: <span style="color:#ae81ff">80</span>
    targetPort: <span style="color:#ae81ff">9898</span>
  analysis:
    interval: 10s
    threshold: <span style="color:#ae81ff">10</span>
    maxWeight: <span style="color:#ae81ff">50</span>
    stepWeight: <span style="color:#ae81ff">5</span>
    metrics:
    - name: request-success-rate
      thresholdRange:
        min: <span style="color:#ae81ff">99</span>
      interval: 1m
    webhooks:
      - name: acceptance-test
        type: pre-rollout
        url: http://flagger-loadtester.test/
        timeout: 30s
        metadata:
          type: bash
          cmd: <span style="color:#e6db74">&#34;curl -sd &#39;test&#39; http://podinfo-canary/token | grep token&#34;</span>
      - name: load-test
        url: http://flagger-loadtester.test/
        timeout: 5s
        metadata:
          cmd: <span style="color:#e6db74">&#34;hey -z 1m -q 10 -c 2 http://app.k8s.mydomain.intra/&#34;</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f podinfo-canary.yaml
</code></pre></div><p>You can now navigate to <code>app.k8s.mydomain.intra</code>. You’ll see the <code>podinfo</code> app:</p>
<p><img src="/img/include/flagger-nginx01.png" alt="appinfo"  class="zoomable" /></p>
<p>You can press the <code>Ping</code> button to refresh version numbers of other pods. Run the following command to set a different version of podinfo:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl set image deployment/podinfo podinfod<span style="color:#f92672">=</span>stefanprodan/podinfo:6.0.3 -n test
</code></pre></div><p>Flagger will detect that the deployment revision number changed, which you can check by listing the events associated with the <code>podinfo</code> canary:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl describe canary/podinfo -n test
<span style="color:#75715e">#Output</span>
Events:
  Type     Reason  Age                From     Message
  ----     ------  ----               ----     -------
  Normal   Synced  117s               flagger  New revision detected! Scaling up podinfo.test
  Warning  Synced  107s               flagger  canary deployment podinfo.test not ready: waiting <span style="color:#66d9ef">for</span> rollout to finish: <span style="color:#ae81ff">0</span> of <span style="color:#ae81ff">2</span> <span style="color:#f92672">(</span>readyThreshold 100%<span style="color:#f92672">)</span> updated replicas are available
  Warning  Synced  97s                flagger  canary deployment podinfo.test not ready: waiting <span style="color:#66d9ef">for</span> rollout to finish: <span style="color:#ae81ff">1</span> of <span style="color:#ae81ff">2</span> <span style="color:#f92672">(</span>readyThreshold 100%<span style="color:#f92672">)</span> updated replicas are available
  Normal   Synced  87s                flagger  Starting canary analysis <span style="color:#66d9ef">for</span> podinfo.test
  Normal   Synced  87s                flagger  Pre-rollout check acceptance-test passed
  Normal   Synced  87s                flagger  Advance podinfo.test canary weight <span style="color:#ae81ff">5</span>
  Warning  Synced  67s <span style="color:#f92672">(</span>x2 over 77s<span style="color:#f92672">)</span>  flagger  Halt advancement no values found <span style="color:#66d9ef">for</span> nginx metric request-success-rate probably podinfo.test is not receiving traffic: running query failed: no values found
  Normal   Synced  57s                flagger  Advance podinfo.test canary weight <span style="color:#ae81ff">10</span>
  Normal   Synced  47s                flagger  Advance podinfo.test canary weight <span style="color:#ae81ff">15</span>
  Normal   Synced  37s                flagger  Advance podinfo.test canary weight <span style="color:#ae81ff">20</span>
  Normal   Synced  27s                flagger  Advance podinfo.test canary weight <span style="color:#ae81ff">25</span>
</code></pre></div><p>Return to your browser and watch the version numbers flicker as the app continually refreshes itself.</p>
<p><img src="/img/include/flagger-nginx02.png" alt="appinfo"  class="zoomable" /></p>
<p>Flagger denotes the traffic shifts with events starting with Advance podinfo.test canary weight, followed by the percentage of traffic being diverted:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl describe canary/podinfo -n test
<span style="color:#75715e">#Output</span>
Output
...
  Normal   Synced  116s                  flagger  Advance podinfo.test canary weight <span style="color:#ae81ff">10</span>
  Normal   Synced  106s                  flagger  Advance podinfo.test canary weight <span style="color:#ae81ff">15</span>
...
</code></pre></div><p>After some time, the canary deployment should succeed and the version numbers will stabilize:</p>
<p><img src="/img/include/flagger-nginx03.png" alt="appinfo"  class="zoomable" /></p>
<p>The final event log of the canary will look similar to this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl describe canary/podinfo -n test
<span style="color:#75715e">#Output</span>
Events:
  Type     Reason  Age                   From     Message
  ----     ------  ----                  ----     -------
  Normal   Synced  2m56s                 flagger  New revision detected! Scaling up podinfo.test
  Warning  Synced  2m46s                 flagger  canary deployment podinfo.test not ready: waiting <span style="color:#66d9ef">for</span> rollout to finish: <span style="color:#ae81ff">0</span> of <span style="color:#ae81ff">2</span> <span style="color:#f92672">(</span>readyThreshold 100%<span style="color:#f92672">)</span> updated replicas are available
  Warning  Synced  2m36s                 flagger  canary deployment podinfo.test not ready: waiting <span style="color:#66d9ef">for</span> rollout to finish: <span style="color:#ae81ff">1</span> of <span style="color:#ae81ff">2</span> <span style="color:#f92672">(</span>readyThreshold 100%<span style="color:#f92672">)</span> updated replicas are available
  Normal   Synced  2m26s                 flagger  Starting canary analysis <span style="color:#66d9ef">for</span> podinfo.test
  Normal   Synced  2m26s                 flagger  Pre-rollout check acceptance-test passed
  Normal   Synced  2m26s                 flagger  Advance podinfo.test canary weight <span style="color:#ae81ff">5</span>
  Warning  Synced  2m6s <span style="color:#f92672">(</span>x2 over 2m16s<span style="color:#f92672">)</span>  flagger  Halt advancement no values found <span style="color:#66d9ef">for</span> nginx metric request-success-rate probably podinfo.test is not receiving traffic: running query failed: no values found
  Normal   Synced  116s                  flagger  Advance podinfo.test canary weight <span style="color:#ae81ff">10</span>
  Normal   Synced  106s                  flagger  Advance podinfo.test canary weight <span style="color:#ae81ff">15</span>
  Normal   Synced  96s                   flagger  Advance podinfo.test canary weight <span style="color:#ae81ff">20</span>
  Normal   Synced  86s                   flagger  Advance podinfo.test canary weight <span style="color:#ae81ff">25</span>
  Normal   Synced  76s                   flagger  Advance podinfo.test canary weight <span style="color:#ae81ff">30</span>
  Warning  Synced  16s                   flagger  podinfo-primary.test not ready: waiting <span style="color:#66d9ef">for</span> rollout to finish: <span style="color:#ae81ff">1</span> old replicas are pending termination
  Normal   Synced  6s <span style="color:#f92672">(</span>x6 over 66s<span style="color:#f92672">)</span>      flagger  <span style="color:#f92672">(</span>combined from similar events<span style="color:#f92672">)</span>: Routing all traffic to primary
</code></pre></div><h3 id="reporting-to-slack">Reporting to Slack</h3>
<p>You can configure Flagger to send its logs to your Slack workspace. To use Slack integration, you’ll need to have an <a href="https://api.slack.com/messaging/webhooks">incoming webhook</a> on Slack for your workspace.</p>
<p>To do so, first log in to Slack and navigate to the <a href="https://api.slack.com/apps?new_app=1">app creation</a> page. Pick a name that you’ll recognize, select the desired workspace, and click <code>Create App</code>.</p>
<p>You’ll be redirected to the settings page for the new app. Click on <code>Incoming Webhooks</code> on the left navigation bar.</p>
<p><img src="/img/include/flagger-nginx04.png" alt="slack webhook"  class="zoomable" /></p>
<p>Enable webhooks by flipping the switch button next to the title <code>Activate Incoming Webhooks</code>.</p>
<p><img src="/img/include/flagger-nginx05.png" alt="slack webhook"  class="zoomable" /></p>
<p>To configure Flagger to send logs to Slack, you’ll need to update its Helm release by running:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">helm upgrade flagger flagger/flagger <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--reuse-values <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set slack.url<span style="color:#f92672">=</span>&lt;your_hook_URL&gt; <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set slack.channel<span style="color:#f92672">=</span>&lt;your_channel_name&gt; <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set slack.user<span style="color:#f92672">=</span>&lt;username&gt;
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl set image deployment/podinfo podinfod<span style="color:#f92672">=</span>stefanprodan/podinfo:3.1.1 -n test
</code></pre></div><p>You’ll soon see messages appearing in Slack:</p>
<p><img src="/img/include/flagger-nginx06.png" alt="slack messages"  class="zoomable" /></p>
<p>When this release deploys, you’ll see a success message:</p>
<p><img src="/img/include/flagger-nginx07.png" alt="slack messages"  class="zoomable" /></p>
<p>For the new release, deploy the <code>6.0.3</code> version again by running:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl set image deployment/podinfo podinfod<span style="color:#f92672">=</span>stefanprodan/podinfo:6.0.3 -n test
</code></pre></div><p>Run the following command to create a large number of HTTP 500 statuses:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">watch curl http://app.your_domain/status/500
</code></pre></div><p>After some time, you’ll see that Flagger decided not to apply the new release.</p>
<p><img src="/img/include/flagger-nginx08.png" alt="slack messages"  class="zoomable" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install an OpenShift 4 cluster with Calico]]></title>
            <link href="https://devopstales.github.io/home/openshift4-calico/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/openshift4-calico/?utm_source=atom_feed" rel="related" type="text/html" title="Install an OpenShift 4 cluster with Calico" />
                <link href="https://devopstales.github.io/home/aws-eks-ingress/?utm_source=atom_feed" rel="related" type="text/html" title="Deploy Ingress Controller to EKS cluster with WAF" />
                <link href="https://devopstales.github.io/home/aws-eks-install/?utm_source=atom_feed" rel="related" type="text/html" title="Create EKS Cluster with eksctl" />
                <link href="https://devopstales.github.io/home/aws-eks-networking/?utm_source=atom_feed" rel="related" type="text/html" title="AWS EKS Network Solutions" />
                <link href="https://devopstales.github.io/home/multus-calico/?utm_source=atom_feed" rel="related" type="text/html" title="Use multus to separate metwork trafics" />
            
                <id>https://devopstales.github.io/home/openshift4-calico/</id>
            
            
            <published>2022-03-12T00:00:00+00:00</published>
            <updated>2022-03-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this Post I will show you How you can Install  OpenShift 4 cluster with Calico.</p>
<H3>Parst of the Openshift 4 series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/openshift4-install/">Install Opeshift 4</a></li>
     <li>Part1b: <a href="../../kubernetes/openshift4-calico/">Install Opeshift 4 with calico</a></li>
     <li>Part2: <a href="../../kubernetes/openshift4-ingress/">Configure OKD OpenShift 4 ingress</a></li>
     <li>Part3: <a href="../../kubernetes/openshift4-auth/">Configure OKD OpenShift 4 authentication</a></li>
     <li>Part4: <a href="../../kubernetes/openshift4-ceph-rbd-csi/">Configure OKD OpenShift 4 Ceph Persisten Storage</a></li>
     <li>Part5a: <a href="../../kubernetes/openshift4-logging/">Install Cluster Logging Operator on OpenShift 4</a></li>
     <li>Part5b: <a href="../../kubernetes/openshift-log4shell/">Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)</a></li>
</ul>


<h3 id="infrastructure">Infrastructure</h3>
<table>
<thead>
<tr>
<th style="text-align:center">Host</th>
<th style="text-align:center">ROLES</th>
<th style="text-align:center">OS</th>
<th style="text-align:center">IP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">pfsense</td>
<td style="text-align:center">Load Balancer, dhcp, dns</td>
<td style="text-align:center">pfsense</td>
<td style="text-align:center">192.168.1.1</td>
</tr>
<tr>
<td style="text-align:center">okd4-services</td>
<td style="text-align:center">pxeboot</td>
<td style="text-align:center">CentOS 7</td>
<td style="text-align:center">192.168.1.200</td>
</tr>
<tr>
<td style="text-align:center">okd4-bootstrap</td>
<td style="text-align:center">bootstrap</td>
<td style="text-align:center">Fedora Core OS</td>
<td style="text-align:center">192.168.1.210</td>
</tr>
<tr>
<td style="text-align:center">okd4-mastr-1</td>
<td style="text-align:center">master</td>
<td style="text-align:center">Fedora Core OS</td>
<td style="text-align:center">192.168.1.201</td>
</tr>
<tr>
<td style="text-align:center">okd4-mastr-2</td>
<td style="text-align:center">master</td>
<td style="text-align:center">Fedora Core OS</td>
<td style="text-align:center">192.168.1.202</td>
</tr>
<tr>
<td style="text-align:center">okd4-mastr-3</td>
<td style="text-align:center">master</td>
<td style="text-align:center">Fedora Core OS</td>
<td style="text-align:center">192.168.1.203</td>
</tr>
<tr>
<td style="text-align:center">okd4-worker-1</td>
<td style="text-align:center">worker</td>
<td style="text-align:center">Fedora Core OS</td>
<td style="text-align:center">192.168.1.204</td>
</tr>
<tr>
<td style="text-align:center">okd4-worker-2</td>
<td style="text-align:center">worker</td>
<td style="text-align:center">Fedora Core OS</td>
<td style="text-align:center">192.168.1.205</td>
</tr>
<tr>
<td style="text-align:center">okd4-worker-4</td>
<td style="text-align:center">worker</td>
<td style="text-align:center">Fedora Core OS</td>
<td style="text-align:center">192.168.1.206</td>
</tr>
<tr>
<td style="text-align:center">okd4-worker-5</td>
<td style="text-align:center">worker</td>
<td style="text-align:center">Fedora Core OS</td>
<td style="text-align:center">192.168.1.207</td>
</tr>
</tbody>
</table>
<h3 id="dns-config">DNS Config</h3>
<pre tabindex="0"><code>; OpenShift Container Platform Cluster - A records
pfsense.okd.mydomain.intra.          IN      A      192.168.1.1
okd4-bootstrap.okd.mydomain.intra.   IN      A      192.168.1.210

okd4-mastr-1.okd.mydomain.intra.        IN      A      192.168.1.201
okd4-mastr-2.okd.mydomain.intra.        IN      A      192.168.1.202
okd4-mastr-3.okd.mydomain.intra.        IN      A      192.168.1.203
okd4-worker-1.okd.mydomain.intra.        IN      A      192.168.1.204
okd4-worker-2.okd.mydomain.intra.        IN      A      192.168.1.205
okd4-worker-3.okd.mydomain.intra.        IN      A      192.168.1.206
okd4-worker-4.okd.mydomain.intra.        IN      A      192.168.1.207


; OpenShift internal cluster IPs - A records
api.okd.mydomain.intra.            IN      A      192.168.1.1
api-int.okd.mydomain.intra.        IN      A      192.168.1.1
etcd-0.okd.mydomain.intra.         IN      A     192.168.1.201
etcd-1.okd.mydomain.intra.         IN      A     192.168.1.202
etcd-2.okd.mydomain.intra.         IN      A     192.168.1.203

okd.mydomain.intra.                IN      A      192.168.1.1
*.okd.mydomain.intra.              IN      A      192.168.1.1

; OpenShift internal cluster IPs - SRV records
_etcd-server-ssl._tcp.okd.mydomain.intra.    86400     IN    SRV     0    10    2380    etcd-0.okd.mydomain.intra.
_etcd-server-ssl._tcp.okd.mydomain.intra.    86400     IN    SRV     0    10    2380    etcd-1.okd.mydomain.intra.
_etcd-server-ssl._tcp.okd.mydomain.intra.    86400     IN    SRV     0    10    2380    etcd-2.okd.mydomain.intra.
</code></pre><h3 id="dhcp-config">DHCP Config:</h3>
<pre tabindex="0"><code>32:89:07:57:27:00  192.168.1.200 	okd4-services
32:89:07:57:27:10  192.168.1.210 	okd4-bootstrap
32:89:07:57:27:01  192.168.1.201 	okd4-mastr-1
32:89:07:57:27:02  192.168.1.202 	okd4-mastr-2
32:89:07:57:27:03  192.168.1.203 	okd4-mastr-3
32:89:07:57:27:04  192.168.1.204 	okd4-worker-1
32:89:07:57:27:05  192.168.1.205 	okd4-worker-2
32:89:07:57:27:06  192.168.1.206 	okd4-worker-3
32:89:07:57:27:07  192.168.1.207 	okd4-worker-4
</code></pre><pre tabindex="0"><code>Next Server: 192.168.1.200
Default BIOS file name: pxelinux.0
</code></pre><h2 id="haproxy-config">HAPROXY Config:</h2>
<pre tabindex="0"><code>192.168.201.1 6443  --&gt;  192.168.1.210   6443
192.168.201.1 6443  --&gt;  192.168.1.201  6443
192.168.201.1 6443  --&gt;  192.168.1.202  6443
192.168.201.1 6443  --&gt;  192.168.1.202  6443
192.168.201.1 22623 --&gt;  192.168.1.210   22623
192.168.201.1 22623 --&gt;  192.168.1.201  22623
192.168.201.1 22623 --&gt;  192.168.1.202  22623
192.168.201.1 22623 --&gt;  192.168.1.202  22623
192.168.201.1 80    --&gt;  192.168.1.204  80
192.168.201.1 80    --&gt;  192.168.1.205  80
192.168.201.1 443   --&gt;  192.168.1.204  443
192.168.201.1 443   --&gt;  192.168.1.205  443
&lt;publicip&gt; 80    --&gt;  192.168.1.206  80
&lt;publicip&gt; 80    --&gt;  192.168.1.207  80
&lt;publicip&gt; 443   --&gt;  192.168.1.206  443
&lt;publicip&gt; 443   --&gt;  192.168.1.207  443
</code></pre><h3 id="install-and-configure-pxeboot">Install and configure pxeboot</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ssh okd4-services

yum install epel-release -y
yum install httpd nano jq -y
dnf install -y tftp-server syslinux-tftpboot
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">mkdir -p /var/lib/tftpboot
cp -v /tftpboot/pxelinux.0 /var/lib/tftpboot/
cp -v /tftpboot/menu.c32 /var/lib/tftpboot/
cp -v /tftpboot/mboot.c32 /var/lib/tftpboot/
cp -v /tftpboot/chain.c32 /var/lib/tftpboot/
cp -v /tftpboot/ldlinux.c32 /var/lib/tftpboot/
cp -v /tftpboot/libutil.c32 /var/lib/tftpboot/

mkdir -p /var/lib/tftpboot/fcsos33
cd /var/lib/tftpboot/fcsos33
RHCOS_BASEURL<span style="color:#f92672">=</span>https://builds.coreos.fedoraproject.org/prod/streams/stable/builds/
wget <span style="color:#e6db74">${</span>RHCOS_BASEURL<span style="color:#e6db74">}</span>33.20210117.3.2/x86_64/fedora-coreos-33.20210117.3.2-live-kernel-x86_64
wget <span style="color:#e6db74">${</span>RHCOS_BASEURL<span style="color:#e6db74">}</span>33.20210117.3.2/x86_64/fedora-coreos-33.20210117.3.2-live-initramfs.x86_64.img
wget <span style="color:#e6db74">${</span>RHCOS_BASEURL<span style="color:#e6db74">}</span>33.20210117.3.2/x86_64/fedora-coreos-33.20210117.3.2-live-rootfs.x86_64.img
cd ~

mkdir /var/lib/tftpboot/pxelinux.cfg
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat &gt; /var/lib/tftpboot/pxelinux.cfg/default <span style="color:#e6db74">&lt;&lt; EOF
</span><span style="color:#e6db74">default menu.c32
</span><span style="color:#e6db74">prompt 0
</span><span style="color:#e6db74">timeout 30
</span><span style="color:#e6db74">menu title PXE Menu
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">label 1
</span><span style="color:#e6db74">menu label ^1) Boot from local drive
</span><span style="color:#e6db74">localboot 0x00
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">label 2
</span><span style="color:#e6db74">menu label ^2) Install OKD Bootstrap
</span><span style="color:#e6db74">KERNEL /fcsos33/fedora-coreos-33.20210117.3.2-live-kernel-x86_64
</span><span style="color:#e6db74">APPEND initrd=/fcsos33/fedora-coreos-33.20210117.3.2-live-initramfs.x86_64.img,/fcsos33/fedora-coreos-33.20210117.3.2-live-rootfs.x86_64.img coreos.inst.install_dev=/dev/vda coreos.inst.image_url=http://192.168.201.4/fcos.raw.xz coreos.inst.ignition_url=http://192.168.201.4/bootstrap.ign
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">label 3
</span><span style="color:#e6db74">menu label ^3) Install OKD Master
</span><span style="color:#e6db74">KERNEL /fcsos33/fedora-coreos-33.20210117.3.2-live-kernel-x86_64
</span><span style="color:#e6db74">APPEND initrd=/fcsos33/fedora-coreos-33.20210117.3.2-live-initramfs.x86_64.img,/fcsos33/fedora-coreos-33.20210117.3.2-live-rootfs.x86_64.img coreos.inst.install_dev=/dev/vda coreos.inst.image_url=http://192.168.201.4/fcos.raw.xz coreos.inst.ignition_url=http://192.168.201.4/master.ign
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">label 4
</span><span style="color:#e6db74">menu label ^4) Install OKD Worker
</span><span style="color:#e6db74">KERNEL /fcsos33/fedora-coreos-33.20210117.3.2-live-kernel-x86_64
</span><span style="color:#e6db74">APPEND initrd=/fcsos33/fedora-coreos-33.20210117.3.2-live-initramfs.x86_64.img,/fcsos33/fedora-coreos-33.20210117.3.2-live-rootfs.x86_64.img coreos.inst.install_dev=/dev/vda coreos.inst.image_url=http://192.168.201.4/fcos.raw.xz coreos.inst.ignition_url=http://192.168.201.4/worker.ign
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><p>Then run: <code>systemctl enable --now tftp.service</code></p>
<h3 id="create-okd-config">Create okd config</h3>
<blockquote>
<p>find the iso:
<a href="https://getfedora.org/en/coreos/download?tab=metal_virtualized&amp;stream=stable">https://getfedora.org/en/coreos/download?tab=metal_virtualized&amp;stream=stable</a>
4K vs non 4K</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">wget fedora-coreos-33.20210117.3.2-live.x86_64.iso
wget https://builds.coreos.fedoraproject.org/prod/streams/stable/builds/33.20210117.3.2/x86_64/fedora-coreos-33.20210117.3.2-metal.x86_64.raw.xz
wget https://builds.coreos.fedoraproject.org/prod/streams/stable/builds/33.20210117.3.2/x86_64/fedora-coreos-33.20210117.3.2-metal.x86_64.raw.xz.sig
cp fedora-coreos-33.20210117.3.2-metal.x86_64.raw.xz /var/www/html/fcos.raw.xz
cp fedora-coreos-33.20210117.3.2-metal.x86_64.raw.xz.sig /var/www/html/fcos.raw.xz.sig
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># find installer</span>
<span style="color:#75715e"># https://github.com/openshift/okd/releases</span>

wget https://github.com/openshift/okd/releases/download/4.6.0-0.okd-2021-02-14-205305/openshift-client-linux-4.6.0-0.okd-2021-02-14-205305.tar.gz
wget https://github.com/openshift/okd/releases/download/4.6.0-0.okd-2021-02-14-205305/openshift-install-linux-4.6.0-0.okd-2021-02-14-205305.tar.gz

tar -xzf openshift-client-linux-4.6.0-0.okd-2021-02-14-205305.tar.gz
tar -xzf openshift-install-linux-4.6.0-0.okd-2021-02-14-205305.tar.gz

sudo mv kubectl oc openshift-install /usr/local/bin/
oc version
openshift-install version

mkdir install_dir
</code></pre></div><p>Use Calico for <code>networkType</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat &gt; install_dir/install-config.yaml <span style="color:#e6db74">&lt;&lt; EOF
</span><span style="color:#e6db74">apiVersion: v1
</span><span style="color:#e6db74">baseDomain: mydomain.intra
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: okd
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">compute:
</span><span style="color:#e6db74">- hyperthreading: Enabled
</span><span style="color:#e6db74">  name: worker
</span><span style="color:#e6db74">  replicas: 0
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">controlPlane:
</span><span style="color:#e6db74">  hyperthreading: Enabled
</span><span style="color:#e6db74">  name: master
</span><span style="color:#e6db74">  replicas: 3
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">networking:
</span><span style="color:#e6db74">  clusterNetwork:
</span><span style="color:#e6db74">  - cidr: 10.128.0.0/14
</span><span style="color:#e6db74">    hostPrefix: 23
</span><span style="color:#e6db74">  networkType: Calico
</span><span style="color:#e6db74">  serviceNetwork:
</span><span style="color:#e6db74">  - 172.30.0.0/16
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">platform:
</span><span style="color:#e6db74">  none: {}
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">fips: false
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">pullSecret: &#39;{&#34;auths&#34;:{&#34;fake&#34;:{&#34;auth&#34;: &#34;bar&#34;}}}&#39;
</span><span style="color:#e6db74">sshKey: &#39;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDK7lDozs9WLJD14H+nz...&#39; 
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">openshift-install create manifests --dir<span style="color:#f92672">=</span>install_dir/
</code></pre></div><p>You may want to provide Calico with additional configuration at install-time. For example, BGP configuration or peers. You can use a Kubernetes ConfigMap with your desired Calico resources in order to set configuration as part of the installation.</p>
<p>To include <a href="https://projectcalico.docs.tigera.io/reference/resources/">Calico resources</a> during installation, edit <code>install_dir/manifests/02-configmap-calico-resources.yaml</code> in order to add your own configuration.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">oc create configmap -n tigera-operator calico-resources <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --from-file<span style="color:#f92672">=</span>&lt;resource-directory&gt; --dry-run -o yaml <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  &gt; install_dir/manifests/02-configmap-calico-resources.yaml
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sed -i <span style="color:#e6db74">&#39;s/mastersSchedulable: true/mastersSchedulable: False/&#39;</span> install_dir/manifests/cluster-scheduler-02-config.yml

openshift-install create ignition-configs --dir<span style="color:#f92672">=</span>install_dir/

sudo cp -R install_dir/*.ign /var/www/html/
sudo cp -R install_dir/metadata.json /var/www/html/
sudo chown -R apache: /var/www/html/
sudo chmod -R <span style="color:#ae81ff">755</span> /var/www/html/
</code></pre></div><blockquote>
<p>The config contains certificates that is walid for 24 hours.</p>
</blockquote>
<h3 id="starting-the-vms">Starting the VMs</h3>
<p>It&rsquo;s time to start the VMs. Select the okd4-bootstrap VM and navigate to Console. Start the VM. Then one by one the masters and the workers too.</p>
<h3 id="bootstrap-okd-cluster">Bootstrap OKD Cluster</h3>
<p>You can monitor the installation progress by running the following command.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">openshift-install --dir<span style="color:#f92672">=</span>install_dir/ wait-for bootstrap-complete --log-level<span style="color:#f92672">=</span>info
</code></pre></div><blockquote>
<p>The certificates in the cluster is not authomaticle approved so I use the abow <code>tmux</code> command to approve</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">tmux
export KUBECONFIG<span style="color:#f92672">=</span>~/install_dir/auth/kubeconfig
<span style="color:#66d9ef">while</span> true; <span style="color:#66d9ef">do</span> echo <span style="color:#e6db74">`</span>oc get csr -o go-template<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;{{range .items}}{{if not .status}}{{.metadata.name}}{{&#34;\n&#34;}}{{end}}{{end}}&#39;</span> | xargs -r oc adm certificate approve<span style="color:#e6db74">`</span>; sleep 60; <span style="color:#66d9ef">done</span>
</code></pre></div><p>Once the bootstrap process completes, you should see the following messages.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">INFO It is now safe to remove the bootstrap resources
</code></pre></div><p>Then stop the bootstrap node.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># debug command to check the health of the cluster.</span>
watch oc get csr
watch oc get node

oc get clusteroperator
oc get clusterversion

watch <span style="color:#e6db74">&#34;oc get clusteroperator&#34;</span>
watch <span style="color:#e6db74">&#34;oc get po -A | grep -v Running | grep -v Completed&#34;</span>


curl -X GET https://api.okd.mydomain.intra:6443/healthz -k
</code></pre></div><p>Verify Calico is installed by verifying the components are available with the following command.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">oc get tigerastatus
</code></pre></div><p>Wait for the console to be available. Once it is available, we can point a browser to <a href="https://console-openshift-console.okd.mydomain.intra">https://console-openshift-console.okd.mydomain.intra</a></p>
<p>You will get an SSL error because the certificate is not valid for this domain. That&rsquo;s normal. Just bypass the SSL error.</p>
<p>Login with user &ldquo;kubeadmin&rdquo;.You can find the kubeadmin password in a file generated during the installation.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat install_dir/auth/kubeadmin-password
</code></pre></div><p>Optionally you can integrate with Operator Lifecycle Manager (OLM). First you will need to create an OperatorGroup for the operator:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">oc apply -f - &lt;&lt;EOF</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">operators.coreos.com/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">OperatorGroup</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">tigera-operator</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">tigera-operator</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">targetNamespaces</span>:
    - <span style="color:#ae81ff">tigera-operator</span>
<span style="color:#ae81ff">EOF</span>
</code></pre></div><p>Next, you will create a Subscription to the operator.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">oc apply -f - &lt;&lt;EOF</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">operators.coreos.com/v1alpha1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Subscription</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">tigera-operator</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">tigera-operator</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">channel</span>: <span style="color:#ae81ff">release-v1.25</span>
  <span style="color:#f92672">installPlanApproval</span>: <span style="color:#ae81ff">Manual</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">tigera-operator</span>
  <span style="color:#f92672">source</span>: <span style="color:#ae81ff">certified-operators</span>
  <span style="color:#f92672">sourceNamespace</span>: <span style="color:#ae81ff">openshift-marketplace</span>
  <span style="color:#f92672">startingCSV</span>: <span style="color:#ae81ff">tigera-operator.v1.25.3</span>
<span style="color:#ae81ff">EOF</span>
</code></pre></div><p>Finally, log in to the OpenShift console, navigate to the Installed Operators section and approve the Install Plan for the operator.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Enable Proxmox PCIe Passthrough]]></title>
            <link href="https://devopstales.github.io/home/proxmox-pci-passthrough/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/proxmox-pci-passthrough/?utm_source=atom_feed" rel="related" type="text/html" title="Enable Proxmox PCIe Passthrough" />
                <link href="https://devopstales.github.io/home/k8s-test-tools/?utm_source=atom_feed" rel="related" type="text/html" title="Validate Kubernetes Deployment in CI/CD" />
                <link href="https://devopstales.github.io/home/aws-eks-ecr/?utm_source=atom_feed" rel="related" type="text/html" title="Elastic Container Registry Integration with EKS" />
                <link href="https://devopstales.github.io/home/aws-s2s-vpn/?utm_source=atom_feed" rel="related" type="text/html" title="AWS - pfsense: Site-to-site VPN using static routes" />
                <link href="https://devopstales.github.io/home/aws-eks-amp-monitoring/?utm_source=atom_feed" rel="related" type="text/html" title="Using AWS Prometheus (AMP) for monitoring AWS EKS cluster." />
            
                <id>https://devopstales.github.io/home/proxmox-pci-passthrough/</id>
            
            
            <published>2022-03-08T00:00:00+00:00</published>
            <updated>2022-03-08T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Proxmox VE allows the passthrough of PCIe devices to individual virtual machines. In this blog post I will show you how you can configure it.</p>
<h3 id="grub-configuration">Grub Configuration</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano /etc/default/grub
<span style="color:#75715e"># Example for an Intel system:</span>
GRUB_CMDLINE_LINUX_DEFAULT<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;quiet intel_iommu=on iommu=pt&#34;</span>

<span style="color:#75715e"># Example for an AMD system:</span>
GRUB_CMDLINE_LINUX_DEFAULT<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;quiet amd_iommu=on iommu=pt&#34;</span>
</code></pre></div><p>Then update the grub:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">update-grub
</code></pre></div><h3 id="add-kernel-modules">Add kernel modules</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano /etc/modules
vfio
vfio_iommu_type1
vfio_pci
vfio_virqfd
</code></pre></div><p>Save the file and update the initramfs:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">update-initramfs -u -k all
</code></pre></div><h3 id="perform-restart">Perform restart</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">init <span style="color:#ae81ff">6</span>
</code></pre></div><h3 id="check-function">Check function</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat /proc/cmdline
BOOT_IMAGE<span style="color:#f92672">=</span>/boot/vmlinuz-5.4.128-1-pve root<span style="color:#f92672">=</span>/dev/mapper/pve-root ro quiet intel_iommu<span style="color:#f92672">=</span>on iommu<span style="color:#f92672">=</span>pt
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">dmesg |grep -e DMAR -e IOMMU -e AMD-Vi
<span style="color:#f92672">[</span>...<span style="color:#f92672">]</span>
<span style="color:#f92672">[</span>    0.273374<span style="color:#f92672">]</span> DMAR: IOMMU enabled
<span style="color:#f92672">[</span>...<span style="color:#f92672">]</span>
<span style="color:#f92672">[</span>    1.722014<span style="color:#f92672">]</span> DMAR: Intel<span style="color:#f92672">(</span>R<span style="color:#f92672">)</span> Virtualization Technology <span style="color:#66d9ef">for</span> Directed I/O
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">lsmod | grep vfio
vfio_pci               <span style="color:#ae81ff">49152</span>  <span style="color:#ae81ff">0</span>
vfio_virqfd            <span style="color:#ae81ff">16384</span>  <span style="color:#ae81ff">1</span> vfio_pci
irqbypass              <span style="color:#ae81ff">16384</span>  <span style="color:#ae81ff">2</span> vfio_pci,kvm
vfio_iommu_type1       <span style="color:#ae81ff">32768</span>  <span style="color:#ae81ff">0</span>
vfio                   <span style="color:#ae81ff">32768</span>  <span style="color:#ae81ff">2</span> vfio_iommu_type1,vfio_pci
</code></pre></div><h3 id="configuration-ethernet-network-card-passthrough">Configuration Ethernet network card passthrough</h3>
<p>Show PCI devices:</p>
<p>Identify network card:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ll /sys/class/net/eno1
lrwxrwxrwx <span style="color:#ae81ff">1</span> root root <span style="color:#ae81ff">0</span> Aug  <span style="color:#ae81ff">6</span> 12:09 /sys/class/net/eno1 -&gt; ../../devices/pci0000:00/0000:00:01.1/0000:09:00.0/net/eno1
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">lspci | grep -iE --color <span style="color:#e6db74">&#39;network|ethernet&#39;</span>
05:00.0 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection <span style="color:#f92672">(</span>rev 01<span style="color:#f92672">)</span>
05:00.1 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection <span style="color:#f92672">(</span>rev 01<span style="color:#f92672">)</span>
09:00.0 Ethernet controller: Intel Corporation I350 Gigabit Network Connection <span style="color:#f92672">(</span>rev 01<span style="color:#f92672">)</span>
09:00.1 Ethernet controller: Intel Corporation I350 Gigabit Network Connection <span style="color:#f92672">(</span>rev 01<span style="color:#f92672">)</span>
</code></pre></div><h3 id="add-device-to-vm">Add device to VM</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano /etc/pve/qemu-server/108.conf
...
hostpci0: 0000:09:00.0
<span style="color:#75715e"># for multiple ones</span>
hostpci0: 0000:09:00.0;0000:09:00.1
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Validate Kubernetes Deployment in CI/CD]]></title>
            <link href="https://devopstales.github.io/home/k8s-test-tools/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-test-tools/?utm_source=atom_feed" rel="related" type="text/html" title="Validate Kubernetes Deployment in CI/CD" />
                <link href="https://devopstales.github.io/home/k8s-security/?utm_source=atom_feed" rel="related" type="text/html" title="Best Practices to keeping Kubernetes Clusters Secure" />
                <link href="https://devopstales.github.io/home/k8s-ps/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Pod Security" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller-v3/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V3" />
                <link href="https://devopstales.github.io/home/continuous-image-security/?utm_source=atom_feed" rel="related" type="text/html" title="Continuous Image Security" />
            
                <id>https://devopstales.github.io/home/k8s-test-tools/</id>
            
            
            <published>2022-03-02T00:00:00+00:00</published>
            <updated>2022-03-02T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I this blog post I will show you how you can validate your kubernetes objects, helm charts, images at CI/CD.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<h3 id="the-yaml">The yaml</h3>
<p>First this is the example yaml that we will use for validation tests.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano base-valid.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">apps/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Deployment</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">http-echo</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">selector</span>:
    <span style="color:#f92672">matchLabels</span>:
      <span style="color:#f92672">app</span>: <span style="color:#ae81ff">http-echo</span>
  <span style="color:#f92672">template</span>:
    <span style="color:#f92672">metadata</span>:
      <span style="color:#f92672">labels</span>:
        <span style="color:#f92672">app</span>: <span style="color:#ae81ff">http-echo</span>
    <span style="color:#f92672">spec</span>:
      <span style="color:#f92672">containers</span>:
      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">http-echo</span>
        <span style="color:#f92672">image</span>: <span style="color:#ae81ff">jxlwqq/http-echo:latest</span>
        <span style="color:#f92672">args</span>: [<span style="color:#e6db74">&#34;-text&#34;</span>, <span style="color:#e6db74">&#34;hello-world&#34;</span>]
        <span style="color:#f92672">ports</span>:
        - <span style="color:#f92672">containerPort</span>: <span style="color:#ae81ff">5678</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Service</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">http-echo</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">ports</span>:
  - <span style="color:#f92672">port</span>: <span style="color:#ae81ff">5678</span>
    <span style="color:#f92672">protocol</span>: <span style="color:#ae81ff">TCP</span>
    <span style="color:#f92672">targetPort</span>: <span style="color:#ae81ff">5678</span>
  <span style="color:#f92672">selector</span>:
    <span style="color:#f92672">app</span>: <span style="color:#ae81ff">http-echo</span>
</code></pre></div><h3 id="kubeval">kubeval</h3>
<p><code>kubeval</code> is a tool for validating a Kubernetes YAML or JSON configuration file. It does so using schemas generated from the Kubernetes OpenAPI specification, and therefore can validate schemas for multiple versions of Kubernetes.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">brew install kubeval
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubeval base-valid.yaml
PASS - base-valid.yaml contains a valid Deployment <span style="color:#f92672">(</span>http-echo<span style="color:#f92672">)</span>
PASS - base-valid.yaml contains a valid Service <span style="color:#f92672">(</span>http-echo<span style="color:#f92672">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubeval --kubernetes-version 1.16.1 base-valid.yaml
</code></pre></div><blockquote>
<p>One limitation of kubeval is that it is currently not able to validate against Custom Resource Definitions (CRDs)</p>
</blockquote>
<h3 id="kube-score">kube-score</h3>
<p><code>Kube-score</code> analyses YAML manifests and scores them against security recommendations and best practices.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">brew install kube-score
<span style="color:#e6db74">``</span>

<span style="color:#e6db74">```</span>bash
kube-score score base-valid.yaml
apps/v1/Deployment http-echo
    <span style="color:#f92672">[</span>CRITICAL<span style="color:#f92672">]</span> Container Resources
        · http-echo -&gt; CPU limit is not set
            Resource limits are recommended to avoid resource DDOS. Set resources.limits.cpu
        · http-echo -&gt; Memory limit is not set
            Resource limits are recommended to avoid resource DDOS. Set resources.limits.memory
        · http-echo -&gt; CPU request is not set
            Resource requests are recommended to make sure that the application can start and run without crashing. Set resources.requests.cpu
        · http-echo -&gt; Memory request is not set
            Resource requests are recommended to make sure that the application can start and run without crashing. Set
            resources.requests.memory
    <span style="color:#f92672">[</span>CRITICAL<span style="color:#f92672">]</span> Pod NetworkPolicy
        · The pod does not have a matching NetworkPolicy
            Create a NetworkPolicy that targets this pod to control who/what can communicate with this pod. Note, this feature needs to be
            supported by the CNI implementation used in the Kubernetes cluster to have an effect.
    <span style="color:#f92672">[</span>CRITICAL<span style="color:#f92672">]</span> Pod Probes
        · Container is missing a readinessProbe
            A readinessProbe should be used to indicate when the service is ready to receive traffic. Without it, the Pod is risking to
            receive traffic before it has booted. It<span style="color:#e6db74">&#39;s also used during rollouts, and can prevent downtime if a new version of the application
</span><span style="color:#e6db74">            is failing.
</span><span style="color:#e6db74">            More information: https://github.com/zegl/kube-score/blob/master/README_PROBES.md
</span><span style="color:#e6db74">    [CRITICAL] Container Security Context User Group ID
</span><span style="color:#e6db74">        · http-echo -&gt; Container has no configured security context
</span><span style="color:#e6db74">            Set securityContext to run the container in a more secure context.
</span><span style="color:#e6db74">    [CRITICAL] Container Image Tag
</span><span style="color:#e6db74">        · http-echo -&gt; Image with latest tag
</span><span style="color:#e6db74">            Using a fixed tag is recommended to avoid accidental upgrades
</span><span style="color:#e6db74">    [CRITICAL] Container Ephemeral Storage Request and Limit
</span><span style="color:#e6db74">        · http-echo -&gt; Ephemeral Storage limit is not set
</span><span style="color:#e6db74">            Resource limits are recommended to avoid resource DDOS. Set resources.limits.ephemeral-storage
</span><span style="color:#e6db74">    [CRITICAL] Container Security Context ReadOnlyRootFilesystem
</span><span style="color:#e6db74">        · http-echo -&gt; Container has no configured security context
</span><span style="color:#e6db74">            Set securityContext to run the container in a more secure context.
</span><span style="color:#e6db74">    [WARNING] Deployment has host PodAntiAffinity
</span><span style="color:#e6db74">        · Deployment does not have a host podAntiAffinity set
</span><span style="color:#e6db74">            It&#39;</span>s recommended to set a podAntiAffinity that stops multiple pods from a deployment from being scheduled on the same node. This
            increases availability in <span style="color:#66d9ef">case</span> the node becomes unavailable.
    <span style="color:#f92672">[</span>CRITICAL<span style="color:#f92672">]</span> Deployment has PodDisruptionBudget
        · No matching PodDisruptionBudget was found
            It<span style="color:#960050;background-color:#1e0010">&#39;</span>s recommended to define a PodDisruptionBudget to avoid unexpected downtime during Kubernetes maintenance operations, such as
            when draining a node.
v1/Service http-echo
</code></pre></div><p>If you plan to use it as part of your Continuous Integration pipeline, you can use a more concise output with the flag <code>--output-format ci</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kube-score score base-valid.yaml --output-format ci
<span style="color:#f92672">[</span>OK<span style="color:#f92672">]</span> http-echo apps/v1/Deployment
<span style="color:#f92672">[</span>OK<span style="color:#f92672">]</span> http-echo apps/v1/Deployment
<span style="color:#f92672">[</span>CRITICAL<span style="color:#f92672">]</span> http-echo apps/v1/Deployment: <span style="color:#f92672">(</span>http-echo<span style="color:#f92672">)</span> CPU limit is not set
<span style="color:#f92672">[</span>CRITICAL<span style="color:#f92672">]</span> http-echo apps/v1/Deployment: <span style="color:#f92672">(</span>http-echo<span style="color:#f92672">)</span> Memory limit is not set
<span style="color:#f92672">[</span>CRITICAL<span style="color:#f92672">]</span> http-echo apps/v1/Deployment: <span style="color:#f92672">(</span>http-echo<span style="color:#f92672">)</span> CPU request is not set
<span style="color:#f92672">[</span>CRITICAL<span style="color:#f92672">]</span> http-echo apps/v1/Deployment: <span style="color:#f92672">(</span>http-echo<span style="color:#f92672">)</span> Memory request is not set
<span style="color:#f92672">[</span>CRITICAL<span style="color:#f92672">]</span> http-echo apps/v1/Deployment: <span style="color:#f92672">(</span>http-echo<span style="color:#f92672">)</span> Image with latest tag
<span style="color:#f92672">[</span>CRITICAL<span style="color:#f92672">]</span> http-echo apps/v1/Deployment: <span style="color:#f92672">(</span>http-echo<span style="color:#f92672">)</span> Ephemeral Storage limit is not set
<span style="color:#f92672">[</span>CRITICAL<span style="color:#f92672">]</span> http-echo apps/v1/Deployment: Container is missing a readinessProbe
<span style="color:#f92672">[</span>OK<span style="color:#f92672">]</span> http-echo apps/v1/Deployment
<span style="color:#f92672">[</span>CRITICAL<span style="color:#f92672">]</span> http-echo apps/v1/Deployment: The pod does not have a matching NetworkPolicy
<span style="color:#f92672">[</span>CRITICAL<span style="color:#f92672">]</span> http-echo apps/v1/Deployment: <span style="color:#f92672">(</span>http-echo<span style="color:#f92672">)</span> Container has no configured security context
<span style="color:#f92672">[</span>OK<span style="color:#f92672">]</span> http-echo apps/v1/Deployment
<span style="color:#f92672">[</span>CRITICAL<span style="color:#f92672">]</span> http-echo apps/v1/Deployment: <span style="color:#f92672">(</span>http-echo<span style="color:#f92672">)</span> Container has no configured security context
<span style="color:#f92672">[</span>CRITICAL<span style="color:#f92672">]</span> http-echo apps/v1/Deployment: No matching PodDisruptionBudget was found
<span style="color:#f92672">[</span>WARNING<span style="color:#f92672">]</span> http-echo apps/v1/Deployment: Deployment does not have a host podAntiAffinity set
<span style="color:#f92672">[</span>SKIPPED<span style="color:#f92672">]</span> http-echo apps/v1/Deployment: Skipped because the deployment is not targeted by a HorizontalPodAutoscaler
<span style="color:#f92672">[</span>OK<span style="color:#f92672">]</span> http-echo apps/v1/Deployment
<span style="color:#f92672">[</span>OK<span style="color:#f92672">]</span> http-echo v1/Service
<span style="color:#f92672">[</span>OK<span style="color:#f92672">]</span> http-echo v1/Service
<span style="color:#f92672">[</span>OK<span style="color:#f92672">]</span> http-echo v1/Service
<span style="color:#f92672">[</span>OK<span style="color:#f92672">]</span> http-echo v1/Service
</code></pre></div><h3 id="trivy">trivy</h3>
<p><code>Trivy</code> (<code>tri</code> pronounced like trigger, <code>vy</code> pronounced like envy) is a simple and comprehensive scanner for vulnerabilities in container images.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">trivy image jxlwqq/http-echo:latest

jxlwqq/http-echo:latest <span style="color:#f92672">(</span>debian 11.1<span style="color:#f92672">)</span>
<span style="color:#f92672">=====================================</span>
Total: <span style="color:#ae81ff">0</span> <span style="color:#f92672">(</span>UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 0<span style="color:#f92672">)</span>


http-echo <span style="color:#f92672">(</span>gobinary<span style="color:#f92672">)</span>
<span style="color:#f92672">====================</span>
Total: <span style="color:#ae81ff">0</span> <span style="color:#f92672">(</span>UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 0<span style="color:#f92672">)</span>
</code></pre></div><p>It also provides built-in policies to detect configuration issues in Docker, Kubernetes and Terraform. Also, you can write your own policies in <code>Rego</code> to scan JSON, YAML, HCL, etc, like Conftest.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">trivy config base-valid.yaml
2022-03-09T18:38:49.725+0100	INFO	Detected config files: <span style="color:#ae81ff">1</span>

base-valid.yaml <span style="color:#f92672">(</span>kubernetes<span style="color:#f92672">)</span>
<span style="color:#f92672">============================</span>
Tests: <span style="color:#ae81ff">39</span> <span style="color:#f92672">(</span>SUCCESSES: 28, FAILURES: 11, EXCEPTIONS: 0<span style="color:#f92672">)</span>
Failures: <span style="color:#ae81ff">11</span> <span style="color:#f92672">(</span>UNKNOWN: 0, LOW: 7, MEDIUM: 4, HIGH: 0, CRITICAL: 0<span style="color:#f92672">)</span>

+---------------------------+------------+----------------------------------------+----------+--------------------------------------------+
|           TYPE            | MISCONF ID |                 CHECK                  | SEVERITY |                  MESSAGE                   |
+---------------------------+------------+----------------------------------------+----------+--------------------------------------------+
| Kubernetes Security Check |   KSV001   | Process can elevate its own privileges |  MEDIUM  | Container <span style="color:#e6db74">&#39;http-echo&#39;</span> of                   |
|                           |            |                                        |          | Deployment <span style="color:#e6db74">&#39;http-echo&#39;</span> should set          |
|                           |            |                                        |          | <span style="color:#e6db74">&#39;securityContext.allowPrivilegeEscalation&#39;</span> |
|                           |            |                                        |          | to false                                   |
|                           |            |                                        |          | --&gt;avd.aquasec.com/appshield/ksv001        |
+                           +------------+----------------------------------------+----------+--------------------------------------------+
|                           |   KSV003   | Default capabilities not dropped       |   LOW    | Container <span style="color:#e6db74">&#39;http-echo&#39;</span> of Deployment        |
|                           |            |                                        |          | <span style="color:#e6db74">&#39;http-echo&#39;</span> should add <span style="color:#e6db74">&#39;ALL&#39;</span> to            |
|                           |            |                                        |          | <span style="color:#e6db74">&#39;securityContext.capabilities.drop&#39;</span>        |
|                           |            |                                        |          | --&gt;avd.aquasec.com/appshield/ksv003        |
+                           +------------+----------------------------------------+          +--------------------------------------------+
|                           |   KSV011   | CPU not limited                        |          | Container <span style="color:#e6db74">&#39;http-echo&#39;</span> of                   |
|                           |            |                                        |          | Deployment <span style="color:#e6db74">&#39;http-echo&#39;</span> should              |
|                           |            |                                        |          | set <span style="color:#e6db74">&#39;resources.limits.cpu&#39;</span>                 |
|                           |            |                                        |          | --&gt;avd.aquasec.com/appshield/ksv011        |
+                           +------------+----------------------------------------+----------+--------------------------------------------+
|                           |   KSV012   | Runs as root user                      |  MEDIUM  | Container <span style="color:#e6db74">&#39;http-echo&#39;</span> of                   |
|                           |            |                                        |          | Deployment <span style="color:#e6db74">&#39;http-echo&#39;</span> should set          |
|                           |            |                                        |          | <span style="color:#e6db74">&#39;securityContext.runAsNonRoot&#39;</span> to true     |
|                           |            |                                        |          | --&gt;avd.aquasec.com/appshield/ksv012        |
+                           +------------+----------------------------------------+----------+--------------------------------------------+
|                           |   KSV013   | Image tag <span style="color:#e6db74">&#39;:latest&#39;</span> used               |   LOW    | Container <span style="color:#e6db74">&#39;http-echo&#39;</span> of Deployment        |
|                           |            |                                        |          | <span style="color:#e6db74">&#39;http-echo&#39;</span> should specify an image tag    |
|                           |            |                                        |          | --&gt;avd.aquasec.com/appshield/ksv013        |
+                           +------------+----------------------------------------+          +--------------------------------------------+
|                           |   KSV014   | Root file system is not read-only      |          | Container <span style="color:#e6db74">&#39;http-echo&#39;</span> of                   |
|                           |            |                                        |          | Deployment <span style="color:#e6db74">&#39;http-echo&#39;</span> should set          |
|                           |            |                                        |          | <span style="color:#e6db74">&#39;securityContext.readOnlyRootFilesystem&#39;</span>   |
|                           |            |                                        |          | to true                                    |
|                           |            |                                        |          | --&gt;avd.aquasec.com/appshield/ksv014        |
+                           +------------+----------------------------------------+          +--------------------------------------------+
|                           |   KSV015   | CPU requests not specified             |          | Container <span style="color:#e6db74">&#39;http-echo&#39;</span> of                   |
|                           |            |                                        |          | Deployment <span style="color:#e6db74">&#39;http-echo&#39;</span> should              |
|                           |            |                                        |          | set <span style="color:#e6db74">&#39;resources.requests.cpu&#39;</span>               |
|                           |            |                                        |          | --&gt;avd.aquasec.com/appshield/ksv015        |
+                           +------------+----------------------------------------+          +--------------------------------------------+
|                           |   KSV016   | Memory requests not specified          |          | Container <span style="color:#e6db74">&#39;http-echo&#39;</span> of                   |
|                           |            |                                        |          | Deployment <span style="color:#e6db74">&#39;http-echo&#39;</span> should              |
|                           |            |                                        |          | set <span style="color:#e6db74">&#39;resources.requests.memory&#39;</span>            |
|                           |            |                                        |          | --&gt;avd.aquasec.com/appshield/ksv016        |
+                           +------------+----------------------------------------+          +--------------------------------------------+
|                           |   KSV018   | Memory not limited                     |          | Container <span style="color:#e6db74">&#39;http-echo&#39;</span> of                   |
|                           |            |                                        |          | Deployment <span style="color:#e6db74">&#39;http-echo&#39;</span> should              |
|                           |            |                                        |          | set <span style="color:#e6db74">&#39;resources.limits.memory&#39;</span>              |
|                           |            |                                        |          | --&gt;avd.aquasec.com/appshield/ksv018        |
+                           +------------+----------------------------------------+----------+--------------------------------------------+
|                           |   KSV020   | Runs with low user ID                  |  MEDIUM  | Container <span style="color:#e6db74">&#39;http-echo&#39;</span> of                   |
|                           |            |                                        |          | Deployment <span style="color:#e6db74">&#39;http-echo&#39;</span> should set          |
|                           |            |                                        |          | <span style="color:#e6db74">&#39;securityContext.runAsUser&#39;</span> &gt; <span style="color:#ae81ff">10000</span>        |
|                           |            |                                        |          | --&gt;avd.aquasec.com/appshield/ksv020        |
+                           +------------+----------------------------------------+          +--------------------------------------------+
|                           |   KSV021   | Runs with low group ID                 |          | Container <span style="color:#e6db74">&#39;http-echo&#39;</span> of                   |
|                           |            |                                        |          | Deployment <span style="color:#e6db74">&#39;http-echo&#39;</span> should set          |
|                           |            |                                        |          | <span style="color:#e6db74">&#39;securityContext.runAsGroup&#39;</span> &gt; <span style="color:#ae81ff">10000</span>       |
|                           |            |                                        |          | --&gt;avd.aquasec.com/appshield/ksv021        |
+---------------------------+------------+----------------------------------------+----------+--------------------------------------------+
</code></pre></div><p>You can integrate trivy to your ci/cd pipeline by using ine of the output template:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">export TRIVY_VERSION<span style="color:#f92672">=</span>0.24.2
wget --no-verbose https://github.com/aquasecurity/trivy/releases/download/v<span style="color:#e6db74">${</span>TRIVY_VERSION<span style="color:#e6db74">}</span>/trivy_<span style="color:#e6db74">${</span>TRIVY_VERSION<span style="color:#e6db74">}</span>_Linux-64bit.tar.gz -O - | tar -zxvf -

trivy image --exit-code <span style="color:#ae81ff">0</span> --no-progress --format template --template <span style="color:#e6db74">&#34;@contrib/gitlab.tpl&#34;</span> -o gl-container-scanning-report.json golang:1.12-alpine

trivy image --exit-code <span style="color:#ae81ff">0</span> --no-progress --format template --template <span style="color:#e6db74">&#34;@contrib/junit.tpl&#34;</span> -o junit-report.xml  golang:1.12-alpine
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Elastic Container Registry Integration with EKS]]></title>
            <link href="https://devopstales.github.io/home/aws-eks-ecr/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/aws-eks-ecr/?utm_source=atom_feed" rel="related" type="text/html" title="Elastic Container Registry Integration with EKS" />
                <link href="https://devopstales.github.io/home/aws-eks-ingress/?utm_source=atom_feed" rel="related" type="text/html" title="Deploy Ingress Controller to EKS cluster with WAF" />
                <link href="https://devopstales.github.io/home/aws-eks-install/?utm_source=atom_feed" rel="related" type="text/html" title="Create EKS Cluster with eksctl" />
                <link href="https://devopstales.github.io/home/aws-eks-amp-monitoring/?utm_source=atom_feed" rel="related" type="text/html" title="Using AWS Prometheus (AMP) for monitoring AWS EKS cluster." />
                <link href="https://devopstales.github.io/cloud/aws-eks-ingress/?utm_source=atom_feed" rel="related" type="text/html" title="Deploy Ingress Controller to EKS cluster with WAF" />
            
                <id>https://devopstales.github.io/home/aws-eks-ecr/</id>
            
            
            <published>2022-02-26T00:00:00+00:00</published>
            <updated>2022-02-26T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this pos I will show you how you can integrate your Elastic Container Registry with EKS.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">aws ecr create-repository --repository-name aws-ecr-kubenginx --region us-east-1
</code></pre></div><p>Build end push image</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Build image with &lt;ECR-REPOSITORY-URI&gt;:&lt;TAG&gt;</span>
docker build -t 180789647333.dkr.ecr.us-east-1.amazonaws.com/aws-ecr-kubenginx:1.0.0 .

<span style="color:#75715e"># Get Login Password</span>
<span style="color:#75715e"># aws ecr get-login-password --region &lt;your-region&gt; | docker login --username AWS --password-stdin &lt;ECR-REPOSITORY-URI&gt;</span>
aws ecr get-login-password --region us-east-1 | <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>docker login --username AWS --password-stdin 180789647333.dkr.ecr.us-east-1.amazonaws.com/aws-ecr-kubenginx

<span style="color:#75715e"># Push the Docker Image</span>
docker push &lt;ECR-REPOSITORY-URI&gt;:&lt;TAG&gt;
docker push 180789647333.dkr.ecr.us-east-1.amazonaws.com/aws-ecr-kubenginx:1.0.0
</code></pre></div><p>Verify ECR Access to EKS Worker Nodes</p>
<ul>
<li>Go to Services -&gt; EC2 -&gt; Running Instances &gt; Select a Worker Node -&gt; Description Tab</li>
<li>Click on value in <code>IAM Role</code> field Role name</li>
<li>In IAM on that <code>specific role</code>, verify <code>permissions</code> tab</li>
<li>Policy with name <code>AmazonEC2ContainerRegistryReadOnly</code>, <code>AmazonEC2ContainerRegistryPowerUser</code> should be associated</li>
</ul>
<p>Use ECR image with Amazon EKS</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#75715e">#01-ECR-Nginx-Deployment.yml</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">apps/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Deployment</span>
<span style="color:#f92672">metadata</span>:
<span style="color:#f92672">name</span>: <span style="color:#ae81ff">kubeapp-ecr</span>
<span style="color:#f92672">labels</span>:
   <span style="color:#f92672">app</span>: <span style="color:#ae81ff">kubeapp-ecr</span>
<span style="color:#f92672">spec</span>:
<span style="color:#f92672">replicas</span>: <span style="color:#ae81ff">2</span>
<span style="color:#f92672">selector</span>:
   <span style="color:#f92672">matchLabels</span>:
      <span style="color:#f92672">app</span>: <span style="color:#ae81ff">kubeapp-ecr</span>
<span style="color:#f92672">template</span>:
   <span style="color:#f92672">metadata</span>:
      <span style="color:#f92672">labels</span>:
      <span style="color:#f92672">app</span>: <span style="color:#ae81ff">kubeapp-ecr</span>
   <span style="color:#f92672">spec</span>:
      <span style="color:#f92672">containers</span>:
      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">kubeapp-ecr</span>
         <span style="color:#f92672">image</span>: <span style="color:#ae81ff">180789647333.</span><span style="color:#ae81ff">dkr.ecr.us-east-1.amazonaws.com/aws-ecr-kubenginx:1.0.0</span>
         <span style="color:#f92672">resources</span>:
            <span style="color:#f92672">requests</span>:
            <span style="color:#f92672">memory</span>: <span style="color:#e6db74">&#34;128Mi&#34;</span>
            <span style="color:#f92672">cpu</span>: <span style="color:#e6db74">&#34;500m&#34;</span>
            <span style="color:#f92672">limits</span>:
            <span style="color:#f92672">memory</span>: <span style="color:#e6db74">&#34;256Mi&#34;</span>
            <span style="color:#f92672">cpu</span>: <span style="color:#e6db74">&#34;1000m&#34;</span>
         <span style="color:#f92672">ports</span>:
            - <span style="color:#f92672">containerPort</span>: <span style="color:#ae81ff">80</span>
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[AWS - pfsense: Site-to-site VPN using static routes]]></title>
            <link href="https://devopstales.github.io/home/aws-s2s-vpn/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/aws-s2s-vpn/?utm_source=atom_feed" rel="related" type="text/html" title="AWS - pfsense: Site-to-site VPN using static routes" />
                <link href="https://devopstales.github.io/home/aws-eks-amp-monitoring/?utm_source=atom_feed" rel="related" type="text/html" title="Using AWS Prometheus (AMP) for monitoring AWS EKS cluster." />
                <link href="https://devopstales.github.io/home/aws-eks-ingress/?utm_source=atom_feed" rel="related" type="text/html" title="Deploy Ingress Controller to EKS cluster with WAF" />
                <link href="https://devopstales.github.io/home/aws-eks-install/?utm_source=atom_feed" rel="related" type="text/html" title="Create EKS Cluster with eksctl" />
                <link href="https://devopstales.github.io/home/aws-eks-networking/?utm_source=atom_feed" rel="related" type="text/html" title="AWS EKS Network Solutions" />
            
                <id>https://devopstales.github.io/home/aws-s2s-vpn/</id>
            
            
            <published>2022-02-22T00:00:00+00:00</published>
            <updated>2022-02-22T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I willll show you how to configure a VPN between pfSense and AWS using static routes.</p>
<p>To create a VPN on AWS side you need the following Components:</p>
<ul>
<li>Customer Gateway - This is represent the on-premise side of the vpn</li>
<li>virtual private gateway - this is a router in the aws</li>
<li>vpn Connection</li>
<li>virtual priveta cloud</li>
</ul>
<p><code>vpc -&gt; virtual private gateway -&gt; vpn Connection -&gt; Customer Gateway</code></p>
<p><img src="/img/include/vpn-how-it-works-vgw.png" alt="vpn infra"  class="zoomable" /></p>
<p>We need to create this components and connect them to each other.</p>
<h3 id="customer-gateway">Customer Gateway</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># set to your own public ip</span>
export CLIENT_PUBLIC_IP<span style="color:#f92672">=</span>1.2.3.4

<span style="color:#75715e"># Create the customer gateway using the following AWS command:</span>
aws ec2 create-customer-gateway --type ipsec.1 --public-ip $CLIENT_PUBLIC_IP

<span style="color:#f92672">{</span>
    <span style="color:#e6db74">&#34;CustomerGateway&#34;</span>: <span style="color:#f92672">{</span>
        <span style="color:#e6db74">&#34;CustomerGatewayId&#34;</span>: <span style="color:#e6db74">&#34;cgw-0e11f167&#34;</span>,
        <span style="color:#e6db74">&#34;IpAddress&#34;</span>: <span style="color:#e6db74">&#34;1.2.3.4&#34;</span>,
        <span style="color:#e6db74">&#34;State&#34;</span>: <span style="color:#e6db74">&#34;available&#34;</span>,
        <span style="color:#e6db74">&#34;Type&#34;</span>: <span style="color:#e6db74">&#34;ipsec.1&#34;</span>,
        <span style="color:#e6db74">&#34;BgpAsn&#34;</span>: <span style="color:#e6db74">&#34;65000&#34;</span>
    <span style="color:#f92672">}</span>
<span style="color:#f92672">}</span>

export CUSTOMER_GATEWAY<span style="color:#f92672">=</span>cgw-0e11f167
</code></pre></div><h3 id="create-a-virtual-private-gateway">Create a Virtual Private Gateway</h3>
<p>Create a target gateway and attach it to your VPC network.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Create a virtual private gateway with a specific AWS-side ASN:</span>
aws ec2 create-vpn-gateway --type ipsec.1

<span style="color:#f92672">{</span>
    <span style="color:#e6db74">&#34;VpnGateway&#34;</span>: <span style="color:#f92672">{</span>
        <span style="color:#e6db74">&#34;AmazonSideAsn&#34;</span>: 64512,
        <span style="color:#e6db74">&#34;State&#34;</span>: <span style="color:#e6db74">&#34;available&#34;</span>,
        <span style="color:#e6db74">&#34;Type&#34;</span>: <span style="color:#e6db74">&#34;ipsec.1&#34;</span>,
        <span style="color:#e6db74">&#34;VpnGatewayId&#34;</span>: <span style="color:#e6db74">&#34;vgw-9a4cacf3&#34;</span>,
        <span style="color:#e6db74">&#34;VpcAttachments&#34;</span>: <span style="color:#f92672">[]</span>
    <span style="color:#f92672">}</span>
<span style="color:#f92672">}</span>

export VPN_GATEWAY_ID<span style="color:#f92672">=</span>vgw-9a4cacf3
export VPC_ID<span style="color:#f92672">=</span>

<span style="color:#75715e"># Attach the virtual private gateway to your VPC network:</span>
aws ec2 attach-vpn-gateway --vpn-gateway-id $VPN_GATEWAY_ID --vpc-id $VPC_ID
</code></pre></div><h3 id="create-a-vpn-connection">Create a VPN Connection</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">export AWS_TIP<span style="color:#f92672">=</span>169.254.0.0/30
<span style="color:#75715e"># random string for secret</span>
export SHARED_SECRET<span style="color:#f92672">=</span>g23r8gr7grg23r8g2fnmf
<span style="color:#75715e"># my network on the on-premise side</span>
export ONPREM_NETWORK<span style="color:#f92672">=</span>192.168.1.0/24

aws ec2 create-vpn-connection <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --type ipsec.1 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --customer-gateway-id $CUSTOMER_GATEWAY <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --vpn-gateway-id $VPN_GATEWAY_ID <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --options TunnelOptions<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;[{TunnelInsideCidr=</span>$AWS_TIP<span style="color:#e6db74">,PreSharedKey=</span>$SHARED_SECRET<span style="color:#e6db74">}]&#34;</span>,StaticRoutesOnly<span style="color:#f92672">=</span>true,LocalIpv4NetworkCidr<span style="color:#f92672">=</span>$ONPREM_NETWORK

<span style="color:#f92672">{</span>
    <span style="color:#e6db74">&#34;VpnConnection&#34;</span>: <span style="color:#f92672">{</span>
        <span style="color:#e6db74">&#34;CustomerGatewayConfiguration&#34;</span>: <span style="color:#e6db74">&#34;...configuration information...&#34;</span>,
        <span style="color:#e6db74">&#34;CustomerGatewayId&#34;</span>: <span style="color:#e6db74">&#34;cgw-0e11f167&#34;</span>,
        <span style="color:#e6db74">&#34;Category&#34;</span>: <span style="color:#e6db74">&#34;VPN&#34;</span>,
        <span style="color:#e6db74">&#34;State&#34;</span>: <span style="color:#e6db74">&#34;pending&#34;</span>,
        <span style="color:#e6db74">&#34;VpnConnectionId&#34;</span>: <span style="color:#e6db74">&#34;vpn-123123123123abcab&#34;</span>,
        <span style="color:#e6db74">&#34;VpnGatewayId&#34;</span>: <span style="color:#e6db74">&#34;vgw-9a4cacf3&#34;</span>,
        <span style="color:#e6db74">&#34;Options&#34;</span>: <span style="color:#f92672">{</span>
            <span style="color:#e6db74">&#34;EnableAcceleration&#34;</span>: false,
            <span style="color:#e6db74">&#34;StaticRoutesOnly&#34;</span>: true,
            <span style="color:#e6db74">&#34;LocalIpv4NetworkCidr&#34;</span>: <span style="color:#e6db74">&#34;192.168.1.0/24&#34;</span>,
            <span style="color:#e6db74">&#34;RemoteIpv4NetworkCidr&#34;</span>: <span style="color:#e6db74">&#34;0.0.0.0/0&#34;</span>,
            <span style="color:#e6db74">&#34;TunnelInsideIpVersion&#34;</span>: <span style="color:#e6db74">&#34;ipv4&#34;</span>,
            <span style="color:#e6db74">&#34;TunnelOptions&#34;</span>: <span style="color:#f92672">[</span>
                <span style="color:#f92672">{</span>
                    <span style="color:#e6db74">&#34;OutsideIpAddress&#34;</span>: <span style="color:#e6db74">&#34;203.0.113.3&#34;</span>,
                    <span style="color:#e6db74">&#34;TunnelInsideCidr&#34;</span>: <span style="color:#e6db74">&#34;169.254.0.0/30&#34;</span>,
                    <span style="color:#e6db74">&#34;PreSharedKey&#34;</span>: <span style="color:#e6db74">&#34;g23r8gr7grg23r8g2fnmf&#34;</span>
                <span style="color:#f92672">}</span>,
                <span style="color:#f92672">{}</span>
            <span style="color:#f92672">]</span>
        <span style="color:#f92672">}</span>,
        <span style="color:#e6db74">&#34;Routes&#34;</span>: <span style="color:#f92672">[]</span>,
        <span style="color:#e6db74">&#34;Tags&#34;</span>: <span style="color:#f92672">[]</span>
    <span style="color:#f92672">}</span>
<span style="color:#f92672">}</span>
</code></pre></div><p>In the <code>TunnelOptions</code> you can configure other options of the vpn like:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">IKEVersions<span style="color:#f92672">=[{</span>Value<span style="color:#f92672">=</span>ikev2<span style="color:#f92672">})</span>
Phase2DHGroupNumbers<span style="color:#f92672">=[{</span>Value<span style="color:#f92672">=</span>15<span style="color:#f92672">})</span>
Phase1DHGroupNumbers<span style="color:#f92672">=[{</span>Value<span style="color:#f92672">=</span>15<span style="color:#f92672">})</span>
Phase2IntegrityAlgorithms<span style="color:#f92672">=[{</span>Value<span style="color:#f92672">=</span>SHA2-256<span style="color:#f92672">})</span>
Phase1IntegrityAlgorithms<span style="color:#f92672">=[{</span>Value<span style="color:#f92672">=</span>SHA2-256<span style="color:#f92672">})</span>
Phase2EncryptionAlgorithms<span style="color:#f92672">=[{</span>Value<span style="color:#f92672">=</span>AES256-GCM-16<span style="color:#f92672">})</span>
Phase1EncryptionAlgorithms<span style="color:#f92672">=[{</span>Value<span style="color:#f92672">=</span>AES256-GCM-16<span style="color:#f92672">})</span>
</code></pre></div><h3 id="configure-routing">Configure Routing</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">aws ec2 create-route --route-table-id rtb-89012345678901234 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--destination-cidr-block 172.31.0.0/16 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--transit-gateway-id tgw-56789012345678901 
</code></pre></div><h3 id="download-the-configuration-file">Download the configuration file</h3>
<p>After you create the Site-to-Site VPN connection, you can download a sample configuration file to use for configuring the customer gateway device.</p>
<blockquote>
<p>The configuration file is an example only and might not match your intended Site-to-Site VPN connection settings entirely. It specifies the minimum requirements for a Site-to-Site VPN connection of AES128, SHA1, and Diffie-Hellman group 2 in most AWS Regions, and AES128, SHA2, and Diffie-Hellman group 14 in the AWS GovCloud Regions. It also specifies pre-shared keys for authentication. You must modify the example configuration file to take advantage of additional security algorithms, Diffie-Hellman groups, private certificates, and IPv6 traffic.</p>
</blockquote>
<ul>
<li>Open the Amazon VPC console at <a href="https://console.aws.amazon.com/vpc/">https://console.aws.amazon.com/vpc/</a></li>
<li>In the navigation pane, choose Site-to-Site VPN Connections.</li>
<li>Select your VPN connection and choose Download Configuration.</li>
</ul>
<h3 id="creating-a-new-ipsec-vpn-on-pfsense">Creating a new IPsec VPN on pfsense</h3>
<p>At <code>VPN &gt; IPsec &gt; Add</code>  Fill out the values from the text file that you just downloaded from AWS. It looks like this.</p>
<p><img src="/img/include/aws-vpn01.jpg" alt="vpn infra"  class="zoomable" /></p>
<p><img src="/img/include/aws-vpn02.jpg" alt="vpn infra"  class="zoomable" /></p>
<p><img src="/img/include/aws-vpn03.jpg" alt="vpn infra"  class="zoomable" /></p>
<p><img src="/img/include/aws-vpn04.jpg" alt="vpn infra"  class="zoomable" /></p>
<p>As with <code>Phase 1</code>, do the same for <code>Phase 2</code>. Read the values from the text file.</p>
<p><img src="/img/include/aws-vpn05.jpg" alt="vpn infra"  class="zoomable" /></p>
<p><img src="/img/include/aws-vpn06.jpg" alt="vpn infra"  class="zoomable" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Using AWS Prometheus (AMP) for monitoring AWS EKS cluster.]]></title>
            <link href="https://devopstales.github.io/home/aws-eks-amp-monitoring/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/aws-eks-amp-monitoring/?utm_source=atom_feed" rel="related" type="text/html" title="Using AWS Prometheus (AMP) for monitoring AWS EKS cluster." />
                <link href="https://devopstales.github.io/home/aws-eks-install/?utm_source=atom_feed" rel="related" type="text/html" title="Create EKS Cluster with eksctl" />
                <link href="https://devopstales.github.io/home/aws-eks-ingress/?utm_source=atom_feed" rel="related" type="text/html" title="Deploy Ingress Controller to EKS cluster with WAF" />
                <link href="https://devopstales.github.io/home/aws-eks-networking/?utm_source=atom_feed" rel="related" type="text/html" title="AWS EKS Network Solutions" />
                <link href="https://devopstales.github.io/cloud/aws-eks-install/?utm_source=atom_feed" rel="related" type="text/html" title="Create EKS Cluster with eksctl" />
            
                <id>https://devopstales.github.io/home/aws-eks-amp-monitoring/</id>
            
            
            <published>2022-02-18T00:00:00+00:00</published>
            <updated>2022-02-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this pos I will show you how you can install the AWS managed Prometheus Service for EKS monitoring.</p>
<h3 id="create-a-new-amp-workspace">Create a new AMP workspace</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">aws amp create-workspace --alias eks-workshop --region $AWS_REGION
</code></pre></div><h3 id="setup-iam-for-prometheus-server-to-send-metrics-to-amp">Setup IAM for Prometheus Server to send metrics to AMP</h3>
<p>After setting the <code>YOUR_EKS_CLUSTER_NAME</code> variable the below shell script can be used to:</p>
<ul>
<li>Creates an IAM role with an IAM policy</li>
<li>Creates a Kubernetes service account</li>
<li>Creates a trust relationship between the IAM role and the OIDC provider</li>
</ul>
<p>for the AMP and EKS cluster.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e">##!/bin/bash</span>
CLUSTER_NAME<span style="color:#f92672">=</span>YOUR_EKS_CLUSTER_NAME
AWS_ACCOUNT_ID<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>aws sts get-caller-identity --query <span style="color:#e6db74">&#34;Account&#34;</span> --output text<span style="color:#66d9ef">)</span>
OIDC_PROVIDER<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>aws eks describe-cluster --name $CLUSTER_NAME --query <span style="color:#e6db74">&#34;cluster.identity.oidc.issuer&#34;</span> --output text | sed -e <span style="color:#e6db74">&#34;s/^https:\/\///&#34;</span><span style="color:#66d9ef">)</span>

PROM_SERVICE_ACCOUNT_NAMESPACE<span style="color:#f92672">=</span>prometheus
GRAFANA_SERVICE_ACCOUNT_NAMESPACE<span style="color:#f92672">=</span>grafana
SERVICE_ACCOUNT_NAME<span style="color:#f92672">=</span>iamproxy-service-account
SERVICE_ACCOUNT_IAM_ROLE<span style="color:#f92672">=</span>EKS-AMP-ServiceAccount-Role
SERVICE_ACCOUNT_IAM_ROLE_DESCRIPTION<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;IAM role to be used by a K8s service account with write access to AMP&#34;</span>
SERVICE_ACCOUNT_IAM_POLICY<span style="color:#f92672">=</span>AWSManagedPrometheusWriteAccessPolicy
SERVICE_ACCOUNT_IAM_POLICY_ARN<span style="color:#f92672">=</span>arn:aws:iam::$AWS_ACCOUNT_ID:policy/$SERVICE_ACCOUNT_IAM_POLICY
<span style="color:#75715e">#</span>
<span style="color:#75715e"># Setup a trust policy designed for a specific combination of K8s service account and namespace to sign in from a Kubernetes cluster which hosts the OIDC Idp.</span>
<span style="color:#75715e"># If the IAM role already exists, then add this new trust policy to the existing trust policy</span>
<span style="color:#75715e">#</span>
echo <span style="color:#e6db74">&#34;Creating a new trust policy&#34;</span>
read -r -d <span style="color:#e6db74">&#39;&#39;</span> NEW_TRUST_RELATIONSHIP <span style="color:#e6db74">&lt;&lt;EOF
</span><span style="color:#e6db74"> [
</span><span style="color:#e6db74">    {
</span><span style="color:#e6db74">      &#34;Effect&#34;: &#34;Allow&#34;,
</span><span style="color:#e6db74">      &#34;Principal&#34;: {
</span><span style="color:#e6db74">        &#34;Federated&#34;: &#34;arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}&#34;
</span><span style="color:#e6db74">      },
</span><span style="color:#e6db74">      &#34;Action&#34;: &#34;sts:AssumeRoleWithWebIdentity&#34;,
</span><span style="color:#e6db74">      &#34;Condition&#34;: {
</span><span style="color:#e6db74">        &#34;StringEquals&#34;: {
</span><span style="color:#e6db74">          &#34;${OIDC_PROVIDER}:sub&#34;: &#34;system:serviceaccount:${GRAFANA_SERVICE_ACCOUNT_NAMESPACE}:${SERVICE_ACCOUNT_NAME}&#34;
</span><span style="color:#e6db74">        }
</span><span style="color:#e6db74">      }
</span><span style="color:#e6db74">    },
</span><span style="color:#e6db74">    {
</span><span style="color:#e6db74">      &#34;Effect&#34;: &#34;Allow&#34;,
</span><span style="color:#e6db74">      &#34;Principal&#34;: {
</span><span style="color:#e6db74">        &#34;Federated&#34;: &#34;arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}&#34;
</span><span style="color:#e6db74">      },
</span><span style="color:#e6db74">      &#34;Action&#34;: &#34;sts:AssumeRoleWithWebIdentity&#34;,
</span><span style="color:#e6db74">      &#34;Condition&#34;: {
</span><span style="color:#e6db74">        &#34;StringEquals&#34;: {
</span><span style="color:#e6db74">          &#34;${OIDC_PROVIDER}:sub&#34;: &#34;system:serviceaccount:${PROM_SERVICE_ACCOUNT_NAMESPACE}:${SERVICE_ACCOUNT_NAME}&#34;
</span><span style="color:#e6db74">        }
</span><span style="color:#e6db74">      }
</span><span style="color:#e6db74">    }
</span><span style="color:#e6db74">  ]
</span><span style="color:#e6db74">EOF</span>
<span style="color:#75715e">#</span>
<span style="color:#75715e"># Get the old trust policy, if one exists, and append it to the new trust policy</span>
<span style="color:#75715e">#</span>
OLD_TRUST_RELATIONSHIP<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>aws iam get-role --role-name $SERVICE_ACCOUNT_IAM_ROLE --query <span style="color:#e6db74">&#39;Role.AssumeRolePolicyDocument.Statement[]&#39;</span> --output json<span style="color:#66d9ef">)</span>
COMBINED_TRUST_RELATIONSHIP<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>echo $OLD_TRUST_RELATIONSHIP $NEW_TRUST_RELATIONSHIP | jq -s add<span style="color:#66d9ef">)</span>
echo <span style="color:#e6db74">&#34;Appending to the existing trust policy&#34;</span>
read -r -d <span style="color:#e6db74">&#39;&#39;</span> TRUST_POLICY <span style="color:#e6db74">&lt;&lt;EOF
</span><span style="color:#e6db74">{
</span><span style="color:#e6db74">  &#34;Version&#34;: &#34;2012-10-17&#34;,
</span><span style="color:#e6db74">  &#34;Statement&#34;: ${COMBINED_TRUST_RELATIONSHIP}
</span><span style="color:#e6db74">}
</span><span style="color:#e6db74">EOF</span>
echo <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">${</span>TRUST_POLICY<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span> &gt; TrustPolicy.json
<span style="color:#75715e">#</span>
<span style="color:#75715e"># Setup the permission policy grants write permissions for all AWS StealFire workspaces</span>
<span style="color:#75715e">#</span>
read -r -d <span style="color:#e6db74">&#39;&#39;</span> PERMISSION_POLICY <span style="color:#e6db74">&lt;&lt;EOF
</span><span style="color:#e6db74">{
</span><span style="color:#e6db74">   &#34;Version&#34;:&#34;2012-10-17&#34;,
</span><span style="color:#e6db74">   &#34;Statement&#34;:[
</span><span style="color:#e6db74">      {
</span><span style="color:#e6db74">         &#34;Effect&#34;:&#34;Allow&#34;,
</span><span style="color:#e6db74">         &#34;Action&#34;:[
</span><span style="color:#e6db74">            &#34;aps:RemoteWrite&#34;,
</span><span style="color:#e6db74">            &#34;aps:QueryMetrics&#34;,
</span><span style="color:#e6db74">            &#34;aps:GetSeries&#34;,
</span><span style="color:#e6db74">            &#34;aps:GetLabels&#34;,
</span><span style="color:#e6db74">            &#34;aps:GetMetricMetadata&#34;
</span><span style="color:#e6db74">         ],
</span><span style="color:#e6db74">         &#34;Resource&#34;:&#34;*&#34;
</span><span style="color:#e6db74">      }
</span><span style="color:#e6db74">   ]
</span><span style="color:#e6db74">}
</span><span style="color:#e6db74">EOF</span>
echo <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">${</span>PERMISSION_POLICY<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span> &gt; PermissionPolicy.json

<span style="color:#75715e">#</span>
<span style="color:#75715e"># Create an IAM permission policy to be associated with the role, if the policy does not already exist</span>
<span style="color:#75715e">#</span>
SERVICE_ACCOUNT_IAM_POLICY_ID<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>aws iam get-policy --policy-arn $SERVICE_ACCOUNT_IAM_POLICY_ARN --query <span style="color:#e6db74">&#39;Policy.PolicyId&#39;</span> --output text<span style="color:#66d9ef">)</span>
<span style="color:#66d9ef">if</span> <span style="color:#f92672">[</span> <span style="color:#e6db74">&#34;</span>$SERVICE_ACCOUNT_IAM_POLICY_ID<span style="color:#e6db74">&#34;</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;</span> <span style="color:#f92672">]</span>; 
<span style="color:#66d9ef">then</span>
  echo <span style="color:#e6db74">&#34;Creating a new permission policy </span>$SERVICE_ACCOUNT_IAM_POLICY<span style="color:#e6db74">&#34;</span>
  aws iam create-policy --policy-name $SERVICE_ACCOUNT_IAM_POLICY --policy-document file://PermissionPolicy.json 
<span style="color:#66d9ef">else</span>
  echo <span style="color:#e6db74">&#34;Permission policy </span>$SERVICE_ACCOUNT_IAM_POLICY<span style="color:#e6db74"> already exists&#34;</span>
<span style="color:#66d9ef">fi</span>

<span style="color:#75715e">#</span>
<span style="color:#75715e"># If the IAM role already exists, then just update the trust policy.</span>
<span style="color:#75715e"># Otherwise create one using the trust policy and permission policy</span>
<span style="color:#75715e">#</span>
SERVICE_ACCOUNT_IAM_ROLE_ARN<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>aws iam get-role --role-name $SERVICE_ACCOUNT_IAM_ROLE --query <span style="color:#e6db74">&#39;Role.Arn&#39;</span> --output text<span style="color:#66d9ef">)</span>
<span style="color:#66d9ef">if</span> <span style="color:#f92672">[</span> <span style="color:#e6db74">&#34;</span>$SERVICE_ACCOUNT_IAM_ROLE_ARN<span style="color:#e6db74">&#34;</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;</span> <span style="color:#f92672">]</span>; 
<span style="color:#66d9ef">then</span>
  echo <span style="color:#e6db74">&#34;</span>$SERVICE_ACCOUNT_IAM_ROLE<span style="color:#e6db74"> role does not exist. Creating a new role with a trust and permission policy&#34;</span>
  <span style="color:#75715e">#</span>
  <span style="color:#75715e"># Create an IAM role for Kubernetes service account </span>
  <span style="color:#75715e">#</span>
  SERVICE_ACCOUNT_IAM_ROLE_ARN<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>aws iam create-role <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --role-name $SERVICE_ACCOUNT_IAM_ROLE <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --assume-role-policy-document file://TrustPolicy.json <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --description <span style="color:#e6db74">&#34;</span>$SERVICE_ACCOUNT_IAM_ROLE_DESCRIPTION<span style="color:#e6db74">&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --query <span style="color:#e6db74">&#34;Role.Arn&#34;</span> --output text<span style="color:#66d9ef">)</span>
  <span style="color:#75715e">#</span>
  <span style="color:#75715e"># Attach the trust and permission policies to the role</span>
  <span style="color:#75715e">#</span>
  aws iam attach-role-policy --role-name $SERVICE_ACCOUNT_IAM_ROLE --policy-arn $SERVICE_ACCOUNT_IAM_POLICY_ARN  
<span style="color:#66d9ef">else</span>
  echo <span style="color:#e6db74">&#34;</span>$SERVICE_ACCOUNT_IAM_ROLE_ARN<span style="color:#e6db74"> role already exists. Updating the trust policy&#34;</span>
  <span style="color:#75715e">#</span>
  <span style="color:#75715e"># Update the IAM role for Kubernetes service account with a with the new trust policy</span>
  <span style="color:#75715e">#</span>
  aws iam update-assume-role-policy --role-name $SERVICE_ACCOUNT_IAM_ROLE --policy-document file://TrustPolicy.json
<span style="color:#66d9ef">fi</span>
echo $SERVICE_ACCOUNT_IAM_ROLE_ARN

<span style="color:#75715e"># EKS cluster hosts an OIDC provider with a public discovery endpoint.</span>
<span style="color:#75715e"># Associate this Idp with AWS IAM so that the latter can validate and accept the OIDC tokens issued by Kubernetes to service accounts.</span>
<span style="color:#75715e"># Doing this with eksctl is the easier and best approach.</span>
<span style="color:#75715e">#</span>
eksctl utils associate-iam-oidc-provider --cluster $CLUSTER_NAME --approve
</code></pre></div><h3 id="setting-up-prometheus-server-in-the-kubernetes-cluster">Setting up Prometheus server in the Kubernetes cluster</h3>
<p>We need to provision a Prometheus server inside our EKS cluster that would be tasked to collect all the cluster metrics and ship them to AMP (or AWS Managed Service for Prometheus)</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
kubectl create ns prometheus
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano amp_ingest_override_values.yaml</span>
---
<span style="color:#f92672">serviceAccounts</span>:
  <span style="color:#75715e">## Disable alert manager roles</span>
  <span style="color:#f92672">server</span>:
        <span style="color:#f92672">name</span>: <span style="color:#e6db74">&#34;iamproxy-service-account&#34;</span>
  <span style="color:#f92672">alertmanager</span>:
    <span style="color:#f92672">create</span>: <span style="color:#66d9ef">false</span>
  <span style="color:#75715e">## Disable pushgateway</span>
  <span style="color:#f92672">pushgateway</span>:
    <span style="color:#f92672">create</span>: <span style="color:#66d9ef">false</span>

<span style="color:#f92672">server</span>:
  <span style="color:#f92672">remoteWrite</span>:
    -
      <span style="color:#f92672">queue_config</span>:
        <span style="color:#f92672">max_samples_per_send</span>: <span style="color:#ae81ff">1000</span>
        <span style="color:#f92672">max_shards</span>: <span style="color:#ae81ff">200</span>
        <span style="color:#f92672">capacity</span>: <span style="color:#ae81ff">2500</span>
  <span style="color:#75715e">## Use a statefulset instead of a deployment for resiliency</span>
  <span style="color:#f92672">statefulSet</span>:
    <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">true</span>
  <span style="color:#75715e">## Store blocks locally for short time period only</span>
  <span style="color:#f92672">retention</span>: <span style="color:#ae81ff">1h</span>
  
<span style="color:#75715e">## Disable alert manager</span>
<span style="color:#f92672">alertmanager</span>:
  <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">false</span>

<span style="color:#75715e">## Disable pushgateway</span>
<span style="color:#f92672">pushgateway</span>:
  <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">false</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">export SERVICE_ACCOUNT_IAM_ROLE<span style="color:#f92672">=</span>EKS-AMP-ServiceAccount-Role
export SERVICE_ACCOUNT_IAM_ROLE_ARN<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>aws iam get-role --role-name $SERVICE_ACCOUNT_IAM_ROLE --query <span style="color:#e6db74">&#39;Role.Arn&#39;</span> --output text<span style="color:#66d9ef">)</span>
export WORKSPACE_ID<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>aws amp list-workspaces --alias eks-workshop | jq .workspaces<span style="color:#f92672">[</span>0<span style="color:#f92672">]</span>.workspaceId -r<span style="color:#66d9ef">)</span>

helm install prometheus-for-amp prometheus-community/prometheus -n prometheus -f ./amp_ingest_override_values.yaml <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set serviceAccounts.server.annotations.<span style="color:#e6db74">&#34;eks\.amazonaws\.com/role-arn&#34;</span><span style="color:#f92672">=</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">${</span>SERVICE_ACCOUNT_IAM_ROLE_ARN<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set server.remoteWrite<span style="color:#f92672">[</span>0<span style="color:#f92672">]</span>.url<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;https://aps-workspaces.</span><span style="color:#e6db74">${</span>AWS_REGION<span style="color:#e6db74">}</span><span style="color:#e6db74">.amazonaws.com/workspaces/</span><span style="color:#e6db74">${</span>WORKSPACE_ID<span style="color:#e6db74">}</span><span style="color:#e6db74">/api/v1/remote_write&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set server.remoteWrite<span style="color:#f92672">[</span>0<span style="color:#f92672">]</span>.sigv4.region<span style="color:#f92672">=</span><span style="color:#e6db74">${</span>AWS_REGION<span style="color:#e6db74">}</span>
</code></pre></div><h3 id="visualizing-metrics-using-grafana">Visualizing metrics using Grafana</h3>
<p>You need a dashboard to visualize the metrics in Promethsus. You can install a Grafana to the EKS Cluster by helm or use AWS Grafana.</p>
<h4 id="install-grafana-by-helm">Install Grafana by helm</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">helm repo add grafana https://grafana.github.io/helm-charts
kubectl create ns grafana
helm install grafana-for-amp grafana/grafana -n grafana
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano amp_query_override_values.yaml</span>
---

<span style="color:#f92672">serviceAccount</span>:
    <span style="color:#f92672">name</span>: <span style="color:#e6db74">&#34;iamproxy-service-account&#34;</span>
    <span style="color:#f92672">annotations</span>:

        <span style="color:#f92672">eks.amazonaws.com/role-arn</span>: <span style="color:#e6db74">&#34;${IAM_PROXY_PROMETHEUS_ROLE_ARN}&#34;</span>
<span style="color:#f92672">grafana.ini</span>:
  <span style="color:#f92672">auth</span>:
    <span style="color:#f92672">sigv4_auth_enabled</span>: <span style="color:#66d9ef">true</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">helm upgrade --install grafana-for-amp grafana/grafana -n grafana -f ./amp_query_override_values.yaml

<span style="color:#75715e"># get passwor</span>
kubectl get secrets grafana-for-amp -n grafana -o jsonpath<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;{.data.admin-password}&#39;</span>|base64 --decode

<span style="color:#75715e"># port-forward to get th ui:</span>
kubectl port-forward -n grafana pods/GRAFANA_POD_NAME 5001:3000 
</code></pre></div><h3 id="configure-grafana">Configure Grafana</h3>
<p>Before we can visualize the metrics in Grafana, it has to be configured with one or more data sources. Here, we will specify the workspace within Amazon Managed Service for Prometheus as a data source, as shown below. In the URL field, specify the Endpoint – query URL displayed in the AMP workspace details page without the /api/v1/query string at the end of the URL.</p>
<p><img src="/img/include/aws-grafana01.png" alt="aws-grafana"  class="zoomable" /></p>
<p>You’re now ready to query metrics data for the Prometheus Counter <code>http_requests_total</code> stored in the managed service workspace and visualize the rate of HTTP requests over a trailing 5-minute period using a Prometheus query as follows:
<code>sum(rate(http_requests_total{exported_job=”recommender”}[5m])) by (path)</code></p>
<p>The figure below illustrates how to visualize this metric in Grafana across the different path labels captured in the Prometheus Counter.</p>
<p><img src="/img/include/aws-grafana02.png" alt="aws-grafana"  class="zoomable" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Deploy Ingress Controller to EKS cluster with WAF]]></title>
            <link href="https://devopstales.github.io/home/aws-eks-ingress/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/aws-eks-install/?utm_source=atom_feed" rel="related" type="text/html" title="Create EKS Cluster with eksctl" />
                <link href="https://devopstales.github.io/cloud/aws-eks-install/?utm_source=atom_feed" rel="related" type="text/html" title="Create EKS Cluster with eksctl" />
                <link href="https://devopstales.github.io/home/aws-eks-networking/?utm_source=atom_feed" rel="related" type="text/html" title="AWS EKS Network Solutions" />
                <link href="https://devopstales.github.io/cloud/aws-eks-ingress/?utm_source=atom_feed" rel="related" type="text/html" title="Deploy Ingress Controller to EKS cluster with WAF" />
                <link href="https://devopstales.github.io/cloud/aws-eks-networking/?utm_source=atom_feed" rel="related" type="text/html" title="AWS EKS Network Solutions" />
            
                <id>https://devopstales.github.io/home/aws-eks-ingress/</id>
            
            
            <published>2022-02-12T00:00:00+00:00</published>
            <updated>2022-02-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this pos I will show you how you can install the AWS Load Balancer Controller on EKS Cluster with WAF protection.</p>
<p>AWS Load Balancer Controller is a controller to help manage Elastic Load Balancers for a Kubernetes cluster. It satisfies Kubernetes <code>Ingress</code> resources by provisioning Application Load Balancers and <code>Service</code> resources by provisioning Network Load Balancers.</p>
<p>AWS Elastic Load Balancing Application Load Balancer (ALB) is a popular AWS service that load balances incoming traffic at the application layer (layer 7) across multiple targets, such as Amazon EC2 instances, in multiple Availability Zones. It supports multiple features including: TLS (Transport Layer Security) termination, AWS WAF (Web Application Firewall) integration and integrated access logs, and health checks.</p>
<h3 id="deploy-the-aws-load-balancer-controller">Deploy the AWS Load Balancer Controller</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Let’s start by setting a few environment variables:</span>
export AWS_REGION<span style="color:#f92672">=</span>eu-central-1
export ACCOUNT_ID<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>aws sts get-caller-identity --query <span style="color:#e6db74">&#39;Account&#39;</span> --output text<span style="color:#66d9ef">)</span>
export EKS_CLUSTER_NAME<span style="color:#f92672">=</span>eks-devopstales

VPC_ID<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>aws eks describe-cluster <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --name $EKS_CLUSTER_NAME <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --region $AWS_REGION <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --query <span style="color:#e6db74">&#39;cluster.resourcesVpcConfig.vpcId&#39;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --output text<span style="color:#66d9ef">)</span>

<span style="color:#75715e"># Associate OIDC provider </span>
eksctl utils associate-iam-oidc-provider <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --cluster $EKS_CLUSTER_NAME <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --region $AWS_REGION <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --approve

<span style="color:#75715e"># Download the IAM policy document</span>
curl -S https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.3.0/docs/install/iam_policy.json -o iam-policy.json

<span style="color:#75715e"># Create an IAM policy</span>
LBC_IAM_POLICY_ARN<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>aws iam create-policy <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --policy-name AWSLoadBalancerControllerIAMPolicy <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --policy-document file://iam-policy.json <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --query <span style="color:#e6db74">&#39;Policy.Arn&#39;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --output text<span style="color:#66d9ef">)</span>

<span style="color:#75715e"># Create a service account </span>
eksctl create iamserviceaccount <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --cluster<span style="color:#f92672">=</span>$EKS_CLUSTER_NAME <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --region $AWS_REGION <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --namespace<span style="color:#f92672">=</span>kube-system <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --name<span style="color:#f92672">=</span>aws-load-balancer-controller <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --override-existing-serviceaccounts <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --attach-policy-arn<span style="color:#f92672">=</span>arn:aws:iam::<span style="color:#e6db74">${</span>ACCOUNT_ID<span style="color:#e6db74">}</span>:policy/AWSLoadBalancerControllerIAMPolicy <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --approve
  
helm repo add eks https://aws.github.io/eks-charts <span style="color:#f92672">&amp;&amp;</span> helm repo update
kubectl apply -k <span style="color:#e6db74">&#34;github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master&#34;</span>
helm install aws-load-balancer-controller <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  eks/aws-load-balancer-controller <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --namespace kube-system <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --set clusterName<span style="color:#f92672">=</span>$EKS_CLUSTER_NAME <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --set serviceAccount.create<span style="color:#f92672">=</span>false <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --set serviceAccount.name<span style="color:#f92672">=</span>aws-load-balancer-controller <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --set vpcId<span style="color:#f92672">=</span>$VPC_ID <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --set region<span style="color:#f92672">=</span>$AWS_REGION
</code></pre></div><p>Let’s create a Kubernetes ingress for an app called Yelb. The AWS Load Balancer Controller will associate the ingress with an Application Load Balancer.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat &lt;&lt; EOF &gt; yelb-ingress.yaml</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">networking.k8s.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Ingress</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">yelb.app</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">yelb</span>
  <span style="color:#f92672">annotations</span>:
    <span style="color:#f92672">kubernetes.io/ingress.class</span>: <span style="color:#ae81ff">alb</span>
    <span style="color:#f92672">alb.ingress.kubernetes.io/scheme</span>: <span style="color:#ae81ff">internet-facing</span>
    <span style="color:#f92672">alb.ingress.kubernetes.io/target-type</span>: <span style="color:#ae81ff">ip</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">rules</span>:
    - <span style="color:#f92672">http</span>:
        <span style="color:#f92672">paths</span>:
          - <span style="color:#f92672">path</span>: <span style="color:#ae81ff">/</span>
            <span style="color:#f92672">pathType</span>: <span style="color:#ae81ff">Prefix</span>
            <span style="color:#f92672">backend</span>:
              <span style="color:#f92672">service</span>:
                <span style="color:#f92672">name</span>: <span style="color:#ae81ff">yelb-ui</span>
                <span style="color:#f92672">port</span>:
                  <span style="color:#f92672">number</span>: <span style="color:#ae81ff">80</span>
<span style="color:#ae81ff">EOF</span>

<span style="color:#ae81ff">kubectl apply -f yelb-ingress.yaml</span>
</code></pre></div><h3 id="add-a-web-application-firewall-to-the-ingress">Add a web application firewall to the ingress</h3>
<p>The first thing we need to do is create a WAS web ACL. In AWS WAF, a web access control list or a web ACL monitors HTTP(S) requests for one or more AWS resources. These resources can be an Amazon API Gateway, AWS AppSync, Amazon CloudFront, or an Application Load Balancer.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Create an AWS WAF web ACL:</span>
WAF_WACL_ARN<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>aws wafv2 create-web-acl <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --name WAF-FOR-YELB <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --region $WAF_AWS_REGION <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --default-action Allow<span style="color:#f92672">={}</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --scope REGIONAL <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --visibility-config SampledRequestsEnabled<span style="color:#f92672">=</span>true,CloudWatchMetricsEnabled<span style="color:#f92672">=</span>true,MetricName<span style="color:#f92672">=</span>YelbWAFAclMetrics <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --description <span style="color:#e6db74">&#34;WAF Web ACL for Yelb&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --query <span style="color:#e6db74">&#39;Summary.ARN&#39;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --output text <span style="color:#66d9ef">)</span>

<span style="color:#75715e"># Store the AWS WAF web ACL’s Id in an environment variable</span>
WAF_WAF_ID<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>aws wafv2 list-web-acls <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --region $WAF_AWS_REGION <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --scope REGIONAL <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --query <span style="color:#e6db74">&#34;WebACLs[?Name==&#39;WAF-for-Yelb&#39;].Id&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --output text<span style="color:#66d9ef">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#75715e"># Update the ingress and associate this AWS WAF web ACL with the ALB that the ingress uses:</span>
<span style="color:#ae81ff">cat &lt;&lt; EOF &gt; yelb-ingress.yaml</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">networking.k8s.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Ingress</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">yelb.app</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">yelb</span>
  <span style="color:#f92672">annotations</span>:
    <span style="color:#f92672">kubernetes.io/ingress.class</span>: <span style="color:#ae81ff">alb</span>
    <span style="color:#f92672">alb.ingress.kubernetes.io/scheme</span>: <span style="color:#ae81ff">internet-facing</span>
    <span style="color:#f92672">alb.ingress.kubernetes.io/target-type</span>: <span style="color:#ae81ff">ip</span>
    <span style="color:#f92672">alb.ingress.kubernetes.io/wafv2-acl-arn</span>: <span style="color:#ae81ff">${WAF_WACL_ARN}</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">rules</span>:
    - <span style="color:#f92672">http</span>:
        <span style="color:#f92672">paths</span>:
          - <span style="color:#f92672">path</span>: <span style="color:#ae81ff">/</span>
            <span style="color:#f92672">pathType</span>: <span style="color:#ae81ff">Prefix</span>
            <span style="color:#f92672">backend</span>:
              <span style="color:#f92672">service</span>:
                <span style="color:#f92672">name</span>: <span style="color:#ae81ff">yelb-ui</span>
                <span style="color:#f92672">port</span>:
                  <span style="color:#f92672">number</span>: <span style="color:#ae81ff">80</span>
<span style="color:#ae81ff">EOF</span>
<span style="color:#ae81ff">kubectl apply -f yelb-ingress.yaml</span>
</code></pre></div><h3 id="enable-traffic-filtering-in-aws-waf">Enable traffic filtering in AWS WAF</h3>
<p>We have associated the ALB that our Kubernetes ingress uses with an AWS WAF web ACL Every request that’s handled by our sample application Yelb pods goes through AWS WAF for inspection. The AWS WAF web ACL is currently allowing every request to pass because we haven’t configured any AWS WAF rules.  In order to filter out potentially malicious traffic, we have to specify rules.</p>
<p>AWS WAF Bot Control is a managed rule group that provides visibility and control over common and pervasive bot traffic to web applications. While Bot Control has been optimized to minimize false positives, we recommend that you deploy Bot Control in count mode first and review CloudWatch metrics and AWS WAF logs to ensure that you are not accidentally blocking legitimate traffic.</p>
<p>Create a rules file and deploy:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat <span style="color:#e6db74">&lt;&lt; EOF &gt; waf-rules.json 
</span><span style="color:#e6db74">[
</span><span style="color:#e6db74">    {
</span><span style="color:#e6db74">      &#34;Name&#34;: &#34;AWS-AWSManagedRulesBotControlRuleSet&#34;,
</span><span style="color:#e6db74">      &#34;Priority&#34;: 0,
</span><span style="color:#e6db74">      &#34;Statement&#34;: {
</span><span style="color:#e6db74">        &#34;ManagedRuleGroupStatement&#34;: {
</span><span style="color:#e6db74">          &#34;VendorName&#34;: &#34;AWS&#34;,
</span><span style="color:#e6db74">          &#34;Name&#34;: &#34;AWSManagedRulesBotControlRuleSet&#34;
</span><span style="color:#e6db74">        }
</span><span style="color:#e6db74">      },
</span><span style="color:#e6db74">      &#34;OverrideAction&#34;: {
</span><span style="color:#e6db74">        &#34;None&#34;: {}
</span><span style="color:#e6db74">      },
</span><span style="color:#e6db74">      &#34;VisibilityConfig&#34;: {
</span><span style="color:#e6db74">        &#34;SampledRequestsEnabled&#34;: true,
</span><span style="color:#e6db74">        &#34;CloudWatchMetricsEnabled&#34;: true,
</span><span style="color:#e6db74">        &#34;MetricName&#34;: &#34;AWS-AWSManagedRulesBotControlRuleSet&#34;
</span><span style="color:#e6db74">      }
</span><span style="color:#e6db74">    }
</span><span style="color:#e6db74">]
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">aws wafv2 update-web-acl <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --name WAF-FOR-YELB <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --scope REGIONAL <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --id $WAF_WAF_ID <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --default-action Allow<span style="color:#f92672">={}</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --lock-token <span style="color:#66d9ef">$(</span>aws wafv2 list-web-acls <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --region $AWS_REGION <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --scope REGIONAL <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --query <span style="color:#e6db74">&#34;WebACLs[?Name==&#39;WAF-for-Yelb&#39;].LockToken&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --output text<span style="color:#66d9ef">)</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --visibility-config SampledRequestsEnabled<span style="color:#f92672">=</span>true,CloudWatchMetricsEnabled<span style="color:#f92672">=</span>true,MetricName<span style="color:#f92672">=</span>YelbWAFAclMetrics <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --region $AWS_REGION <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --rules file://waf-rules.json
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Create EKS Cluster with eksctl]]></title>
            <link href="https://devopstales.github.io/home/aws-eks-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/aws-eks-install/?utm_source=atom_feed" rel="related" type="text/html" title="Create EKS Cluster with eksctl" />
                <link href="https://devopstales.github.io/home/aws-eks-networking/?utm_source=atom_feed" rel="related" type="text/html" title="AWS EKS Network Solutions" />
                <link href="https://devopstales.github.io/cloud/aws-eks-networking/?utm_source=atom_feed" rel="related" type="text/html" title="AWS EKS Network Solutions" />
                <link href="https://devopstales.github.io/home/kubernetes-networking-2/?utm_source=atom_feed" rel="related" type="text/html" title="Understanding kubernetes networking: owerlay networks" />
                <link href="https://devopstales.github.io/home/kubernetes-networking-1/?utm_source=atom_feed" rel="related" type="text/html" title="Understanding kubernetes networking: pods and services" />
            
                <id>https://devopstales.github.io/home/aws-eks-install/</id>
            
            
            <published>2022-02-11T00:00:00+00:00</published>
            <updated>2022-02-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this pos I will show you how you can install an AWS managed Elastic Kubernetes Service with ekscli.</p>
<h3 id="create-an-aws-kms-custom-managed-key">Create an AWS KMS Custom Managed Key</h3>
<p>Create a CMK for the EKS cluster to use when encrypting your Kubernetes secrets:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">aws kms create-alias --alias-name alias/devopstales --target-key-id <span style="color:#66d9ef">$(</span>aws kms create-key --query KeyMetadata.Arn --output text<span style="color:#66d9ef">)</span>
</code></pre></div><h3 id="create-eks-cluster-with-eksctl">Create EKS Cluster with eksctl</h3>
<p><code>eksctl</code> is a simple CLI tool for creating clusters on EKS. To download the latest release, run:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl --silent --location <span style="color:#e6db74">&#34;https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_</span><span style="color:#66d9ef">$(</span>uname -s<span style="color:#66d9ef">)</span><span style="color:#e6db74">_amd64.tar.gz&#34;</span> | tar xz -C /tmp
sudo mv /tmp/eksctl /usr/local/bin
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Let’s start by setting a few environment variables:</span>
export AWS_REGION<span style="color:#f92672">=</span>eu-central-1
export ACCOUNT_ID<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>aws sts get-caller-identity --query <span style="color:#e6db74">&#39;Account&#39;</span> --output text<span style="color:#66d9ef">)</span>
export EKS_CLUSTER_NAME<span style="color:#f92672">=</span>eks-devopstales

<span style="color:#75715e"># Get the KMS Custom Managed Key resource name:</span>
export MASTER_ARN<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>aws kms describe-key --key-id alias/devopstales --query KeyMetadata.Arn --output text<span style="color:#66d9ef">)</span>

<span style="color:#75715e"># Create a cluster</span>
eksctl create cluster <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --name $EKS_CLUSTER_NAME <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --region $AWS_REGION <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --managed
</code></pre></div><p>If ylou need mor advanced configuration You can create a deployment file for <code>eksctl</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat &lt;&lt; EOF &gt; eks_devopstales.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">eksctl.io/v1alpha5</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterConfig</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">eks-devopstales</span>
  <span style="color:#f92672">region</span>: <span style="color:#ae81ff">${AWS_REGION}</span>
  <span style="color:#f92672">version</span>: <span style="color:#e6db74">&#34;1.21&#34;</span>
<span style="color:#f92672">availabilityZones</span>: [<span style="color:#e6db74">&#34;eu-central-1a&#34;</span>]
<span style="color:#f92672">managedNodeGroups</span>:
- <span style="color:#f92672">name</span>: <span style="color:#ae81ff">workers</span>
  <span style="color:#f92672">labels</span>: 
      <span style="color:#f92672">role</span>: <span style="color:#ae81ff">workers</span>
      <span style="color:#f92672">environment</span>: <span style="color:#ae81ff">poc</span>
  <span style="color:#f92672">desiredCapacity</span>: <span style="color:#ae81ff">1</span>
  <span style="color:#f92672">instanceType</span>: <span style="color:#ae81ff">t3.small</span>
  <span style="color:#f92672">volumeEncrypted</span>: <span style="color:#66d9ef">true</span>
  <span style="color:#f92672">volumeSize</span>: <span style="color:#ae81ff">30</span>
  <span style="color:#f92672">ssh</span>:
    <span style="color:#f92672">enableSsm</span>: <span style="color:#66d9ef">true</span>
<span style="color:#f92672">secretsEncryption</span>:
  <span style="color:#f92672">keyARN</span>: <span style="color:#ae81ff">${MASTER_ARN}</span>
<span style="color:#75715e"># To enable all of the control plane logs, uncomment below:</span>
<span style="color:#75715e"># cloudWatch:</span>
<span style="color:#75715e">#  clusterLogging:</span>
<span style="color:#75715e">#    enableTypes: [&#34;*&#34;]</span>
<span style="color:#ae81ff">EOF</span>
</code></pre></div><p>Next, use the file you created as the input for the eksctl cluster creation:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">eksctl create cluster -f eks_devopstales.yaml
</code></pre></div><p>Test The cluster</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get nodes
</code></pre></div><h3 id="using-iam-groups-to-manage-kubernetes-cluster-access">Using IAM Groups to manage Kubernetes cluster access</h3>
<p>We are going to create 3 roles:</p>
<ul>
<li><strong>k8sAdmin</strong> role which will have admin rights in our EKS cluster</li>
<li><strong>k8sDev</strong> role which will give access to the developers namespace in our EKS cluster</li>
<li><strong>k8sInteg</strong> role which will give access to the integration namespace in our EKS cluster</li>
</ul>
<p>Create the roles:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">POLICY<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>echo -n <span style="color:#e6db74">&#39;{&#34;Version&#34;:&#34;2012-10-17&#34;,&#34;Statement&#34;:[{&#34;Effect&#34;:&#34;Allow&#34;,&#34;Principal&#34;:{&#34;AWS&#34;:&#34;arn:aws:iam::&#39;</span>; echo -n <span style="color:#e6db74">&#34;</span>$ACCOUNT_ID<span style="color:#e6db74">&#34;</span>; echo -n <span style="color:#e6db74">&#39;:root&#34;},&#34;Action&#34;:&#34;sts:AssumeRole&#34;,&#34;Condition&#34;:{}}]}&#39;</span><span style="color:#66d9ef">)</span>

echo ACCOUNT_ID<span style="color:#f92672">=</span>$ACCOUNT_ID
echo POLICY<span style="color:#f92672">=</span>$POLICY

aws iam create-role <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --role-name k8sAdmin <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --description <span style="color:#e6db74">&#34;Kubernetes administrator role (for AWS IAM Authenticator for Kubernetes).&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --assume-role-policy-document <span style="color:#e6db74">&#34;</span>$POLICY<span style="color:#e6db74">&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --output text <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --query <span style="color:#e6db74">&#39;Role.Arn&#39;</span>

aws iam create-role <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --role-name k8sDev <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --description <span style="color:#e6db74">&#34;Kubernetes developer role (for AWS IAM Authenticator for Kubernetes).&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --assume-role-policy-document <span style="color:#e6db74">&#34;</span>$POLICY<span style="color:#e6db74">&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --output text <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --query <span style="color:#e6db74">&#39;Role.Arn&#39;</span>
  
aws iam create-role <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --role-name k8sInteg <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --description <span style="color:#e6db74">&#34;Kubernetes role for integration namespace in quick cluster.&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --assume-role-policy-document <span style="color:#e6db74">&#34;</span>$POLICY<span style="color:#e6db74">&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --output text <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --query <span style="color:#e6db74">&#39;Role.Arn&#39;</span>

</code></pre></div><p>We will define 3 groups:</p>
<ul>
<li><strong>k8sAdmin</strong> - users from this group will have admin rights on the kubernetes cluster</li>
<li><strong>k8sDe</strong> - users from this group will have full access only in the development namespace of the cluster</li>
<li><strong>k8sInteg</strong> - users from this group will have access to integration namespace.</li>
</ul>
<p>Create k8sAdmin IAM Group:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">aws iam create-group --group-name k8sAdmin

ADMIN_GROUP_POLICY<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>echo -n <span style="color:#e6db74">&#39;{
</span><span style="color:#e6db74">  &#34;Version&#34;: &#34;2012-10-17&#34;,
</span><span style="color:#e6db74">  &#34;Statement&#34;: [
</span><span style="color:#e6db74">    {
</span><span style="color:#e6db74">      &#34;Sid&#34;: &#34;AllowAssumeOrganizationAccountRole&#34;,
</span><span style="color:#e6db74">      &#34;Effect&#34;: &#34;Allow&#34;,
</span><span style="color:#e6db74">      &#34;Action&#34;: &#34;sts:AssumeRole&#34;,
</span><span style="color:#e6db74">      &#34;Resource&#34;: &#34;arn:aws:iam::&#39;</span>; echo -n <span style="color:#e6db74">&#34;</span>$ACCOUNT_ID<span style="color:#e6db74">&#34;</span>; echo -n <span style="color:#e6db74">&#39;:role/k8sAdmin&#34;
</span><span style="color:#e6db74">    }
</span><span style="color:#e6db74">  ]
</span><span style="color:#e6db74">}&#39;</span><span style="color:#66d9ef">)</span>
echo ADMIN_GROUP_POLICY<span style="color:#f92672">=</span>$ADMIN_GROUP_POLICY

aws iam put-group-policy <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--group-name k8sAdmin <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--policy-name k8sAdmin-policy <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--policy-document <span style="color:#e6db74">&#34;</span>$ADMIN_GROUP_POLICY<span style="color:#e6db74">&#34;</span>
</code></pre></div><p>Create k8sDev IAM Group:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">aws iam create-group --group-name k8sDev

DEV_GROUP_POLICY<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>echo -n <span style="color:#e6db74">&#39;{
</span><span style="color:#e6db74">  &#34;Version&#34;: &#34;2012-10-17&#34;,
</span><span style="color:#e6db74">  &#34;Statement&#34;: [
</span><span style="color:#e6db74">    {
</span><span style="color:#e6db74">      &#34;Sid&#34;: &#34;AllowAssumeOrganizationAccountRole&#34;,
</span><span style="color:#e6db74">      &#34;Effect&#34;: &#34;Allow&#34;,
</span><span style="color:#e6db74">      &#34;Action&#34;: &#34;sts:AssumeRole&#34;,
</span><span style="color:#e6db74">      &#34;Resource&#34;: &#34;arn:aws:iam::&#39;</span>; echo -n <span style="color:#e6db74">&#34;</span>$ACCOUNT_ID<span style="color:#e6db74">&#34;</span>; echo -n <span style="color:#e6db74">&#39;:role/k8sDev&#34;
</span><span style="color:#e6db74">    }
</span><span style="color:#e6db74">  ]
</span><span style="color:#e6db74">}&#39;</span><span style="color:#66d9ef">)</span>
echo DEV_GROUP_POLICY<span style="color:#f92672">=</span>$DEV_GROUP_POLICY

aws iam put-group-policy <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--group-name k8sDev <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--policy-name k8sDev-policy <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--policy-document <span style="color:#e6db74">&#34;</span>$DEV_GROUP_POLICY<span style="color:#e6db74">&#34;</span>
</code></pre></div><p>Create k8sInteg IAM Group:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">aws iam create-group --group-name k8sInteg

INTEG_GROUP_POLICY<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>echo -n <span style="color:#e6db74">&#39;{
</span><span style="color:#e6db74">  &#34;Version&#34;: &#34;2012-10-17&#34;,
</span><span style="color:#e6db74">  &#34;Statement&#34;: [
</span><span style="color:#e6db74">    {
</span><span style="color:#e6db74">      &#34;Sid&#34;: &#34;AllowAssumeOrganizationAccountRole&#34;,
</span><span style="color:#e6db74">      &#34;Effect&#34;: &#34;Allow&#34;,
</span><span style="color:#e6db74">      &#34;Action&#34;: &#34;sts:AssumeRole&#34;,
</span><span style="color:#e6db74">      &#34;Resource&#34;: &#34;arn:aws:iam::&#39;</span>; echo -n <span style="color:#e6db74">&#34;</span>$ACCOUNT_ID<span style="color:#e6db74">&#34;</span>; echo -n <span style="color:#e6db74">&#39;:role/k8sInteg&#34;
</span><span style="color:#e6db74">    }
</span><span style="color:#e6db74">  ]
</span><span style="color:#e6db74">}&#39;</span><span style="color:#66d9ef">)</span>
echo INTEG_GROUP_POLICY<span style="color:#f92672">=</span>$INTEG_GROUP_POLICY

aws iam put-group-policy <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--group-name k8sInteg <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--policy-name k8sInteg-policy <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--policy-document <span style="color:#e6db74">&#34;</span>$INTEG_GROUP_POLICY<span style="color:#e6db74">&#34;</span>
</code></pre></div><p>Create IAM Users and add to the groups:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">aws iam create-user --user-name PaulAdmin
aws iam create-user --user-name JeanDev
aws iam create-user --user-name PierreInteg

aws iam add-user-to-group --group-name k8sAdmin --user-name PaulAdmin
aws iam add-user-to-group --group-name k8sDev --user-name JeanDev
aws iam add-user-to-group --group-name k8sInteg --user-name PierreInteg

<span style="color:#75715e"># Get Access key for the users</span>
aws iam create-access-key --user-name PaulAdmin | tee /tmp/PaulAdmin.json
aws iam create-access-key --user-name JeanDev | tee /tmp/JeanDev.json
aws iam create-access-key --user-name PierreInteg | tee /tmp/PierreInteg.json
</code></pre></div><h3 id="configure-kubernetes-rbac">Configure Kubernetes RBAC</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl create namespace integration
kubectl create namespace development
</code></pre></div><p>Configuring access to development namespace:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat &lt;&lt; EOF | kubectl apply -f - -n development</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Role</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">dev-role</span>
<span style="color:#f92672">rules</span>:
  - <span style="color:#f92672">apiGroups</span>:
      - <span style="color:#e6db74">&#34;&#34;</span>
      - <span style="color:#e6db74">&#34;apps&#34;</span>
      - <span style="color:#e6db74">&#34;batch&#34;</span>
      - <span style="color:#e6db74">&#34;extensions&#34;</span>
    <span style="color:#f92672">resources</span>:
      - <span style="color:#e6db74">&#34;configmaps&#34;</span>
      - <span style="color:#e6db74">&#34;cronjobs&#34;</span>
      - <span style="color:#e6db74">&#34;deployments&#34;</span>
      - <span style="color:#e6db74">&#34;events&#34;</span>
      - <span style="color:#e6db74">&#34;ingresses&#34;</span>
      - <span style="color:#e6db74">&#34;jobs&#34;</span>
      - <span style="color:#e6db74">&#34;pods&#34;</span>
      - <span style="color:#e6db74">&#34;pods/attach&#34;</span>
      - <span style="color:#e6db74">&#34;pods/exec&#34;</span>
      - <span style="color:#e6db74">&#34;pods/log&#34;</span>
      - <span style="color:#e6db74">&#34;pods/portforward&#34;</span>
      - <span style="color:#e6db74">&#34;secrets&#34;</span>
      - <span style="color:#e6db74">&#34;services&#34;</span>
    <span style="color:#f92672">verbs</span>:
      - <span style="color:#e6db74">&#34;create&#34;</span>
      - <span style="color:#e6db74">&#34;delete&#34;</span>
      - <span style="color:#e6db74">&#34;describe&#34;</span>
      - <span style="color:#e6db74">&#34;get&#34;</span>
      - <span style="color:#e6db74">&#34;list&#34;</span>
      - <span style="color:#e6db74">&#34;patch&#34;</span>
      - <span style="color:#e6db74">&#34;update&#34;</span>
---
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">RoleBinding</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">dev-role-binding</span>
<span style="color:#f92672">subjects</span>:
- <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">User</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">dev-user</span>
<span style="color:#f92672">roleRef</span>:
  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Role</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">dev-role</span>
  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
<span style="color:#ae81ff">EOF</span>
</code></pre></div><p>Configuring access to integration namespace:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat &lt;&lt; EOF | kubectl apply -f - -n integration</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Role</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">integ-role</span>
<span style="color:#f92672">rules</span>:
  - <span style="color:#f92672">apiGroups</span>:
      - <span style="color:#e6db74">&#34;&#34;</span>
      - <span style="color:#e6db74">&#34;apps&#34;</span>
      - <span style="color:#e6db74">&#34;batch&#34;</span>
      - <span style="color:#e6db74">&#34;extensions&#34;</span>
    <span style="color:#f92672">resources</span>:
      - <span style="color:#e6db74">&#34;configmaps&#34;</span>
      - <span style="color:#e6db74">&#34;cronjobs&#34;</span>
      - <span style="color:#e6db74">&#34;deployments&#34;</span>
      - <span style="color:#e6db74">&#34;events&#34;</span>
      - <span style="color:#e6db74">&#34;ingresses&#34;</span>
      - <span style="color:#e6db74">&#34;jobs&#34;</span>
      - <span style="color:#e6db74">&#34;pods&#34;</span>
      - <span style="color:#e6db74">&#34;pods/attach&#34;</span>
      - <span style="color:#e6db74">&#34;pods/exec&#34;</span>
      - <span style="color:#e6db74">&#34;pods/log&#34;</span>
      - <span style="color:#e6db74">&#34;pods/portforward&#34;</span>
      - <span style="color:#e6db74">&#34;secrets&#34;</span>
      - <span style="color:#e6db74">&#34;services&#34;</span>
    <span style="color:#f92672">verbs</span>:
      - <span style="color:#e6db74">&#34;create&#34;</span>
      - <span style="color:#e6db74">&#34;delete&#34;</span>
      - <span style="color:#e6db74">&#34;describe&#34;</span>
      - <span style="color:#e6db74">&#34;get&#34;</span>
      - <span style="color:#e6db74">&#34;list&#34;</span>
      - <span style="color:#e6db74">&#34;patch&#34;</span>
      - <span style="color:#e6db74">&#34;update&#34;</span>
---
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">RoleBinding</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">integ-role-binding</span>
<span style="color:#f92672">subjects</span>:
- <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">User</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">integ-user</span>
<span style="color:#f92672">roleRef</span>:
  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Role</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">integ-role</span>
  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
<span style="color:#ae81ff">EOF</span>
</code></pre></div><p>Gives Access to our IAM Roles to EKS Cluster:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">eksctl create iamidentitymapping <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --cluster eksworkshop-eksctl <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --arn arn:aws:iam::<span style="color:#e6db74">${</span>ACCOUNT_ID<span style="color:#e6db74">}</span>:role/k8sDev <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --username dev-user

eksctl create iamidentitymapping <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --cluster eksworkshop-eksctl <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --arn arn:aws:iam::<span style="color:#e6db74">${</span>ACCOUNT_ID<span style="color:#e6db74">}</span>:role/k8sInteg <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --username integ-user

eksctl create iamidentitymapping <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --cluster eksworkshop-eksctl <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --arn arn:aws:iam::<span style="color:#e6db74">${</span>ACCOUNT_ID<span style="color:#e6db74">}</span>:role/k8sAdmin <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --username admin <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --group system:masters
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[AWS EKS Network Solutions]]></title>
            <link href="https://devopstales.github.io/home/aws-eks-networking/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/aws-eks-networking/?utm_source=atom_feed" rel="related" type="text/html" title="AWS EKS Network Solutions" />
                <link href="https://devopstales.github.io/home/kubernetes-networking-2/?utm_source=atom_feed" rel="related" type="text/html" title="Understanding kubernetes networking: owerlay networks" />
                <link href="https://devopstales.github.io/home/kubernetes-networking-1/?utm_source=atom_feed" rel="related" type="text/html" title="Understanding kubernetes networking: pods and services" />
                <link href="https://devopstales.github.io/kubernetes/kubernetes-networking-2/?utm_source=atom_feed" rel="related" type="text/html" title="Understanding kubernetes networking: owerlay networks" />
                <link href="https://devopstales.github.io/kubernetes/kubernetes-networking-1/?utm_source=atom_feed" rel="related" type="text/html" title="Understanding kubernetes networking: pods and services" />
            
                <id>https://devopstales.github.io/home/aws-eks-networking/</id>
            
            
            <published>2022-02-10T00:00:00+00:00</published>
            <updated>2022-02-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will analyse the available CNI plugins for Amazon Elastic Kubernetes Service.</p>
<p>Cloud providers like AWS, Google are in the market with their managed Kubernetes offering, which is one of the easiest ways to getting started with the Kubernetes journey. The cloud provider manage most of the components, but you still get a chance to choose your own CNI. Different providers offers different CNI plugins. AWS give you two option Calico or VPC CNI.</p>
<h3 id="vpc-cni">VPC CNI</h3>
<p>Amazon VPC CNI Plugin assigns a private IPv4 or IPv6 address from your VPC to each pod. By default, the number of pods can you run on a worker is based on the number of IP addresses assigned to Elastic network interfaces and the number of network interfaces attached to your Amazon EC2 node. From the v1.19.0 support higher pod density per node by allow to assign /28 (16 IP addresses) prefixes, instead of assigning individual IP addresses to network interfaces. PAMD will derive a (/32) IP from these prefixes for pod IP allocation.</p>
<p>To add or update the add-on on your cluster, see <a href="https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html">Managing the Amazon VPC CNI add-on</a>. If you want to update the version in the existing cluster, run the below command.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">eksctl update addon <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span> --name vpc-cni <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span> --version &lt;1.9.x-eksbuild.y&gt; <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span> --cluster &lt;my-cluster&gt; <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span> --force

kubectl set env daemonset aws-node <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span> -n kube-system ENABLE_PREFIX_DELEGATION<span style="color:#f92672">=</span>true
</code></pre></div><p>To use this CNI you need an <a href="https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html">Amazon EC2 Nitro instances</a> for node groups in the cluster.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">eksctl create nodegroup <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --cluster &lt;my-cluster&gt; <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --region &lt;us-east-1&gt; <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --name &lt;my-nodegroup&gt; <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --node-type &lt;m5.large&gt; <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --managed <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --max-pods-per-node &lt;110&gt; -- Count from the max pods calculator
</code></pre></div><h3 id="calico">Calico</h3>
<p>We can yous any other standerd CNI plugin, but the other suppertid CNI in EKS is Caliso. The huge advantage of Calico is the ability to create Network policies. Network policies are similar to AWS security groups in that you can create network ingress and egress rules. Instead of assigning instances to a security group, you assign network policies to pods using pod selectors and labels.</p>
<p>If you go with your own CNI, AWS will not be supporting any issues related to networking, as they don’t manage it. They are right at their end but definitely something you should watch out for.</p>
<p>The Other Problem is the communication with the api server from the pods. For every EKS cluster, AWS launches the control plane in a VPC which is managed by AWS. At the time of creating the control plane AWS creates a route table entry for your VPC CNI CIDR to the ENI of the control plane. This is how the API server is able to communicate to all the Nodes and pods in the cluster, but if we decide to use our own CNI, like calico, we change the POD CIDR, and now the API server has no clue on how to route the traffic to the pod IP.</p>
<p><img src="/img/include/aws-eks-vpc-cni.png" alt="AWS VPN CNI Routing"  class="zoomable" /></p>
<p>Some situation It can be a problem fi your pods can&rsquo;t communicate with te api server. This means you cannot use admission webhooks, operators or Service Mesh. On the other hand it can be a security feature too.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to create your own Homebrew repo?]]></title>
            <link href="https://devopstales.github.io/home/brew-repo/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/brew-repo/?utm_source=atom_feed" rel="related" type="text/html" title="How to create your own Homebrew repo?" />
                <link href="https://devopstales.github.io/home/trivy-operator-2.3/?utm_source=atom_feed" rel="related" type="text/html" title="trivy-operator 2.3: Patch release for Admisssion controller" />
                <link href="https://devopstales.github.io/home/gke-gitlab-terraform/?utm_source=atom_feed" rel="related" type="text/html" title="Create K8S cluster with Terraform and GitlabCI" />
                <link href="https://devopstales.github.io/home/multus-calico/?utm_source=atom_feed" rel="related" type="text/html" title="Use multus to separate metwork trafics" />
                <link href="https://devopstales.github.io/home/multus/?utm_source=atom_feed" rel="related" type="text/html" title="Use Multus CNI in Kubernetes" />
            
                <id>https://devopstales.github.io/home/brew-repo/</id>
            
            
            <published>2022-02-05T00:00:00+00:00</published>
            <updated>2022-02-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I this post I will show you in a Step-by-Step guide how cou can create your own Homebrew repository on GitHub.</p>
<h3 id="what-is-homebrew">What is Homebrew?</h3>
<p>Homebrew is the missing package manager for macOS. It installs packages with a simple command like <code>brew install curl</code>. It can be used on Linux too. Homebrew taps are third-party repositories. You can create your own repository and host on GitHub.</p>
<p>In this article I will use <code>tmux-cssh</code> script as the program I will share by Homebrew.</p>
<h3 id="create-git-repo">Create Git repo</h3>
<p>To host a Homebrew tap you just need to create a GitGub git repository with the naming convention <code>homebrew-&lt;name&gt;</code>. I called <code>homebrew-devopstales</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">git clone git@github.com:devopstales/homebrew-devopstales.gi
cd homebrew-devopstales
echo <span style="color:#e6db74">&#34;# homebrew-devopstales&#34;</span> &gt;&gt; README.md                                  
git init
git add README.md
git commit -m <span style="color:#e6db74">&#34;first commit&#34;</span>
git branch -M main
git remote add origin git@github.com:devopstales/homebrew-devopstales.git
git push -u origin main
</code></pre></div><p>I will use this repo to host the <code>tmux-cssh</code> script too on another branch, but you can use It&rsquo;s own repo.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">git checkout -b tmux-cssh
cp /usr/local/sbin/tmux-cssh .
git add -A
git commit -m <span style="color:#e6db74">&#34;tmux-cssh&#34;</span>
git push --set-upstream origin tmux-cssh
git tag tmux-cssh_1.0
git commit -m <span style="color:#e6db74">&#34;tmux-cssh_1.0&#34;</span>
git push origin --tags
</code></pre></div><p>I cerated a teg to use It&rsquo;s tar.gz in the release file of the package in Homebrew tap.</p>
<p><img src="/img/include/brew01.png" alt="release tar.gzm"  class="zoomable" /></p>
<p>Right click on the tar.gz and sopy the link address. Now we go back the main branch and generate the release file for brew.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">git checkout main
brew create https://github.com/devopstales/homebrew-devopstales/archive/refs/tags/tmux-cssh_1.0.tar.gz
</code></pre></div><p>It will show the content of the created file in your default text editor. Save the file and move to this git repo.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cp /home/linuxbrew/.linuxbrew/Homebrew/Library/Taps/homebrew/homebrew-core/Formula/homebrew-devopstales.rb tmux-cssh.rb
</code></pre></div><p>Edit the <code>tmux-cssh.rb</code> file:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-ruby" data-lang="ruby"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">TmuxCssh</span> <span style="color:#f92672">&lt;</span> <span style="color:#66d9ef">Formula</span>
  desc <span style="color:#e6db74">&#34;tmux-css allow parallel execution of cmmands on remote servers&#34;</span>
  homepage <span style="color:#e6db74">&#34;https://github.com/knakayama/tmux-cssh&#34;</span>
  url <span style="color:#e6db74">&#34;https://github.com/devopstales/homebrew-devopstales/archive/refs/tags/tmux-cssh_1.0.tar.gz&#34;</span>
  sha256 <span style="color:#e6db74">&#34;ce761f21fa0fe7050f408e687f9a4964acf95736c85678e3cc27cee88bd6ceed&#34;</span>
  version <span style="color:#e6db74">&#34;1.0&#34;</span>

  depends_on <span style="color:#e6db74">&#34;tmux&#34;</span>

  <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">install</span>
    bin<span style="color:#f92672">.</span>install <span style="color:#e6db74">&#34;tmux-cssh&#34;</span>
  <span style="color:#66d9ef">end</span>

  test <span style="color:#66d9ef">do</span>
    system <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">#{</span>bin<span style="color:#e6db74">}</span><span style="color:#e6db74">/tmux-cssh&#34;</span>, <span style="color:#e6db74">&#34;--help&#34;</span>
  <span style="color:#66d9ef">end</span>
<span style="color:#66d9ef">end</span>
</code></pre></div><p>If you have directories, you can use <code>Dir[&quot;lib&quot;]</code> to install directories or use <code>prefix.install</code> to copy files:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">def install    
    bin.install <span style="color:#e6db74">&#34;tmux-cssh&#34;</span>    
    bin.install Dir<span style="color:#f92672">[</span><span style="color:#e6db74">&#34;lib&#34;</span><span style="color:#f92672">]</span>    
    bin.install Dir<span style="color:#f92672">[</span><span style="color:#e6db74">&#34;files&#34;</span><span style="color:#f92672">]</span>
    prefix.install <span style="color:#e6db74">&#34;README.md&#34;</span>
    prefix.install <span style="color:#e6db74">&#34;LICENSE&#34;</span>  
end
</code></pre></div><p>Once it is ready, add the file, commit, and push.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">git add -A
git commit -m <span style="color:#e6db74">&#34;tmux-cssh_1.0&#34;</span>
git push
</code></pre></div><p>Now your tap is ready. Run the following commands to use it:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">brew tap devopstales/devopstales
brew install tmux-cssh
</code></pre></div><p>For update you just generate a new rb file with <code>brew create</code> command and the new tar.gz archive. Then change the <code>url</code> and <code>sha256</code> fields in tmux-cssh.rb.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[trivy-operator 2.3: Patch release for Admisssion controller]]></title>
            <link href="https://devopstales.github.io/home/trivy-operator-2.3/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/trivy-operator-2.2/?utm_source=atom_feed" rel="related" type="text/html" title="trivy-operator 2.2: Patch release for Admisssion controller" />
                <link href="https://devopstales.github.io/home/trivy-operator-2.1/?utm_source=atom_feed" rel="related" type="text/html" title="trivy-operator 2.1: Trivy-operator is now an Admisssion controller too!!!" />
                <link href="https://devopstales.github.io/home/trivy-operator-1.0/?utm_source=atom_feed" rel="related" type="text/html" title="trivy-operator 1.0" />
                <link href="https://devopstales.github.io/kubernetes/trivy-operator-2.3/?utm_source=atom_feed" rel="related" type="text/html" title="trivy-operator 2.3: Patch release for Admisssion controller" />
                <link href="https://devopstales.github.io/kubernetes/trivy-operator-2.2/?utm_source=atom_feed" rel="related" type="text/html" title="trivy-operator 2.2: Patch release for Admisssion controller" />
            
                <id>https://devopstales.github.io/home/trivy-operator-2.3/</id>
            
            
            <published>2022-02-04T00:00:00+00:00</published>
            <updated>2022-02-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Today I happy to announce the release of trivy-operator 2.3. This blog post focuses on the functionality provided by the trivy-operator 2.3 release.</p>
<h3 id="what-is-trivy-operator">What is trivy-operator?</h3>
<p>Trivy-operator is a Kubernetes Operator based on the open-source container vulnerability scanner Trivy. The goal of this project is to provide a vulnerability scanner that continuously scans containers deployed in a Kubernetes cluster. <a href="https://github.com/nolar/kopf">Built with Kubernetes Operator Pythonic Framework (Kopf)</a> There are a few solution for checking the images when you deploy them to the Kubernetes cluster, but fighting against vulnerabilities is a day to day task. Check once is not enough when every day is a new das for frats. That is why I created trivy-operator so you can create scheduled image scans on your running pods.</p>
<h3 id="bugfixes">Bugfixes</h3>
<p>With the release of trivy-operator 2.3 I fixed a few minor problems:</p>
<h3 id="bugfix">Bugfix</h3>
<ul>
<li>Add ability to Clusterwide Admission Controller</li>
<li>Logging improvements</li>
<li>Remove duplication on scanning, cronejob</li>
</ul>
<h3 id="trivy-image-validator">Trivy Image Validator</h3>
<p>The admission controller function can be configured as a ValidatingWebhook in a k8s cluster. Kubernetes will send requests to the admission server when a Pod creation is initiated. The admission controller checks the image using trivy if it is in a namespace with the label <code>trivy-operator-validation=true</code>.</p>
<p>You can define policy to the Admission Controller, by adding annotation to the pod trough the deployment:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">spec</span>:
  <span style="color:#ae81ff">...</span>
  <span style="color:#f92672">template</span>:
    <span style="color:#f92672">metadata</span>:
      <span style="color:#f92672">annotations</span>:
        <span style="color:#f92672">trivy.security.devopstales.io/medium</span>: <span style="color:#e6db74">&#34;5&#34;</span>
        <span style="color:#f92672">trivy.security.devopstales.io/low</span>: <span style="color:#e6db74">&#34;10&#34;</span>
        <span style="color:#f92672">trivy.security.devopstales.io/critical</span>: <span style="color:#e6db74">&#34;2&#34;</span>
...
</code></pre></div><h3 id="where-you-can-find">Where you can find:</h3>
<p>With the release of trivy-operator 2.3 I published trivy-operator with OperatorFramework to OperatorHub:</p>
<p><img src="/img/include/trivy-operator-OH.png" alt="OperatorHub"  class="zoomable" /></p>
<p><img src="/img/include/trivy-operator-OH2.png" alt="OperatorHub"  class="zoomable" /></p>
<h2 id="usage">Usage</h2>
<p>To ease deployment I created a helm chart for trivy-operator.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">helm repo add devopstales https://devopstales.github.io/helm-charts
helm repo update
</code></pre></div><p>Create a value file for deploy:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat &lt;&lt;&#39;EOF&#39;&gt; values.yaml</span>
<span style="color:#f92672">image</span>:
  <span style="color:#f92672">repository</span>: <span style="color:#ae81ff">devopstales/trivy-operator</span>
  <span style="color:#f92672">pullPolicy</span>: <span style="color:#ae81ff">Always</span>
  <span style="color:#f92672">tag</span>: <span style="color:#e6db74">&#34;2.3&#34;</span>

<span style="color:#f92672">imagePullSecrets</span>: []
<span style="color:#f92672">podSecurityContext</span>:
  <span style="color:#f92672">fsGroup</span>: <span style="color:#ae81ff">10001</span>
  <span style="color:#f92672">fsGroupChangePolicy</span>: <span style="color:#e6db74">&#34;OnRootMismatch&#34;</span>

<span style="color:#f92672">serviceAccount</span>:
  <span style="color:#f92672">create</span>: <span style="color:#66d9ef">true</span>
  <span style="color:#f92672">annotations</span>: {}
  <span style="color:#f92672">name</span>: <span style="color:#e6db74">&#34;trivy-operator&#34;</span>

<span style="color:#f92672">monitoring</span>:
  <span style="color:#f92672">port</span>: <span style="color:#e6db74">&#34;9115&#34;</span>

<span style="color:#f92672">serviceMonitor</span>:
  <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">false</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#e6db74">&#34;kube-system&#34;</span>

<span style="color:#f92672">storage</span>:
  <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">true</span>
  <span style="color:#f92672">size</span>: <span style="color:#ae81ff">1Gi</span>

<span style="color:#f92672">NamespaceScanner</span>:
  <span style="color:#f92672">crontab</span>: <span style="color:#e6db74">&#34;*/5 * * * *&#34;</span>
  <span style="color:#f92672">namespaceSelector</span>: <span style="color:#e6db74">&#34;trivy-scan&#34;</span>

<span style="color:#f92672">registryAuth</span>:
  <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">false</span>
  <span style="color:#f92672">registry</span>:
  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">docker.io</span>
    <span style="color:#f92672">user</span>: <span style="color:#e6db74">&#34;user&#34;</span>
    <span style="color:#f92672">password</span>: <span style="color:#e6db74">&#34;password&#34;</span>

<span style="color:#f92672">githubToken</span>:
  <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">false</span>
  <span style="color:#f92672">token</span>: <span style="color:#e6db74">&#34;&#34;</span>
<span style="color:#ae81ff">EOF</span>
</code></pre></div><p>When the trivy in the container want to scan an image first download the vulnerability database from github. If you test many images you need a  <code>githubToken</code> overcome the github rate limit and dockerhub username and password for overcome the dockerhub rate limit. If your store you images in a private repository you need to add an username and password for authentication.</p>
<p>The following tables lists configurable parameters of the trivy-operator chart and their default values.</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>image.repository</td>
<td>image</td>
<td>devopstales/trivy-operator</td>
</tr>
<tr>
<td>image.pullPolicy</td>
<td>pullPolicy</td>
<td>Always</td>
</tr>
<tr>
<td>image.tag</td>
<td>image tag</td>
<td>2.1</td>
</tr>
<tr>
<td>imagePullSecrets</td>
<td>imagePullSecrets list</td>
<td>[]</td>
</tr>
<tr>
<td>podSecurityContext.fsGroup</td>
<td>mount id</td>
<td>10001</td>
</tr>
<tr>
<td>serviceAccount.create</td>
<td>create serviceAccount</td>
<td>true</td>
</tr>
<tr>
<td>serviceAccount.annotations</td>
<td>add annotation to serviceAccount</td>
<td>{}</td>
</tr>
<tr>
<td>serviceAccount.name</td>
<td>name of the serviceAccount</td>
<td>trivy-operator</td>
</tr>
<tr>
<td>monitoring.port</td>
<td>prometheus endpoint port</td>
<td>9115</td>
</tr>
<tr>
<td>serviceMonitor.enabled</td>
<td>enable serviceMonitor object creation</td>
<td>false</td>
</tr>
<tr>
<td>serviceMonitor.namespace</td>
<td>where to create serviceMonitor object</td>
<td>kube-system</td>
</tr>
<tr>
<td>storage.enabled</td>
<td>enable pv to store trivy database</td>
<td>true</td>
</tr>
<tr>
<td>storage.size</td>
<td>pv size</td>
<td>1Gi</td>
</tr>
<tr>
<td>NamespaceScanner.crontab</td>
<td>cronjob scheduler</td>
<td>&ldquo;*/5 * * * *&rdquo;</td>
</tr>
<tr>
<td>NamespaceScanner.namespaceSelector</td>
<td>Namespace Selector</td>
<td>&ldquo;trivy-scan&rdquo;</td>
</tr>
<tr>
<td>registryAuth.enabled</td>
<td>enable registry authentication in operator</td>
<td>false</td>
</tr>
<tr>
<td>registryAuth.registry</td>
<td>registry name for authentication</td>
<td></td>
</tr>
<tr>
<td>registryAuth.user</td>
<td>username for authentication</td>
<td></td>
</tr>
<tr>
<td>registryAuth.password</td>
<td>password for authentication</td>
<td></td>
</tr>
<tr>
<td>githubToken.enabled</td>
<td>Enable githubToken usage for trivy database update</td>
<td>false</td>
</tr>
<tr>
<td>githubToken.token</td>
<td>githubToken value</td>
<td>&quot;&quot;</td>
</tr>
</tbody>
</table>
<h3 id="monitoring">Monitoring</h3>
<p>Trivy-operatos has a prometheus endpoint op port <code>9115</code> and can be deployed wit <code>ServiceMonitor</code> for automated scrapping.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -s http://10.43.179.39:9115/metrics | grep trivy_vulnerabilities
<span style="color:#75715e"># HELP trivy_vulnerabilities_sum Container vulnerabilities</span>
<span style="color:#75715e"># TYPE trivy_vulnerabilities_sum gauge</span>
trivy_vulnerabilities_sum<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/openshift/mysql-56-centos7:latest&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;scanning_error&#34;</span><span style="color:#f92672">}</span> 1.0
trivy_vulnerabilities_sum<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/nginxinc/nginx-unprivileged:latest&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;UNKNOWN&#34;</span><span style="color:#f92672">}</span> 0.0
trivy_vulnerabilities_sum<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/nginxinc/nginx-unprivileged:latest&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;LOW&#34;</span><span style="color:#f92672">}</span> 83.0
trivy_vulnerabilities_sum<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/nginxinc/nginx-unprivileged:latest&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;MEDIUM&#34;</span><span style="color:#f92672">}</span> 5.0
trivy_vulnerabilities_sum<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/nginxinc/nginx-unprivileged:latest&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;HIGH&#34;</span><span style="color:#f92672">}</span> 7.0
trivy_vulnerabilities_sum<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/nginxinc/nginx-unprivileged:latest&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;CRITICAL&#34;</span><span style="color:#f92672">}</span> 4.0
trivy_vulnerabilities_sum<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/library/nginx:1.18&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;UNKNOWN&#34;</span><span style="color:#f92672">}</span> 0.0
trivy_vulnerabilities_sum<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/library/nginx:1.18&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;LOW&#34;</span><span style="color:#f92672">}</span> 126.0
trivy_vulnerabilities_sum<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/library/nginx:1.18&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;MEDIUM&#34;</span><span style="color:#f92672">}</span> 25.0
trivy_vulnerabilities_sum<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/library/nginx:1.18&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;HIGH&#34;</span><span style="color:#f92672">}</span> 43.0
trivy_vulnerabilities_sum<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/library/nginx:1.18&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;CRITICAL&#34;</span><span style="color:#f92672">}</span> 21.0
<span style="color:#75715e"># HELP trivy_vulnerabilities Container vulnerabilities</span>
<span style="color:#75715e"># TYPE trivy_vulnerabilities gauge</span>
trivy_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/nginxinc/nginx-unprivileged:latest&#34;</span>,installedVersion<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;2.3.4&#34;</span>,pkgName<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pkgName&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;LOW&#34;</span>,vulnerabilityId<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;CVE-2011-3374&#34;</span><span style="color:#f92672">}</span> 1.0
trivy_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/nginxinc/nginx-unprivileged:latest&#34;</span>,installedVersion<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;8.32-4&#34;</span>,pkgName<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pkgName&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;LOW&#34;</span>,vulnerabilityId<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;CVE-2016-2781&#34;</span><span style="color:#f92672">}</span> 1.0
trivy_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/nginxinc/nginx-unprivileged:latest&#34;</span>,installedVersion<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;8.32-4&#34;</span>,pkgName<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pkgName&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;LOW&#34;</span>,vulnerabilityId<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;CVE-2017-18018&#34;</span><span style="color:#f92672">}</span> 1.0
trivy_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/nginxinc/nginx-unprivileged:latest&#34;</span>,installedVersion<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;7.74.0-1.3&#34;</span>,pkgName<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pkgName&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;CRITICAL&#34;</span>,vulnerabilityId<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;CVE-2021-22945&#34;</span><span style="color:#f92672">}</span> 1.0
trivy_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/nginxinc/nginx-unprivileged:latest&#34;</span>,installedVersion<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;7.74.0-1.3&#34;</span>,pkgName<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pkgName&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;HIGH&#34;</span>,vulnerabilityId<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;CVE-2021-22946&#34;</span><span style="color:#f92672">}</span> 1.0
trivy_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/nginxinc/nginx-unprivileged:latest&#34;</span>,installedVersion<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;7.74.0-1.3&#34;</span>,pkgName<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pkgName&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;MEDIUM&#34;</span>,vulnerabilityId<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;CVE-2021-22947&#34;</span><span style="color:#f92672">}</span> 1.0
trivy_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/nginxinc/nginx-unprivileged:latest&#34;</span>,installedVersion<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;7.74.0-1.3&#34;</span>,pkgName<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pkgName&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;LOW&#34;</span>,vulnerabilityId<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;CVE-2021-22898&#34;</span><span style="color:#f92672">}</span> 1.0
</code></pre></div><p><img src="/img/include/trivy-exporter.png" alt="Grafana Dashboard"  class="zoomable" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Create K8S cluster with Terraform and GitlabCI]]></title>
            <link href="https://devopstales.github.io/home/gke-gitlab-terraform/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/gke-gitlab-terraform/?utm_source=atom_feed" rel="related" type="text/html" title="Create K8S cluster with Terraform and GitlabCI" />
                <link href="https://devopstales.github.io/home/multus-calico/?utm_source=atom_feed" rel="related" type="text/html" title="Use multus to separate metwork trafics" />
                <link href="https://devopstales.github.io/home/multus/?utm_source=atom_feed" rel="related" type="text/html" title="Use Multus CNI in Kubernetes" />
                <link href="https://devopstales.github.io/home/trivy-operator-2.2/?utm_source=atom_feed" rel="related" type="text/html" title="trivy-operator 2.2: Patch release for Admisssion controller" />
                <link href="https://devopstales.github.io/home/trivy-operator-2.1/?utm_source=atom_feed" rel="related" type="text/html" title="trivy-operator 2.1: Trivy-operator is now an Admisssion controller too!!!" />
            
                <id>https://devopstales.github.io/home/gke-gitlab-terraform/</id>
            
            
            <published>2022-01-30T00:00:00+00:00</published>
            <updated>2022-01-30T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how how you can create a K8S cluster with Terraform and GitlabCI.</p>
<h3 id="create-service-account-on-gcp">create service account on GCP</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">gcloud config set project gke-terraform-test

gcloud iam service-accounts create gitlab-terraform <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--display-name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;gitlab-terraform&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--project<span style="color:#f92672">=</span>gke-terraform-test

gcloud iam service-accounts add-iam-policy-binding <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>gitlab-terraform@gke-terraform-test.iam.gserviceaccount.com <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--member<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;serviceAccount:gitlab-terraform@gke-terraform-test.iam.gserviceaccount.com&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--role<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;roles/compute.networkAdmin&#39;</span>

gcloud iam service-accounts add-iam-policy-binding <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>gitlab-terraform@gke-terraform-test.iam.gserviceaccount.com <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--member<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;serviceAccount:gitlab-terraform@gke-terraform-test.iam.gserviceaccount.com&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--role<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;roles/container.admin&#39;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--project<span style="color:#f92672">=</span>gke-terraform-test

gcloud iam service-accounts add-iam-policy-binding <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>gitlab-terraform@gke-terraform-test.iam.gserviceaccount.com <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--member<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;serviceAccount:gitlab-terraform@gke-terraform-test.iam.gserviceaccount.com&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--role<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;roles/iam.serviceAccountUser&#39;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--project<span style="color:#f92672">=</span>gke-terraform-test

gcloud iam service-accounts add-iam-policy-binding <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>gitlab-terraform@gke-terraform-test.iam.gserviceaccount.com <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--member<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;serviceAccount:gitlab-terraform@gke-terraform-test.iam.gserviceaccount.com&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--role<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;roles/iam.serviceAccountAdmin&#39;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--project<span style="color:#f92672">=</span>gke-terraform-test

gcloud iam service-accounts keys create serviceaccount.json <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--iam-account <span style="color:#e6db74">&#34;gitlab-terraform@gke-terraform-test.iam.gserviceaccount.com&#34;</span>
</code></pre></div><p>Store the content of the <code>serviceaccount.json</code> as base64 encoded value in gitlab variable called <code>BASE64_GOOGLE_CREDENTIALS</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">base64 serviceaccount.json | tr -d <span style="color:#ae81ff">\\</span>n
</code></pre></div><h3 id="create-gitlab-ciyaml">Create gitlab-ci.yaml</h3>
<p>Create CI environment variable:</p>
<p><code>TF_VAR_gitlab_token</code>:  GitLab personal access token with api scope to add the provisioned cluster to your GitLab group.
<code>TF_ROOT</code>: terraform
<code>TF_VAR_gcp_project</code>: gke-terraform-test</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano gitlab-ci.yaml</span>
<span style="color:#f92672">include</span>:
  - <span style="color:#f92672">template</span>: <span style="color:#ae81ff">Terraform.gitlab-ci.yml</span>

<span style="color:#f92672">variables</span>:
  <span style="color:#f92672">TF_STATE_NAME</span>: <span style="color:#ae81ff">production</span>
  <span style="color:#f92672">TF_CACHE_KEY</span>: <span style="color:#ae81ff">production</span>
  <span style="color:#f92672">TF_ROOT</span>: <span style="color:#ae81ff">terraform</span>
  <span style="color:#f92672">TF_VAR_gcp_project</span>: <span style="color:#ae81ff">gke-terraform-test</span>

<span style="color:#f92672">before_script</span>:
  - <span style="color:#ae81ff">export GOOGLE_CREDENTIALS=$(echo $BASE64_GOOGLE_CREDENTIALS | base64 -d)</span>
</code></pre></div><h3 id="create-terraform-file">Create terraform file</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">mkdir  terraform
cd
</code></pre></div><p>Import the <code>hashicorp/google</code> provider:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano providers.tf</span>
<span style="color:#ae81ff">terraform {</span>
  <span style="color:#ae81ff">required_providers {</span>
    <span style="color:#ae81ff">google = {</span>
      <span style="color:#ae81ff">source  = &#34;hashicorp/google&#34;</span>
      <span style="color:#ae81ff">version = &#34;3.79.0&#34;</span>
    }
  }

  <span style="color:#ae81ff">required_version = &#34;~&gt; 1.0.3&#34;</span>

  <span style="color:#ae81ff">backend &#34;http&#34; {</span>
  }
}

<span style="color:#ae81ff">provider &#34;google&#34; {</span>
  <span style="color:#ae81ff">project = var.project_id</span>
  <span style="color:#ae81ff">region  = var.region</span>
}
</code></pre></div><p>Generate Variables:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano variables.tf</span>
<span style="color:#ae81ff">variable &#34;project_id&#34; {</span>
  <span style="color:#ae81ff">description = &#34;project id&#34;</span>
}

<span style="color:#ae81ff">variable &#34;region&#34; {</span>
  <span style="color:#ae81ff">description = &#34;region&#34;</span>
}

<span style="color:#ae81ff">variable &#34;gke_num_nodes&#34; {</span>
  <span style="color:#ae81ff">default     = 1</span>
  <span style="color:#ae81ff">description = &#34;number of gke nodes per zone&#34;</span>
}

<span style="color:#ae81ff">variable &#34;machine_type&#34; {</span>
  <span style="color:#ae81ff">type        = string</span>
  <span style="color:#ae81ff">description = &#34;Type of the node compute engines.&#34;</span>
}

<span style="color:#ae81ff">variable &#34;disk_size_gb&#34; {</span>
  <span style="color:#ae81ff">type        = number</span>
  <span style="color:#ae81ff">description = &#34;Size of the node&#39;s disk.&#34;</span>
}

<span style="color:#ae81ff">variable &#34;disk_type&#34; {</span>
  <span style="color:#ae81ff">type        = string</span>
  <span style="color:#ae81ff">description = &#34;Type of the node&#39;s disk.&#34;</span>
}

<span style="color:#ae81ff">variable &#34;cluster_version&#34; {</span>
  <span style="color:#ae81ff">default = &#34;1.20&#34;</span>
}
</code></pre></div><p>Create network and GKE Cluster:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano main.json</span>
<span style="color:#ae81ff">resource &#34;google_compute_network&#34; &#34;vpc&#34; {</span>
  <span style="color:#ae81ff">name                    = &#34;gke-test-vpc&#34;</span>
  <span style="color:#ae81ff">auto_create_subnetworks = &#34;false&#34;</span>
}

<span style="color:#ae81ff">resource &#34;google_compute_subnetwork&#34; &#34;gke-test-network&#34; {</span>
  <span style="color:#ae81ff">name          = &#34;gke-test-network&#34;</span>
  <span style="color:#ae81ff">region        = var.region</span>
  <span style="color:#ae81ff">network       = google_compute_network.vpc.name</span>
  <span style="color:#ae81ff">ip_cidr_range = &#34;10.10.10.0/24&#34;</span>
}

<span style="color:#ae81ff">resource &#34;google_service_account&#34; &#34;default&#34; {</span>
  <span style="color:#ae81ff">account_id   = &#34;service-account-id&#34;</span>
  <span style="color:#ae81ff">display_name = &#34;Service Account&#34;</span>
}

<span style="color:#ae81ff">resource &#34;google_container_cluster&#34; &#34;primary&#34; {</span>
  <span style="color:#ae81ff">name     = &#34;gke-test&#34;</span>
  <span style="color:#ae81ff">location = var.region</span>

  <span style="color:#ae81ff">min_master_version       = var.cluster_version</span>
  <span style="color:#ae81ff">remove_default_node_pool = true</span>
  <span style="color:#ae81ff">initial_node_count       = 1</span>

  <span style="color:#ae81ff">network    = google_compute_network.vpc.name</span>
  <span style="color:#ae81ff">subnetwork = &#34;gke-test-network&#34;</span>
}


<span style="color:#ae81ff">resource &#34;google_container_node_pool&#34; &#34;primary_nodes&#34; {</span>
  <span style="color:#ae81ff">name       = &#34;${google_container_cluster.primary.name}-node-pool&#34;</span>
  <span style="color:#ae81ff">location   = var.region</span>
  <span style="color:#ae81ff">cluster    = google_container_cluster.primary.name</span>
  <span style="color:#ae81ff">node_count = var.gke_num_nodes</span>
  <span style="color:#ae81ff">version    = var.cluster_version</span>

  <span style="color:#ae81ff">management {</span>
    <span style="color:#ae81ff">auto_repair  = &#34;true&#34;</span>
    <span style="color:#ae81ff">auto_upgrade = &#34;true&#34;</span>
  }

  <span style="color:#ae81ff">node_config {</span>
    <span style="color:#ae81ff">oauth_scopes = [</span>
      <span style="color:#e6db74">&#34;https://www.googleapis.com/auth/cloud-platform&#34;</span>,
    ]

    <span style="color:#ae81ff">labels = {</span>
      <span style="color:#ae81ff">env = var.project_id</span>
    }

    <span style="color:#ae81ff">service_account = google_service_account.default.email</span>
    <span style="color:#ae81ff">image_type      = &#34;COS&#34;</span>
    <span style="color:#ae81ff">machine_type    = var.machine_type</span>
    <span style="color:#ae81ff">disk_size_gb    = var.disk_size_gb</span>
    <span style="color:#ae81ff">disk_type       = var.disk_type</span>
    <span style="color:#ae81ff">preemptible     = false</span>
    <span style="color:#ae81ff">tags         = [</span>
      <span style="color:#e6db74">&#34;gke-node&#34;</span>,
    ]

    <span style="color:#ae81ff">metadata = {</span>
      <span style="color:#ae81ff">disable-legacy-endpoints = &#34;true&#34;</span>
    }
  }
}
</code></pre></div><p>Add value for the variables:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano terraform.tfvars</span>
<span style="color:#ae81ff">project_id   = &#34;gke-terraform-test&#34;</span>
<span style="color:#ae81ff">region       = &#34;europe-west1&#34;</span>
<span style="color:#ae81ff">gke_num_nodes = 1</span>
<span style="color:#ae81ff">machine_type  = &#34;g1-small&#34;</span>
<span style="color:#ae81ff">disk_type     = &#34;pd-standard&#34;</span>
<span style="color:#ae81ff">disk_size_gb  = 10</span>
</code></pre></div><p>Configure what data will print in the end of the deploy:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano outputs.tf</span>
<span style="color:#ae81ff">output &#34;region&#34; {</span>
  <span style="color:#ae81ff">value       = var.region</span>
  <span style="color:#ae81ff">description = &#34;GCloud Region&#34;</span>
}

<span style="color:#ae81ff">output &#34;project_id&#34; {</span>
  <span style="color:#ae81ff">value       = var.project_id</span>
  <span style="color:#ae81ff">description = &#34;GCloud Project ID&#34;</span>
}

<span style="color:#ae81ff">output &#34;kubernetes_cluster_name&#34; {</span>
  <span style="color:#ae81ff">value       = google_container_cluster.primary.name</span>
  <span style="color:#ae81ff">description = &#34;GKE Cluster Name&#34;</span>
}
</code></pre></div><p>If you created all the fles it looks like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cd ..
tree .
.
├── README.md
└── terraform
    ├── providers.tf
    ├── main.tf
    ├── outputs.tf
    ├── terraform.tfvars
    ├── variables.tf

<span style="color:#ae81ff">1</span> directory, <span style="color:#ae81ff">5</span> files
</code></pre></div><p>Now you can push to the gitlab project:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">git add -A
git commit -m <span style="color:#e6db74">&#34;base terraform commit&#34;</span>
git push
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Use multus to separate metwork trafics]]></title>
            <link href="https://devopstales.github.io/home/multus-calico/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/multus/?utm_source=atom_feed" rel="related" type="text/html" title="Use Multus CNI in Kubernetes" />
                <link href="https://devopstales.github.io/kubernetes/multus-calico/?utm_source=atom_feed" rel="related" type="text/html" title="Use multus to separate metwork trafics" />
                <link href="https://devopstales.github.io/kubernetes/multus/?utm_source=atom_feed" rel="related" type="text/html" title="Use Multus CNI in Kubernetes" />
                <link href="https://devopstales.github.io/home/cilium-clustermesh/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Multicluster with Cilium Cluster Mesh" />
                <link href="https://devopstales.github.io/home/cilium-opnsense-bgp/?utm_source=atom_feed" rel="related" type="text/html" title="Use Cilium BGP integration with OPNsense" />
            
                <id>https://devopstales.github.io/home/multus-calico/</id>
            
            
            <published>2022-01-15T00:00:00+00:00</published>
            <updated>2022-01-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can use Multus CNI and Calico to create Kubernetes pods in different networks.</p>
<h3 id="install-default-network">Install default network</h3>
<blockquote>
<p>The kubernetes cluster is installed with <code>kubeadm</code> and <code>--pod-network-cidr=10.244.0.0/16</code> option</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f https://docs.projectcalico.org/manifests/tigera-operator.yaml

nano custom-resources.yaml
---
apiVersion: operator.tigera.io/v1
kind: Installation
metadata:
  name: default
spec:
  <span style="color:#75715e"># Configures Calico networking.</span>
  calicoNetwork:
    bgp: Disabled
    <span style="color:#75715e"># linuxDataplane: BPF</span>
    <span style="color:#75715e"># Note: The ipPools section cannot be modified post-install.</span>
    ipPools:
    - blockSize: <span style="color:#ae81ff">26</span>
      cidr: 10.244.0.0/16
      encapsulation: VXLANCrossSubnet
      natOutgoing: Enabled
      nodeSelector: all<span style="color:#f92672">()</span>

kubectl apply -f custom-resources.yaml
</code></pre></div><h3 id="install-secondary-network">Install Secondary network</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ip -d link show vxlan.calico
9: vxlan.calico: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span style="color:#ae81ff">1450</span> qdisc noqueue state UNKNOWN mode DEFAULT group default
    link/ether 66:2f:69:dc:0c:cc brd ff:ff:ff:ff:ff:ff promiscuity <span style="color:#ae81ff">0</span> minmtu <span style="color:#ae81ff">68</span> maxmtu <span style="color:#ae81ff">65535</span>
    vxlan id <span style="color:#ae81ff">4096</span> local 192.168.200.10 dev enp0s9 srcport <span style="color:#ae81ff">0</span> <span style="color:#ae81ff">0</span> dstport <span style="color:#ae81ff">4789</span> nolearning ttl auto ageing <span style="color:#ae81ff">300</span> udpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode eui64 numtxqueues <span style="color:#ae81ff">1</span> numrxqueues <span style="color:#ae81ff">1</span> gso_max_size <span style="color:#ae81ff">65536</span> gso_max_segs <span style="color:#ae81ff">65535</span>

ip a show vxlan.calico
9: vxlan.calico: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span style="color:#ae81ff">1450</span> qdisc noqueue state UNKNOWN group default
    link/ether 66:2f:69:dc:0c:cc brd ff:ff:ff:ff:ff:ff
    inet 10.250.58.192/32 scope global vxlan.calico
       valid_lft forever preferred_lft forever
    inet6 fe80::642f:69ff:fedc:ccc/64 scope link
       valid_lft forever preferred_lft forever
</code></pre></div><h3 id="deploy-multus">Deploy multus</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f https://raw.githubusercontent.com/k8snetworkplumbingwg/multus-cni/master/deployments/multus-daemonset-thick-plugin.yml

kubectl get pods --all-namespaces | grep -i multus
cat /etc/cni/net.d/00-multus.conf | jq
</code></pre></div><p>check multus config find calico as the default config.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat <span style="color:#e6db74">&lt;&lt;EOF | kubectl apply -f -
</span><span style="color:#e6db74">apiVersion: &#34;k8s.cni.cncf.io/v1&#34;
</span><span style="color:#e6db74">kind: NetworkAttachmentDefinition
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: 22-calico
</span><span style="color:#e6db74">spec: 
</span><span style="color:#e6db74">  config: &#39;{
</span><span style="color:#e6db74">    &#34;cniVersion&#34;: &#34;0.3.1&#34;,
</span><span style="color:#e6db74">    &#34;type&#34;: &#34;calico&#34;,
</span><span style="color:#e6db74">    &#34;log_level&#34;: &#34;info&#34;,
</span><span style="color:#e6db74">    &#34;datastore_type&#34;: &#34;kubernetes&#34;,
</span><span style="color:#e6db74">    &#34;ipam&#34;: {
</span><span style="color:#e6db74">      &#34;type&#34;: &#34;host-local&#34;,
</span><span style="color:#e6db74">      &#34;subnet&#34;: &#34;172.22.0.0/16&#34;
</span><span style="color:#e6db74">    },
</span><span style="color:#e6db74">    &#34;policy&#34;: {
</span><span style="color:#e6db74">      &#34;type&#34;: &#34;k8s&#34;
</span><span style="color:#e6db74">    },
</span><span style="color:#e6db74">    &#34;kubernetes&#34;: {
</span><span style="color:#e6db74">      &#34;kubeconfig&#34;: &#34;/etc/cni/net.d/calico-kubeconfig&#34;
</span><span style="color:#e6db74">    }
</span><span style="color:#e6db74">  }&#39;
</span><span style="color:#e6db74">EOF</span>

cat <span style="color:#e6db74">&lt;&lt;EOF | kubectl apply -f -
</span><span style="color:#e6db74">apiVersion: &#34;k8s.cni.cncf.io/v1&#34;
</span><span style="color:#e6db74">kind: NetworkAttachmentDefinition
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: 26-calico
</span><span style="color:#e6db74">  namespace: kube-system
</span><span style="color:#e6db74">spec: 
</span><span style="color:#e6db74">  config: &#39;{
</span><span style="color:#e6db74">    &#34;cniVersion&#34;: &#34;0.3.1&#34;,
</span><span style="color:#e6db74">    &#34;type&#34;: &#34;calico&#34;,
</span><span style="color:#e6db74">    &#34;log_level&#34;: &#34;info&#34;,
</span><span style="color:#e6db74">    &#34;datastore_type&#34;: &#34;kubernetes&#34;,
</span><span style="color:#e6db74">    &#34;ipam&#34;: {
</span><span style="color:#e6db74">      &#34;type&#34;: &#34;host-local&#34;,
</span><span style="color:#e6db74">      &#34;subnet&#34;: &#34;172.26.0.0/16&#34;
</span><span style="color:#e6db74">    },
</span><span style="color:#e6db74">    &#34;policy&#34;: {
</span><span style="color:#e6db74">      &#34;type&#34;: &#34;k8s&#34;
</span><span style="color:#e6db74">    },
</span><span style="color:#e6db74">    &#34;kubernetes&#34;: {
</span><span style="color:#e6db74">      &#34;kubeconfig&#34;: &#34;/etc/cni/net.d/calico-kubeconfig&#34;
</span><span style="color:#e6db74">    }
</span><span style="color:#e6db74">  }&#39;
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat <span style="color:#e6db74">&lt;&lt;EOF | kubectl apply -f -
</span><span style="color:#e6db74">apiVersion: v1
</span><span style="color:#e6db74">kind: Pod
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: net-pod
</span><span style="color:#e6db74">  annotations:
</span><span style="color:#e6db74">    v1.multus-cni.io/default-network: default/26-calico
</span><span style="color:#e6db74">spec:
</span><span style="color:#e6db74">  containers:
</span><span style="color:#e6db74">  - name: netshoot-pod
</span><span style="color:#e6db74">    image: nicolaka/netshoot
</span><span style="color:#e6db74">    command: [&#34;tail&#34;]
</span><span style="color:#e6db74">    args: [&#34;-f&#34;, &#34;/dev/null&#34;]
</span><span style="color:#e6db74">  terminationGracePeriodSeconds: 0
</span><span style="color:#e6db74">---
</span><span style="color:#e6db74">apiVersion: v1
</span><span style="color:#e6db74">kind: Pod
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: net-pod2
</span><span style="color:#e6db74">  annotations:
</span><span style="color:#e6db74">    v1.multus-cni.io/default-network: default/22-calico
</span><span style="color:#e6db74">spec:
</span><span style="color:#e6db74">  containers:
</span><span style="color:#e6db74">  - name: netshoot-pod
</span><span style="color:#e6db74">    image: nicolaka/netshoot
</span><span style="color:#e6db74">    command: [&#34;tail&#34;]
</span><span style="color:#e6db74">    args: [&#34;-f&#34;, &#34;/dev/null&#34;]
</span><span style="color:#e6db74">  terminationGracePeriodSeconds: 0
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl exec -it net-pod -- ip addr
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu <span style="color:#ae81ff">65536</span> qdisc noqueue state UNKNOWN group default qlen <span style="color:#ae81ff">1000</span>
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
3: eth0@if66: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span style="color:#ae81ff">1450</span> qdisc noqueue state UP group default
    link/ether fa:26:21:4b:3c:94 brd ff:ff:ff:ff:ff:ff link-netnsid <span style="color:#ae81ff">0</span>
    inet 172.26.0.4/32 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::f826:21ff:fe4b:3c94/64 scope link
       valid_lft forever preferred_lft forever

kubectl exec -it net-pod2 -- ip addr
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu <span style="color:#ae81ff">65536</span> qdisc noqueue state UNKNOWN group default qlen <span style="color:#ae81ff">1000</span>
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
3: eth0@if67: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span style="color:#ae81ff">1450</span> qdisc noqueue state UP group default
    link/ether fe:16:f1:9d:5e:40 brd ff:ff:ff:ff:ff:ff link-netnsid <span style="color:#ae81ff">0</span>
    inet 172.22.0.6/32 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::fc16:f1ff:fe9d:5e40/64 scope link
       valid_lft forever preferred_lft forever
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl exec -it net-pod -- ping -c <span style="color:#ae81ff">1</span> 172.22.0.6
PING 172.22.0.6 <span style="color:#f92672">(</span>172.22.0.6<span style="color:#f92672">)</span> 56<span style="color:#f92672">(</span>84<span style="color:#f92672">)</span> bytes of data.
<span style="color:#ae81ff">64</span> bytes from 172.22.0.6: icmp_seq<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> ttl<span style="color:#f92672">=</span><span style="color:#ae81ff">63</span> time<span style="color:#f92672">=</span>0.099 ms

--- 172.22.0.6 ping statistics ---
<span style="color:#ae81ff">1</span> packets transmitted, <span style="color:#ae81ff">1</span> received, 0% packet loss, time 0ms
rtt min/avg/max/mdev <span style="color:#f92672">=</span> 0.099/0.099/0.099/0.000 ms
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Use Multus CNI in Kubernetes]]></title>
            <link href="https://devopstales.github.io/home/multus/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/multus/?utm_source=atom_feed" rel="related" type="text/html" title="Use Multus CNI in Kubernetes" />
                <link href="https://devopstales.github.io/home/cilium-clustermesh/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Multicluster with Cilium Cluster Mesh" />
                <link href="https://devopstales.github.io/home/cilium-opnsense-bgp/?utm_source=atom_feed" rel="related" type="text/html" title="Use Cilium BGP integration with OPNsense" />
                <link href="https://devopstales.github.io/home/lazyimage/?utm_source=atom_feed" rel="related" type="text/html" title="Speed up docker pull with lazypull" />
                <link href="https://devopstales.github.io/home/k8s-metallb-bgp-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Self-hosted Load Balancer for bare metal Kubernetes" />
            
                <id>https://devopstales.github.io/home/multus/</id>
            
            
            <published>2022-01-14T00:00:00+00:00</published>
            <updated>2022-01-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can use Multus CNI to create Kubernetes pods with multiple interfaces.</p>
<h3 id="install-a-default-network">Install a default network</h3>
<p>Our installation method requires that you first have installed Kubernetes and have configured a default network &ndash; that is, a CNI plugin that&rsquo;s used for your pod-to-pod connectivity. After installing Kubernetes, you must install a default network CNI plugin. In this demo I will use Flannel for the sake of simplicity.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># install flanel:</span>
wget https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml
nano kube-flannel.yml
...
        args:
        - --ip-masq
        - --kube-subnet-mgr
        - --iface<span style="color:#f92672">=</span>enp0s8

kubectl apply -f kube-flannel.yml
</code></pre></div><h3 id="install-multus">Install Multus</h3>
<p>Now we can install Multus. The recommended method to deploy Multus is to deploy using a Daemonset, this spins up pods which install a Multus binary and configure Multus for usage.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f https://raw.githubusercontent.com/k8snetworkplumbingwg/multus-cni/master/deployments/multus-daemonset-thick-plugin.yml

kubectl get pods --all-namespaces | grep -i multus
</code></pre></div><p>You may further validate that it has ran by looking at the <code>/etc/cni/net.d/</code> directory and ensure that the auto-generated <code>/etc/cni/net.d/00-multus.conf</code> exists. Check the <code>multus</code> binary is exists under <code>/opt/cni/bin</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ll /opt/cni/bin/
total <span style="color:#ae81ff">98044</span>
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root  <span style="color:#ae81ff">3254624</span> Sep  <span style="color:#ae81ff">9</span>  <span style="color:#ae81ff">2020</span> bandwidth
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root  <span style="color:#ae81ff">3581192</span> Sep  <span style="color:#ae81ff">9</span>  <span style="color:#ae81ff">2020</span> bridge
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root  <span style="color:#ae81ff">9837552</span> Sep  <span style="color:#ae81ff">9</span>  <span style="color:#ae81ff">2020</span> dhcp
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root  <span style="color:#ae81ff">4699824</span> Sep  <span style="color:#ae81ff">9</span>  <span style="color:#ae81ff">2020</span> firewall
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root  <span style="color:#ae81ff">2650368</span> Sep  <span style="color:#ae81ff">9</span>  <span style="color:#ae81ff">2020</span> flannel
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root  <span style="color:#ae81ff">3274160</span> Sep  <span style="color:#ae81ff">9</span>  <span style="color:#ae81ff">2020</span> host-device
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root  <span style="color:#ae81ff">2847152</span> Sep  <span style="color:#ae81ff">9</span>  <span style="color:#ae81ff">2020</span> host-local
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root  <span style="color:#ae81ff">3377272</span> Sep  <span style="color:#ae81ff">9</span>  <span style="color:#ae81ff">2020</span> ipvlan
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root  <span style="color:#ae81ff">2715600</span> Sep  <span style="color:#ae81ff">9</span>  <span style="color:#ae81ff">2020</span> loopback
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root  <span style="color:#ae81ff">3440168</span> Sep  <span style="color:#ae81ff">9</span>  <span style="color:#ae81ff">2020</span> macvlan
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root <span style="color:#ae81ff">42554869</span> Jan <span style="color:#ae81ff">15</span> 10:44 multus
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root  <span style="color:#ae81ff">3048528</span> Sep  <span style="color:#ae81ff">9</span>  <span style="color:#ae81ff">2020</span> portmap
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root  <span style="color:#ae81ff">3528800</span> Sep  <span style="color:#ae81ff">9</span>  <span style="color:#ae81ff">2020</span> ptp
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root  <span style="color:#ae81ff">2849328</span> Sep  <span style="color:#ae81ff">9</span>  <span style="color:#ae81ff">2020</span> sbr
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root  <span style="color:#ae81ff">2503512</span> Sep  <span style="color:#ae81ff">9</span>  <span style="color:#ae81ff">2020</span> static
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root  <span style="color:#ae81ff">2820128</span> Sep  <span style="color:#ae81ff">9</span>  <span style="color:#ae81ff">2020</span> tuning
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root  <span style="color:#ae81ff">3377120</span> Sep  <span style="color:#ae81ff">9</span>  <span style="color:#ae81ff">2020</span> vlan
</code></pre></div><h3 id="create-networkattachmentdefinition">Create NetworkAttachmentDefinition</h3>
<p>The first thing we&rsquo;ll do is create configurations for each of the additional interfaces that we attach to pods. We&rsquo;ll do this by creating Custom Resources. Each configuration we well add is a CNI configuration. If you&rsquo;re not familiar with them, let&rsquo;s break them down quickly.Here&rsquo;s an example CNI configuration:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat &lt;&lt;EOF | kubectl create -f -</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#e6db74">&#34;k8s.cni.cncf.io/v1&#34;</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">NetworkAttachmentDefinition</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">macvlan-conf</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">config</span>: <span style="color:#e6db74">&#39;{
</span><span style="color:#e6db74">      &#34;cniVersion&#34;: &#34;0.3.0&#34;,
</span><span style="color:#e6db74">      &#34;type&#34;: &#34;macvlan&#34;,
</span><span style="color:#e6db74">      &#34;master&#34;: &#34;enp0s9&#34;,
</span><span style="color:#e6db74">      &#34;mode&#34;: &#34;bridge&#34;,
</span><span style="color:#e6db74">      &#34;ipam&#34;: {
</span><span style="color:#e6db74">        &#34;type&#34;: &#34;host-local&#34;,
</span><span style="color:#e6db74">        &#34;subnet&#34;: &#34;172.17.9.0/24&#34;,
</span><span style="color:#e6db74">        &#34;rangeStart&#34;: &#34;172.17.9.240&#34;,
</span><span style="color:#e6db74">        &#34;rangeEnd&#34;: &#34;172.17.9.250&#34;,
</span><span style="color:#e6db74">        &#34;routes&#34;: [
</span><span style="color:#e6db74">          { &#34;dst&#34;: &#34;0.0.0.0/0&#34; }
</span><span style="color:#e6db74">        ],
</span><span style="color:#e6db74">        &#34;gateway&#34;: &#34;172.17.9.1&#34;
</span><span style="color:#e6db74">      }
</span><span style="color:#e6db74">    }&#39;</span>
<span style="color:#ae81ff">EOF</span>
</code></pre></div><ul>
<li><code>cniVersion</code>: Tells each CNI plugin which version is being used and can give the plugin information if it&rsquo;s using a too late (or too early) version.</li>
<li><code>master</code>: this parameter should match the interface name on the hosts in your cluster. Can not be the same interface used by the default network!!!</li>
<li><code>type</code>: This tells CNI which binary to call on disk. Each CNI plugin is a binary that&rsquo;s called. Typically, these binaries are stored in <code>/opt/cni/bin</code> on each node, and CNI executes this binary. In this case we&rsquo;ve specified the <code>macvlan</code> binary. If this is your first time installing Multus, you might want to verify that the plugins that are in the &ldquo;type&rdquo; field are actually on disk in the <code>/opt/cni/bin</code> directory.</li>
<li><code>ipam</code>: IP address allocation configuration. The <code>type</code> can an be the following:
<ul>
<li><code>dhcp</code>: Runs a daemon on the host to make DHCP requests on behalf of a container</li>
<li><code>host-local</code>: Maintains a local database of allocated IPs</li>
<li><code>static</code>: Allocates static IPv4/IPv6 addresses to containers</li>
<li><a href="https://github.com/k8snetworkplumbingwg/whereabouts">whereabouts</a>: A CNI IPAM plugin that assigns IP addresses cluster-wide</li>
</ul>
</li>
</ul>
<h3 id="networkattachmentdefinition-cni-types">NetworkAttachmentDefinition CNI Types</h3>
<h5 id="bridge">Bridge:</h5>
<p>This acts as a network switch between multiple pods on the same node host. In its current form, a bridge interface is created that does not link any physical host interface. As a result, connections are not made to any external networks including other pods on the other host nodes:</p>
<p><img src="/img/include/multus01.png" alt="Brudge"  class="zoomable" /></p>
<p>Configure the bridge plug-in with host-local IPAM. The default bridge name is cni0 by default if the name is not specified using bridge parameter:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#e6db74">&#34;k8s.cni.cncf.io/v1&#34;</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">NetworkAttachmentDefinition</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">bridge-conf</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">config</span>: <span style="color:#e6db74">&#39;{
</span><span style="color:#e6db74">      &#34;cniVersion&#34;: &#34;0.3.1&#34;,
</span><span style="color:#e6db74">      &#34;type&#34;: &#34;bridge&#34;,
</span><span style="color:#e6db74">      &#34;bridge&#34;: &#34;mybr0&#34;,
</span><span style="color:#e6db74">      &#34;ipam&#34;: {
</span><span style="color:#e6db74">          &#34;type&#34;: &#34;host-local&#34;,
</span><span style="color:#e6db74">          &#34;subnet&#34;: &#34;192.168.12.0/24&#34;,
</span><span style="color:#e6db74">          &#34;rangeStart&#34;: &#34;192.168.12.10&#34;,
</span><span style="color:#e6db74">          &#34;rangeEnd&#34;: &#34;192.168.12.200&#34;
</span><span style="color:#e6db74">      }
</span><span style="color:#e6db74">    }&#39;</span>
</code></pre></div><h5 id="host-device">Host-device:</h5>
<p>This plug-in makes a physical host interface move to a pod network namespace. When enabled, the specified host interface disappears in the root network namespace (default host network namespace). This behavior might affect re-creating the pod in place on the same host as the host interface may not be found as it is specified by host-device plug-in configuration.</p>
<p><img src="/img/include/multus02.png" alt="Brudge"  class="zoomable" /></p>
<p>This time, dhcp IPAM is configured, and it would trigger the creation of the dhcp-daemon daemonset pods. The pod in daemon mode listens for an address from a DHCP server on Kubernetes, whereas the DHCP server itself is not provided. In other words, it requires an existing DHCP server in the same network. This demonstration shows you the MAC address of the parent is kept in the pod network namespace. Additionally, the source IP and MAC address can be identified by using an external web server access test.</p>
<p>Add the following configurations:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#e6db74">&#34;k8s.cni.cncf.io/v1&#34;</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">NetworkAttachmentDefinition</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">host-device</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">config</span>: <span style="color:#e6db74">&#39;{
</span><span style="color:#e6db74">      &#34;cniVersion&#34;: &#34;0.3.1&#34;,
</span><span style="color:#e6db74">      &#34;name&#34;: &#34;host-device-main&#34;,
</span><span style="color:#e6db74">      &#34;type&#34;: &#34;host-device&#34;,
</span><span style="color:#e6db74">      &#34;device&#34;: &#34;enp0s9&#34;,
</span><span style="color:#e6db74">      &#34;ipam&#34;: {
</span><span style="color:#e6db74">          &#34;type&#34;: &#34;dhcp&#34;
</span><span style="color:#e6db74">      }
</span><span style="color:#e6db74">    }&#39;</span>
</code></pre></div><h5 id="ipvlan">ipvlan:</h5>
<p>The ipvlan plug-in may be used in the event that there are limited MAC addresses that can be used. This issue is common on some switch devices that restrict the maximum number of MAC addresses per physical port due to port security configurations. When operating in most cloud providers, you should  consider using ipvlan instead of macvlan as unknown MAC addresses are forbidden in VPC networks:</p>
<p><img src="/img/include/multus03.png" alt="Brudge"  class="zoomable" /></p>
<p>The sub-interface of the ipvlan can use distinct IP addresses with the same MAC address of the parent host interface. So, it would not work well with a DHCP server which depends on the MAC addresses. Parent host interface acts as a bridge (switch) with L2 mode, and it acts as a router with L3 mode of the ipvlan plug-in.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#e6db74">&#34;k8s.cni.cncf.io/v1&#34;</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">NetworkAttachmentDefinition</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">ipvlan</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">config</span>: <span style="color:#e6db74">&#39;{
</span><span style="color:#e6db74">   &#34;cniVersion&#34;: &#34;0.3.1&#34;,
</span><span style="color:#e6db74">   &#34;name&#34;: &#34;ipvlan-main&#34;,
</span><span style="color:#e6db74">   &#34;type&#34;: &#34;ipvlan&#34;,
</span><span style="color:#e6db74">   &#34;mode&#34;: &#34;l2&#34;,
</span><span style="color:#e6db74">   &#34;master&#34;: &#34;enp0s9&#34;,
</span><span style="color:#e6db74">     &#34;ipam&#34;: {
</span><span style="color:#e6db74">          &#34;type&#34;: &#34;host-local&#34;,
</span><span style="color:#e6db74">          &#34;subnet&#34;: &#34;172.17.9.0/24&#34;,
</span><span style="color:#e6db74">          &#34;rangeStart&#34;: &#34;172.17.9.201&#34;,
</span><span style="color:#e6db74">          &#34;rangeEnd&#34;: &#34;172.17.9.205&#34;,
</span><span style="color:#e6db74">          &#34;gateway&#34;: &#34;172.17.9.1&#34;
</span><span style="color:#e6db74">     }
</span><span style="color:#e6db74">   }&#39;</span>
</code></pre></div><h5 id="macvlan">Macvlan:</h5>
<p>With macvlan, it&rsquo;s simple to use as it aligns to traditional network connectivity. Since the connectivity is directly bound with the underlying network using sub-interfaces each having MAC address.</p>
<p><img src="/img/include/multus04.png" alt="Brudge"  class="zoomable" /></p>
<p>macvlan generates MAC addresses per the sub-interfaces, and in most cases, Public Cloud Platforms are not allowed due to their security policy and Hypervisors have limited capabilities. For the <code>RHV</code> (Red Hat Virtualization) use case, you will need to change <code>No network filter</code> on your network profile before executing the test. For <code>vSwitch</code> in vSphere environments, similar relaxed policies need to be applied. The test procedure is almost the same as ipvlan, so it is easy to compare both plug-ins.</p>
<p>Macvlan has multiple modes. In this example, bridge mode will be configured. Refer to MACVLAN documentation for more information on the other mode which will not be demonstrated.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#e6db74">&#34;k8s.cni.cncf.io/v1&#34;</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">NetworkAttachmentDefinition</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">macvlan</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">config</span>: <span style="color:#e6db74">&#39;{
</span><span style="color:#e6db74">    &#34;cniVersion&#34;: &#34;0.3.1&#34;,
</span><span style="color:#e6db74">    &#34;name&#34;: &#34;macvlan-main&#34;,
</span><span style="color:#e6db74">    &#34;type&#34;: &#34;macvlan&#34;,
</span><span style="color:#e6db74">    &#34;mode&#34;: &#34;bridge&#34;,
</span><span style="color:#e6db74">    &#34;master&#34;: &#34;enp0s9&#34;,
</span><span style="color:#e6db74">      &#34;ipam&#34;: {
</span><span style="color:#e6db74">            &#34;type&#34;: &#34;static&#34;
</span><span style="color:#e6db74">      }
</span><span style="color:#e6db74">    }&#39;</span>
</code></pre></div><h3 id="creating-a-pod-that-attaches-an-additional-interface">Creating a pod that attaches an additional interface</h3>
<p><img src="/img/include/multus00.png" alt="Infra"  class="zoomable" /></p>
<p>Deploy a IPVLAN Type NetworkAttachmentDefinition:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat &lt;&lt;EOF | kubectl create -f -</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#e6db74">&#34;k8s.cni.cncf.io/v1&#34;</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">NetworkAttachmentDefinition</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">ipvlan-def</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">config</span>: <span style="color:#e6db74">&#39;{
</span><span style="color:#e6db74">      &#34;cniVersion&#34;: &#34;0.3.1&#34;,
</span><span style="color:#e6db74">      &#34;type&#34;: &#34;ipvlan&#34;,
</span><span style="color:#e6db74">      &#34;master&#34;: &#34;enp0s9&#34;,
</span><span style="color:#e6db74">      &#34;mode&#34;: &#34;l2&#34;,
</span><span style="color:#e6db74">      &#34;ipam&#34;: {
</span><span style="color:#e6db74">        &#34;type&#34;: &#34;host-local&#34;,
</span><span style="color:#e6db74">        &#34;subnet&#34;: &#34;192.168.200.0/24&#34;,
</span><span style="color:#e6db74">        &#34;rangeStart&#34;: &#34;192.168.200.201&#34;,
</span><span style="color:#e6db74">        &#34;rangeEnd&#34;: &#34;192.168.200.205&#34;,
</span><span style="color:#e6db74">        &#34;gateway&#34;: &#34;192.168.200.1&#34;
</span><span style="color:#e6db74">      }
</span><span style="color:#e6db74">    }&#39;</span>
<span style="color:#ae81ff">EOF</span>
</code></pre></div><p>Let&rsquo;s go ahead and create a pod (that just sleeps for a really long time) with this command:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat &lt;&lt;EOF | kubectl create -f -</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Pod</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">net-pod</span>
  <span style="color:#f92672">annotations</span>:
    <span style="color:#f92672">k8s.v1.cni.cncf.io/networks</span>: <span style="color:#ae81ff">ipvlan-def</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">containers</span>:
  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">netshoot-pod</span>
    <span style="color:#f92672">image</span>: <span style="color:#ae81ff">nicolaka/netshoot</span>
    <span style="color:#f92672">command</span>: [<span style="color:#e6db74">&#34;tail&#34;</span>]
    <span style="color:#f92672">args</span>: [<span style="color:#e6db74">&#34;-f&#34;</span>, <span style="color:#e6db74">&#34;/dev/null&#34;</span>]
  <span style="color:#f92672">terminationGracePeriodSeconds</span>: <span style="color:#ae81ff">0</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Pod</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">net-pod2</span>
  <span style="color:#f92672">annotations</span>:
    <span style="color:#f92672">k8s.v1.cni.cncf.io/networks</span>: <span style="color:#ae81ff">ipvlan-def</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">containers</span>:
  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">netshoot-pod</span>
    <span style="color:#f92672">image</span>: <span style="color:#ae81ff">nicolaka/netshoot</span>
    <span style="color:#f92672">command</span>: [<span style="color:#e6db74">&#34;tail&#34;</span>]
    <span style="color:#f92672">args</span>: [<span style="color:#e6db74">&#34;-f&#34;</span>, <span style="color:#e6db74">&#34;/dev/null&#34;</span>]
  <span style="color:#f92672">terminationGracePeriodSeconds</span>: <span style="color:#ae81ff">0</span>
<span style="color:#ae81ff">EOF</span>
</code></pre></div><p>Check the ips in the pod:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl exec -it net-pod -- ip addr
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu <span style="color:#ae81ff">65536</span> qdisc noqueue state UNKNOWN group default qlen <span style="color:#ae81ff">1000</span>
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
3: eth0@if10: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span style="color:#ae81ff">1450</span> qdisc noqueue state UP group default 
    link/ether 06:56:cf:cb:3e:75 brd ff:ff:ff:ff:ff:ff link-netnsid <span style="color:#ae81ff">0</span>
    inet 10.244.0.5/24 brd 10.244.0.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::456:cfff:fecb:3e75/64 scope link 
       valid_lft forever preferred_lft forever
4: net1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span style="color:#ae81ff">1500</span> qdisc noqueue state UNKNOWN group default 
    link/ether 08:00:27:a0:41:35 brd ff:ff:ff:ff:ff:ff
    inet 192.168.200.201/24 brd 192.168.200.255 scope global net1
       valid_lft forever preferred_lft forever
    inet6 fe80::800:2700:1a0:4135/64 scope link 
       valid_lft forever preferred_lft forever

kubectl exec -it net-pod2 -- ip addr
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu <span style="color:#ae81ff">65536</span> qdisc noqueue state UNKNOWN group default qlen <span style="color:#ae81ff">1000</span>
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
3: eth0@if9: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span style="color:#ae81ff">1450</span> qdisc noqueue state UP group default 
    link/ether 8e:8f:68:f8:80:2c brd ff:ff:ff:ff:ff:ff link-netnsid <span style="color:#ae81ff">0</span>
    inet 10.244.0.4/24 brd 10.244.0.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::8c8f:68ff:fef8:802c/64 scope link 
       valid_lft forever preferred_lft forever
4: net1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span style="color:#ae81ff">1500</span> qdisc noqueue state UNKNOWN group default 
    link/ether 08:00:27:a0:41:35 brd ff:ff:ff:ff:ff:ff
    inet 192.168.200.202/24 brd 192.168.200.255 scope global net1
       valid_lft forever preferred_lft forever
    inet6 fe80::800:2700:2a0:4135/64 scope link 
       valid_lft forever preferred_lft forever
</code></pre></div><p>Ping test:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># ping own ip</span>
kubectl exec -it net-pod -- ping -c <span style="color:#ae81ff">1</span> -I net1 192.168.200.201
PING 192.168.200.201 <span style="color:#f92672">(</span>192.168.200.201<span style="color:#f92672">)</span> from 192.168.200.201 net1: 56<span style="color:#f92672">(</span>84<span style="color:#f92672">)</span> bytes of data.
<span style="color:#ae81ff">64</span> bytes from 192.168.200.201: icmp_seq<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> ttl<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span> time<span style="color:#f92672">=</span>0.024 ms

--- 192.168.200.201 ping statistics ---
<span style="color:#ae81ff">1</span> packets transmitted, <span style="color:#ae81ff">1</span> received, 0% packet loss, time 0ms
rtt min/avg/max/mdev <span style="color:#f92672">=</span> 0.024/0.024/0.024/0.000 ms

<span style="color:#75715e"># ping net-pod2&#39;s ip</span>
kubectl exec -it net-pod -- ping -c <span style="color:#ae81ff">1</span> -I net1 192.168.200.201
PING 192.168.200.202 <span style="color:#f92672">(</span>192.168.200.202<span style="color:#f92672">)</span> from 192.168.200.201 net1: 56<span style="color:#f92672">(</span>84<span style="color:#f92672">)</span> bytes of data.
<span style="color:#ae81ff">64</span> bytes from 192.168.200.202: icmp_seq<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> ttl<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span> time<span style="color:#f92672">=</span>0.040 ms

--- 192.168.200.202 ping statistics ---
<span style="color:#ae81ff">1</span> packets transmitted, <span style="color:#ae81ff">1</span> received, 0% packet loss, time 0ms
rtt min/avg/max/mdev <span style="color:#f92672">=</span> 0.040/0.040/0.040/0.000 ms

<span style="color:#75715e"># ping dw</span>
kubectl exec -it net-pod -- ping -c <span style="color:#ae81ff">1</span> -I net1 192.168.200.10
PING 192.168.200.1 <span style="color:#f92672">(</span>192.168.200.1<span style="color:#f92672">)</span> from 192.168.200.201 net1: 56<span style="color:#f92672">(</span>84<span style="color:#f92672">)</span> bytes of data.
<span style="color:#ae81ff">64</span> bytes from 192.168.200.1: icmp_seq<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> ttl<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span> time<span style="color:#f92672">=</span>0.217 ms

--- 192.168.200.1 ping statistics ---
<span style="color:#ae81ff">1</span> packets transmitted, <span style="color:#ae81ff">1</span> received, 0% packet loss, time 4ms
rtt min/avg/max/mdev <span style="color:#f92672">=</span> 0.217/0.217/0.217/0.000 ms
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to migrate PostgreSQL databases to Google Cloud SQL?]]></title>
            <link href="https://devopstales.github.io/home/gcp-cloud-sql-postgresql-migration/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/gcp-cloud-sql-postgresql-migration/</id>
            
            
            <published>2022-01-13T00:00:00+00:00</published>
            <updated>2022-01-13T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can migrate your PostgreSQL database to GCP Cloud SQL.</p>
<h3 id="what-is-cloud-sql">What is Cloud SQL?</h3>
<h3 id="create-a-postgresql-instance">Create a PostgreSQL Instance</h3>
<p>I will create the PostgreSQL instance called postgresql-prod-n8gh.</p>
<p><img src="/img/include/gcp-psql01.png" alt="Create psql server"  class="zoomable" /></p>
<p>I will also create the db_admin user for postgresql-prod-n8gh instance in Google CloudSQL platform USERS section.</p>
<p><img src="/img/include/gcp-psql02.png" alt="Create psql server"  class="zoomable" /></p>
<p>I will give the access my IP for connection in Cloud SQL platform AUTHORIZATION section.</p>
<p><img src="/img/include/gcp-psql03.png" alt="Create psql server"  class="zoomable" /></p>
<p>I will log in with the db_admin user to postgresql-prod-n8gh.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">psql -h 38.56.110.96 -U db_admin -d postgres -W
</code></pre></div><h2 id="how-to-migrate">How to migrate?</h2>
<p>You hawe to option to migrate to Cloud SQL. The first is the standard <code>pg_dump</code>. The second is Replication with Seamless Cutover.</p>
<h3 id="dump-transfer-and-import">Dump, Transfer, and Import</h3>
<p>First, we would export the data using <code>pg_dump</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">pg_dump -U <span style="color:#f92672">[</span>USERNAME<span style="color:#f92672">]</span> --format<span style="color:#f92672">=</span>plain --no-owner --no-acl <span style="color:#f92672">[</span>DATABASE_NAME<span style="color:#f92672">]</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>| sed -E <span style="color:#e6db74">&#39;s/(DROP|CREATE|COMMENT ON) EXTENSION/-- \1 EXTENSION/g&#39;</span> &gt; <span style="color:#f92672">[</span>SQL_FILE<span style="color:#f92672">]</span>.sql
</code></pre></div><p>Then we have to transfer the dump to Google Cloud Storage. I will go to the Cloud Storage to create Bucket on Google Cloud Platform.</p>
<p><img src="/img/include/gcp-psql04.png" alt="Create Bucket"  class="zoomable" /></p>
<p><img src="/img/include/gcp-psql05.png" alt="Create Bucket"  class="zoomable" /></p>
<p>I will right-click on the postgres-prod-master-dump to edit bucket&rsquo;s permissions.</p>
<p><img src="/img/include/gcp-psql06.png" alt="Create Bucket"  class="zoomable" /></p>
<p>I need to change the Cloud API access scope of the remote-server instance to access and write to Cloud Storage.</p>
<p><img src="/img/include/gcp-psql07.png" alt="Create Bucket"  class="zoomable" /></p>
<p>I will run the following command for moving prod.sql dump to the postgres-prod-master-dump bucket in the remote-server instance.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">gsutil cp prod.sql gs://postgres-prod-master-dump
</code></pre></div><p>Finally, we would run the import. I will return back to the postgresql-prod-n8gh instance that created on Cloud SQL. I will click the IMPORT section in the Cloud SQL platform.</p>
<p><img src="/img/include/gcp-psql08.png" alt="Import db"  class="zoomable" /></p>
<p><img src="/img/include/gcp-psql09.png" alt="Import db"  class="zoomable" /></p>
<h3 id="replication-with-seamless-cutover">Replication with Seamless Cutover</h3>
<p>The Database Migration service of GCP is a logical replication between source and destination database and then first take the full dump from source and restore it on destination, and then it also keep replicating the ongoing data from source to destination and keep destination in sync with source which avoid downtime, once we see there is no lag between both the databases (we can see it in dms console) we will promote the cloud sql instance and redirect the application to cloud sql.</p>
<p>First we need some configuration on the On-premise or self-managed source PostgreSQL server:</p>
<p>Install the pglogical package on the server.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">apt install postgresql-11-pglogical -y

nano postgresql.conf
<span style="color:#75715e"># logical replication</span>
wal_level <span style="color:#f92672">=</span> logical
shared_preload_libraries <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;pglogical&#39;</span>
track_commit_timestamp <span style="color:#f92672">=</span> on
pglogical.conflict_resolution <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;last_update_wins&#39;</span>
</code></pre></div><p>Connect to the instance and set the following parameters, as needed:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo su - postgres
psql

ALTER SYSTEM SET shared_preload_libraries <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;pglogical&#39;</span>;
ALTER SYSTEM SET wal_level <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;logical&#39;</span>;
ALTER SYSTEM SET wal_sender_timeout <span style="color:#f92672">=</span> 0;


GRANT USAGE on public to replicator;
GRANT USAGE on pglogical to replicator;

<span style="color:#ae81ff">\c</span> backup_test
CREATE EXTENSION IF NOT EXISTS pglogical;
GRANT USAGE on SCHEMA pglogical to PUBLIC;
GRANT SELECT on ALL TABLES in SCHEMA pglogical to replicator;
GRANT SELECT on ALL TABLES in SCHEMA public to replicator;
GRANT SELECT on ALL SEQUENCES in SCHEMA public to replicator;
GRANT SELECT on ALL SEQUENCES in SCHEMA pglogical to replicator;

<span style="color:#ae81ff">\c</span> postgres
CREATE EXTENSION IF NOT EXISTS pglogical;
GRANT USAGE on SCHEMA pglogical to PUBLIC;
GRANT SELECT on ALL TABLES in SCHEMA pglogical to replicator;
GRANT SELECT on ALL TABLES in SCHEMA public to replicator;
GRANT SELECT on ALL SEQUENCES in SCHEMA public to replicator;
GRANT SELECT on ALL SEQUENCES in SCHEMA pglogical to replicator;
<span style="color:#ae81ff">\q</span>
</code></pre></div><p>Now we need to create a source connection profile: Go to the <code>Database Migration</code> page in the <code>Google Cloud Console</code> and select <code>Connection profiles</code> page and click on <code>create profile</code>.</p>
<p><img src="/img/include/gcp-psql10.png" alt="Import db"  class="zoomable" /></p>
<p>Select a Postgresql as Sourcedatabase engine. Enter hostname, port name, username and password to connect to the source database. Select the region and click on create.</p>
<p><img src="/img/include/gcp-psql11.png" alt="Import db"  class="zoomable" /></p>
<p>Now we can use this <code>create profile</code> to create a <code> Migration jobs</code>. Go to the <code>Migration jobs</code> page in the <code>Google Cloud Console</code> the click <code>CREATE MIGRATION JOB</code> at the top of the page.</p>
<p><img src="/img/include/gcp-psql12.png" alt="Import db"  class="zoomable" /></p>
<p>Select the source database engine the select the destination region for your migration. This is where the Database Migration Service instance is created, and the new database engine will be created. Specify the migration job type: Continuous (snapshot + ongoing changes).</p>
<p><img src="/img/include/gcp-psql13.png" alt="Import db"  class="zoomable" /></p>
<p>As we have already created a connection profile, then select it from the list of existing connection profiles.</p>
<p><img src="/img/include/gcp-psql14.png" alt="Import db"  class="zoomable" /></p>
<p>Configure a Destination cloud sql instance</p>
<p><img src="/img/include/gcp-psql15.png" alt="Import db"  class="zoomable" /></p>
<p>Choose whether to connect to this instance via private or public IP address. It is recommended to use Private IP. If you are using Private IP then select the VPC name as well, if you are using public IP just select it and move on to set the machine type.</p>
<p><img src="/img/include/gcp-psql16.png" alt="Import db"  class="zoomable" /></p>
<p>Select the machine type for the Cloud SQL instance. The disk size must be equal to or greater than the source database size, make sure automatic storage increase is enabled.</p>
<p><img src="/img/include/gcp-psql17.png" alt="Import db"  class="zoomable" /></p>
<p>Define connectivity method: From the Connectivity method drop-down menu, select a network connectivity method. This method defines how the newly created Cloud SQL instance will connect to the source database. If you have selected public ip at the time of destination instance creation then use the connectivity method as IP allowlist.</p>
<p><img src="/img/include/gcp-psql18.png" alt="Import db"  class="zoomable" /></p>
<p>Test and create the migration job:</p>
<p><img src="/img/include/gcp-psql19.png" alt="Import db"  class="zoomable" /></p>
<p>Once the job has been started after some time the status of job will be changes as below</p>
<p><img src="/img/include/gcp-psql20.png" alt="Import db"  class="zoomable" /></p>
<p>And then once dump and restore completed job status will be changed as below</p>
<p><img src="/img/include/gcp-psql21.png" alt="Import db"  class="zoomable" /></p>
<p>Once you see replication as 0 bytes for a long time, it means all the data has been migrated to the destination instance.</p>
<p><img src="/img/include/gcp-psql22.png" alt="Import db"  class="zoomable" /></p>
<p>Promoting a migration:</p>
<p><img src="/img/include/gcp-psql23.png" alt="Import db"  class="zoomable" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Cluster Pools got marked read only, OSDs are near full.]]></title>
            <link href="https://devopstales.github.io/home/ceph-full-osd/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/ceph-full-osd/</id>
            
            
            <published>2022-01-12T00:00:00+00:00</published>
            <updated>2022-01-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you what can you do whet an OSD is full and the ceph cluster is locked.</p>
<p>OSDs should never be full in theory and administrators should monitor how full OSDs are. If OSDs are approaching 80% full, it’s time for the administrator to take action to prevent OSDs from filling up.  Action can include re-weighting the OSDs in question and or adding more OSDs to the cluster.</p>
<pre tabindex="0"><code># ceph osd dump | grep ratio
full_ratio 0.95
backfillfull_ratio 0.9
nearfull_ratio 0.85
</code></pre><p>By default, when OSDs reach 85% capacity, <code>nearfull_ratio warning</code> is triggered.
By default when OSDs reach 90% capacity, <code>backfillfull_ratio warning</code> is triggered.  At this point the cluster will deny backfilling to the OSD in question.
By default when OSDs reach 95% capacity, <code>full_ratio is triggered</code>, all PGs (Placement Groups) on the OSDs in question will be marked Read Only, as well as all pools which are associated with the PGs on the OSD. The cluster is marked Read Only, to prevent corruption from occurring.</p>
<p>Check Osd usage:</p>
<pre tabindex="0"><code>ceph --connect-timeout=5 osd df tree
</code></pre><p>To get the cluster out of this state, data needs to be pushed away or removed from the OSDs in question. In the below example it is a single OSD in question (osd.52), but there could be many OSDs that are marked full.<br>
The first objective is to get the OSDs that are full below 95% capacity, so the cluster is not marked Read Only. It is possible to achieve this goal with a lower Weight value, .90, .85, .80, etc.</p>
<pre tabindex="0"><code>ceph osd set noout
ceph osd reweight 52 .85
</code></pre><p><code>ceph osd set-full-ratio .96</code> will change the <code>full_ratio</code> to 96% and remove the Read Only flag on OSDs which are 95% -96% full. If OSDs are 96% full it&rsquo;s possible to set <code>ceph osd set-full-ratio .97</code>, however, do NOT set this value too high.</p>
<p><code>ceph osd set-backfillfull-ratio 91</code> will change the <code>backfillfull_ratio</code> to 91% and allow backfill to occur on OSDs which are 90-91% full.  This setting is helpful when there are multiple OSDs which are full.</p>
<pre tabindex="0"><code>ceph osd set-nearfull-ratio .90
ceph osd set-backfillfull-ratio .95
ceph osd set-full-ratio .97
</code></pre><p>Now we can add more OSD to the cluster or force the rebalance of the data. In this case I will do the second because I hawe no more space in my server for more OSD disks.</p>
<pre tabindex="0"><code>ceph balancer on
ceph balancer mode upmap
ceph balancer status
ceph balancer ls
</code></pre><p>If the percentig is below the 96 treshold you can configure back the ratios:</p>
<pre tabindex="0"><code>ceph osd set-nearfull-ratio .85
ceph osd set-backfillfull-ratio .90
ceph osd set-full-ratio .95
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes Single Sign-on with Pinniped OpenID Connect]]></title>
            <link href="https://devopstales.github.io/home/k8s-pinniped/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-pinniped/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Single Sign-on with Pinniped OpenID Connect" />
                <link href="https://devopstales.github.io/home/cilium-clustermesh/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Multicluster with Cilium Cluster Mesh" />
                <link href="https://devopstales.github.io/home/k8s-git-backup/?utm_source=atom_feed" rel="related" type="text/html" title="How to Backup Kubernetes to git?" />
                <link href="https://devopstales.github.io/home/cilium-opnsense-bgp/?utm_source=atom_feed" rel="related" type="text/html" title="Use Cilium BGP integration with OPNsense" />
                <link href="https://devopstales.github.io/home/lazyimage/?utm_source=atom_feed" rel="related" type="text/html" title="Speed up docker pull with lazypull" />
            
                <id>https://devopstales.github.io/home/k8s-pinniped/</id>
            
            
            <published>2021-12-29T00:00:00+00:00</published>
            <updated>2021-12-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this tutorial I will setup Pinniped, a Single Sign-on solution from the VMware Tanzu project.</p>
<h3 id="wtah-is-pinniped">Wtah is Pinniped</h3>
<p>Pinniped gives you a unified login experience across all your clusters, including on-premises and managed cloud environments.</p>
<p>Pinniped consists of two components, Supervisor and Concierge:</p>
<ul>
<li>
<p>The Pinniped Supervisor is an OIDC server which allows users to authenticate with an external identity provider (IDP), and then issues its own federation ID tokens to be passed on to clusters based on the user information from the IDP.</p>
</li>
<li>
<p>The Pinniped Concierge is a credential exchange API which takes as input a credential from an identity source (e.g., Pinniped Supervisor, proprietary IDP), authenticates the user via that credential, and returns another credential which is understood by the host Kubernetes cluster or by an impersonation proxy which acts on behalf of the user.</p>
</li>
</ul>
<p><img src="/img/include/pinniped01.svg" alt="Pinniped Architecture"  class="zoomable" /></p>
<h3 id="install-the-pinniped-supervisor">Install the Pinniped Supervisor</h3>
<p>Run below command to install Supervisor, this will install everything to the pinniped-supervisor namespace.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f https://get.pinniped.dev/latest/install-pinniped-supervisor.yaml

kubens pinniped-supervisor
kubectl get po
</code></pre></div><p>After installing Pinniped, create a secret with the client ID and secret from your identity provider. In below command I have named the secret <code>oidc-client</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl create secret generic oidc-client <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --namespace pinniped-supervisor <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --type secrets.pinniped.dev/oidc-client <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --from-literal<span style="color:#f92672">=</span>clientID<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;&lt;client-id-goes-here&gt;&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --from-literal<span style="color:#f92672">=</span>clientSecret<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;&lt;secret-goes-here&gt;&#34;</span>
</code></pre></div><p>The supervisor needs to be reachable from other clusters, we need to create a service and an ingress resource.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano pinniped-supervisor-ingress.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Service</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">pinniped-supervisor</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">pinniped-supervisor</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">selector</span>:
    <span style="color:#f92672">app</span>: <span style="color:#ae81ff">pinniped-supervisor</span>
  <span style="color:#f92672">ports</span>:
    - <span style="color:#f92672">port</span>: <span style="color:#ae81ff">80</span>
      <span style="color:#f92672">targetPort</span>: <span style="color:#ae81ff">8080</span>
      <span style="color:#f92672">protocol</span>: <span style="color:#ae81ff">TCP</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">networking.k8s.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Ingress</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">pinniped-supervisor</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">pinniped-supervisor</span>
  <span style="color:#f92672">annotations</span>:
    <span style="color:#f92672">kubernetes.io/ingress.class</span>: <span style="color:#ae81ff">nginx</span>
    <span style="color:#f92672">cert-manager.io/cluster-issuer</span>: <span style="color:#ae81ff">ca-issuer</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">tls</span>:
  - <span style="color:#f92672">hosts</span>:
    - <span style="color:#ae81ff">supervisor.k8s.intra</span>
    <span style="color:#f92672">secretName</span>: <span style="color:#ae81ff">supervisor-cert</span>
  <span style="color:#f92672">rules</span>:
  - <span style="color:#f92672">host</span>: <span style="color:#ae81ff">supervisor.k8s.intra</span>
    <span style="color:#f92672">http</span>:
      <span style="color:#f92672">paths</span>:
      - <span style="color:#f92672">path</span>: <span style="color:#ae81ff">/</span>
        <span style="color:#f92672">pathType</span>: <span style="color:#ae81ff">Prefix</span>
        <span style="color:#f92672">backend</span>:
          <span style="color:#f92672">service</span>:
            <span style="color:#f92672">name</span>: <span style="color:#ae81ff">pinniped-supervisor</span>
            <span style="color:#f92672">port</span>:
              <span style="color:#f92672">number</span>: <span style="color:#ae81ff">80</span>
</code></pre></div><p>Create and apply a <code>FederationDomain</code> resource. Give it a name and set the issuer to the URL you will be using for Supervisor.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano FederationDomain.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">config.supervisor.pinniped.dev/v1alpha1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">FederationDomain</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">keycloak</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">pinniped-supervisor</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">issuer</span>: <span style="color:#ae81ff">https://supervisor.k8s.intra</span>
</code></pre></div><p>Create and apply a <code>OIDCIdentityProvider</code> resource.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano OIDCIdentityProvider.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">idp.supervisor.pinniped.dev/v1alpha1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">OIDCIdentityProvider</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">keycloak-idp</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">pinniped-supervisor</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">issuer</span>: <span style="color:#ae81ff">https://sso.k8s.intra/auth/realms/k8s</span>
  <span style="color:#f92672">claims</span>:
    <span style="color:#f92672">username</span>: <span style="color:#ae81ff">email</span>
    <span style="color:#f92672">groups</span>: <span style="color:#ae81ff">groups</span>
  <span style="color:#f92672">authorizationConfig</span>:
    <span style="color:#f92672">additionalScopes</span>: [<span style="color:#e6db74">&#39;email&#39;</span>, <span style="color:#e6db74">&#39;profile&#39;</span>]
  <span style="color:#f92672">client</span>:
    <span style="color:#f92672">secretName</span>: <span style="color:#ae81ff">oidc-client</span>
</code></pre></div><h3 id="install-the-pinniped-concierge">Install the Pinniped Concierge</h3>
<p>Install Concierge on any managed or unmanaged cluster you would like to use OIDC login on. This can include the cluster where Supervisor is running.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f https://get.pinniped.dev/latest/install-pinniped-concierge-crds.yaml
kubectl apply -f https://get.pinniped.dev/latest/install-pinniped-concierge.yaml

kubens pinniped-concierge
kubectl get po
</code></pre></div><p>Create and apply a <code>JWTAuthenticator</code> resource, it is cluster scoped so no need for namespaces. Create a random string for the audience property using <code>openssl rand -base64 24</code> command.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">openssl rand -base64 <span style="color:#ae81ff">24</span>
jCIGaxMT5Yw9NvafTTVoXxGLkviPPyg6
</code></pre></div><blockquote>
<p>If you ise a self signed certificate for the ingress you need o add the base64 encoded CA pem to the JWTAuthenticator.</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat k8s.pem | base64
LS0tLS1CR...
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano JWTAuthenticator.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">authentication.concierge.pinniped.dev/v1alpha1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">JWTAuthenticator</span>
<span style="color:#f92672">metadata</span>:
   <span style="color:#f92672">name</span>: <span style="color:#ae81ff">supervisor-jwt-authenticator</span>
   <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">pinniped-concierge</span>
<span style="color:#f92672">spec</span>:
   <span style="color:#f92672">issuer</span>: <span style="color:#ae81ff">https://supervisor.k8s.intra</span>
   <span style="color:#f92672">audience</span>: <span style="color:#ae81ff">jCIGaxMT5Yw9NvafTTVoXxGLkviPPyg6</span>
   <span style="color:#f92672">claims</span>:
     <span style="color:#f92672">username</span>: <span style="color:#ae81ff">username</span>
     <span style="color:#f92672">groups</span>: <span style="color:#ae81ff">groups</span>
  <span style="color:#75715e"># tls:</span>
  <span style="color:#75715e">#   certificateAuthorityData: LS0tLS1CR... # optional base64 CA data if using a self-signed certificate</span>
</code></pre></div><blockquote>
<p>The <code>supervisor.k8s.intra</code> domain must be resolwable from the pinniped-concierge.
You can add <code>supervisor.k8s.intra</code> as like hists file to the coredns <a href="https://devopstales.github.io/home/k8s-custom-host/">like this</a>.</p>
</blockquote>
<p>Below is an example of a <code>ClusterRoleBinding</code> that binds the role <code>cluster-admin</code> to the Keycloak group <code>devops-team</code>. (In my case it came from ldap) Create your own role bindings to fit your needs and apply them to the cluster.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano devops-team_ClusterRoleBinding.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRoleBinding</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cluster-admin-it-afdeling</span>
<span style="color:#f92672">roleRef</span>:
  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRole</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cluster-admin</span>
<span style="color:#f92672">subjects</span>:
  - <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
    <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Group</span>
    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">devops-team</span>
</code></pre></div><h3 id="install-the-pinniped-command-line-tool">Install the Pinniped command-line tool</h3>
<p>For osx you can use <code>brew</code> to install the command-line tool:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">brew install vmware-tanzu/pinniped/pinniped-cli
</code></pre></div><p>Or find the appropriate binary for your platform from the <a href="https://github.com/vmware-tanzu/pinniped/releases">latest release</a> the put the command-line tool somewhere on your <code>$PATH</code>.</p>
<p>Generate the <code>kubeconfig</code> file wigh the default admin <code>kubeconfig</code> on the cluster&rsquo;s master node:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">pinniped get kubeconfig <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--oidc-ca-bundle /opt/k8s_sec_lab/cert/k8s.pem <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--output pinniped-kubeconfig

kubectl --kubeconfig pinniped-kubeconfig get pods -n pinniped-concierge
NAME                                                  READY   STATUS    RESTARTS   AGE
pinniped-concierge-9bc8bbdc5-qg75p                    1/1     Running   <span style="color:#ae81ff">0</span>          16m
pinniped-concierge-9bc8bbdc5-zxsjj                    1/1     Running   <span style="color:#ae81ff">0</span>          16m
pinniped-concierge-kube-cert-agent-5dcccbbb4b-56jbj   1/1     Running   <span style="color:#ae81ff">0</span>          79m
</code></pre></div><p>The <code>k8s.pem</code> file is containes the  CA certificate from <code>ca-issuer</code>.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[trivy-operator 2.2: Patch release for Admisssion controller]]></title>
            <link href="https://devopstales.github.io/home/trivy-operator-2.2/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/trivy-operator-2.1/?utm_source=atom_feed" rel="related" type="text/html" title="trivy-operator 2.1: Trivy-operator is now an Admisssion controller too!!!" />
                <link href="https://devopstales.github.io/home/trivy-operator-1.0/?utm_source=atom_feed" rel="related" type="text/html" title="trivy-operator 1.0" />
                <link href="https://devopstales.github.io/kubernetes/trivy-operator-2.2/?utm_source=atom_feed" rel="related" type="text/html" title="trivy-operator 2.2: Patch release for Admisssion controller" />
                <link href="https://devopstales.github.io/kubernetes/trivy-operator-2.1/?utm_source=atom_feed" rel="related" type="text/html" title="trivy-operator 2.1: Trivy-operator is now an Admisssion controller too!!!" />
                <link href="https://devopstales.github.io/kubernetes/trivy-operator-1.0/?utm_source=atom_feed" rel="related" type="text/html" title="trivy-operator 1.0" />
            
                <id>https://devopstales.github.io/home/trivy-operator-2.2/</id>
            
            
            <published>2021-12-27T00:00:00+00:00</published>
            <updated>2021-12-27T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Today I happy to announce the release of trivy-operator 2.2. This blog post focuses on the functionality provided by the trivy-operator 2.2 release.</p>
<h3 id="what-is-trivy-operator">What is trivy-operator?</h3>
<p>Trivy-operator is a Kubernetes Operator based on the open-source container vulnerability scanner Trivy. The goal of this project is to provide a vulnerability scanner that continuously scans containers deployed in a Kubernetes cluster. <a href="https://github.com/nolar/kopf">Built with Kubernetes Operator Pythonic Framework (Kopf)</a> There are a few solution for checking the images when you deploy them to the Kubernetes cluster, but fighting against vulnerabilities is a day to day task. Check once is not enough when every day is a new das for frats. That is why I created trivy-operator so you can create scheduled image scans on your running pods.</p>
<h3 id="bugfixes">Bugfixes</h3>
<p>With the release of trivy-operator 2.2 I fixed a few minor problems:</p>
<ul>
<li>Add Advanced Grafana Dashboard and Change Prometheus Endpoint</li>
<li>Now the Admission Controller Function generates it&rsquo;s own self signed certificate for the <code>ValidatingWebhook</code></li>
</ul>
<p><img src="/img/include/trivy-exporter.png" alt="Grafana Dashboard"  class="zoomable" /></p>
<h3 id="trivy-image-validator">Trivy Image Validator</h3>
<p>The admission controller function can be configured as a ValidatingWebhook in a k8s cluster. Kubernetes will send requests to the admission server when a Pod creation is initiated. The admission controller checks the image using trivy if it is in a namespace with the label <code>trivy-operator-validation=true</code>.</p>
<p>You can define policy to the Admission Controller, by adding annotation to the pod trough the deployment:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">spec</span>:
  <span style="color:#ae81ff">...</span>
  <span style="color:#f92672">template</span>:
    <span style="color:#f92672">metadata</span>:
      <span style="color:#f92672">annotations</span>:
        <span style="color:#f92672">trivy.security.devopstales.io/medium</span>: <span style="color:#e6db74">&#34;5&#34;</span>
        <span style="color:#f92672">trivy.security.devopstales.io/low</span>: <span style="color:#e6db74">&#34;10&#34;</span>
        <span style="color:#f92672">trivy.security.devopstales.io/critical</span>: <span style="color:#e6db74">&#34;2&#34;</span>
...
</code></pre></div><h3 id="where-you-can-find">Where you can find:</h3>
<p>With the release of trivy-operator 2.2 I published trivy-operator with OperatorFramework to OperatorHub:</p>
<p><img src="/img/include/trivy-operator-OH.png" alt="OperatorHub"  class="zoomable" /></p>
<p><img src="/img/include/trivy-operator-OH2.png" alt="OperatorHub"  class="zoomable" /></p>
<h2 id="usage">Usage</h2>
<p>To ease deployment I created a helm chart for trivy-operator.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">helm repo add devopstales https://devopstales.github.io/helm-charts
helm repo update
</code></pre></div><p>Create a value file for deploy:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat &lt;&lt;&#39;EOF&#39;&gt; values.yaml</span>
<span style="color:#f92672">image</span>:
  <span style="color:#f92672">repository</span>: <span style="color:#ae81ff">devopstales/trivy-operator</span>
  <span style="color:#f92672">pullPolicy</span>: <span style="color:#ae81ff">Always</span>
  <span style="color:#f92672">tag</span>: <span style="color:#e6db74">&#34;2.2&#34;</span>

<span style="color:#f92672">imagePullSecrets</span>: []
<span style="color:#f92672">podSecurityContext</span>:
  <span style="color:#f92672">fsGroup</span>: <span style="color:#ae81ff">10001</span>
  <span style="color:#f92672">fsGroupChangePolicy</span>: <span style="color:#e6db74">&#34;OnRootMismatch&#34;</span>

<span style="color:#f92672">serviceAccount</span>:
  <span style="color:#f92672">create</span>: <span style="color:#66d9ef">true</span>
  <span style="color:#f92672">annotations</span>: {}
  <span style="color:#f92672">name</span>: <span style="color:#e6db74">&#34;trivy-operator&#34;</span>

<span style="color:#f92672">monitoring</span>:
  <span style="color:#f92672">port</span>: <span style="color:#e6db74">&#34;9115&#34;</span>

<span style="color:#f92672">serviceMonitor</span>:
  <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">false</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#e6db74">&#34;kube-system&#34;</span>

<span style="color:#f92672">storage</span>:
  <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">true</span>
  <span style="color:#f92672">size</span>: <span style="color:#ae81ff">1Gi</span>

<span style="color:#f92672">NamespaceScanner</span>:
  <span style="color:#f92672">crontab</span>: <span style="color:#e6db74">&#34;*/5 * * * *&#34;</span>
  <span style="color:#f92672">namespaceSelector</span>: <span style="color:#e6db74">&#34;trivy-scan&#34;</span>

<span style="color:#f92672">registryAuth</span>:
  <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">false</span>
  <span style="color:#f92672">registry</span>:
  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">docker.io</span>
    <span style="color:#f92672">user</span>: <span style="color:#e6db74">&#34;user&#34;</span>
    <span style="color:#f92672">password</span>: <span style="color:#e6db74">&#34;password&#34;</span>

<span style="color:#f92672">githubToken</span>:
  <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">false</span>
  <span style="color:#f92672">token</span>: <span style="color:#e6db74">&#34;&#34;</span>
<span style="color:#ae81ff">EOF</span>
</code></pre></div><p>When the trivy in the container want to scan an image first download the vulnerability database from github. If you test many images you need a  <code>githubToken</code> overcome the github rate limit and dockerhub username and password for overcome the dockerhub rate limit. If your store you images in a private repository you need to add an username and password for authentication.</p>
<p>The following tables lists configurable parameters of the trivy-operator chart and their default values.</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>image.repository</td>
<td>image</td>
<td>devopstales/trivy-operator</td>
</tr>
<tr>
<td>image.pullPolicy</td>
<td>pullPolicy</td>
<td>Always</td>
</tr>
<tr>
<td>image.tag</td>
<td>image tag</td>
<td>2.1</td>
</tr>
<tr>
<td>imagePullSecrets</td>
<td>imagePullSecrets list</td>
<td>[]</td>
</tr>
<tr>
<td>podSecurityContext.fsGroup</td>
<td>mount id</td>
<td>10001</td>
</tr>
<tr>
<td>serviceAccount.create</td>
<td>create serviceAccount</td>
<td>true</td>
</tr>
<tr>
<td>serviceAccount.annotations</td>
<td>add annotation to serviceAccount</td>
<td>{}</td>
</tr>
<tr>
<td>serviceAccount.name</td>
<td>name of the serviceAccount</td>
<td>trivy-operator</td>
</tr>
<tr>
<td>monitoring.port</td>
<td>prometheus endpoint port</td>
<td>9115</td>
</tr>
<tr>
<td>serviceMonitor.enabled</td>
<td>enable serviceMonitor object creation</td>
<td>false</td>
</tr>
<tr>
<td>serviceMonitor.namespace</td>
<td>where to create serviceMonitor object</td>
<td>kube-system</td>
</tr>
<tr>
<td>storage.enabled</td>
<td>enable pv to store trivy database</td>
<td>true</td>
</tr>
<tr>
<td>storage.size</td>
<td>pv size</td>
<td>1Gi</td>
</tr>
<tr>
<td>NamespaceScanner.crontab</td>
<td>cronjob scheduler</td>
<td>&ldquo;*/5 * * * *&rdquo;</td>
</tr>
<tr>
<td>NamespaceScanner.namespaceSelector</td>
<td>Namespace Selector</td>
<td>&ldquo;trivy-scan&rdquo;</td>
</tr>
<tr>
<td>registryAuth.enabled</td>
<td>enable registry authentication in operator</td>
<td>false</td>
</tr>
<tr>
<td>registryAuth.registry</td>
<td>registry name for authentication</td>
<td></td>
</tr>
<tr>
<td>registryAuth.user</td>
<td>username for authentication</td>
<td></td>
</tr>
<tr>
<td>registryAuth.password</td>
<td>password for authentication</td>
<td></td>
</tr>
<tr>
<td>githubToken.enabled</td>
<td>Enable githubToken usage for trivy database update</td>
<td>false</td>
</tr>
<tr>
<td>githubToken.token</td>
<td>githubToken value</td>
<td>&quot;&quot;</td>
</tr>
</tbody>
</table>
<h3 id="monitoring">Monitoring</h3>
<p>Trivy-operatos has a prometheus endpoint op port <code>9115</code> and can be deployed wit <code>ServiceMonitor</code> for automated scrapping.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -s http://10.43.179.39:9115/metrics | grep trivy_vulnerabilities
<span style="color:#75715e"># HELP trivy_vulnerabilities_sum Container vulnerabilities</span>
<span style="color:#75715e"># TYPE trivy_vulnerabilities_sum gauge</span>
trivy_vulnerabilities_sum<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/openshift/mysql-56-centos7:latest&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;scanning_error&#34;</span><span style="color:#f92672">}</span> 1.0
trivy_vulnerabilities_sum<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/nginxinc/nginx-unprivileged:latest&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;UNKNOWN&#34;</span><span style="color:#f92672">}</span> 0.0
trivy_vulnerabilities_sum<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/nginxinc/nginx-unprivileged:latest&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;LOW&#34;</span><span style="color:#f92672">}</span> 83.0
trivy_vulnerabilities_sum<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/nginxinc/nginx-unprivileged:latest&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;MEDIUM&#34;</span><span style="color:#f92672">}</span> 5.0
trivy_vulnerabilities_sum<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/nginxinc/nginx-unprivileged:latest&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;HIGH&#34;</span><span style="color:#f92672">}</span> 7.0
trivy_vulnerabilities_sum<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/nginxinc/nginx-unprivileged:latest&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;CRITICAL&#34;</span><span style="color:#f92672">}</span> 4.0
trivy_vulnerabilities_sum<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/library/nginx:1.18&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;UNKNOWN&#34;</span><span style="color:#f92672">}</span> 0.0
trivy_vulnerabilities_sum<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/library/nginx:1.18&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;LOW&#34;</span><span style="color:#f92672">}</span> 126.0
trivy_vulnerabilities_sum<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/library/nginx:1.18&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;MEDIUM&#34;</span><span style="color:#f92672">}</span> 25.0
trivy_vulnerabilities_sum<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/library/nginx:1.18&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;HIGH&#34;</span><span style="color:#f92672">}</span> 43.0
trivy_vulnerabilities_sum<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/library/nginx:1.18&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;CRITICAL&#34;</span><span style="color:#f92672">}</span> 21.0
<span style="color:#75715e"># HELP trivy_vulnerabilities Container vulnerabilities</span>
<span style="color:#75715e"># TYPE trivy_vulnerabilities gauge</span>
trivy_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/nginxinc/nginx-unprivileged:latest&#34;</span>,installedVersion<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;2.2.4&#34;</span>,pkgName<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pkgName&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;LOW&#34;</span>,vulnerabilityId<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;CVE-2011-3374&#34;</span><span style="color:#f92672">}</span> 1.0
trivy_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/nginxinc/nginx-unprivileged:latest&#34;</span>,installedVersion<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;8.32-4&#34;</span>,pkgName<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pkgName&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;LOW&#34;</span>,vulnerabilityId<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;CVE-2016-2781&#34;</span><span style="color:#f92672">}</span> 1.0
trivy_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/nginxinc/nginx-unprivileged:latest&#34;</span>,installedVersion<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;8.32-4&#34;</span>,pkgName<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pkgName&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;LOW&#34;</span>,vulnerabilityId<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;CVE-2017-18018&#34;</span><span style="color:#f92672">}</span> 1.0
trivy_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/nginxinc/nginx-unprivileged:latest&#34;</span>,installedVersion<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;7.74.0-1.3&#34;</span>,pkgName<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pkgName&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;CRITICAL&#34;</span>,vulnerabilityId<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;CVE-2021-22945&#34;</span><span style="color:#f92672">}</span> 1.0
trivy_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/nginxinc/nginx-unprivileged:latest&#34;</span>,installedVersion<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;7.74.0-1.3&#34;</span>,pkgName<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pkgName&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;HIGH&#34;</span>,vulnerabilityId<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;CVE-2021-22946&#34;</span><span style="color:#f92672">}</span> 1.0
trivy_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/nginxinc/nginx-unprivileged:latest&#34;</span>,installedVersion<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;7.74.0-1.3&#34;</span>,pkgName<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pkgName&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;MEDIUM&#34;</span>,vulnerabilityId<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;CVE-2021-22947&#34;</span><span style="color:#f92672">}</span> 1.0
trivy_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/nginxinc/nginx-unprivileged:latest&#34;</span>,installedVersion<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;7.74.0-1.3&#34;</span>,pkgName<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pkgName&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;LOW&#34;</span>,vulnerabilityId<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;CVE-2021-22898&#34;</span><span style="color:#f92672">}</span> 1.0
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Publish Kubernetes Operator to OperatorHub]]></title>
            <link href="https://devopstales.github.io/home/oml/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/ansible-operator-overview/?utm_source=atom_feed" rel="related" type="text/html" title="Ansible Operator Overview" />
                <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
                <link href="https://devopstales.github.io/kubernetes/oml/?utm_source=atom_feed" rel="related" type="text/html" title="Publish Kubernetes Operator to OperatorHub" />
                <link href="https://devopstales.github.io/home/openshift-log4shell/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)" />
                <link href="https://devopstales.github.io/home/openshift-elasticsearch-error/?utm_source=atom_feed" rel="related" type="text/html" title="Opeshift elasticsearch search-guard error" />
            
                <id>https://devopstales.github.io/home/oml/</id>
            
            
            <published>2021-12-21T00:00:00+00:00</published>
            <updated>2021-12-21T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can publish your operator to OperatorHub.</p>
<p>I assume you have a pre built and publised docker image with the operator in it.</p>
<h3 id="install-requirement">Install requirement</h3>
<ul>
<li><a href="https://github.com/operator-framework/operator-registry/releases">operator-registry-tools</a></li>
<li><a href="https://github.com/operator-framework/operator-sdk/releases">operator-sdk</a></li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">brew install opm
brew install operator-sdk
</code></pre></div><h3 id="stucture">Stucture</h3>
<p>Your Operator submission can be formatted following the <code>bundle</code> or <code>packagemanifest</code> format. he <code>packagemanifest</code> format is a legacy format which is kept for backwards compatibility only so I will use <code>bundle</code> format.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">tree OLM

OLM
└── 0.0.1
    ├── manifests
    │   ├── trivy-operator.v0.0.1.clusterserviceversion.yaml
    │   └── crds.yaml
    └── metadata
        └── annotations.yaml
</code></pre></div><h3 id="create-clusterserviceversion">Create clusterserviceversion</h3>
<p>To add your operator to any of the supported platforms, you will need to submit metadata for your Operator to be used by the <code>Operator Lifecycle Manager</code> (OLM). This is YAML file called <code>ClusterServiceVersion</code> which contains references to all of the CRDs, RBAC rules, Deployment and container image needed to install and securely run your Operator.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano OLM/0.0.1/manifests/trivy-operator.v0.0.1.clusterserviceversion.yaml</span>
</code></pre></div><p>First start with a CSV that only contains some descriptive metadata:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">operators.coreos.com/v1alpha1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterServiceVersion</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">annotations</span>:
    <span style="color:#f92672">capabilities</span>: <span style="color:#ae81ff">Basic Install</span>
    <span style="color:#f92672">description</span>: <span style="color:#ae81ff">Trivy Operator for scheduled imagescans and an Admission Control.</span>
    <span style="color:#f92672">certified</span>: <span style="color:#e6db74">&#34;false&#34;</span>
    <span style="color:#f92672">containerImage</span>: <span style="color:#ae81ff">devopstales/trivy-operator:0.0.1</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">trivy-operator.v0.0.1</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">description</span>: <span style="color:#ae81ff">Trivy Operator for scheduled imagescans and an Admission Control.</span>
  <span style="color:#f92672">displayName</span>: <span style="color:#ae81ff">Trivy Operator</span>
  <span style="color:#f92672">keywords</span>:
  - <span style="color:#e6db74">&#34;devops&#34;</span>
  - <span style="color:#e6db74">&#34;tales&#34;</span>
  - <span style="color:#ae81ff">trivy</span>
  - <span style="color:#ae81ff">security</span>
  <span style="color:#f92672">maintainers</span>:
  - <span style="color:#f92672">email</span>: <span style="color:#ae81ff">devopstales@mydomain.intra</span>
    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">devopstales</span>
  <span style="color:#f92672">maturity</span>: <span style="color:#ae81ff">alpha</span>
  <span style="color:#f92672">provider</span>:
    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">Blog</span>
    <span style="color:#f92672">url</span>: <span style="color:#ae81ff">devopstales.hithub.io</span>
  <span style="color:#f92672">version</span>: <span style="color:#ae81ff">0.10.0</span>
</code></pre></div><p>The next section to add to the CSV is the Install Strategy - this tells OLM about the runtime components of your operator and their requirements.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">operators.coreos.com/v1alpha1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterServiceVersion</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">annotations</span>:
  <span style="color:#75715e"># ... </span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">trivy-operator.v0.0.1</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">install</span>:
  <span style="color:#75715e"># ... </span>
    <span style="color:#75715e"># strategy indicates what type of deployment artifacts are used</span>
    <span style="color:#f92672">strategy</span>: <span style="color:#ae81ff">deployment</span>
    <span style="color:#75715e"># spec for the deployment strategy is a list of deployment specs and required permissions - similar to a pod template used in a deployment</span>
    <span style="color:#f92672">spec</span>:
      <span style="color:#f92672">permissions</span>:
      - <span style="color:#f92672">serviceAccountName</span>: <span style="color:#ae81ff">trivy-operator </span>
        <span style="color:#f92672">rules</span>:
        - <span style="color:#f92672">apiGroups</span>:
          - <span style="color:#e6db74">&#34;&#34;</span>
          <span style="color:#f92672">resources</span>:
          - <span style="color:#ae81ff">pods</span>
          <span style="color:#f92672">verbs</span>:
          - <span style="color:#e6db74">&#39;*&#39;</span>
          <span style="color:#75715e"># the rest of the rules</span>
      <span style="color:#75715e"># permissions required at the cluster scope</span>
      <span style="color:#f92672">clusterPermissions</span>:
      - <span style="color:#f92672">serviceAccountName</span>: <span style="color:#ae81ff">trivy-operator </span>
        <span style="color:#f92672">rules</span>:
        - <span style="color:#f92672">apiGroups</span>:
          - <span style="color:#e6db74">&#34;&#34;</span>
          <span style="color:#f92672">resources</span>:
          - <span style="color:#ae81ff">serviceaccounts</span>
          <span style="color:#f92672">verbs</span>:
          - <span style="color:#e6db74">&#39;*&#39;</span>
          <span style="color:#75715e"># the rest of the rules</span>
      <span style="color:#f92672">deployments</span>:
      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">trivy-operator</span>
        <span style="color:#f92672">spec</span>:
          <span style="color:#f92672">replicas</span>: <span style="color:#ae81ff">1</span>
          <span style="color:#75715e"># the rest of a deployment spec</span>
</code></pre></div><blockquote>
<p>For Openshift you can append SCC to the serviceaccount in the rules section</p>
</blockquote>
<p>It’s also important to tell OLM the ways in which your operator can be deployed:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">operators.coreos.com/v1alpha1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterServiceVersion</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">annotations</span>:
  <span style="color:#75715e"># ... </span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">trivy-operator.v0.0.1</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#75715e"># ... </span>
  <span style="color:#f92672">installModes</span>:
  - <span style="color:#f92672">supported</span>: <span style="color:#66d9ef">true</span>
    <span style="color:#f92672">type</span>: <span style="color:#ae81ff">OwnNamespace</span>
  - <span style="color:#f92672">supported</span>: <span style="color:#66d9ef">true</span>
    <span style="color:#f92672">type</span>: <span style="color:#ae81ff">SingleNamespace</span>
  - <span style="color:#f92672">supported</span>: <span style="color:#66d9ef">false</span>
    <span style="color:#f92672">type</span>: <span style="color:#ae81ff">MultiNamespace</span>
  - <span style="color:#f92672">supported</span>: <span style="color:#66d9ef">true</span>
    <span style="color:#f92672">type</span>: <span style="color:#ae81ff">AllNamespaces</span>
</code></pre></div><p>By definition, operators are programs that can talk to the Kubernetes API and they are also extend the Kubernetes API, by <code>CustomResourceDefinitions</code>. Which APIs are watched or owned is important metadata for OLM so we need to define thease too.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">operators.coreos.com/v1alpha1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterServiceVersion</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">annotations</span>:
  <span style="color:#75715e"># ... </span>
    <span style="color:#f92672">alm-examples</span>: |-<span style="color:#e6db74">
</span><span style="color:#e6db74">      [
</span><span style="color:#e6db74">        {
</span><span style="color:#e6db74">          &#34;apiVersion&#34;: &#34;trivy-operator.devopstales.io/v1&#34;,
</span><span style="color:#e6db74">          &#34;kind&#34;: &#34;NamespaceScanner&#34;,
</span><span style="color:#e6db74">          &#34;metadata&#34;: {
</span><span style="color:#e6db74">            &#34;name&#34;: &#34;trivy-operator-main-config&#34;
</span><span style="color:#e6db74">          },
</span><span style="color:#e6db74">          &#34;spec&#34;: {
</span><span style="color:#e6db74">            &#34;crontab&#34;: &#34;*/5 * * * *&#34;,
</span><span style="color:#e6db74">            &#34;namespace_selector&#34;: &#34;trivy-scan&#34;,
</span><span style="color:#e6db74">            &#34;registry&#34;: [
</span><span style="color:#e6db74">              {
</span><span style="color:#e6db74">                &#34;name&#34;: &#34;docker.io&#34;,
</span><span style="color:#e6db74">                &#34;user&#34;: &#34;&#34;,
</span><span style="color:#e6db74">                &#34;password&#34;: &#34;&#34;
</span><span style="color:#e6db74">              }
</span><span style="color:#e6db74">            ]
</span><span style="color:#e6db74">          }
</span><span style="color:#e6db74">        }
</span><span style="color:#e6db74">      ]</span>      
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">trivy-operator.v0.0.1</span>
<span style="color:#f92672">spec</span>:
<span style="color:#f92672">spec</span>:
  <span style="color:#75715e"># ... </span>
  <span style="color:#f92672">customresourcedefinitions</span>:
    <span style="color:#f92672">owned</span>:
    <span style="color:#75715e"># a list of CRDs that this operator owns </span>
      <span style="color:#75715e"># name is the metadata.name of the CRD</span>
    - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">namespace-scanners.trivy-operator.devopstales.io</span>
      <span style="color:#75715e"># version is the version of the CRD (one per entry)</span>
      <span style="color:#f92672">version</span>: <span style="color:#ae81ff">v1</span>
      <span style="color:#75715e"># spec.names.kind from the CRD</span>
      <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">NamespaceScanner</span>
</code></pre></div><p>You must place the <code>CustomResourceDefinitions</code> to the <code>manifests</code> folder andy any other object like <code>service</code> what you want to deploy by the <code>Operator Framework</code>. If you created your `` you can test with OperatorHub&rsquo;s <a href="https://operatorhub.io/preview">Operator Preview</a> page.</p>
<h1 id="test-operator">Test operator</h1>
<p>You can test your files with <code>operator-sdk</code> locally:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">operator-sdk bundle validate ./OLM/0.0.1 --select-optional suite<span style="color:#f92672">=</span>operatorframework
INFO<span style="color:#f92672">[</span>0000<span style="color:#f92672">]</span> All validation tests have completed successfully
</code></pre></div><h3 id="generate-bundle">Generate bundle</h3>
<p>To test the functions all working correctly you need first to install the operator with Operato framework. To do so you need a build the <code>bundle</code> image the the catalog index:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cd OML
opm alpha bundle build -c stable -d 0.0.1/manifests -t devopstales/trivy-operator-bundle:0.0.1 -p trivy-operator
docker push docker.io/devopstales/trivy-operator-bundle:0.0.1

opm index add -b docker.io/devopstales/trivy-operator-bundle:0.0.1 -t docker.io/devopstales/trivy-operator-index:0.0.1 -c docker
docker push docker.io/devopstales/trivy-operator-index:0.0.1
</code></pre></div><h3 id="install-oml">Install OML</h3>
<p>To install the operator from your catalog index first you need to install the <code>operator-lifecycle-manager</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -sL https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.19.1/install.sh | bash -s v0.19.1
</code></pre></div><h3 id="add-your-index-as-catalogsource">Add your index as CatalogSource</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano cs-trivy-operator.yaml</span>

<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">operators.coreos.com/v1alpha1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">CatalogSource</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">devopstales-catalog</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">olm</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">displayName</span>: <span style="color:#ae81ff">devopstales</span>
  <span style="color:#f92672">publisher</span>: <span style="color:#ae81ff">devopstales</span>
  <span style="color:#f92672">sourceType</span>: <span style="color:#ae81ff">grpc</span>
  <span style="color:#f92672">image</span>: <span style="color:#ae81ff">docker.io/devopstales/trivy-operator-index:0.0.1</span>
  <span style="color:#f92672">updateStrategy</span>:
    <span style="color:#f92672">registryPoll</span>:
      <span style="color:#f92672">interval</span>: <span style="color:#ae81ff">1m</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f cs-trivy-operator.yaml

kubectl get catalogsource --all-namespaces
NAMESPACE   NAME                    DISPLAY               TYPE      PUBLISHER        AGE
olm         devopstales-catalog     devopstales           grpc      devopstales      13m
olm         operatorhubio-catalog   Community Operators   grpc      OperatorHub.io   7h20m

kubectl get packagemanifest | grep trivy
trivy-operator                             devopstales           6h54m
</code></pre></div><p>Subscribe to this operator:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">$ cat og.yaml </span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">operators.coreos.com/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">OperatorGroup</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">test-og</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">default</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">targetNamespaces</span>:
  - <span style="color:#ae81ff">default</span>

<span style="color:#ae81ff">$ cat sub_devopstales-catalog.yaml </span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">operators.coreos.com/v1alpha1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Subscription</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">trivy-operator</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">default</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">channel</span>: <span style="color:#ae81ff">alpha</span>
  <span style="color:#f92672">installPlanApproval</span>: <span style="color:#ae81ff">Automatic</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">trivy-operator</span>
  <span style="color:#f92672">source</span>: <span style="color:#ae81ff">devopstales-catalog</span>
  <span style="color:#f92672">sourceNamespace</span>: <span style="color:#ae81ff">olm</span>
  <span style="color:#f92672">startingCSV</span>: <span style="color:#ae81ff">trivy-operator.v0.0.1</span>
</code></pre></div><p>After create them, you will get the below objects:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">$ oc get sub -n default</span>
<span style="color:#ae81ff">NAME             PACKAGE          SOURCE                CHANNEL</span>
<span style="color:#ae81ff">trivy-operator   trivy-operator   devopstales-catalog   alpha</span>

<span style="color:#ae81ff">$ oc get ip -n default</span>
<span style="color:#ae81ff">NAME            CSV                     APPROVAL    APPROVED</span>
<span style="color:#ae81ff">install-tvpq4   trivy-operator.v0.0.1   Automatic   true</span>

<span style="color:#ae81ff">$ oc get csv -n default</span>
<span style="color:#ae81ff">NAME                    DISPLAY          VERSION   REPLACES   PHASE</span>
<span style="color:#ae81ff">trivy-operator.v0.0.1   Trivy Operator   0.0.1                Succeeded</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"></code></pre></div><p>The you can create the Custo Resource to test the functionality.</p>
<h3 id="publicate-operato-on-operatohub">Publicate operato on OperatoHub</h3>
<p>To publish your bundled operator you need to create a pull requuest to the <a href="https://github.com/k8s-operatorhub/community-operators">operatorhub  community-operators</a> repo or <a href="https://github.com/redhat-openshift-ecosystem/community-operators-prod">RedHat Opeshift community-operators</a> repo.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[trivy-operator 2.1: Trivy-operator is now an Admisssion controller too!!!]]></title>
            <link href="https://devopstales.github.io/home/trivy-operator-2.1/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/trivy-operator-1.0/?utm_source=atom_feed" rel="related" type="text/html" title="trivy-operator 1.0" />
                <link href="https://devopstales.github.io/kubernetes/trivy-operator-2.1/?utm_source=atom_feed" rel="related" type="text/html" title="trivy-operator 2.1: Trivy-operator is now an Admisssion controller too!!!" />
                <link href="https://devopstales.github.io/kubernetes/trivy-operator-1.0/?utm_source=atom_feed" rel="related" type="text/html" title="trivy-operator 1.0" />
                <link href="https://devopstales.github.io/home/k8s-central-oauth/?utm_source=atom_feed" rel="related" type="text/html" title="Central authentication with oauth2-proxy" />
                <link href="https://devopstales.github.io/home/k8s-rbac-gen/?utm_source=atom_feed" rel="related" type="text/html" title="How to create kubeconfig?" />
            
                <id>https://devopstales.github.io/home/trivy-operator-2.1/</id>
            
            
            <published>2021-12-17T00:00:00+00:00</published>
            <updated>2021-12-17T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Today I happy to announce the release of trivy-operator 2.1. This blog post focuses on the functionality provided by the trivy-operator 2.1 release.</p>
<h3 id="what-is-trivy-operator">What is trivy-operator?</h3>
<p>Trivy-operator is a Kubernetes Operator based on the open-source container vulnerability scanner Trivy. The goal of this project is to provide a vulnerability scanner that continuously scans containers deployed in a Kubernetes cluster. <a href="https://github.com/nolar/kopf">Built with Kubernetes Operator Pythonic Framework (Kopf)</a> There are a few solution for checking the images when you deploy them to the Kubernetes cluster, but fighting against vulnerabilities is a day to day task. Check once is not enough when every day is a new das for frats. That is why I created trivy-operator so you can create scheduled image scans on your running pods.</p>
<h3 id="new-functions">New Functions</h3>
<p>With the release of trivy-operator 2.1 It is not just an operator but an Admisssion controller too!!! Now you can check the Images at deploy time with the same tool. You didn&rsquo;t need to deploy multiple applications for image scanning.</p>
<h3 id="trivy-image-validator">Trivy Image Validator</h3>
<p>The admission controller function can be configured as a <code>ValidatingWebhook</code> in a k8s cluster. Kubernetes will send requests to the admission server when a Pod creation is initiated. The admission controller checks the image using trivy.</p>
<p>You can define policy to the Admission Controller, by adding annotation to the pod trough the deployment:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">spec</span>:
  <span style="color:#ae81ff">...</span>
  <span style="color:#f92672">template</span>:
    <span style="color:#f92672">metadata</span>:
      <span style="color:#f92672">annotations</span>:
        <span style="color:#f92672">trivy.security.devopstales.io/medium</span>: <span style="color:#e6db74">&#34;5&#34;</span>
        <span style="color:#f92672">trivy.security.devopstales.io/low</span>: <span style="color:#e6db74">&#34;10&#34;</span>
        <span style="color:#f92672">trivy.security.devopstales.io/critical</span>: <span style="color:#e6db74">&#34;2&#34;</span>
...
</code></pre></div><h3 id="where-you-can-find">Where you can find:</h3>
<p>With the release of trivy-operator 2.1 I published trivy-operator with OperatorFramework to OperatorHub:</p>
<p><img src="/img/include/trivy-operator-OH.png" alt="OperatorHub"  class="zoomable" /></p>
<h2 id="usage">Usage</h2>
<p>To ease deployment I created a helm chart for trivy-operator.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">helm repo add devopstales https://devopstales.github.io/helm-charts
helm repo update
</code></pre></div><p>Create a value file for deploy:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat &lt;&lt;&#39;EOF&#39;&gt; values.yaml</span>
<span style="color:#f92672">image</span>:
  <span style="color:#f92672">repository</span>: <span style="color:#ae81ff">devopstales/trivy-operator</span>
  <span style="color:#f92672">pullPolicy</span>: <span style="color:#ae81ff">Always</span>
  <span style="color:#f92672">tag</span>: <span style="color:#e6db74">&#34;2.1&#34;</span>

<span style="color:#f92672">imagePullSecrets</span>: []
<span style="color:#f92672">podSecurityContext</span>:
  <span style="color:#f92672">fsGroup</span>: <span style="color:#ae81ff">10001</span>
  <span style="color:#f92672">fsGroupChangePolicy</span>: <span style="color:#e6db74">&#34;OnRootMismatch&#34;</span>

<span style="color:#f92672">serviceAccount</span>:
  <span style="color:#f92672">create</span>: <span style="color:#66d9ef">true</span>
  <span style="color:#f92672">annotations</span>: {}
  <span style="color:#f92672">name</span>: <span style="color:#e6db74">&#34;trivy-operator&#34;</span>

<span style="color:#f92672">monitoring</span>:
  <span style="color:#f92672">port</span>: <span style="color:#e6db74">&#34;9115&#34;</span>

<span style="color:#f92672">serviceMonitor</span>:
  <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">false</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#e6db74">&#34;kube-system&#34;</span>

<span style="color:#f92672">storage</span>:
  <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">true</span>
  <span style="color:#f92672">size</span>: <span style="color:#ae81ff">1Gi</span>

<span style="color:#f92672">NamespaceScanner</span>:
  <span style="color:#f92672">crontab</span>: <span style="color:#e6db74">&#34;*/5 * * * *&#34;</span>
  <span style="color:#f92672">namespaceSelector</span>: <span style="color:#e6db74">&#34;trivy-scan&#34;</span>

<span style="color:#f92672">registryAuth</span>:
  <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">false</span>
  <span style="color:#f92672">registry</span>:
  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">docker.io</span>
    <span style="color:#f92672">user</span>: <span style="color:#e6db74">&#34;user&#34;</span>
    <span style="color:#f92672">password</span>: <span style="color:#e6db74">&#34;password&#34;</span>

<span style="color:#f92672">githubToken</span>:
  <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">false</span>
  <span style="color:#f92672">token</span>: <span style="color:#e6db74">&#34;&#34;</span>
<span style="color:#ae81ff">EOF</span>
</code></pre></div><p>When the trivy in the container want to scan an image first download the vulnerability database from github. If you test many images you need a  <code>githubToken</code> overcome the github rate limit and dockerhub username and password for overcome the dockerhub rate limit. If your store you images in a private repository you need to add an username and password for authentication.</p>
<p>The following tables lists configurable parameters of the trivy-operator chart and their default values.</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>image.repository</td>
<td>image</td>
<td>devopstales/trivy-operator</td>
</tr>
<tr>
<td>image.pullPolicy</td>
<td>pullPolicy</td>
<td>Always</td>
</tr>
<tr>
<td>image.tag</td>
<td>image tag</td>
<td>2.1</td>
</tr>
<tr>
<td>imagePullSecrets</td>
<td>imagePullSecrets list</td>
<td>[]</td>
</tr>
<tr>
<td>podSecurityContext.fsGroup</td>
<td>mount id</td>
<td>10001</td>
</tr>
<tr>
<td>serviceAccount.create</td>
<td>create serviceAccount</td>
<td>true</td>
</tr>
<tr>
<td>serviceAccount.annotations</td>
<td>add annotation to serviceAccount</td>
<td>{}</td>
</tr>
<tr>
<td>serviceAccount.name</td>
<td>name of the serviceAccount</td>
<td>trivy-operator</td>
</tr>
<tr>
<td>monitoring.port</td>
<td>prometheus endpoint port</td>
<td>9115</td>
</tr>
<tr>
<td>serviceMonitor.enabled</td>
<td>enable serviceMonitor object creation</td>
<td>false</td>
</tr>
<tr>
<td>serviceMonitor.namespace</td>
<td>where to create serviceMonitor object</td>
<td>kube-system</td>
</tr>
<tr>
<td>storage.enabled</td>
<td>enable pv to store trivy database</td>
<td>true</td>
</tr>
<tr>
<td>storage.size</td>
<td>pv size</td>
<td>1Gi</td>
</tr>
<tr>
<td>NamespaceScanner.crontab</td>
<td>cronjob scheduler</td>
<td>&ldquo;*/5 * * * *&rdquo;</td>
</tr>
<tr>
<td>NamespaceScanner.namespaceSelector</td>
<td>Namespace Selector</td>
<td>&ldquo;trivy-scan&rdquo;</td>
</tr>
<tr>
<td>registryAuth.enabled</td>
<td>enable registry authentication in operator</td>
<td>false</td>
</tr>
<tr>
<td>registryAuth.registry</td>
<td>registry name for authentication</td>
<td></td>
</tr>
<tr>
<td>registryAuth.user</td>
<td>username for authentication</td>
<td></td>
</tr>
<tr>
<td>registryAuth.password</td>
<td>password for authentication</td>
<td></td>
</tr>
<tr>
<td>githubToken.enabled</td>
<td>Enable githubToken usage for trivy database update</td>
<td>false</td>
</tr>
<tr>
<td>githubToken.token</td>
<td>githubToken value</td>
<td>&quot;&quot;</td>
</tr>
</tbody>
</table>
<h3 id="monitoring">Monitoring</h3>
<p>Trivy-operatos has a prometheus endpoint op port <code>9115</code> and can be deployed wit <code>ServiceMonitor</code> for automated scrapping.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -s http://10.43.179.39:9115/metrics | grep so_vulnerabilities

so_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/library/nginx:1.18&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;UNKNOWN&#34;</span><span style="color:#f92672">}</span> <span style="color:#ae81ff">0</span>
so_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/library/nginx:1.18&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;LOW&#34;</span><span style="color:#f92672">}</span> <span style="color:#ae81ff">23</span>
so_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/library/nginx:1.18&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;MEDIUM&#34;</span><span style="color:#f92672">}</span> <span style="color:#ae81ff">93</span>
so_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/library/nginx:1.18&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;HIGH&#34;</span><span style="color:#f92672">}</span> <span style="color:#ae81ff">76</span>
so_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/library/nginx:1.18&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;CRITICAL&#34;</span><span style="color:#f92672">}</span> <span style="color:#ae81ff">25</span>
so_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/library/nginx:latest&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;UNKNOWN&#34;</span><span style="color:#f92672">}</span> <span style="color:#ae81ff">0</span>
so_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/library/nginx:latest&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;LOW&#34;</span><span style="color:#f92672">}</span> <span style="color:#ae81ff">23</span>
so_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/library/nginx:latest&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;MEDIUM&#34;</span><span style="color:#f92672">}</span> <span style="color:#ae81ff">88</span>
so_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/library/nginx:latest&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;HIGH&#34;</span><span style="color:#f92672">}</span> <span style="color:#ae81ff">60</span>
so_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/library/nginx:latest&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;CRITICAL&#34;</span><span style="color:#f92672">}</span> <span style="color:#ae81ff">8</span>

</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)]]></title>
            <link href="https://devopstales.github.io/home/openshift-log4shell/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/openshift-log4shell/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)" />
                <link href="https://devopstales.github.io/home/openshift-elasticsearch-error/?utm_source=atom_feed" rel="related" type="text/html" title="Opeshift elasticsearch search-guard error" />
                <link href="https://devopstales.github.io/home/how-to-fix-ansible-service-broker-in-openshift-3-11/?utm_source=atom_feed" rel="related" type="text/html" title="How to fix Ansible Service Broker in OpenShift 3.11" />
                <link href="https://devopstales.github.io/home/how-to-fix-cluster-monitoring-operator-in-openshift-3-11/?utm_source=atom_feed" rel="related" type="text/html" title="How to fix cluster-monitoring-operator in OpenShift 3.11" />
                <link href="https://devopstales.github.io/home/how-to-fix-registry-console-ui-in-openshift-3-11/?utm_source=atom_feed" rel="related" type="text/html" title="How to fix registry console UI in OpenShift 3.11" />
            
                <id>https://devopstales.github.io/home/openshift-log4shell/</id>
            
            
            <published>2021-12-15T00:00:00+00:00</published>
            <updated>2021-12-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>On OpenShift 4 and OpenShift 3.11 in OpenShift Logging the above mitigation can be applied to the affected Elasticsearch component.</p>
<H3>Parst of the Openshift series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
     <li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
     <li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
     <li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
     <li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
     <li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
     <li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
     <li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
     <li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
     <li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
     <li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
     <li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
     <li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
     <li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
     <li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
     <li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
     <li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
     <li>Part16a: <a href="../../kubernetes/openshift-elasticsearch-error/">Opeshift elasticsearch search-guard error</a></li>
     <li>Part16b: <a href="../../kubernetes/openshift-log4shell/">Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)</a></li>
</ul>


<h3 id="openshift-311">OpenShift 3.11</h3>
<p>Resolution:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">oc project openshift-logging

oc get dc -l component<span style="color:#f92672">=</span>es
NAME                              REVISION   DESIRED   CURRENT   TRIGGERED BY
logging-es-data-master-9fgtlhi4   <span style="color:#ae81ff">1</span>          <span style="color:#ae81ff">1</span>         <span style="color:#ae81ff">1</span>

oc set env -c elasticsearch dc/logging-es-data-master-9fgtlhi4 ES_JAVA_OPTS<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;-Dlog4j2.formatMsgNoLookups=true&#34;</span>

<span style="color:#75715e"># test the configuration</span>
oc set env -c elasticsearch dc -l component<span style="color:#f92672">=</span>es --list | grep ES_JAVA_OPTS

oc scale dc/logging-es-data-master-9fgtlhi4 --replicas<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>
oc rollout latest dc/logging-es-data-master-9fgtlhi4
oc scale dc/logging-es-data-master-9fgtlhi4 --replicas<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>
</code></pre></div><p>Afther the pod is recreated test the variable in the pods:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#66d9ef">for</span> es_pod in <span style="color:#66d9ef">$(</span>oc get pods -l component<span style="color:#f92672">=</span>es --no-headers -o jsonpath<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;{range .items[?(@.status.phase==&#34;Running&#34;)]}{.metadata.name}{&#34;\n&#34;}{end}&#39;</span><span style="color:#66d9ef">)</span>; <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>   <span style="color:#66d9ef">do</span> echo <span style="color:#e6db74">&#34;Confirm changes on </span>$es_pod<span style="color:#e6db74">&#34;</span> ;  sleep <span style="color:#ae81ff">1</span> ; <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>   oc rsh -Tc elasticsearch $es_pod ps auxwww | grep log4j2.formatMsgNoLookups ; sleep 3; <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>   <span style="color:#66d9ef">done</span>


<span style="color:#66d9ef">for</span> es_pod in <span style="color:#66d9ef">$(</span>oc get pods -l component<span style="color:#f92672">=</span>es --no-headers -o jsonpath<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;{range .items[?(@.status.phase==&#34;Running&#34;)]}{.metadata.name}{&#34;\n&#34;}{end}&#39;</span><span style="color:#66d9ef">)</span>; <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>   <span style="color:#66d9ef">do</span> echo <span style="color:#e6db74">&#34;Confirm changes on </span>$es_pod<span style="color:#e6db74">&#34;</span> ;  sleep <span style="color:#ae81ff">1</span> ; <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>   oc rsh -Tc elasticsearch $es_pod printenv | grep ES_JAVA_OPTS ; sleep 3; <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>   <span style="color:#66d9ef">done</span>
</code></pre></div><h3 id="openshift-4">OpenShift 4</h3>
<p>Resolution:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">oc project openshift-logging

oc get deployment -l component<span style="color:#f92672">=</span>elasticsearch
NAME                                      REVISION   DESIRED   CURRENT   TRIGGERED BY
elasticsearch-cdm-ba9c6evk-1-796f6cfdbc   <span style="color:#ae81ff">1</span>          <span style="color:#ae81ff">1</span>         <span style="color:#ae81ff">1</span>

oc patch deployment/elasticsearch-cdm-ba9c6evk-1-796f6cfdbc --type<span style="color:#f92672">=</span>merge -p <span style="color:#e6db74">&#39;{&#34;spec&#34;:{&#34;paused&#34;: false}}&#39;</span>
oc set env deployment/elasticsearch-cdm-ba9c6evk-1-796f6cfdbc -c elasticsearch ES_JAVA_OPTS<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;-Dlog4j2.formatMsgNoLookups=true&#34;</span>

oc set env -c elasticsearch deployment -l component<span style="color:#f92672">=</span>elasticsearch --list | grep ES_JAVA_OPTS

oc scale deployment/elasticsearch-cdm-ba9c6evk-1-796f6cfdbc --replicas<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>
</code></pre></div><p>Afther the pod is recreated test the variable in the pods:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">oc get pods -l component<span style="color:#f92672">=</span>elasticsearch

oc  set env -c elasticsearch pods -l component<span style="color:#f92672">=</span>elasticsearch --list | grep ES_JAVA_OPTS

oc exec -c elasticsearch elasticsearch-cdm-ba9c6evk-1-796f6cfdbc-4dqc6 -- grep -a log4j2.formatMsgNoLookups /proc/1/cmdline
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Opeshift elasticsearch search-guard error]]></title>
            <link href="https://devopstales.github.io/home/openshift-elasticsearch-error/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-log4shell/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)" />
                <link href="https://devopstales.github.io/home/how-to-fix-ansible-service-broker-in-openshift-3-11/?utm_source=atom_feed" rel="related" type="text/html" title="How to fix Ansible Service Broker in OpenShift 3.11" />
                <link href="https://devopstales.github.io/home/how-to-fix-cluster-monitoring-operator-in-openshift-3-11/?utm_source=atom_feed" rel="related" type="text/html" title="How to fix cluster-monitoring-operator in OpenShift 3.11" />
                <link href="https://devopstales.github.io/home/how-to-fix-registry-console-ui-in-openshift-3-11/?utm_source=atom_feed" rel="related" type="text/html" title="How to fix registry console UI in OpenShift 3.11" />
                <link href="https://devopstales.github.io/home/openshift-cert-manager/?utm_source=atom_feed" rel="related" type="text/html" title="Install cert-manager to Openshift" />
            
                <id>https://devopstales.github.io/home/openshift-elasticsearch-error/</id>
            
            
            <published>2021-12-15T00:00:00+00:00</published>
            <updated>2021-12-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show You How you can fix elasticsearch search-guard index error.</p>
<H3>Parst of the Openshift series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
     <li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
     <li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
     <li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
     <li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
     <li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
     <li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
     <li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
     <li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
     <li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
     <li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
     <li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
     <li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
     <li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
     <li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
     <li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
     <li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
     <li>Part16a: <a href="../../kubernetes/openshift-elasticsearch-error/">Opeshift elasticsearch search-guard error</a></li>
     <li>Part16b: <a href="../../kubernetes/openshift-log4shell/">Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)</a></li>
</ul>


<p>If you get the following error:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">[</span>2021-12-15 09:10:17,949<span style="color:#f92672">][</span>INFO <span style="color:#f92672">][</span>container.run            <span style="color:#f92672">]</span> Seeding the searchguard ACL index.  Will wait up to <span style="color:#ae81ff">604800</span> seconds.
<span style="color:#f92672">[</span>2021-12-15 09:10:18,027<span style="color:#f92672">][</span>INFO <span style="color:#f92672">][</span>container.run            <span style="color:#f92672">]</span> Seeding the searchguard ACL index.  Will wait up to <span style="color:#ae81ff">604800</span> seconds.
/etc/elasticsearch ~
Search Guard Admin v5
Will connect to localhost:9300 ... <span style="color:#66d9ef">done</span>
ERROR StatusLogger No Log4j <span style="color:#ae81ff">2</span> configuration file found. Using default configuration <span style="color:#f92672">(</span>logging only errors to the console<span style="color:#f92672">)</span>, or user programmatically provided configurations. Set system property <span style="color:#e6db74">&#39;log4j2.debug&#39;</span> to show Log4j <span style="color:#ae81ff">2</span> internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html <span style="color:#66d9ef">for</span> instructions on how to configure Log4j <span style="color:#ae81ff">2</span>
Elasticsearch Version: 5.6.13
Search Guard Version: &lt;unknown&gt;
Contacting elasticsearch cluster <span style="color:#e6db74">&#39;elasticsearch&#39;</span> ...
Clustername: logging-es
Clusterstate: RED
Number of nodes: <span style="color:#ae81ff">1</span>
Number of data nodes: <span style="color:#ae81ff">1</span>
</code></pre></div><p>Try to rerun the inicialization script:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">oc get pods -l component<span style="color:#f92672">=</span>es
NAME                                      READY     STATUS    RESTARTS   AGE
logging-es-data-master-9fgtlhi4-3-d48rs   2/2       Running   <span style="color:#ae81ff">0</span>          21m

oc exec -c elasticsearch logging-es-data-master-9fgtlhi4-3-d48rs -- es_seed_acl
</code></pre></div><p>If you get the same log we need to delete the searchguard index and reinicilaize:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">oc exec -c elasticearch logging-es-data-master-9fgtlhi4-3-d48rs --es_util --query<span style="color:#f92672">=</span>.searchguard -XDELETE
<span style="color:#f92672">{</span><span style="color:#e6db74">&#34;acknowledged&#34;</span>:true<span style="color:#f92672">}</span>

oc exec -c elasticsearch logging-es-data-master-9fgtlhi4-3-d48rs -- es_seed_acl
<span style="color:#f92672">[</span>2021-12-15 09:15:47,762<span style="color:#f92672">][</span>INFO <span style="color:#f92672">][</span>container.run            <span style="color:#f92672">]</span> Seeding the searchguard ACL index.  Will wait up to <span style="color:#ae81ff">604800</span> seconds.
<span style="color:#f92672">[</span>2021-12-15 09:15:47,931<span style="color:#f92672">][</span>INFO <span style="color:#f92672">][</span>container.run            <span style="color:#f92672">]</span> Seeding the searchguard ACL index.  Will wait up to <span style="color:#ae81ff">604800</span> seconds.
/etc/elasticsearch ~
Search Guard Admin v5
Will connect to localhost:9300 ... <span style="color:#66d9ef">done</span>
ERROR StatusLogger No Log4j <span style="color:#ae81ff">2</span> configuration file found. Using default configuration <span style="color:#f92672">(</span>logging only errors to the console<span style="color:#f92672">)</span>, or user programmatically provided configurations. Set system property <span style="color:#e6db74">&#39;log4j2.debug&#39;</span> to show Log4j <span style="color:#ae81ff">2</span> internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html <span style="color:#66d9ef">for</span> instructions on how to configure Log4j <span style="color:#ae81ff">2</span>
Elasticsearch Version: 5.6.16
Search Guard Version: &lt;unknown&gt;
Contacting elasticsearch cluster <span style="color:#e6db74">&#39;elasticsearch&#39;</span> ...
Clustername: logging-es
Clusterstate: RED
Number of nodes: <span style="color:#ae81ff">1</span>
Number of data nodes: <span style="color:#ae81ff">1</span>
.searchguard index does not exists, attempt to create it ...
Populate config from /opt/app-root/src/sgconfig/
Will update <span style="color:#e6db74">&#39;config&#39;</span> with /opt/app-root/src/sgconfig/sg_config.yml
   SUCC: Configuration <span style="color:#66d9ef">for</span> <span style="color:#e6db74">&#39;config&#39;</span> created or updated
Will update <span style="color:#e6db74">&#39;roles&#39;</span> with /opt/app-root/src/sgconfig/sg_roles.yml
   SUCC: Configuration <span style="color:#66d9ef">for</span> <span style="color:#e6db74">&#39;roles&#39;</span> created or updated
Will update <span style="color:#e6db74">&#39;rolesmapping&#39;</span> with /opt/app-root/src/sgconfig/sg_roles_mapping.yml
   SUCC: Configuration <span style="color:#66d9ef">for</span> <span style="color:#e6db74">&#39;rolesmapping&#39;</span> created or updated
Will update <span style="color:#e6db74">&#39;internalusers&#39;</span> with /opt/app-root/src/sgconfig/sg_internal_users.yml
   SUCC: Configuration <span style="color:#66d9ef">for</span> <span style="color:#e6db74">&#39;internalusers&#39;</span> created or updated
Will update <span style="color:#e6db74">&#39;actiongroups&#39;</span> with /opt/app-root/src/sgconfig/sg_action_groups.yml
   SUCC: Configuration <span style="color:#66d9ef">for</span> <span style="color:#e6db74">&#39;actiongroups&#39;</span> created or updated
Done with success
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configure OKD OpenShift 4 authentication]]></title>
            <link href="https://devopstales.github.io/home/openshift4-auth/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift4-ceph-rbd-csi/?utm_source=atom_feed" rel="related" type="text/html" title="Configure OKD OpenShift 4 Ceph Persisten Storage" />
                <link href="https://devopstales.github.io/home/openshift4-ingress/?utm_source=atom_feed" rel="related" type="text/html" title="Configure OKD OpenShift 4 ingress" />
                <link href="https://devopstales.github.io/home/openshift4-install/?utm_source=atom_feed" rel="related" type="text/html" title="How To Install OKD OpenShift 4 on premise" />
                <link href="https://devopstales.github.io/home/openshift4-logging/?utm_source=atom_feed" rel="related" type="text/html" title="Install Cluster Logging Operator on OpenShift 4" />
                <link href="https://devopstales.github.io/home/openshift_4/?utm_source=atom_feed" rel="related" type="text/html" title="OpenShift 4.2 with Red Hat CodeReady Containers" />
            
                <id>https://devopstales.github.io/home/openshift4-auth/</id>
            
            
            <published>2021-12-13T00:00:00+00:00</published>
            <updated>2021-12-13T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this Post I will show you how you can create multiple ingress route on an OpenShift 4 on premise.</p>
<H3>Parst of the Openshift 4 series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/openshift4-install/">Install Opeshift 4</a></li>
     <li>Part1b: <a href="../../kubernetes/openshift4-calico/">Install Opeshift 4 with calico</a></li>
     <li>Part2: <a href="../../kubernetes/openshift4-ingress/">Configure OKD OpenShift 4 ingress</a></li>
     <li>Part3: <a href="../../kubernetes/openshift4-auth/">Configure OKD OpenShift 4 authentication</a></li>
     <li>Part4: <a href="../../kubernetes/openshift4-ceph-rbd-csi/">Configure OKD OpenShift 4 Ceph Persisten Storage</a></li>
     <li>Part5a: <a href="../../kubernetes/openshift4-logging/">Install Cluster Logging Operator on OpenShift 4</a></li>
     <li>Part5b: <a href="../../kubernetes/openshift-log4shell/">Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)</a></li>
</ul>


<h3 id="create-a-cluster-admin-user">Create a Cluster Admin User</h3>
<p>The current kubeadmin user that we are using in the previous step is temporary. We need to create a permanent cluster administrator user. The fastest way to do this is by using htpasswd as an authentication provider. We will create a secret under the openshift-config namespace and add a htpasswd provider in the cluster oAuth.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">htpasswd -c -B -b users.htpasswd adminuser adminpassword
htpasswd -c -B -b users.htpasswd testuser testpassword
oc create secret generic htpass-secret --from-file<span style="color:#f92672">=</span>htpasswd<span style="color:#f92672">=</span>users.htpasswd -n <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>openshift-config
oc apply -f htpasswd_provider.yaml
</code></pre></div><p>You can create groups to the cluster:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano groups.yaml</span>
---
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Group</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">user.openshift.io/v1</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">admins</span>
<span style="color:#f92672">users</span>:
  - <span style="color:#ae81ff">adminuser</span>
---
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Group</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">user.openshift.io/v1</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cluster-admins</span>
<span style="color:#f92672">users</span>:
  - <span style="color:#ae81ff">adminuser</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">user.openshift.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Group</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">developers</span>
<span style="color:#f92672">users</span>:
  - <span style="color:#ae81ff">testuser</span>
---
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRoleBinding</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">okd-admins</span>
<span style="color:#f92672">subjects</span>:
  - <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Group</span>
    <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">admins</span>
<span style="color:#f92672">roleRef</span>:
  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRole</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">admin</span>
---
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRoleBinding</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">okd-cluster-admin</span>
<span style="color:#f92672">subjects</span>:
  - <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Group</span>
    <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cluster-admins</span>
<span style="color:#f92672">roleRef</span>:
  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRole</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cluster-admin</span>
---
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRoleBinding</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">okd-cluster-developers</span>
<span style="color:#f92672">subjects</span>:
  - <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Group</span>
    <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">developers</span>
<span style="color:#f92672">roleRef</span>:
  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRole</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">basic-user</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">oc apply -f group-admins.yaml
</code></pre></div><h3 id="enable-oauth-on-okd-cluster">Enable Oauth On OKD Cluster</h3>
<p>I will use Keycloak as the oauth prowider. In Keycloak <code>okd</code> realm I created a client called <code>okd4</code>. The <code>openid-client-secret</code> is the base64 encodid secret for the <code>okd4</code> Client ID.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano oauth-config.yaml</span>
---
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Secret</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">openid-client-secret</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">openshift-config</span>
<span style="color:#f92672">data</span>:
  <span style="color:#f92672">clientSecret</span>: <span style="color:#ae81ff">OWI4OWUyZjgtYrM6ZC10ODU2LTgyN3YtN2ZiODUzNDUyZDc4</span>
<span style="color:#f92672">type</span>: <span style="color:#ae81ff">Opaque</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">config.openshift.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">OAuth</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cluster</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">identityProviders</span>:
    - <span style="color:#f92672">mappingMethod</span>: <span style="color:#ae81ff">add</span>
      <span style="color:#f92672">name</span>: <span style="color:#ae81ff">sso.mydomain.intra</span>
      <span style="color:#f92672">openID</span>:
        <span style="color:#f92672">claims</span>:
          <span style="color:#f92672">email</span>:
            - <span style="color:#ae81ff">email</span>
          <span style="color:#f92672">name</span>:
            - <span style="color:#ae81ff">name</span>
          <span style="color:#f92672">preferredUsername</span>:
            - <span style="color:#ae81ff">email</span>
            - <span style="color:#ae81ff">preferred_username</span>
        <span style="color:#f92672">clientID</span>: <span style="color:#ae81ff">okd4</span>
        <span style="color:#f92672">clientSecret</span>:
          <span style="color:#f92672">name</span>: <span style="color:#ae81ff">openid-client-secret</span>
        <span style="color:#f92672">extraScopes</span>: []
        <span style="color:#f92672">issuer</span>: <span style="color:#e6db74">&#39;https://sso.mydomain.intra/auth/realms/okd&#39;</span>
      <span style="color:#f92672">type</span>: <span style="color:#ae81ff">OpenID</span>
</code></pre></div><p>Then apply the config <code>oc apply -f oauth-config.yaml</code></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configure OKD OpenShift 4 Ceph Persisten Storage]]></title>
            <link href="https://devopstales.github.io/home/openshift4-ceph-rbd-csi/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift4-auth/?utm_source=atom_feed" rel="related" type="text/html" title="Configure OKD OpenShift 4 authentication" />
                <link href="https://devopstales.github.io/home/openshift4-ingress/?utm_source=atom_feed" rel="related" type="text/html" title="Configure OKD OpenShift 4 ingress" />
                <link href="https://devopstales.github.io/home/openshift4-install/?utm_source=atom_feed" rel="related" type="text/html" title="How To Install OKD OpenShift 4 on premise" />
                <link href="https://devopstales.github.io/home/openshift4-logging/?utm_source=atom_feed" rel="related" type="text/html" title="Install Cluster Logging Operator on OpenShift 4" />
                <link href="https://devopstales.github.io/home/openshift_4/?utm_source=atom_feed" rel="related" type="text/html" title="OpenShift 4.2 with Red Hat CodeReady Containers" />
            
                <id>https://devopstales.github.io/home/openshift4-ceph-rbd-csi/</id>
            
            
            <published>2021-12-13T00:00:00+00:00</published>
            <updated>2021-12-13T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this Post I will show you how you can create peristent storage on an OpenShift 4 with Ceph RBD CSI Driver.</p>
<H3>Parst of the Openshift 4 series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/openshift4-install/">Install Opeshift 4</a></li>
     <li>Part1b: <a href="../../kubernetes/openshift4-calico/">Install Opeshift 4 with calico</a></li>
     <li>Part2: <a href="../../kubernetes/openshift4-ingress/">Configure OKD OpenShift 4 ingress</a></li>
     <li>Part3: <a href="../../kubernetes/openshift4-auth/">Configure OKD OpenShift 4 authentication</a></li>
     <li>Part4: <a href="../../kubernetes/openshift4-ceph-rbd-csi/">Configure OKD OpenShift 4 Ceph Persisten Storage</a></li>
     <li>Part5a: <a href="../../kubernetes/openshift4-logging/">Install Cluster Logging Operator on OpenShift 4</a></li>
     <li>Part5b: <a href="../../kubernetes/openshift-log4shell/">Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)</a></li>
</ul>


<table>
<thead>
<tr>
<th style="text-align:center">Host</th>
<th style="text-align:center">ROLES</th>
<th style="text-align:center">OS</th>
<th style="text-align:center">IP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">okd4-ceph1</td>
<td style="text-align:center">master</td>
<td style="text-align:center">CentOS 7</td>
<td style="text-align:center">192.168.1.221</td>
</tr>
<tr>
<td style="text-align:center">okd4-ceph2</td>
<td style="text-align:center">master</td>
<td style="text-align:center">CentOS 7</td>
<td style="text-align:center">192.168.1.222</td>
</tr>
<tr>
<td style="text-align:center">okd4-ceph3</td>
<td style="text-align:center">master</td>
<td style="text-align:center">CentOS 7</td>
<td style="text-align:center">192.168.1.223</td>
</tr>
<tr>
<td style="text-align:center">okd4-ceph4</td>
<td style="text-align:center">OSD</td>
<td style="text-align:center">CentOS 7</td>
<td style="text-align:center">192.168.1.224</td>
</tr>
<tr>
<td style="text-align:center">okd4-ceph5</td>
<td style="text-align:center">OSD</td>
<td style="text-align:center">CentOS 7</td>
<td style="text-align:center">192.168.1.225</td>
</tr>
</tbody>
</table>
<p>First we need a priject where we will install the Ceph Driver:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">oc adm new-project ceph-csi-rbd
oc project ceph-csi-rbd
</code></pre></div><p>We need the cluster id from the ceph cluster so get it from the ceph cluster</p>
<pre tabindex="0"><code>ceph -s
  cluster:
    id:     f8b13ea1-2s52-4fe8-bd67-e7ddf259122b
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum okd4-ceph1,okd4-ceph2,okd4-ceph3
    mgr: okd4-ceph3(active), standbys: okd4-ceph2, okd4-ceph1
    mds: cephfs-1/1/1 up  {0=okd4-ceph4=up:active}, 3 up:standby
    osd: 8 osds: 8 up, 8 in
    rgw: 4 daemons active

  data:
    pools:   14 pools, 664 pgs
    objects: 1.72M objects, 6.26TiB
    usage:   16.7TiB used, 26.9TiB / 43.7TiB avail
    pgs:     663 active+clean
             1   active+clean+scrubbing+deep

  io:
    client:   253B/s rd, 2.04MiB/s wr, 0op/s rd, 49op/s wr
</code></pre><p>With the cluster id we can create the value file for the helm chart:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano values.yaml</span>
<span style="color:#f92672">csiConfig</span>:
  - <span style="color:#f92672">clusterID</span>: <span style="color:#e6db74">&#34;f8b13ea1-2s52-4fe8-bd67-e7ddf259122b&#34;</span>
    <span style="color:#f92672">monitors</span>:
      - <span style="color:#e6db74">&#34;192.168.1.221:6789&#34;</span>
      - <span style="color:#e6db74">&#34;192.168.1.222:6789&#34;</span>
      - <span style="color:#e6db74">&#34;192.168.1.223:6789&#34;</span>
</code></pre></div><p>Create the <code>SecurityContextConstraints</code> fot the <code>ceph-csi-rbd-provisioner</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano provisioner-scc.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">security.openshift.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">SecurityContextConstraints</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">annotations</span>:
    <span style="color:#f92672">kubernetes.io/description</span>: <span style="color:#ae81ff">ceph-csi-rbd-provisioner scc is used for ceph-csi-rbd-provisioner</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">ceph-csi-rbd-provisioner</span>
<span style="color:#f92672">allowHostDirVolumePlugin</span>: <span style="color:#66d9ef">true</span>
<span style="color:#f92672">allowHostIPC</span>: <span style="color:#66d9ef">false</span>
<span style="color:#f92672">allowHostNetwork</span>: <span style="color:#66d9ef">true</span>
<span style="color:#f92672">allowHostPID</span>: <span style="color:#66d9ef">true</span>
<span style="color:#f92672">allowHostPorts</span>: <span style="color:#66d9ef">true</span>
<span style="color:#f92672">allowPrivilegeEscalation</span>: <span style="color:#66d9ef">true</span>
<span style="color:#f92672">allowPrivilegedContainer</span>: <span style="color:#66d9ef">true</span>
<span style="color:#f92672">allowedCapabilities</span>:
  - <span style="color:#e6db74">&#39;SYS_ADMIN&#39;</span>
<span style="color:#f92672">priority</span>: <span style="color:#66d9ef">null</span>
<span style="color:#f92672">readOnlyRootFilesystem</span>: <span style="color:#66d9ef">false</span>
<span style="color:#f92672">requiredDropCapabilities</span>: <span style="color:#66d9ef">null</span>
<span style="color:#f92672">defaultAddCapabilities</span>: <span style="color:#66d9ef">null</span>
<span style="color:#f92672">runAsUser</span>:
  <span style="color:#f92672">type</span>: <span style="color:#ae81ff">RunAsAny</span>
<span style="color:#f92672">seLinuxContext</span>:
  <span style="color:#f92672">type</span>: <span style="color:#ae81ff">RunAsAny</span>
<span style="color:#f92672">fsGroup</span>:
  <span style="color:#f92672">type</span>: <span style="color:#ae81ff">RunAsAny</span>
<span style="color:#f92672">supplementalGroups</span>:
  <span style="color:#f92672">type</span>: <span style="color:#ae81ff">RunAsAny</span>
<span style="color:#f92672">volumes</span>:
  - <span style="color:#e6db74">&#39;configMap&#39;</span>
  - <span style="color:#e6db74">&#39;emptyDir&#39;</span>
  - <span style="color:#e6db74">&#39;projected&#39;</span>
  - <span style="color:#e6db74">&#39;secret&#39;</span>
  - <span style="color:#e6db74">&#39;downwardAPI&#39;</span>
  - <span style="color:#e6db74">&#39;hostPath&#39;</span>
<span style="color:#f92672">users</span>:
  - <span style="color:#ae81ff">system:serviceaccount:ceph-csi-rbd:ceph-csi-rbd-provisioner</span>
<span style="color:#f92672">groups</span>: []
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">oc apply -f provisioner-scc.yaml

helm repo add ceph-csi https://ceph.github.io/csi-charts 
helm install --namespace <span style="color:#e6db74">&#34;ceph-csi-rbd&#34;</span> <span style="color:#e6db74">&#34;ceph-csi-rbd&#34;</span> ceph-csi/ceph-csi-rbd -f values.yaml
</code></pre></div><p>Now we need to configure the athentication for the ceph cluster. First get the ceph cluster admin userKey:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">echo <span style="color:#e6db74">&#34;admin&#34;</span> | base64
YWRtaW4K

ceph auth get-key client.admin|base64
QVFDTDliVmNEb21I32SHoPxXNGhmRkczTFNtcXM0ZW5VaXlTZEE977<span style="color:#f92672">==</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano secret.yaml</span>
---
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Secret</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">csi-rbd-secret</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">default</span>
<span style="color:#f92672">data</span>:
  <span style="color:#f92672">userID</span>: <span style="color:#ae81ff">YWRtaW4=</span>
  <span style="color:#f92672">userKey</span>: <span style="color:#ae81ff">QVFDTDliVmNEb21I32SHoPxXNGhmRkczTFNtcXM0ZW5VaXlTZEE977==</span>
<span style="color:#f92672">type</span>: <span style="color:#ae81ff">Opaque</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano okd4-pool.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">storage.k8s.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">StorageClass</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">csi-rbd-sc</span>
  <span style="color:#f92672">annotations</span>:
    <span style="color:#f92672">storageclass.kubernetes.io/is-default-class</span>: <span style="color:#e6db74">&#39;true&#39;</span>
<span style="color:#f92672">provisioner</span>: <span style="color:#ae81ff">rbd.csi.ceph.com</span>
<span style="color:#f92672">parameters</span>:
  <span style="color:#f92672">pool</span>: <span style="color:#ae81ff">okd4-pool</span>
  <span style="color:#f92672">clusterID</span>: <span style="color:#ae81ff">f8b13ea1-2s52-4fe8-bd67-e7ddf259122b</span>
  <span style="color:#f92672">volumeNamePrefix</span>: <span style="color:#ae81ff">okd4-vol-</span>
  <span style="color:#f92672">imageFeatures</span>: <span style="color:#ae81ff">layering</span>
  <span style="color:#f92672">csi.storage.k8s.io/fstype</span>: <span style="color:#ae81ff">ext4</span>
  <span style="color:#f92672">csi.storage.k8s.io/provisioner-secret-namespace</span>: <span style="color:#ae81ff">default</span>
  <span style="color:#f92672">csi.storage.k8s.io/provisioner-secret-name</span>: <span style="color:#ae81ff">csi-rbd-secret</span>
  <span style="color:#f92672">csi.storage.k8s.io/node-stage-secret-namespace</span>: <span style="color:#ae81ff">default</span>
  <span style="color:#f92672">csi.storage.k8s.io/node-stage-secret-name</span>: <span style="color:#ae81ff">csi-rbd-secret</span>
  <span style="color:#f92672">csi.storage.k8s.io/controller-expand-secret-namespace</span>: <span style="color:#ae81ff">default</span>
  <span style="color:#f92672">csi.storage.k8s.io/controller-expand-secret-name</span>: <span style="color:#ae81ff">csi-rbd-secret</span>
<span style="color:#f92672">volumeBindingMode</span>: <span style="color:#ae81ff">Immediate</span>
<span style="color:#f92672">reclaimPolicy</span>: <span style="color:#ae81ff">Delete</span>
<span style="color:#f92672">allowVolumeExpansion</span>: <span style="color:#66d9ef">true</span>
<span style="color:#f92672">mountOptions</span>:
  - <span style="color:#ae81ff">discard</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">oc apply -f secret.yaml
oc apply -f okd4-pool.yaml
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano raw-block-pvc.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">PersistentVolumeClaim</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">raw-block-pvc</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">accessModes</span>:
    - <span style="color:#ae81ff">ReadWriteOnce</span>
  <span style="color:#f92672">volumeMode</span>: <span style="color:#ae81ff">Block</span>
  <span style="color:#f92672">resources</span>:
    <span style="color:#f92672">requests</span>:
      <span style="color:#f92672">storage</span>: <span style="color:#ae81ff">50Mi</span>
  <span style="color:#f92672">storageClassName</span>: <span style="color:#ae81ff">csi-rbd-sc</span>
</code></pre></div><p>The test with a pvc <code>oc apply raw-block-pvc.yaml</code></p>
<h3 id="configuring-registry-storage">Configuring registry storage</h3>
<p>Edit the imageregistry-operator:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch <span style="color:#e6db74">&#39;{&#34;spec&#34;:{&#34;managementState&#34;:&#34;Managed&#34;}}&#39;</span>
oc edit configs.imageregistry.operator.openshift.io

<span style="color:#75715e"># from</span>
...
spec:
  storage: <span style="color:#f92672">{}</span>

<span style="color:#75715e"># to</span>
spec:
  storage:
    pvc:
      claim:
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">oc get pod -n openshift-image-registry

NAME                                               READY     STATUS    RESTARTS      AGE
cluster-image-registry-operator-5897c9d897-46rg8   1/1       Running   <span style="color:#ae81ff">1</span> <span style="color:#f92672">(</span>12h ago<span style="color:#f92672">)</span>   33h
image-registry-7467dd65f9-vhnvx                    1/1       Pending   <span style="color:#ae81ff">0</span>             5m
node-ca-6q6bh                                      1/1       Running   <span style="color:#ae81ff">2</span>             9d
node-ca-7hphd                                      1/1       Running   <span style="color:#ae81ff">2</span>             9d
node-ca-cbt2x                                      1/1       Running   <span style="color:#ae81ff">2</span>             9d
node-ca-cfqf5                                      1/1       Running   <span style="color:#ae81ff">2</span>             9d
node-ca-gk5ps                                      1/1       Running   <span style="color:#ae81ff">2</span>             9d
node-ca-r5jnx                                      1/1       Running   <span style="color:#ae81ff">2</span>             9d
</code></pre></div><h3 id="enable-the-image-registry-default-route">Enable the Image Registry default route</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">oc patch configs.imageregistry.operator.openshift.io/cluster --type merge -p <span style="color:#e6db74">&#39;{&#34;spec&#34;:{&#34;defaultRoute&#34;:true}}&#39;</span>
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configure OKD OpenShift 4 ingress]]></title>
            <link href="https://devopstales.github.io/home/openshift4-ingress/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift4-ceph-rbd-csi/?utm_source=atom_feed" rel="related" type="text/html" title="Configure OKD OpenShift 4 Ceph Persisten Storage" />
                <link href="https://devopstales.github.io/home/openshift4-auth/?utm_source=atom_feed" rel="related" type="text/html" title="Configure OKD OpenShift 4 authentication" />
                <link href="https://devopstales.github.io/home/openshift4-install/?utm_source=atom_feed" rel="related" type="text/html" title="How To Install OKD OpenShift 4 on premise" />
                <link href="https://devopstales.github.io/home/openshift4-logging/?utm_source=atom_feed" rel="related" type="text/html" title="Install Cluster Logging Operator on OpenShift 4" />
                <link href="https://devopstales.github.io/home/openshift_4/?utm_source=atom_feed" rel="related" type="text/html" title="OpenShift 4.2 with Red Hat CodeReady Containers" />
            
                <id>https://devopstales.github.io/home/openshift4-ingress/</id>
            
            
            <published>2021-12-13T00:00:00+00:00</published>
            <updated>2021-12-13T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this Post I will show you how you can create multiple ingress route on an OpenShift 4 on premise.</p>
<H3>Parst of the Openshift 4 series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/openshift4-install/">Install Opeshift 4</a></li>
     <li>Part1b: <a href="../../kubernetes/openshift4-calico/">Install Opeshift 4 with calico</a></li>
     <li>Part2: <a href="../../kubernetes/openshift4-ingress/">Configure OKD OpenShift 4 ingress</a></li>
     <li>Part3: <a href="../../kubernetes/openshift4-auth/">Configure OKD OpenShift 4 authentication</a></li>
     <li>Part4: <a href="../../kubernetes/openshift4-ceph-rbd-csi/">Configure OKD OpenShift 4 Ceph Persisten Storage</a></li>
     <li>Part5a: <a href="../../kubernetes/openshift4-logging/">Install Cluster Logging Operator on OpenShift 4</a></li>
     <li>Part5b: <a href="../../kubernetes/openshift-log4shell/">Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)</a></li>
</ul>


<h3 id="what-is-ingress-operator">What is Ingress Operator</h3>
<p>Ingress Operator is an OpenShift component which enables external access to cluster services by configuring Ingress Controllers, which route traffic as specified by OpenShift Route and Kubernetes Ingress resources.</p>
<p>To provide this functionality, Ingress Operator deploys and manages an OpenShift router — a HAProxy-based Kubernetes ingress controller.</p>
<h3 id="add-default-certificate-for-ingress-operator">Add default certificate for Ingress Operator</h3>
<p>Create the secret containing the certificate:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat tls.crt | base64
LS0tLS1CRUdJTiBDRVJUSUZ...
cat tls.key | base64
LS0tLS1CRUdJTiBQUklWQVR...
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano cert.yaml</span>
---
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Secret</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">default-tls-cert</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">openshift-ingress</span>
<span style="color:#f92672">data</span>:
  <span style="color:#f92672">tls.crt</span>: <span style="color:#ae81ff">LS0tLS1CRUdJTiBDRVJUSUZ...</span>
  <span style="color:#f92672">tls.key</span>: <span style="color:#ae81ff">LS0tLS1CRUdJTiBQUklWQVR...</span>
<span style="color:#f92672">type</span>: <span style="color:#ae81ff">Opaque</span>
</code></pre></div><h3 id="create-multiple-ingress-route">Create multiple ingress route</h3>
<p>For the example I will create a private and a public rout for the cluster</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano default.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">operator.openshift.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">IngressController</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">default</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">openshift-ingress-operator</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">defaultCertificate</span>:
    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">default-tls-cert</span>
  <span style="color:#f92672">nodePlacement</span>:
    <span style="color:#f92672">nodeSelector</span>:
      <span style="color:#f92672">matchLabels</span>:
        <span style="color:#f92672">node-role.kubernetes.io/ingress-internal</span>: <span style="color:#e6db74">&#39;&#39;</span>
  <span style="color:#f92672">replicas</span>: <span style="color:#ae81ff">2</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano public.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">operator.openshift.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">IngressController</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">public</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">openshift-ingress-operator</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">defaultCertificate</span>:
    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">default-tls-cert</span>
  <span style="color:#f92672">domain</span>: <span style="color:#ae81ff">external.okd.mydomain.intra</span>
  <span style="color:#f92672">nodePlacement</span>:
    <span style="color:#f92672">nodeSelector</span>:
      <span style="color:#f92672">matchLabels</span>:
        <span style="color:#f92672">node-role.kubernetes.io/ingress-public</span>: <span style="color:#e6db74">&#39;&#39;</span>
  <span style="color:#f92672">replicas</span>: <span style="color:#ae81ff">2</span>
  <span style="color:#f92672">routeSelector</span>:
    <span style="color:#f92672">matchLabels</span>:
      <span style="color:#f92672">router</span>: <span style="color:#ae81ff">public</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">oc label nodes okd4-worker-1 node-role.kubernetes.io/ingress-internal<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;&#34;</span>
oc label nodes okd4-worker-2 node-role.kubernetes.io/ingress-internal<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;&#34;</span>
oc label nodes okd4-worker-3 node-role.kubernetes.io/ingress-public<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;&#34;</span>
oc label nodes okd4-worker-4 node-role.kubernetes.io/ingress-public<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;&#34;</span>

oc apply -f cert.yaml
oc apply -f ingress/
oc patch ingresscontroller.operator default --type<span style="color:#f92672">=</span>merge -p <span style="color:#e6db74">&#39;{&#34;spec&#34;:{&#34;defaultCertificate&#34;: {&#34;name&#34;: &#34;mydióomain-intra-certs&#34;}}}&#39;</span> -n openshift-ingress-operator
</code></pre></div><h3 id="enable-http2">enable HTTP/2</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">oc annotate ingresses.config/cluster ingress.operator.openshift.io/default-enable-http2<span style="color:#f92672">=</span>true
</code></pre></div><h3 id="add-okd-311-type-conole-url">Add OKD 3.11 type conole url</h3>
<p>I used OKD 3.11 and ther the conosle usrl wa master.okd.mydomain.intra so I desided to create the same route for okd4:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano master-okf-mydomain-intra.yaml</span>
---
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Route</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">route.openshift.io/v1</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">master-okd-mydomain-intra</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">openshift-console</span>
  <span style="color:#f92672">labels</span>:
    <span style="color:#f92672">app</span>: <span style="color:#ae81ff">console</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">host</span>: <span style="color:#ae81ff">master.okd.mydomain.intra</span>
  <span style="color:#f92672">to</span>:
    <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Service</span>
    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">console</span>
    <span style="color:#f92672">weight</span>: <span style="color:#ae81ff">100</span>
  <span style="color:#f92672">port</span>:
    <span style="color:#f92672">targetPort</span>: <span style="color:#ae81ff">https</span>
  <span style="color:#f92672">tls</span>:
    <span style="color:#f92672">termination</span>: <span style="color:#ae81ff">reencrypt</span>
    <span style="color:#f92672">insecureEdgeTerminationPolicy</span>: <span style="color:#ae81ff">Redirect</span>
  <span style="color:#f92672">wildcardPolicy</span>: <span style="color:#ae81ff">None</span>
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How To Install OKD OpenShift 4 on premise]]></title>
            <link href="https://devopstales.github.io/home/openshift4-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift4-ceph-rbd-csi/?utm_source=atom_feed" rel="related" type="text/html" title="Configure OKD OpenShift 4 Ceph Persisten Storage" />
                <link href="https://devopstales.github.io/home/openshift4-auth/?utm_source=atom_feed" rel="related" type="text/html" title="Configure OKD OpenShift 4 authentication" />
                <link href="https://devopstales.github.io/home/openshift4-ingress/?utm_source=atom_feed" rel="related" type="text/html" title="Configure OKD OpenShift 4 ingress" />
                <link href="https://devopstales.github.io/home/openshift4-logging/?utm_source=atom_feed" rel="related" type="text/html" title="Install Cluster Logging Operator on OpenShift 4" />
                <link href="https://devopstales.github.io/home/openshift_4/?utm_source=atom_feed" rel="related" type="text/html" title="OpenShift 4.2 with Red Hat CodeReady Containers" />
            
                <id>https://devopstales.github.io/home/openshift4-install/</id>
            
            
            <published>2021-12-13T00:00:00+00:00</published>
            <updated>2021-12-13T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this Post I will show you how you can install the an OpenShift 4 on premise.</p>
<H3>Parst of the Openshift 4 series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/openshift4-install/">Install Opeshift 4</a></li>
     <li>Part1b: <a href="../../kubernetes/openshift4-calico/">Install Opeshift 4 with calico</a></li>
     <li>Part2: <a href="../../kubernetes/openshift4-ingress/">Configure OKD OpenShift 4 ingress</a></li>
     <li>Part3: <a href="../../kubernetes/openshift4-auth/">Configure OKD OpenShift 4 authentication</a></li>
     <li>Part4: <a href="../../kubernetes/openshift4-ceph-rbd-csi/">Configure OKD OpenShift 4 Ceph Persisten Storage</a></li>
     <li>Part5a: <a href="../../kubernetes/openshift4-logging/">Install Cluster Logging Operator on OpenShift 4</a></li>
     <li>Part5b: <a href="../../kubernetes/openshift-log4shell/">Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)</a></li>
</ul>


<h3 id="infrastructure">Infrastructure</h3>
<table>
<thead>
<tr>
<th style="text-align:center">Host</th>
<th style="text-align:center">ROLES</th>
<th style="text-align:center">OS</th>
<th style="text-align:center">IP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">pfsense</td>
<td style="text-align:center">Load Balancer, dhcp, dns</td>
<td style="text-align:center">pfsense</td>
<td style="text-align:center">192.168.1.1</td>
</tr>
<tr>
<td style="text-align:center">okd4-services</td>
<td style="text-align:center">pxeboot</td>
<td style="text-align:center">CentOS 7</td>
<td style="text-align:center">192.168.1.200</td>
</tr>
<tr>
<td style="text-align:center">okd4-bootstrap</td>
<td style="text-align:center">bootstrap</td>
<td style="text-align:center">Fedora Core OS</td>
<td style="text-align:center">192.168.1.210</td>
</tr>
<tr>
<td style="text-align:center">okd4-mastr-1</td>
<td style="text-align:center">master</td>
<td style="text-align:center">Fedora Core OS</td>
<td style="text-align:center">192.168.1.201</td>
</tr>
<tr>
<td style="text-align:center">okd4-mastr-2</td>
<td style="text-align:center">master</td>
<td style="text-align:center">Fedora Core OS</td>
<td style="text-align:center">192.168.1.202</td>
</tr>
<tr>
<td style="text-align:center">okd4-mastr-3</td>
<td style="text-align:center">master</td>
<td style="text-align:center">Fedora Core OS</td>
<td style="text-align:center">192.168.1.203</td>
</tr>
<tr>
<td style="text-align:center">okd4-worker-1</td>
<td style="text-align:center">worker</td>
<td style="text-align:center">Fedora Core OS</td>
<td style="text-align:center">192.168.1.204</td>
</tr>
<tr>
<td style="text-align:center">okd4-worker-2</td>
<td style="text-align:center">worker</td>
<td style="text-align:center">Fedora Core OS</td>
<td style="text-align:center">192.168.1.205</td>
</tr>
<tr>
<td style="text-align:center">okd4-worker-4</td>
<td style="text-align:center">worker</td>
<td style="text-align:center">Fedora Core OS</td>
<td style="text-align:center">192.168.1.206</td>
</tr>
<tr>
<td style="text-align:center">okd4-worker-5</td>
<td style="text-align:center">worker</td>
<td style="text-align:center">Fedora Core OS</td>
<td style="text-align:center">192.168.1.207</td>
</tr>
</tbody>
</table>
<h3 id="dns-config">DNS Config</h3>
<pre tabindex="0"><code>; OpenShift Container Platform Cluster - A records
pfsense.okd.mydomain.intra.          IN      A      192.168.1.1
okd4-bootstrap.okd.mydomain.intra.   IN      A      192.168.1.210

okd4-mastr-1.okd.mydomain.intra.        IN      A      192.168.1.201
okd4-mastr-2.okd.mydomain.intra.        IN      A      192.168.1.202
okd4-mastr-3.okd.mydomain.intra.        IN      A      192.168.1.203
okd4-worker-1.okd.mydomain.intra.        IN      A      192.168.1.204
okd4-worker-2.okd.mydomain.intra.        IN      A      192.168.1.205
okd4-worker-3.okd.mydomain.intra.        IN      A      192.168.1.206
okd4-worker-4.okd.mydomain.intra.        IN      A      192.168.1.207


; OpenShift internal cluster IPs - A records
api.okd.mydomain.intra.            IN      A      192.168.1.1
api-int.okd.mydomain.intra.        IN      A      192.168.1.1
etcd-0.okd.mydomain.intra.         IN      A     192.168.1.201
etcd-1.okd.mydomain.intra.         IN      A     192.168.1.202
etcd-2.okd.mydomain.intra.         IN      A     192.168.1.203

okd.mydomain.intra.                IN      A      192.168.1.1
*.okd.mydomain.intra.              IN      A      192.168.1.1

; OpenShift internal cluster IPs - SRV records
_etcd-server-ssl._tcp.okd.mydomain.intra.    86400     IN    SRV     0    10    2380    etcd-0.okd.mydomain.intra.
_etcd-server-ssl._tcp.okd.mydomain.intra.    86400     IN    SRV     0    10    2380    etcd-1.okd.mydomain.intra.
_etcd-server-ssl._tcp.okd.mydomain.intra.    86400     IN    SRV     0    10    2380    etcd-2.okd.mydomain.intra.
</code></pre><h3 id="dhcp-config">DHCP Config:</h3>
<pre tabindex="0"><code>32:89:07:57:27:00  192.168.1.200 	okd4-services
32:89:07:57:27:10  192.168.1.210 	okd4-bootstrap
32:89:07:57:27:01  192.168.1.201 	okd4-mastr-1
32:89:07:57:27:02  192.168.1.202 	okd4-mastr-2
32:89:07:57:27:03  192.168.1.203 	okd4-mastr-3
32:89:07:57:27:04  192.168.1.204 	okd4-worker-1
32:89:07:57:27:05  192.168.1.205 	okd4-worker-2
32:89:07:57:27:06  192.168.1.206 	okd4-worker-3
32:89:07:57:27:07  192.168.1.207 	okd4-worker-4
</code></pre><pre tabindex="0"><code>Next Server: 192.168.1.200
Default BIOS file name: pxelinux.0
</code></pre><h2 id="haproxy-config">HAPROXY Config:</h2>
<pre tabindex="0"><code>192.168.201.1 6443  --&gt;  192.168.1.210   6443
192.168.201.1 6443  --&gt;  192.168.1.201  6443
192.168.201.1 6443  --&gt;  192.168.1.202  6443
192.168.201.1 6443  --&gt;  192.168.1.202  6443
192.168.201.1 22623 --&gt;  192.168.1.210   22623
192.168.201.1 22623 --&gt;  192.168.1.201  22623
192.168.201.1 22623 --&gt;  192.168.1.202  22623
192.168.201.1 22623 --&gt;  192.168.1.202  22623
192.168.201.1 80    --&gt;  192.168.1.204  80
192.168.201.1 80    --&gt;  192.168.1.205  80
192.168.201.1 443   --&gt;  192.168.1.204  443
192.168.201.1 443   --&gt;  192.168.1.205  443
&lt;publicip&gt; 80    --&gt;  192.168.1.206  80
&lt;publicip&gt; 80    --&gt;  192.168.1.207  80
&lt;publicip&gt; 443   --&gt;  192.168.1.206  443
&lt;publicip&gt; 443   --&gt;  192.168.1.207  443
</code></pre><h3 id="install-and-configure-pxeboot">Install and configure pxeboot</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ssh okd4-services

yum install epel-release -y
yum install httpd nano jq -y
dnf install -y tftp-server syslinux-tftpboot
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">mkdir -p /var/lib/tftpboot
cp -v /tftpboot/pxelinux.0 /var/lib/tftpboot/
cp -v /tftpboot/menu.c32 /var/lib/tftpboot/
cp -v /tftpboot/mboot.c32 /var/lib/tftpboot/
cp -v /tftpboot/chain.c32 /var/lib/tftpboot/
cp -v /tftpboot/ldlinux.c32 /var/lib/tftpboot/
cp -v /tftpboot/libutil.c32 /var/lib/tftpboot/

mkdir -p /var/lib/tftpboot/fcsos33
cd /var/lib/tftpboot/fcsos33
RHCOS_BASEURL<span style="color:#f92672">=</span>https://builds.coreos.fedoraproject.org/prod/streams/stable/builds/
wget <span style="color:#e6db74">${</span>RHCOS_BASEURL<span style="color:#e6db74">}</span>33.20210117.3.2/x86_64/fedora-coreos-33.20210117.3.2-live-kernel-x86_64
wget <span style="color:#e6db74">${</span>RHCOS_BASEURL<span style="color:#e6db74">}</span>33.20210117.3.2/x86_64/fedora-coreos-33.20210117.3.2-live-initramfs.x86_64.img
wget <span style="color:#e6db74">${</span>RHCOS_BASEURL<span style="color:#e6db74">}</span>33.20210117.3.2/x86_64/fedora-coreos-33.20210117.3.2-live-rootfs.x86_64.img
cd ~

mkdir /var/lib/tftpboot/pxelinux.cfg
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat &gt; /var/lib/tftpboot/pxelinux.cfg/default <span style="color:#e6db74">&lt;&lt; EOF
</span><span style="color:#e6db74">default menu.c32
</span><span style="color:#e6db74">prompt 0
</span><span style="color:#e6db74">timeout 30
</span><span style="color:#e6db74">menu title PXE Menu
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">label 1
</span><span style="color:#e6db74">menu label ^1) Boot from local drive
</span><span style="color:#e6db74">localboot 0x00
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">label 2
</span><span style="color:#e6db74">menu label ^2) Install OKD Bootstrap
</span><span style="color:#e6db74">KERNEL /fcsos33/fedora-coreos-33.20210117.3.2-live-kernel-x86_64
</span><span style="color:#e6db74">APPEND initrd=/fcsos33/fedora-coreos-33.20210117.3.2-live-initramfs.x86_64.img,/fcsos33/fedora-coreos-33.20210117.3.2-live-rootfs.x86_64.img coreos.inst.install_dev=/dev/vda coreos.inst.image_url=http://192.168.201.4/fcos.raw.xz coreos.inst.ignition_url=http://192.168.201.4/bootstrap.ign
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">label 3
</span><span style="color:#e6db74">menu label ^3) Install OKD Master
</span><span style="color:#e6db74">KERNEL /fcsos33/fedora-coreos-33.20210117.3.2-live-kernel-x86_64
</span><span style="color:#e6db74">APPEND initrd=/fcsos33/fedora-coreos-33.20210117.3.2-live-initramfs.x86_64.img,/fcsos33/fedora-coreos-33.20210117.3.2-live-rootfs.x86_64.img coreos.inst.install_dev=/dev/vda coreos.inst.image_url=http://192.168.201.4/fcos.raw.xz coreos.inst.ignition_url=http://192.168.201.4/master.ign
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">label 4
</span><span style="color:#e6db74">menu label ^4) Install OKD Worker
</span><span style="color:#e6db74">KERNEL /fcsos33/fedora-coreos-33.20210117.3.2-live-kernel-x86_64
</span><span style="color:#e6db74">APPEND initrd=/fcsos33/fedora-coreos-33.20210117.3.2-live-initramfs.x86_64.img,/fcsos33/fedora-coreos-33.20210117.3.2-live-rootfs.x86_64.img coreos.inst.install_dev=/dev/vda coreos.inst.image_url=http://192.168.201.4/fcos.raw.xz coreos.inst.ignition_url=http://192.168.201.4/worker.ign
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><p>Then run: <code>systemctl enable --now tftp.service</code></p>
<h3 id="create-okd-config">Create okd config</h3>
<blockquote>
<p>find the iso:
<a href="https://getfedora.org/en/coreos/download?tab=metal_virtualized&amp;stream=stable">https://getfedora.org/en/coreos/download?tab=metal_virtualized&amp;stream=stable</a>
4K vs non 4K</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">wget fedora-coreos-33.20210117.3.2-live.x86_64.iso
wget https://builds.coreos.fedoraproject.org/prod/streams/stable/builds/33.20210117.3.2/x86_64/fedora-coreos-33.20210117.3.2-metal.x86_64.raw.xz
wget https://builds.coreos.fedoraproject.org/prod/streams/stable/builds/33.20210117.3.2/x86_64/fedora-coreos-33.20210117.3.2-metal.x86_64.raw.xz.sig
cp fedora-coreos-33.20210117.3.2-metal.x86_64.raw.xz /var/www/html/fcos.raw.xz
cp fedora-coreos-33.20210117.3.2-metal.x86_64.raw.xz.sig /var/www/html/fcos.raw.xz.sig
</code></pre></div><pre tabindex="0"><code># find installer
# https://github.com/openshift/okd/releases

wget https://github.com/openshift/okd/releases/download/4.6.0-0.okd-2021-02-14-205305/openshift-client-linux-4.6.0-0.okd-2021-02-14-205305.tar.gz
wget https://github.com/openshift/okd/releases/download/4.6.0-0.okd-2021-02-14-205305/openshift-install-linux-4.6.0-0.okd-2021-02-14-205305.tar.gz

tar -xzf openshift-client-linux-4.6.0-0.okd-2021-02-14-205305.tar.gz
tar -xzf openshift-install-linux-4.6.0-0.okd-2021-02-14-205305.tar.gz

sudo mv kubectl oc openshift-install /usr/local/bin/
oc version
openshift-install version

mkdir install_dir
</code></pre><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat &gt; install_dir/install-config.yaml <span style="color:#e6db74">&lt;&lt; EOF
</span><span style="color:#e6db74">apiVersion: v1
</span><span style="color:#e6db74">baseDomain: mydomain.intra
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: okd
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">compute:
</span><span style="color:#e6db74">- hyperthreading: Enabled
</span><span style="color:#e6db74">  name: worker
</span><span style="color:#e6db74">  replicas: 0
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">controlPlane:
</span><span style="color:#e6db74">  hyperthreading: Enabled
</span><span style="color:#e6db74">  name: master
</span><span style="color:#e6db74">  replicas: 3
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">networking:
</span><span style="color:#e6db74">  clusterNetwork:
</span><span style="color:#e6db74">  - cidr: 10.128.0.0/14
</span><span style="color:#e6db74">    hostPrefix: 23
</span><span style="color:#e6db74">  networkType: OpenShiftSDN
</span><span style="color:#e6db74">  serviceNetwork:
</span><span style="color:#e6db74">  - 172.30.0.0/16
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">platform:
</span><span style="color:#e6db74">  none: {}
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">fips: false
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">pullSecret: &#39;{&#34;auths&#34;:{&#34;fake&#34;:{&#34;auth&#34;: &#34;bar&#34;}}}&#39;
</span><span style="color:#e6db74">sshKey: &#39;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDK7lDozs9WLJD14H+nz...&#39; 
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">openshift-install create manifests --dir<span style="color:#f92672">=</span>install_dir/

sed -i <span style="color:#e6db74">&#39;s/mastersSchedulable: true/mastersSchedulable: False/&#39;</span> install_dir/manifests/cluster-scheduler-02-config.yml

openshift-install create ignition-configs --dir<span style="color:#f92672">=</span>install_dir/

sudo cp -R install_dir/*.ign /var/www/html/
sudo cp -R install_dir/metadata.json /var/www/html/
sudo chown -R apache: /var/www/html/
sudo chmod -R <span style="color:#ae81ff">755</span> /var/www/html/
</code></pre></div><blockquote>
<p>The config contains certificates that is walid for 24 hours.</p>
</blockquote>
<h3 id="starting-the-vms">Starting the VMs</h3>
<p>It&rsquo;s time to start the VMs. Select the okd4-bootstrap VM and navigate to Console. Start the VM. Then one by one the masters and the workers too.</p>
<h3 id="bootstrap-okd-cluster">Bootstrap OKD Cluster</h3>
<p>You can monitor the installation progress by running the following command.</p>
<pre tabindex="0"><code>openshift-install --dir=install_dir/ wait-for bootstrap-complete --log-level=info
</code></pre><blockquote>
<p>The certificates in the cluster is not authomaticle approved so I use the abow <code>tmux</code> command to approve</p>
</blockquote>
<pre tabindex="0"><code>tmux
export KUBECONFIG=~/install_dir/auth/kubeconfig
while true; do echo `oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{&quot;\n&quot;}}{{end}}{{end}}' | xargs -r oc adm certificate approve`; sleep 60; done
</code></pre><p>Once the bootstrap process completes, you should see the following messages.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">INFO It is now safe to remove the bootstrap resources
</code></pre></div><p>Then stop the bootstrap node.</p>
<pre tabindex="0"><code># debug command to check the health of the cluster.
watch oc get csr
watch oc get node

oc get clusteroperator
oc get clusterversion

watch &quot;oc get clusteroperator&quot;
watch &quot;oc get po -A | grep -v Running | grep -v Completed&quot;


curl -X GET https://api.okd.mydomain.intra:6443/healthz -k
</code></pre><p>Wait for the console to be available. Once it is available, we can point a browser to <a href="https://console-openshift-console.okd.mydomain.intra">https://console-openshift-console.okd.mydomain.intra</a></p>
<p>You will get an SSL error because the certificate is not valid for this domain. That&rsquo;s normal. Just bypass the SSL error.</p>
<p>Login with user &ldquo;kubeadmin&rdquo;.You can find the kubeadmin password in a file generated during the installation.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat install_dir/auth/kubeadmin-password
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Cluster Logging Operator on OpenShift 4]]></title>
            <link href="https://devopstales.github.io/home/openshift4-logging/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift_4/?utm_source=atom_feed" rel="related" type="text/html" title="OpenShift 4.2 with Red Hat CodeReady Containers" />
                <link href="https://devopstales.github.io/kubernetes/openshift4-logging/?utm_source=atom_feed" rel="related" type="text/html" title="Install Cluster Logging Operator on OpenShift 4" />
                <link href="https://devopstales.github.io/home/how-to-fix-ansible-service-broker-in-openshift-3-11/?utm_source=atom_feed" rel="related" type="text/html" title="How to fix Ansible Service Broker in OpenShift 3.11" />
                <link href="https://devopstales.github.io/home/how-to-fix-cluster-monitoring-operator-in-openshift-3-11/?utm_source=atom_feed" rel="related" type="text/html" title="How to fix cluster-monitoring-operator in OpenShift 3.11" />
                <link href="https://devopstales.github.io/home/how-to-fix-registry-console-ui-in-openshift-3-11/?utm_source=atom_feed" rel="related" type="text/html" title="How to fix registry console UI in OpenShift 3.11" />
            
                <id>https://devopstales.github.io/home/openshift4-logging/</id>
            
            
            <published>2021-12-12T00:00:00+00:00</published>
            <updated>2021-12-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this Post I will show you how you can install the Cluster Logging Operator on an OpenShift 4.</p>
<H3>Parst of the Openshift 4 series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/openshift4-install/">Install Opeshift 4</a></li>
     <li>Part1b: <a href="../../kubernetes/openshift4-calico/">Install Opeshift 4 with calico</a></li>
     <li>Part2: <a href="../../kubernetes/openshift4-ingress/">Configure OKD OpenShift 4 ingress</a></li>
     <li>Part3: <a href="../../kubernetes/openshift4-auth/">Configure OKD OpenShift 4 authentication</a></li>
     <li>Part4: <a href="../../kubernetes/openshift4-ceph-rbd-csi/">Configure OKD OpenShift 4 Ceph Persisten Storage</a></li>
     <li>Part5a: <a href="../../kubernetes/openshift4-logging/">Install Cluster Logging Operator on OpenShift 4</a></li>
     <li>Part5b: <a href="../../kubernetes/openshift-log4shell/">Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)</a></li>
</ul>


<h3 id="redhat-pull-secret-and-enable-red-hat-operators">RedHat pull secret and enable Red Hat Operators</h3>
<p>To install the <a href="https://console.redhat.com/openshift/install/pull-secret">RedHat pull secret</a>. First download it from the url.</p>
<p>If you did&rsquo;t addid at the install create the secret kike this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano pull-secret.json</span>
{
  <span style="color:#f92672">&#34;auths&#34;: </span>{
    <span style="color:#f92672">&#34;registry.redhat.io&#34;: </span>{
      <span style="color:#f92672">&#34;username&#34;: </span><span style="color:#e6db74">&#34;redhat-user&#34;</span>,
      <span style="color:#f92672">&#34;password&#34;: </span><span style="color:#e6db74">&#34;redhat-user-pass&#34;</span>,
      <span style="color:#f92672">&#34;email&#34;: </span><span style="color:#e6db74">&#34;redhat-user-email&#34;</span>,
      <span style="color:#f92672">&#34;auth&#34;: </span><span style="color:#e6db74">&#34;redhat-user-auth-key&#34;</span>
    }
  }
}

<span style="color:#ae81ff">oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=./pull-secret.json</span>
</code></pre></div><p>Modify the following objects to enable Red Hat Operators.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">oc edit configs.samples.operator.openshift.io/cluster -o yaml</span>

<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">samples.operator.openshift.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Config</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#ae81ff">...</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">architectures</span>:
  - <span style="color:#ae81ff">x86_64</span>
  <span style="color:#f92672">managementState</span>: <span style="color:#ae81ff">Managed</span>
<span style="color:#f92672">status</span>:
  <span style="color:#f92672">architectures</span>:
  <span style="color:#ae81ff">...</span>
  <span style="color:#f92672">managementState</span>: <span style="color:#ae81ff">Managed</span>
  <span style="color:#f92672">version</span>: <span style="color:#ae81ff">4.9.0-0.</span><span style="color:#ae81ff">okd-2021-11-28-035710</span>
</code></pre></div><p>Second, run the following command to edit the Operator Hub configuration file and set all source to <code>disabled: false</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">oc edit operatorhub cluster -o yaml</span>
...
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">disableAllDefaultSources</span>: <span style="color:#66d9ef">true</span>
  <span style="color:#f92672">sources</span>:
  - <span style="color:#f92672">disabled</span>: <span style="color:#66d9ef">false</span>
    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">community-operators</span>
  - <span style="color:#f92672">disabled</span>: <span style="color:#66d9ef">false</span>
    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">certified-operators</span>
  - <span style="color:#f92672">disabled</span>: <span style="color:#66d9ef">false</span>
    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">redhat-operators</span>
  - <span style="color:#f92672">disabled</span>: <span style="color:#66d9ef">false</span>
    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">redhat-marketplace</span>
<span style="color:#f92672">status</span>:
  <span style="color:#f92672">sources</span>:
  - <span style="color:#f92672">disabled</span>: <span style="color:#66d9ef">false</span>
    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">community-operators</span>
    <span style="color:#f92672">status</span>: <span style="color:#ae81ff">Success</span>
  - <span style="color:#f92672">disabled</span>: <span style="color:#66d9ef">false</span>
    <span style="color:#f92672">message</span>: <span style="color:#ae81ff">CatalogSource.operators.coreos.com &#34;redhat-marketplace&#34; not found</span>
    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">redhat-marketplace</span>
    <span style="color:#f92672">status</span>: <span style="color:#ae81ff">Error</span>
  - <span style="color:#f92672">disabled</span>: <span style="color:#66d9ef">false</span>
    <span style="color:#f92672">message</span>: <span style="color:#ae81ff">CatalogSource.operators.coreos.com &#34;redhat-operators&#34; not found</span>
    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">redhat-operators</span>
    <span style="color:#f92672">status</span>: <span style="color:#ae81ff">Error</span>
  - <span style="color:#f92672">disabled</span>: <span style="color:#66d9ef">false</span>
    <span style="color:#f92672">message</span>: <span style="color:#ae81ff">CatalogSource.operators.coreos.com &#34;certified-operators&#34; not found</span>
    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">certified-operators</span>
    <span style="color:#f92672">status</span>: <span style="color:#ae81ff">Error</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">oc get catalogsource --all-namespaces
NAMESPACE               NAME                  DISPLAY               TYPE      PUBLISHER   AGE
openshift-marketplace   certified-operators   Certified Operators   grpc      Red Hat     37m
openshift-marketplace   community-operators   Community Operators   grpc      Red Hat     289d
openshift-marketplace   redhat-marketplace    Red Hat Marketplace   grpc      Red Hat     37m
openshift-marketplace   redhat-operators      Red Hat Operators     grpc      Red Hat     37m
</code></pre></div><h3 id="install-ellasticsearch-operator">Install Ellasticsearch Operator</h3>
<p>Create Operators namespaces:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat &lt;&lt; EOF &gt;penshift_operators_redhatnamespace.yaml</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Namespace</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">openshift-operators-redhat </span>
  <span style="color:#f92672">annotations</span>:
    <span style="color:#f92672">openshift.io/node-selector</span>: <span style="color:#e6db74">&#34;&#34;</span>
  <span style="color:#f92672">labels</span>:
    <span style="color:#f92672">openshift.io/cluster-monitoring</span>: <span style="color:#e6db74">&#34;true&#34;</span> 
<span style="color:#ae81ff">EOF</span>

<span style="color:#ae81ff">oc apply -f penshift_operators_redhatnamespace.yaml</span>
<span style="color:#ae81ff">oc project openshift-logging</span>
</code></pre></div><p>Create OperatorGroup object</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat &lt;&lt; EOF &gt;openshift-operators-redhat-operatorgroup.yaml</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">operators.coreos.com/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">OperatorGroup</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">openshift-operators-redhat</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">openshift-operators-redhat </span>
<span style="color:#f92672">spec</span>: {}
<span style="color:#ae81ff">EOF</span>

<span style="color:#ae81ff">oc apply -f openshift-operators-redhat-operatorgroup.yaml</span>
</code></pre></div><p>Subscribe a Namespace to the Cluster Logging Operator:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat &lt;&lt; EOF &gt;cluster-logging-sub.yaml</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">operators.coreos.com/v1alpha1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Subscription</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">elasticsearch-operator</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">openshift-operators-redhat</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">channel</span>: <span style="color:#ae81ff">stable</span>
  <span style="color:#f92672">installPlanApproval</span>: <span style="color:#ae81ff">Automatic</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">elasticsearch-operator</span>
  <span style="color:#f92672">source</span>: <span style="color:#ae81ff">redhat-operators</span>
  <span style="color:#f92672">sourceNamespace</span>: <span style="color:#ae81ff">openshift-marketplace</span>
<span style="color:#ae81ff">EOF</span>

<span style="color:#ae81ff">oc apply -f cluster-logging-sub.yaml</span>

<span style="color:#ae81ff">oc get csv</span>
<span style="color:#ae81ff">NAME                               DISPLAY                            VERSION    REPLACES                           PHASE</span>
<span style="color:#ae81ff">elasticsearch-operator.5.3.1-12    OpenShift Elasticsearch Operator   5.3.1-12                                      Succeeded</span>
</code></pre></div><h3 id="install-logging-operator">Install Logging Operator</h3>
<p>Create Operators namespaces:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat &lt;&lt; EOF &gt;ocp_cluster_logging_namespace.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Namespace</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">openshift-logging</span>
  <span style="color:#f92672">annotations</span>:
    <span style="color:#f92672">openshift.io/node-selector</span>: <span style="color:#e6db74">&#34;&#34;</span>
  <span style="color:#f92672">labels</span>:
    <span style="color:#f92672">openshift.io/cluster-logging</span>: <span style="color:#e6db74">&#34;true&#34;</span>
    <span style="color:#f92672">openshift.io/cluster-monitoring</span>: <span style="color:#e6db74">&#34;true&#34;</span>
<span style="color:#ae81ff">EOF</span>

<span style="color:#ae81ff">oc apply -f ocp_cluster_logging_namespace.yaml</span>
<span style="color:#ae81ff">oc project openshift-logging</span>
</code></pre></div><p>Create OperatorGroup object</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat &lt;&lt; EOF &gt;cluster-logging-operatorgroup.yaml</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">operators.coreos.com/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">OperatorGroup</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cluster-logging</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">openshift-logging </span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">targetNamespaces</span>:
  - <span style="color:#ae81ff">openshift-logging</span>
<span style="color:#ae81ff">EOF</span>

<span style="color:#ae81ff">oc apply -f cluster-logging-operatorgroup.yaml</span>
</code></pre></div><p>Subscribe a Namespace to the Cluster Logging Operator:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat &lt;&lt; EOF &gt;cluster-logging-sub.yaml</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">operators.coreos.com/v1alpha1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Subscription</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cluster-logging</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">openshift-logging</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">channel</span>: <span style="color:#e6db74">&#34;stable&#34;</span> <span style="color:#75715e"># Set Channel</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cluster-logging</span>
  <span style="color:#f92672">source</span>: <span style="color:#ae81ff">redhat-operators</span>
  <span style="color:#f92672">sourceNamespace</span>: <span style="color:#ae81ff">openshift-marketplace</span>
<span style="color:#ae81ff">EOF</span>

<span style="color:#ae81ff">oc apply -f cluster-logging-sub.yaml</span>

<span style="color:#ae81ff">oc get csv</span>
<span style="color:#ae81ff">NAME                               DISPLAY                            VERSION    REPLACES                           PHASE</span>
<span style="color:#ae81ff">cluster-logging.5.3.1-12           Red Hat OpenShift Logging          5.3.1-12                                      Succeeded</span>
<span style="color:#ae81ff">elasticsearch-operator.5.3.1-12    OpenShift Elasticsearch Operator   5.3.1-12                                      Succeeded</span>
</code></pre></div><h3 id="deploy-cluster-logging-stack">Deploy Cluster logging stack</h3>
<p>Create a Cluster Logging instance:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat &lt;&lt; EOF &gt;cluster-logging-instance.yaml</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#e6db74">&#34;logging.openshift.io/v1&#34;</span>
<span style="color:#f92672">kind</span>: <span style="color:#e6db74">&#34;ClusterLogging&#34;</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#e6db74">&#34;instance&#34;</span> 
  <span style="color:#f92672">namespace</span>: <span style="color:#e6db74">&#34;openshift-logging&#34;</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">managementState</span>: <span style="color:#e6db74">&#34;Managed&#34;</span>  
  <span style="color:#f92672">logStore</span>:
    <span style="color:#f92672">type</span>: <span style="color:#e6db74">&#34;elasticsearch&#34;</span>  
    <span style="color:#f92672">retentionPolicy</span>: 
      <span style="color:#f92672">application</span>:
        <span style="color:#f92672">maxAge</span>: <span style="color:#ae81ff">1d</span>
      <span style="color:#f92672">infra</span>:
        <span style="color:#f92672">maxAge</span>: <span style="color:#ae81ff">7d</span>
      <span style="color:#f92672">audit</span>:
        <span style="color:#f92672">maxAge</span>: <span style="color:#ae81ff">7d</span>
    <span style="color:#f92672">elasticsearch</span>:
      <span style="color:#f92672">nodeCount</span>: <span style="color:#ae81ff">3</span> 
      <span style="color:#f92672">storage</span>:
        <span style="color:#f92672">storageClassName</span>: <span style="color:#e6db74">&#34;&lt;storage_class_name&gt;&#34;</span> 
        <span style="color:#f92672">size</span>: <span style="color:#ae81ff">200G</span>
      <span style="color:#f92672">resources</span>: 
          <span style="color:#f92672">limits</span>:
            <span style="color:#f92672">memory</span>: <span style="color:#e6db74">&#34;16Gi&#34;</span>
          <span style="color:#f92672">requests</span>:
            <span style="color:#f92672">memory</span>: <span style="color:#e6db74">&#34;16Gi&#34;</span>
      <span style="color:#f92672">proxy</span>: 
        <span style="color:#f92672">resources</span>:
          <span style="color:#f92672">limits</span>:
            <span style="color:#f92672">memory</span>: <span style="color:#ae81ff">256Mi</span>
          <span style="color:#f92672">requests</span>:
            <span style="color:#f92672">memory</span>: <span style="color:#ae81ff">256Mi</span>
      <span style="color:#f92672">redundancyPolicy</span>: <span style="color:#e6db74">&#34;SingleRedundancy&#34;</span>
  <span style="color:#f92672">visualization</span>:
    <span style="color:#f92672">type</span>: <span style="color:#e6db74">&#34;kibana&#34;</span>  
    <span style="color:#f92672">kibana</span>:
      <span style="color:#f92672">replicas</span>: <span style="color:#ae81ff">1</span>
  <span style="color:#f92672">curation</span>:
    <span style="color:#f92672">type</span>: <span style="color:#e6db74">&#34;curator&#34;</span>  
    <span style="color:#f92672">curator</span>:
      <span style="color:#f92672">schedule</span>: <span style="color:#e6db74">&#34;30 3 * * *&#34;</span>
  <span style="color:#f92672">collection</span>:
    <span style="color:#f92672">logs</span>:
      <span style="color:#f92672">type</span>: <span style="color:#e6db74">&#34;fluentd&#34;</span>  
      <span style="color:#f92672">fluentd</span>: {}
<span style="color:#ae81ff">EOF</span>

<span style="color:#ae81ff">oc apply -f cluster-logging-instance.yaml</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">oc get deployment</span>
<span style="color:#ae81ff">cluster-logging-operator       1/1     1            1           18h</span>
<span style="color:#ae81ff">elasticsearch-cd-x6kdekli-1    0/1     1            0           6m54s</span>
<span style="color:#ae81ff">elasticsearch-cdm-x6kdekli-1   1/1     1            1           18h</span>
<span style="color:#ae81ff">elasticsearch-cdm-x6kdekli-2   0/1     1            0           6m49s</span>
<span style="color:#ae81ff">elasticsearch-cdm-x6kdekli-3   0/1     1            0           6m44s</span>
</code></pre></div><h3 id="defining-kibana-index-patterns">Defining Kibana index patterns</h3>
<p>In the OpenShift Container Platform console, click the <code>Application Launcher</code> and select <code>Logging</code>.</p>
<p><img src="/img/include/logging-openshift-1.png" alt="Logging Operator"  class="zoomable" /></p>
<p>Create your Kibana index patterns by clicking <code>Management</code> → <code>Index Patterns</code> → <code>Create index pattern</code>:</p>
<ul>
<li>Each user must manually create index patterns when logging into Kibana the first time in order to see logs for their projects. Users must create an index pattern named <code>app</code> and use the <code>@timestamp</code> time field to view their container logs.</li>
<li>Each admin user must create index patterns when logged into Kibana the first time for the <code>app</code>, <code>infra</code>, and <code>audit</code> indices using the <code>@timestamp</code> time field.</li>
</ul>
<p><img src="/img/include/logging-openshift-2.png" alt="Logging Operator"  class="zoomable" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Replace CEPH SSD journal disk]]></title>
            <link href="https://devopstales.github.io/home/ceph-change-journal-ssd/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/ceph-change-journal-ssd/</id>
            
            
            <published>2021-12-12T00:00:00+00:00</published>
            <updated>2021-12-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can change the end of life journal SSD in Ceph.</p>
<h3 id="replace-a-ssd-disk-used-as-journal-for-filestore">Replace a SSD disk used as journal for filestore</h3>
<p>Let&rsquo;s suppose that we need to replace /dev/nvme0n1. This device is used for journal for osd.10 and osd.11:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">[</span>root@ceph-osd-02 ~<span style="color:#f92672">]</span><span style="color:#75715e"># ceph device ls | grep ceph-osd-02</span>
ST6000NM0115-1YZ110_ZAD5KF07                    ceph-osd-02:sda                        osd.10
ST6000NM0115-1YZ110_ZAD5N8P7                    ceph-osd-02:sdb                        osd.11
Samsung_SSD_970_EVO_Plus_250GB_S4EUNJ0N111052K  ceph-osd-02:nvme0n1                    osd.10 osd.11
</code></pre></div><p>Let&rsquo;s tell ceph to not rebalance the cluster as we stop these OSDs for maintenance:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">[</span>root@ceph-osd-02 ~<span style="color:#f92672">]</span><span style="color:#75715e"># ceph osd set noout</span>
</code></pre></div><p>Let&rsquo;s stop the affected OSDs:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">[</span>root@ceph-osd-02 ~<span style="color:#f92672">]</span><span style="color:#75715e"># systemctl stop ceph-osd@10.service</span>
<span style="color:#f92672">[</span>root@ceph-osd-02 ~<span style="color:#f92672">]</span><span style="color:#75715e"># systemctl stop ceph-osd@11.service</span>
</code></pre></div><p>Let&rsquo;s flush the journals for these OSDs:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">[</span>root@ceph-osd-02 ~<span style="color:#f92672">]</span><span style="color:#75715e"># ceph-osd -i 10 --flush-journal</span>
<span style="color:#f92672">[</span>root@ceph-osd-02 ~<span style="color:#f92672">]</span><span style="color:#75715e"># ceph-osd -i 11 --flush-journal</span>
</code></pre></div><p>Backup nvme0n1 partition table.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">[</span>root@ceph-osd-02 ~<span style="color:#f92672">]</span><span style="color:#75715e"># sfdisk -l /dev/nvme0n1 &gt; nvme0n1.partition.table.txt</span>
</code></pre></div><p>Let&rsquo;s replace the device nvme0n1. In case, let&rsquo;s zap it:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">[</span>root@ceph-osd-02 ~<span style="color:#f92672">]</span><span style="color:#75715e"># ceph-disk zap /dev/nvme0n1</span>
</code></pre></div><p>Let&rsquo;s partition the new disk, using this script:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e">#!/bin/bash
</span><span style="color:#75715e"></span> 
osds<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;10 11&#34;</span>
journal_disk<span style="color:#f92672">=</span>/dev/nvme0n1
part_number<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>
<span style="color:#66d9ef">for</span> osd_id in $osds; <span style="color:#66d9ef">do</span>
  part_number<span style="color:#f92672">=</span><span style="color:#66d9ef">$((</span>part_number+1<span style="color:#66d9ef">))</span>
  journal_uuid<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>cat /var/lib/ceph/osd/ceph-$osd_id/journal_uuid<span style="color:#66d9ef">)</span>
  echo <span style="color:#e6db74">&#34;journal_uuid: </span><span style="color:#e6db74">${</span>journal_uuid<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
  echo <span style="color:#e6db74">&#34;part_number: </span><span style="color:#e6db74">${</span>part_number<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
  sgdisk --new<span style="color:#f92672">=</span><span style="color:#e6db74">${</span>part_number<span style="color:#e6db74">}</span>:0:+30720M --change-name<span style="color:#f92672">=</span><span style="color:#e6db74">${</span>part_number<span style="color:#e6db74">}</span>:<span style="color:#e6db74">&#39;ceph journal&#39;</span> --partition-guid<span style="color:#f92672">=</span><span style="color:#e6db74">${</span>part_number<span style="color:#e6db74">}</span>:$journal_uuid --typecode<span style="color:#f92672">=</span><span style="color:#e6db74">${</span>part_number<span style="color:#e6db74">}</span>:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- $journal_disk
<span style="color:#66d9ef">done</span>
</code></pre></div><p>OR with backup:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">[</span>root@ceph-osd-02 ~<span style="color:#f92672">]</span><span style="color:#75715e"># sfdisk /dev/nvme0n1 &lt; nvme0n1.partition.table.txt</span>
</code></pre></div><p>Then:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">[</span>root@ceph-osd-02 ~<span style="color:#f92672">]</span><span style="color:#75715e"># ceph-osd --mkjournal -i 10</span>
<span style="color:#f92672">[</span>root@ceph-osd-02 ~<span style="color:#f92672">]</span><span style="color:#75715e"># ceph-osd --mkjournal -i 11</span>
</code></pre></div><p>Let&rsquo;s restart the osds:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">[</span>root@ceph-osd-02 ~<span style="color:#f92672">]</span><span style="color:#75715e"># systemctl restart ceph-osd@10.service</span>
<span style="color:#f92672">[</span>root@ceph-osd-02 ~<span style="color:#f92672">]</span><span style="color:#75715e"># systemctl restart ceph-osd@11.service</span>
</code></pre></div><p>Finally:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">[</span>root@ceph-osd-02 ~<span style="color:#f92672">]</span><span style="color:#75715e"># ceph osd unset noout</span>
</code></pre></div><h3 id="replace-a-ssd-disk-used-as-db-for-bluestore">Replace a SSD disk used as db for bluestore</h3>
<p>Let&rsquo;s suppose that we need to replace /dev/nvme0n1. This device is used for journal for osd.10 and osd.11:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">[</span>root@ceph-osd-02 ~<span style="color:#f92672">]</span><span style="color:#75715e"># ceph device ls | grep ceph-osd-02</span>
ST6000NM0115-1YZ110_ZAD5KF07                    ceph-osd-02:sda                        osd.10
ST6000NM0115-1YZ110_ZAD5N8P7                    ceph-osd-02:sdb                        osd.11
Samsung_SSD_970_EVO_Plus_250GB_S4EUNJ0N111052K  ceph-osd-02:nvme0n1                    osd.10 osd.11
</code></pre></div><p>Check the LVM paricioning on the nvme0n1:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">[</span>root@ceph-osd-02 ~<span style="color:#f92672">]</span><span style="color:#75715e"># lsblk</span>
sda                                                                                                     8:0    <span style="color:#ae81ff">0</span>   5.5T  <span style="color:#ae81ff">0</span> disk
└─ceph--a2d09b40--caa4--4720--8953--5e86750da005-osd--block--de012ee4--60c4--4623--a98c--20b3256a6587 253:6    <span style="color:#ae81ff">0</span>   5.5T  <span style="color:#ae81ff">0</span> lvm
sdb                                                                                                     8:16   <span style="color:#ae81ff">0</span>   5.5T  <span style="color:#ae81ff">0</span> disk
└─ceph--01fefec3--2549--40dc--b03e--ea1cbf0c22f1-osd--block--df90bd50--cd26--4306--8c4a--6d97148870e8 253:8    <span style="color:#ae81ff">0</span>   5.5T  <span style="color:#ae81ff">0</span> lvm
...
nvme0n1                                                                                               259:0    <span style="color:#ae81ff">0</span> 232.9G  <span style="color:#ae81ff">0</span> disk
├─ceph--2dd99fb0--5e5a--4795--a14d--8fea42f9b4e9-osd--db--6463679d--ccd6--4988--a4fa--6bb0037b8f7a    253:5    <span style="color:#ae81ff">0</span>   115G  <span style="color:#ae81ff">0</span> lvm
└─ceph--2dd99fb0--5e5a--4795--a14d--8fea42f9b4e9-osd--db--3b39c364--92cb--41c4--8150--ce7f4bdb4b2c    253:7    <span style="color:#ae81ff">0</span>   115G  <span style="color:#ae81ff">0</span> lvm

vgdisplay -v
</code></pre></div><p>We find the volume groups used for db and block. This physical devices in our case it is:
db: <code>ceph-2dd99fb0-5e5a-4795-a14d-8fea42f9b4e9</code>
block1: <code>ceph-a2d09b40-caa4-4720-8953-5e86750da005</code>
block2: <code>ceph-01fefec3-2549-40dc-b03e-ea1cbf0c22f1</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">[</span>root@ceph-osd-02 ~<span style="color:#f92672">]</span><span style="color:#75715e"># vgdisplay -v ceph-2dd99fb0-5e5a-4795-a14d-8fea42f9b4e9</span>
  --- Volume group ---
  VG Name               ceph-2dd99fb0-5e5a-4795-a14d-8fea42f9b4e9
  System ID
  Format                lvm2
  Metadata Areas        <span style="color:#ae81ff">1</span>
  Metadata Sequence No  <span style="color:#ae81ff">5</span>
  VG Access             read/write
  VG Status             resizable
  MAX LV                <span style="color:#ae81ff">0</span>
  Cur LV                <span style="color:#ae81ff">2</span>
  Open LV               <span style="color:#ae81ff">2</span>
  Max PV                <span style="color:#ae81ff">0</span>
  Cur PV                <span style="color:#ae81ff">1</span>
  Act PV                <span style="color:#ae81ff">1</span>
  VG Size               232.88 GiB
  PE Size               4.00 MiB
  Total PE              <span style="color:#ae81ff">59618</span>
  Alloc PE / Size       <span style="color:#ae81ff">58880</span> / 230.00 GiB
  Free  PE / Size       <span style="color:#ae81ff">738</span> / 2.88 GiB
  VG UUID               f29Vag-1PrI-fo7x-Dvhm-TNDl-2cfY-5hFY33

  --- Logical volume ---
  LV Path                /dev/ceph-2dd99fb0-5e5a-4795-a14d-8fea42f9b4e9/osd-db-6463679d-ccd6-4988-a4fa-6bb0037b8f7a
  LV Name                osd-db-6463679d-ccd6-4988-a4fa-6bb0037b8f7a
  VG Name                ceph-2dd99fb0-5e5a-4795-a14d-8fea42f9b4e9
  LV UUID                UWFXM5-5ZmF-Kb4f-jTqc-KuqZ-IWc7-UjHhXY
  LV Write Access        read/write
  LV Creation host, time ceph-osd-02, 2021-06-04 21:48:01 +0200
  LV Status              available
  <span style="color:#75715e"># open                 12</span>
  LV Size                115.00 GiB
  Current LE             <span style="color:#ae81ff">29440</span>
  Segments               <span style="color:#ae81ff">1</span>
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     <span style="color:#ae81ff">256</span>
  Block device           253:5

  --- Logical volume ---
  LV Path                /dev/ceph-2dd99fb0-5e5a-4795-a14d-8fea42f9b4e9/osd-db-3b39c364-92cb-41c4-8150-ce7f4bdb4b2c
  LV Name                osd-db-3b39c364-92cb-41c4-8150-ce7f4bdb4b2c
  VG Name                ceph-2dd99fb0-5e5a-4795-a14d-8fea42f9b4e9
  LV UUID                e52dYE-FuRK-TMPv-U8vx-38pv-KdfE-R0fwBo
  LV Write Access        read/write
  LV Creation host, time ceph-osd-02, 2021-06-04 21:48:30 +0200
  LV Status              available
  <span style="color:#75715e"># open                 12</span>
  LV Size                115.00 GiB
  Current LE             <span style="color:#ae81ff">29440</span>
  Segments               <span style="color:#ae81ff">1</span>
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     <span style="color:#ae81ff">256</span>
  Block device           253:7

  --- Physical volumes ---
  PV Name               /dev/nvme0n1
  PV UUID               4bVZmc-Vku7-rWPd-RHrn-xUFf-WrYb-yidijN
  PV Status             allocatable
  Total PE / Free PE    <span style="color:#ae81ff">59618</span> / <span style="color:#ae81ff">738</span>
</code></pre></div><p>I.e. it is used as db for OSD 10-11</p>
<p>Let&rsquo;s &lsquo;disable these OSDs&rsquo;:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">[</span>root@c-osd-5 /<span style="color:#f92672">]</span><span style="color:#75715e"># ceph osd crush reweight osd.10 0</span>
reweighted item id <span style="color:#ae81ff">10</span> name <span style="color:#e6db74">&#39;osd.10&#39;</span> to <span style="color:#ae81ff">0</span> in crush map
<span style="color:#f92672">[</span>root@c-osd-5 /<span style="color:#f92672">]</span><span style="color:#75715e"># ceph osd crush reweight osd.11 0</span>
reweighted item id <span style="color:#ae81ff">11</span> name <span style="color:#e6db74">&#39;osd.11&#39;</span> to <span style="color:#ae81ff">0</span> in crush map
</code></pre></div><p>Wait that the status is HEALTH-OK. Then destroy the osds(be sure to have saved the mappings first !!):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">[</span>root@c-osd-5 /<span style="color:#f92672">]</span><span style="color:#75715e"># ll /var/lib/ceph/osd/ceph-10/ | grep block</span>
lrwxrwxrwx <span style="color:#ae81ff">1</span> ceph ceph  <span style="color:#ae81ff">93</span> Jun  <span style="color:#ae81ff">4</span> 21:48 block -&gt; /dev/ceph-a2d09b40-caa4-4720-8953-5e86750da005/osd-block-de012ee4-60c4-4623-a98c-20b3256a6587
lrwxrwxrwx <span style="color:#ae81ff">1</span> ceph ceph  <span style="color:#ae81ff">90</span> Jun  <span style="color:#ae81ff">4</span> 21:48 block.db -&gt; /dev/ceph-2dd99fb0-5e5a-4795-a14d-8fea42f9b4e9/osd-db-6463679d-ccd6-4988-a4fa-6bb0037b8f7a

<span style="color:#f92672">[</span>root@c-osd-5 /<span style="color:#f92672">]</span><span style="color:#75715e"># ll /var/lib/ceph/osd/ceph-11/ | grep block</span>
lrwxrwxrwx <span style="color:#ae81ff">1</span> ceph ceph  <span style="color:#ae81ff">93</span> Jun  <span style="color:#ae81ff">4</span> 21:48 block -&gt; /dev/ceph-01fefec3-2549-40dc-b03e-ea1cbf0c22f1/osd-block-df90bd50-cd26-4306-8c4a-6d97148870e8
lrwxrwxrwx <span style="color:#ae81ff">1</span> ceph ceph  <span style="color:#ae81ff">90</span> Jun  <span style="color:#ae81ff">4</span> 21:48 block.db -&gt; /dev/ceph-2dd99fb0-5e5a-4795-a14d-8fea42f9b4e9/osd-db-3b39c364-92cb-41c4-8150-ce7f4bdb4b2c
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">[</span>root@ceph-osd-02 ~<span style="color:#f92672">]</span><span style="color:#75715e"># ceph osd out osd.10</span>
<span style="color:#f92672">[</span>root@ceph-osd-02 ~<span style="color:#f92672">]</span><span style="color:#75715e"># ceph osd out osd.11</span>

<span style="color:#f92672">[</span>root@ceph-osd-02 ~<span style="color:#f92672">]</span><span style="color:#75715e"># ceph osd crush remove osd.10</span>
<span style="color:#f92672">[</span>root@ceph-osd-02 ~<span style="color:#f92672">]</span><span style="color:#75715e"># ceph osd crush remove osd.11</span>

<span style="color:#f92672">[</span>root@ceph-osd-02 ~<span style="color:#f92672">]</span><span style="color:#75715e"># systemctl stop ceph-osd@10.service</span>
<span style="color:#f92672">[</span>root@ceph-osd-02 ~<span style="color:#f92672">]</span><span style="color:#75715e"># systemctl stop ceph-osd@11.service</span>

<span style="color:#f92672">[</span>root@ceph-osd-02 ~<span style="color:#f92672">]</span><span style="color:#75715e"># ceph auth del osd.10</span>
<span style="color:#f92672">[</span>root@ceph-osd-02 ~<span style="color:#f92672">]</span><span style="color:#75715e"># ceph auth del osd.11</span>

<span style="color:#f92672">[</span>root@ceph-osd-02 ~<span style="color:#f92672">]</span><span style="color:#75715e"># ceph osd rm osd.10</span>
<span style="color:#f92672">[</span>root@ceph-osd-02 ~<span style="color:#f92672">]</span><span style="color:#75715e"># ceph osd rm osd.11</span>

<span style="color:#f92672">[</span>root@ceph-osd-02 ~<span style="color:#f92672">]</span><span style="color:#75715e"># umount /var/lib/ceph/osd/ceph-10</span>
<span style="color:#f92672">[</span>root@ceph-osd-02 ~<span style="color:#f92672">]</span><span style="color:#75715e"># umount /var/lib/ceph/osd/ceph-11</span>
</code></pre></div><p>Destroy the volume group created on this SSD disk (be sure to have saved the vgdisplay output first !):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">[</span>root@ceph-osd-02 ~<span style="color:#f92672">]</span><span style="color:#75715e"># vgdisplay -v ceph-2dd99fb0-5e5a-4795-a14d-8fea42f9b4e9 &gt; ceph-vg.txt</span>

<span style="color:#f92672">[</span>root@ceph-osd-02 ~<span style="color:#f92672">]</span><span style="color:#75715e"># vgremove ceph-2dd99fb0-5e5a-4795-a14d-8fea42f9b4e9</span>
</code></pre></div><p>Replace the SSD disk. Suppose the new one is always called vdk. Recreate volume group and logical volume (refer to the previous vgdisplay output):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">[</span>root@c-osd-5 /<span style="color:#f92672">]</span><span style="color:#75715e"># vgcreate ceph-2dd99fb0-5e5a-4795-a14d-8fea42f9b4e9 /dev/vdk </span>
  Physical volume <span style="color:#e6db74">&#34;/dev/vdk&#34;</span> successfully created.
  Volume group <span style="color:#e6db74">&#34;ceph-2dd99fb0-5e5a-4795-a14d-8fea42f9b4e9&#34;</span> successfully created
<span style="color:#f92672">[</span>root@c-osd-5 /<span style="color:#f92672">]</span><span style="color:#75715e"># lvcreate -L 115GB -n osd-db-6463679d-ccd6-4988-a4fa-6bb0037b8f7a ceph-2dd99fb0-5e5a-4795-a14d-8fea42f9b4e9</span>
  Logical volume <span style="color:#e6db74">&#34;osd-db-6463679d-ccd6-4988-a4fa-6bb0037b8f7a&#34;</span> created.
<span style="color:#f92672">[</span>root@c-osd-5 /<span style="color:#f92672">]</span><span style="color:#75715e"># lvcreate -L 115GB -n osd-db-3b39c364-92cb-41c4-8150-ce7f4bdb4b2c ceph-2dd99fb0-5e5a-4795-a14d-8fea42f9b4e9</span>
<span style="color:#f92672">[</span>root@c-osd-5 /<span style="color:#f92672">]</span><span style="color:#75715e"># </span>
</code></pre></div><p>Let&rsquo;s do a lvm zap:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">[</span>root@c-osd-5 /<span style="color:#f92672">]</span><span style="color:#75715e"># ceph-volume lvm zap /var/lib/ceph/osd/ceph-10/block</span>
<span style="color:#f92672">[</span>root@c-osd-5 /<span style="color:#f92672">]</span><span style="color:#75715e"># ceph-volume lvm zap /var/lib/ceph/osd/ceph-11/block</span>
</code></pre></div><p>Let&rsquo;s create the OSD:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#f92672">[</span>root@c-osd-5 /<span style="color:#f92672">]</span><span style="color:#75715e"># ceph-volume lvm create --bluestore --data ceph-a2d09b40-caa4-4720-8953-5e86750da005/osd-block-de012ee4-60c4-4623-a98c-20b3256a6587 --block.db ceph-2dd99fb0-5e5a-4795-a14d-8fea42f9b4e9/osd-db-6463679d-ccd6-4988-a4fa-6bb0037b8f7a</span>
<span style="color:#f92672">[</span>root@c-osd-5 /<span style="color:#f92672">]</span><span style="color:#75715e"># ceph-volume lvm create --bluestore --data ceph-01fefec3-2549-40dc-b03e-ea1cbf0c22f1/osd-block-df90bd50-cd26-4306-8c4a-6d97148870e8 --block.db ceph-2dd99fb0-5e5a-4795-a14d-8fea42f9b4e9/ceph-2dd99fb0-5e5a-4795-a14d-8fea42f9b4e9</span>
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes Pod Security]]></title>
            <link href="https://devopstales.github.io/home/k8s-ps/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/rke2-pod-security-policy/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Pod Security Policy" />
                <link href="https://devopstales.github.io/kubernetes/k8s-ps/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Pod Security" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller-v3/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V3" />
                <link href="https://devopstales.github.io/home/continuous-image-security/?utm_source=atom_feed" rel="related" type="text/html" title="Continuous Image Security" />
                <link href="https://devopstales.github.io/home/k8s-prometheus-stack/?utm_source=atom_feed" rel="related" type="text/html" title="K8S Logging And Monitoring" />
            
                <id>https://devopstales.github.io/home/k8s-ps/</id>
            
            
            <published>2021-12-10T00:00:00+00:00</published>
            <updated>2021-12-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>With the release of Kubernetes v1.23, Pod Security admission has now entered beta. In this article, we cover the key concepts of Pod Security along with how to use it.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<h3 id="what-is-a-pod-security-policy">What is a Pod Security Policy?</h3>
<p>A Pod Security Policy is a cluster-level resource that controls security sensitive aspects of the pod specification. RBAC Controlls the usable Kubernetes objects for a user but nt the conditions of a specific ofject like allow run as root or not in a container.  PSP objects define a set of conditions that a pod must run with in order to be accepted into the system, as well as defaults for their related fields. PodSecurityPolicy is an optional admission controller that is enabled by default through the API, thus policies can be deployed without the PSP admission plugin enabled.</p>
<h3 id="what-is-a-pod-security">What is a Pod Security</h3>
<p>Pod Security is the successor to <code>PodSecurityPolicy</code> which was deprecated in the v1.21 release, and will be removed in Kubernetes v1.25. Pod Security overcomes key shortcomings of Kubernetes' existing, PodSecurityPolicy (PSP) mechanism like: challenging to deploy with controllers and teh lack of dry-run/audit capabilities made it hard to enable PodSecurityPolicy.</p>
<h3 id="configuring-pod-security">Configuring Pod Security</h3>
<p>Pod Security use profiles based on <a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/">Pod Security Standards</a>. These standards define three different policy levels:</p>
<ul>
<li>privileged - The Privileged policy is purposely-open, and entirely unrestricted.</li>
<li>baseline - The Baseline policy is aimed at ease of adoption for common containerized workloads while preventing known privilege escalations.
<ul>
<li>Disables <code>HostProcess</code> for windows</li>
<li>Disables <code>Host Namespaces</code> on linux like: <code>hostNetwork</code>, <code>hostPID</code> and <code>hostIPC</code></li>
<li>Disables <code>Privileged Containers</code></li>
<li>Disallow adding of <code>Capabilities</code></li>
<li>Disallow mounting of <code>HostPath Volumes</code></li>
<li>Disallow usage of <code>Host Ports</code></li>
<li>On supported hosts, the <code>runtime/default</code> AppArmor profile is applied by default.</li>
<li>Setting the SELinux type is restricted, and setting a custom SELinux user or role option is forbidden.</li>
<li>Seccomp profile must not be explicitly set to <code>Unconfined</code>.</li>
<li>Disallow the configuration of <code>Sysctls</code></li>
</ul>
</li>
<li>restricted - The Restricted policy is aimed at enforcing current Pod hardening best practices.
<ul>
<li>Disallow <code>Privilege Escalation</code></li>
<li>Disallow running cotainer as root user, group and uid or guid as 0</li>
<li>Seccomp profile must be explicitly set to one of the allowed values.</li>
<li>Containers must drop <code>ALL</code> capabilities, and are only permitted to add back the <code>NET_BIND_SERVICE</code> capability.</li>
</ul>
</li>
</ul>
<p>Policies are applied in a specific mode. The modes are:</p>
<ul>
<li><code>enforce</code> — Any Pods that violate the policy will be rejected</li>
<li><code>audit</code> — Violations will be recorded as an annotation in the audit logs, but don&rsquo;t affect whether the pod is allowed.</li>
<li><code>warn</code> — Violations will send a warning message back to the user, but don&rsquo;t affect whether the pod is allowed.</li>
</ul>
<h3 id="demo">Demo:</h3>
<p>Start a cluster with kind for the demo:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kind create cluster --image kindest/node:v1.23.0
kubectl cluster-info --context kind-kind
kubectx kind-kind
</code></pre></div><p>Find the enabled admission plugins:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ kubectl -n kube-system exec kube-apiserver-kind-control-plane -it -- kube-apiserver -h | grep <span style="color:#e6db74">&#34;default enabled ones&#34;</span>
...
      --enable-admission-plugins strings
admission plugins that should be enabled in addition
to default enabled ones <span style="color:#f92672">(</span>NamespaceLifecycle, LimitRanger,
ServiceAccount, TaintNodesByCondition, PodSecurity, Priority,
DefaultTolerationSeconds, DefaultStorageClass,
StorageObjectInUseProtection, PersistentVolumeClaimResize,
RuntimeClass, CertificateApproval, CertificateSigning,
CertificateSubjectRestriction, DefaultIngressClass,
MutatingAdmissionWebhook, ValidatingAdmissionWebhook,
ResourceQuota<span style="color:#f92672">)</span>.
...
</code></pre></div><p>Policies are applied to a namespace via labels. These labels are as follows:</p>
<ul>
<li>pod-security.kubernetes.io/<!-- raw HTML omitted -->: <!-- raw HTML omitted --> (required to enable pod security)</li>
<li>pod-security.kubernetes.io/<!-- raw HTML omitted -->-version: <!-- raw HTML omitted --> (optional, defaults to latest)</li>
</ul>
<p>Deploy demo workload:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl create ns verify-pod-security
kubens verify-pod-security

<span style="color:#75715e"># enforces a &#34;restricted&#34; security policy and audits on restricted</span>
kubectl label --overwrite ns verify-pod-security <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  pod-security.kubernetes.io/enforce<span style="color:#f92672">=</span>restricted <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  pod-security.kubernetes.io/audit<span style="color:#f92672">=</span>restricted
</code></pre></div><p>Next, try to deploy a privileged pod:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat &lt;&lt;EOF | kubectl -n verify-pod-security apply -f -</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Pod</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">busybox-privileged</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">containers</span>:
  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">busybox</span>
    <span style="color:#f92672">image</span>: <span style="color:#ae81ff">busybox</span>
    <span style="color:#f92672">args</span>:
    - <span style="color:#ae81ff">sleep</span>
    - <span style="color:#e6db74">&#34;1000000&#34;</span>
    <span style="color:#f92672">securityContext</span>:
      <span style="color:#f92672">allowPrivilegeEscalation</span>: <span style="color:#66d9ef">true</span>
<span style="color:#ae81ff">EOF</span>
</code></pre></div><p>The output is similar to this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">Error from server <span style="color:#f92672">(</span>Forbidden<span style="color:#f92672">)</span>: error when creating <span style="color:#e6db74">&#34;STDIN&#34;</span>: pods <span style="color:#e6db74">&#34;busybox-privileged&#34;</span> is forbidden: violates PodSecurity <span style="color:#e6db74">&#34;restricted:latest&#34;</span>: allowPrivilegeEscalation !<span style="color:#f92672">=</span> false <span style="color:#f92672">(</span>container <span style="color:#e6db74">&#34;busybox&#34;</span> must set securityContext.allowPrivilegeEscalation<span style="color:#f92672">=</span>false<span style="color:#f92672">)</span>, unrestricted capabilities <span style="color:#f92672">(</span>container <span style="color:#e6db74">&#34;busybox&#34;</span> must set securityContext.capabilities.drop<span style="color:#f92672">=[</span><span style="color:#e6db74">&#34;ALL&#34;</span><span style="color:#f92672">])</span>, runAsNonRoot !<span style="color:#f92672">=</span> true <span style="color:#f92672">(</span>pod or container <span style="color:#e6db74">&#34;busybox&#34;</span> must set securityContext.runAsNonRoot<span style="color:#f92672">=</span>true<span style="color:#f92672">)</span>, seccompProfile <span style="color:#f92672">(</span>pod or container <span style="color:#e6db74">&#34;busybox&#34;</span> must set securityContext.seccompProfile.type to <span style="color:#e6db74">&#34;RuntimeDefault&#34;</span> or <span style="color:#e6db74">&#34;Localhost&#34;</span><span style="color:#f92672">)</span>
</code></pre></div><p>Now let&rsquo;s apply the privileged Pod Security level and try tp deploy again.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># enforces a &#34;privileged&#34; security policy and warns / audits on baseline</span>
kubectl label --overwrite ns verify-pod-security <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  pod-security.kubernetes.io/enforce<span style="color:#f92672">=</span>privileged <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  pod-security.kubernetes.io/warn<span style="color:#f92672">=</span>baseline <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  pod-security.kubernetes.io/audit<span style="color:#f92672">=</span>baseline
</code></pre></div><p>Now the pod is created:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">pod/busybox-privileged created
</code></pre></div><p>Let&rsquo;s apply the baseline Pod Security level and try again.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># enforces a &#34;baseline&#34; security policy and warns / audits on restricted</span>
kubectl label --overwrite ns verify-pod-security <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  pod-security.kubernetes.io/enforce<span style="color:#f92672">=</span>baseline <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  pod-security.kubernetes.io/warn<span style="color:#f92672">=</span>restricted <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  pod-security.kubernetes.io/audit<span style="color:#f92672">=</span>restricted
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat &lt;&lt;EOF | kubectl -n verify-pod-security apply -f -</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Pod</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">busybox-baseline</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">containers</span>:
  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">busybox</span>
    <span style="color:#f92672">image</span>: <span style="color:#ae81ff">busybox</span>
    <span style="color:#f92672">args</span>:
    - <span style="color:#ae81ff">sleep</span>
    - <span style="color:#e6db74">&#34;1000000&#34;</span>
    <span style="color:#f92672">securityContext</span>:
      <span style="color:#f92672">allowPrivilegeEscalation</span>: <span style="color:#66d9ef">false</span>
      <span style="color:#f92672">capabilities</span>:
        <span style="color:#f92672">add</span>:
          - <span style="color:#ae81ff">NET_BIND_SERVICE</span>
          - <span style="color:#ae81ff">CHOWN</span>
<span style="color:#ae81ff">EOF</span>
</code></pre></div><p>The output is similar to the following. Note that the warnings match the error message from the restricted policy, but the pod is still successfully created.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">Warning: would violate PodSecurity <span style="color:#e6db74">&#34;restricted:latest&#34;</span>: unrestricted capabilities <span style="color:#f92672">(</span>container <span style="color:#e6db74">&#34;busybox&#34;</span> must set securityContext.capabilities.drop<span style="color:#f92672">=[</span><span style="color:#e6db74">&#34;ALL&#34;</span><span style="color:#f92672">]</span>; container <span style="color:#e6db74">&#34;busybox&#34;</span> must not include <span style="color:#e6db74">&#34;CHOWN&#34;</span> in securityContext.capabilities.add<span style="color:#f92672">)</span>, runAsNonRoot !<span style="color:#f92672">=</span> true <span style="color:#f92672">(</span>pod or container <span style="color:#e6db74">&#34;busybox&#34;</span> must set securityContext.runAsNonRoot<span style="color:#f92672">=</span>true<span style="color:#f92672">)</span>, seccompProfile <span style="color:#f92672">(</span>pod or container <span style="color:#e6db74">&#34;busybox&#34;</span> must set securityContext.seccompProfile.type to <span style="color:#e6db74">&#34;RuntimeDefault&#34;</span> or <span style="color:#e6db74">&#34;Localhost&#34;</span><span style="color:#f92672">)</span>
pod/busybox-baseline created
</code></pre></div><p>You ken use Kyverno to autamate the creation of the labels at namespace. The usage Kyverno is out of the scope of this post but if you want to know more about about this topic check <a href="https://devopstales.github.io/kubernetes/k8s-kyverno-cosign/">my other post</a>.</p>
<h3 id="applying-a-cluster-wide-policy">Applying a cluster-wide policy</h3>
<p>In addition to applying labels to namespaces to configure policy you can also configure cluster-wide policies and exemptions using the AdmissionConfiguration resource.</p>
<p>First create a new kind cluster with :</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kind delete cluster
</code></pre></div><p>Create a Pod Security configuration that enforce and audit baseline policies while using a restricted profile to warn the end user.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat &lt;&lt;EOF &gt; pod-security.yaml</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">apiserver.config.k8s.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">AdmissionConfiguration</span>
<span style="color:#f92672">plugins</span>:
- <span style="color:#f92672">name</span>: <span style="color:#ae81ff">PodSecurity</span>
  <span style="color:#f92672">configuration</span>:
    <span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">pod-security.admission.config.k8s.io/v1beta1</span>
    <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">PodSecurityConfiguration</span>
    <span style="color:#f92672">defaults</span>:
      <span style="color:#f92672">enforce</span>: <span style="color:#e6db74">&#34;baseline&#34;</span>
      <span style="color:#f92672">enforce-version</span>: <span style="color:#e6db74">&#34;latest&#34;</span>
      <span style="color:#f92672">audit</span>: <span style="color:#e6db74">&#34;baseline&#34;</span>
      <span style="color:#f92672">audit-version</span>: <span style="color:#e6db74">&#34;latest&#34;</span>
      <span style="color:#f92672">warn</span>: <span style="color:#e6db74">&#34;restricted&#34;</span>
      <span style="color:#f92672">warn-version</span>: <span style="color:#e6db74">&#34;latest&#34;</span>
      <span style="color:#f92672">audit</span>: <span style="color:#e6db74">&#34;restricted&#34;</span>
      <span style="color:#f92672">audit-version</span>: <span style="color:#e6db74">&#34;latest&#34;</span>
    <span style="color:#f92672">exemptions</span>:
      <span style="color:#75715e"># Array of authenticated usernames to exempt.</span>
      <span style="color:#f92672">usernames</span>: []
      <span style="color:#75715e"># Array of runtime class names to exempt.</span>
      <span style="color:#f92672">runtimeClasses</span>: []
      <span style="color:#75715e"># Array of namespaces to exempt.</span>
      <span style="color:#f92672">namespaces</span>: [<span style="color:#ae81ff">kube-system]</span>
<span style="color:#ae81ff">EOF</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat &lt;&lt;EOF &gt; kind-config.yaml</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Cluster</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">kind.x-k8s.io/v1alpha4</span>
<span style="color:#f92672">nodes</span>:
- <span style="color:#f92672">role</span>: <span style="color:#ae81ff">control-plane</span>
  <span style="color:#f92672">kubeadmConfigPatches</span>:
  - |<span style="color:#e6db74">
</span><span style="color:#e6db74">    kind: ClusterConfiguration
</span><span style="color:#e6db74">    apiServer:
</span><span style="color:#e6db74">        # enable admission-control-config flag on the API server
</span><span style="color:#e6db74">        extraArgs:
</span><span style="color:#e6db74">          admission-control-config-file: /etc/kubernetes/policies/pod-security.yaml
</span><span style="color:#e6db74">        # mount new file / directories on the control plane
</span><span style="color:#e6db74">        extraVolumes:
</span><span style="color:#e6db74">          - name: policies
</span><span style="color:#e6db74">            hostPath: /etc/kubernetes/policies
</span><span style="color:#e6db74">            mountPath: /etc/kubernetes/policies
</span><span style="color:#e6db74">            readOnly: true
</span><span style="color:#e6db74">            pathType: &#34;DirectoryOrCreate&#34;</span>    
  <span style="color:#75715e"># mount the local file on the control plane</span>
  <span style="color:#f92672">extraMounts</span>:
  - <span style="color:#f92672">hostPath</span>: <span style="color:#ae81ff">./pod-security.yaml</span>
    <span style="color:#f92672">containerPath</span>: <span style="color:#ae81ff">/etc/kubernetes/policies/pod-security.yaml</span>
    <span style="color:#f92672">readOnly</span>: <span style="color:#66d9ef">true</span>
<span style="color:#ae81ff">EOF</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kind create cluster --image kindest/node:v1.23.0 --config kind-config.yaml
kubectl cluster-info --context kind-kind
kubectx kind-kind
</code></pre></div><p>Let&rsquo;s create a new namespace and see if the labels apply there.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ kubectl create namespace test-defaults
namespace/test-defaults created

$ kubectl describe namespace test-defaults
Name:         test-defaults
Labels:       kubernetes.io/metadata.name<span style="color:#f92672">=</span>test-defaults
Annotations:  &lt;none&gt;
Status:       Active

No resource quota.

No LimitRange resource.
</code></pre></div><p>Can a privileged workload be deployed?</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat &lt;&lt;EOF | kubectl -n test-defaults apply -f -</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Pod</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">busybox-privileged</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">containers</span>:
  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">busybox</span>
    <span style="color:#f92672">image</span>: <span style="color:#ae81ff">busybox</span>
    <span style="color:#f92672">args</span>:
    - <span style="color:#ae81ff">sleep</span>
    - <span style="color:#e6db74">&#34;1000000&#34;</span>
    <span style="color:#f92672">securityContext</span>:
      <span style="color:#f92672">allowPrivilegeEscalation</span>: <span style="color:#66d9ef">true</span>
<span style="color:#ae81ff">EOF</span>
</code></pre></div><p>The default warn level is working.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">Warning: would violate PodSecurity <span style="color:#e6db74">&#34;restricted:latest&#34;</span>: allowPrivilegeEscalation !<span style="color:#f92672">=</span> false <span style="color:#f92672">(</span>container <span style="color:#e6db74">&#34;busybox&#34;</span> must set securityContext.allowPrivilegeEscalation<span style="color:#f92672">=</span>false<span style="color:#f92672">)</span>, unrestricted capabilities <span style="color:#f92672">(</span>container <span style="color:#e6db74">&#34;busybox&#34;</span> must set securityContext.capabilities.drop<span style="color:#f92672">=[</span><span style="color:#e6db74">&#34;ALL&#34;</span><span style="color:#f92672">])</span>, runAsNonRoot !<span style="color:#f92672">=</span> true <span style="color:#f92672">(</span>pod or container <span style="color:#e6db74">&#34;busybox&#34;</span> must set securityContext.runAsNonRoot<span style="color:#f92672">=</span>true<span style="color:#f92672">)</span>, seccompProfile <span style="color:#f92672">(</span>pod or container <span style="color:#e6db74">&#34;busybox&#34;</span> must set securityContext.seccompProfile.type to <span style="color:#e6db74">&#34;RuntimeDefault&#34;</span> or <span style="color:#e6db74">&#34;Localhost&#34;</span><span style="color:#f92672">)</span>
pod/busybox-privileged created
</code></pre></div><p>Check the API server metrics endpoint:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get --raw /metrics | grep pod_security_evaluations_total

<span style="color:#75715e"># HELP pod_security_evaluations_total [ALPHA] Number of policy evaluations that occurred, not counting ignored or exempt requests.</span>
<span style="color:#75715e"># TYPE pod_security_evaluations_total counter</span>
pod_security_evaluations_total<span style="color:#f92672">{</span>decision<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;allow&#34;</span>,mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;enforce&#34;</span>,policy_level<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;baseline&#34;</span>,policy_version<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;latest&#34;</span>,request_operation<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;create&#34;</span>,resource<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pod&#34;</span>,subresource<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;&#34;</span><span style="color:#f92672">}</span> <span style="color:#ae81ff">2</span>
pod_security_evaluations_total<span style="color:#f92672">{</span>decision<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;allow&#34;</span>,mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;enforce&#34;</span>,policy_level<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;privileged&#34;</span>,policy_version<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;latest&#34;</span>,request_operation<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;create&#34;</span>,resource<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pod&#34;</span>,subresource<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;&#34;</span><span style="color:#f92672">}</span> <span style="color:#ae81ff">0</span>
pod_security_evaluations_total<span style="color:#f92672">{</span>decision<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;allow&#34;</span>,mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;enforce&#34;</span>,policy_level<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;privileged&#34;</span>,policy_version<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;latest&#34;</span>,request_operation<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;update&#34;</span>,resource<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pod&#34;</span>,subresource<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;&#34;</span><span style="color:#f92672">}</span> <span style="color:#ae81ff">0</span>
pod_security_evaluations_total<span style="color:#f92672">{</span>decision<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;deny&#34;</span>,mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;audit&#34;</span>,policy_level<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;baseline&#34;</span>,policy_version<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;latest&#34;</span>,request_operation<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;create&#34;</span>,resource<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pod&#34;</span>,subresource<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;&#34;</span><span style="color:#f92672">}</span> <span style="color:#ae81ff">1</span>
pod_security_evaluations_total<span style="color:#f92672">{</span>decision<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;deny&#34;</span>,mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;enforce&#34;</span>,policy_level<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;baseline&#34;</span>,policy_version<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;latest&#34;</span>,request_operation<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;create&#34;</span>,resource<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pod&#34;</span>,subresource<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;&#34;</span><span style="color:#f92672">}</span> <span style="color:#ae81ff">1</span>
pod_security_evaluations_total<span style="color:#f92672">{</span>decision<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;deny&#34;</span>,mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;warn&#34;</span>,policy_level<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;restricted&#34;</span>,policy_version<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;latest&#34;</span>,request_operation<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;create&#34;</span>,resource<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;controller&#34;</span>,subresource<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;&#34;</span><span style="color:#f92672">}</span> <span style="color:#ae81ff">2</span>
pod_security_evaluations_total<span style="color:#f92672">{</span>decision<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;deny&#34;</span>,mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;warn&#34;</span>,policy_level<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;restricted&#34;</span>,policy_version<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;latest&#34;</span>,request_operation<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;create&#34;</span>,resource<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pod&#34;</span>,subresource<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;&#34;</span><span style="color:#f92672">}</span> <span style="color:#ae81ff">2</span>
</code></pre></div><h3 id="auditing">Auditing</h3>
<p>Example audit-policy.yaml configuration tuned for Pod Security events:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">apiVersion: audit.k8s.io/v1
kind: Policy
rules:
- level: RequestResponse
  resources:
    - group: <span style="color:#e6db74">&#34;&#34;</span> <span style="color:#75715e"># core API group</span>
      resources: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;pods&#34;</span>, <span style="color:#e6db74">&#34;pods/ephemeralcontainers&#34;</span>, <span style="color:#e6db74">&#34;podtemplates&#34;</span>, <span style="color:#e6db74">&#34;replicationcontrollers&#34;</span><span style="color:#f92672">]</span>
    - group: <span style="color:#e6db74">&#34;apps&#34;</span>
      resources: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;daemonsets&#34;</span>, <span style="color:#e6db74">&#34;deployments&#34;</span>, <span style="color:#e6db74">&#34;replicasets&#34;</span>, <span style="color:#e6db74">&#34;statefulsets&#34;</span><span style="color:#f92672">]</span>
    - group: <span style="color:#e6db74">&#34;batch&#34;</span>
      resources: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;cronjobs&#34;</span>, <span style="color:#e6db74">&#34;jobs&#34;</span><span style="color:#f92672">]</span>
  verbs: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;create&#34;</span>, <span style="color:#e6db74">&#34;update&#34;</span><span style="color:#f92672">]</span>
  omitStages:
    - <span style="color:#e6db74">&#34;RequestReceived&#34;</span>
    - <span style="color:#e6db74">&#34;ResponseStarted&#34;</span>
    - <span style="color:#e6db74">&#34;Panic&#34;</span>
</code></pre></div><p>The enableing of the audit-policy function is out of the scope of this post but if you want to know more about about this topic check <a href="https://devopstales.github.io/kubernetes/k8s-falco/">my other post</a>.</p>
<h3 id="psp-migrations">PSP migrations</h3>
<p>If you&rsquo;re already using PSP, SIG Auth has created a guide and <a href="https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/">published the steps to migrate off of PSP</a>.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to deploy a Domain Controller on Microsoft Azure]]></title>
            <link href="https://devopstales.github.io/home/azure-dc-deploy/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/azure-dc-deploy/?utm_source=atom_feed" rel="related" type="text/html" title="How to deploy a Domain Controller on Microsoft Azure" />
                <link href="https://devopstales.github.io/home/aks-registry/?utm_source=atom_feed" rel="related" type="text/html" title="Azure Conainer Registry integration for AKS" />
                <link href="https://devopstales.github.io/home/aks-ingress-controller/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ingress to AKS" />
                <link href="https://devopstales.github.io/home/aks/?utm_source=atom_feed" rel="related" type="text/html" title="Install AKS Cluster" />
                <link href="https://devopstales.github.io/home/k8s-install-containerd/?utm_source=atom_feed" rel="related" type="text/html" title="Install kubernetes with kubeadm" />
            
                <id>https://devopstales.github.io/home/azure-dc-deploy/</id>
            
            
            <published>2021-12-07T00:00:00+00:00</published>
            <updated>2021-12-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this pos I will show you how you can create a hybrid Acrive Directory Domain with on-premiss and Azure DCs.</p>
<h3 id="requirements">Requirements</h3>
<ul>
<li>An Azure AD tenant with an active subscription.</li>
<li>A Virtual Network in Azure that doesn’t overlap with your on-premises network.</li>
<li>A continuous line of sight between your on-premises domain controller and Microsoft Azure (Azure VPN Gateway, ExpressRoute or an NVA).</li>
</ul>
<h3 id="deploy-a-virtual-machine">Deploy A Virtual Machine</h3>
<ul>
<li>Go to the azure portal (<a href="https://portal.azure.com">https://portal.azure.com</a>) and login</li>
<li>Create a new Windows Server resource. I Recommened using Windows Server 2019.</li>
</ul>
<p><img src="/img/include/azuredc01.jpg" alt="Bacic info"  class="zoomable" /></p>
<p>For safety reasons, you should set <code>allow selected ports</code> to <code>none</code>.</p>
<p><img src="/img/include/azuredc02.jpg" alt="disallow selected ports"  class="zoomable" /></p>
<ul>
<li>Click Next to configure vm disks.</li>
</ul>
<blockquote>
<p>A Single VM without premium SSD’s has an SLA of 99.95%. A Single VM with premium SSD’s (all disks) has an SLA of 99.99%. I Recommend using premium disks for your domain controller.</p>
</blockquote>
<ul>
<li>Add a second (premium ssd) disk with host caching set to none. This disk will contain the database, logs and sysvol folders.</li>
</ul>
<p><img src="/img/include/azuredc03.jpg" alt="add premium ssd"  class="zoomable" /></p>
<ul>
<li>Click Next to configure networking. Attach the VM to your existing vNet that’s connected with your on-premises domain. Don’t assign a public IP address to your virtual machine as recommended by Microsoft – use a VPN or Azure Bastion to connect to the machine.</li>
</ul>
<p><img src="/img/include/azuredc04.jpg" alt="configure network"  class="zoomable" /></p>
<ul>
<li>Finish all steps to create the virtual machine. Don’t enable <code>Login with AAD credentials</code> or <code>Auto-shutdown</code>.</li>
</ul>
<h3 id="configure-static-ip">Configure static IP</h3>
<p>The virtual machine must have a static IP address.</p>
<ul>
<li>Select network interface of your new virtual machine</li>
</ul>
<p><img src="/img/include/azuredc05.jpg" alt="static ip"  class="zoomable" /></p>
<p><img src="/img/include/azuredc06.jpg" alt="static ip"  class="zoomable" /></p>
<ul>
<li>Select Static and configure the IP address. Don’t forget to click save – a reboot may be required. You should never configure the static IP address on the VM itself as you do on-premises.</li>
</ul>
<p><img src="/img/include/azuredc07.jpg" alt="static ip"  class="zoomable" /></p>
<h3 id="domain-join">Domain join</h3>
<ul>
<li>Test if you can ping the VM from your on-premises domain controller and the other way around.</li>
<li>Open Active Directory Sites &amp; Services on your on-premises domain controller.</li>
<li>Create a new site</li>
</ul>
<p><img src="/img/include/azuredc08.jpg" alt="new site"  class="zoomable" /></p>
<p><img src="/img/include/azuredc09.jpg" alt="new site"  class="zoomable" /></p>
<ul>
<li>Right click Subnets and select New Subnet.</li>
</ul>
<p><img src="/img/include/azuredc10.jpg" alt="new subnet"  class="zoomable" /></p>
<p><img src="/img/include/azuredc11.jpg" alt="new subnet"  class="zoomable" /></p>
<ul>
<li>Start Add Roles and Features on the Azure VM.</li>
<li>Add the Active Directory Domain Services role and all necessary features.</li>
<li>Promote this server to a domain controller.</li>
<li>Select Add a domain controller to an existing domain.</li>
</ul>
<p><img src="/img/include/azuredc12.jpg" alt="join domain"  class="zoomable" /></p>
<p><img src="/img/include/azuredc13.jpg" alt="join domain"  class="zoomable" /></p>
<p><img src="/img/include/azuredc14.jpg" alt="join domain"  class="zoomable" /></p>
<ul>
<li>Reboot the virtual machine.</li>
</ul>
<h3 id="validate-dc-dns-settings-on-azure">Validate DC DNS Settings on Azure</h3>
<p>When the virtual machine is back online, it probably has static DNS servers configured – this happened because of the AD DC roles. Change this back to Obtain DNS server address automatically.</p>
<p><img src="/img/include/azuredc15.jpg" alt="configure dns server"  class="zoomable" /></p>
<p><img src="/img/include/azuredc16.jpg" alt="configure dns server"  class="zoomable" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Firewall Ports for AD Domain Join]]></title>
            <link href="https://devopstales.github.io/home/pfsense-ad-join/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/pfsense-ad-join/</id>
            
            
            <published>2021-11-24T00:00:00+00:00</published>
            <updated>2021-11-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you what port you need to enable for AD Domain Join.</p>
<h3 id="firewall-ports-required-to-join-ad-domain-minimum">Firewall Ports required to join AD Domain (Minimum)</h3>
<p>Windows 10 Client can join to Windows 2019 AD Domain with the following Ports allow in Firewall</p>
<ul>
<li>TCP 88 (Kerberos Key Distribution Center)</li>
<li>TCP 135 (Remote Procedure Call)</li>
<li>TCP 139 (NetBIOS Session Service)</li>
<li>TCP 389 (LDAP)</li>
<li>TCP 445 (SMB,Net Logon)</li>
<li>UDP 53 (DNS)</li>
<li>UDP 389 (LDAP, DC Locator, Net Logon)</li>
<li>TCP 49152-65535 (Randomly allocated high TCP ports)</li>
</ul>
<p>Without TCP High Ports open the following Message appear even join to domain successfully:</p>
<p><img src="/img/include/AD-FW-01.png" alt="message"  class="zoomable" /></p>
<p>there is a lot of TCP high ports are blocked in Firewall:</p>
<p><img src="/img/include/AD-FW-02.png" alt="Port Blocking"  class="zoomable" /></p>
<h3 id="optional-ports">Optional Ports</h3>
<ul>
<li>UDP 123 (NTP)</li>
<li>TCP 53 (DNS)</li>
<li>TCP 464 ( Kerberos Password V5 – Used when user change their password from desktop)</li>
<li>UDP 137 (NetBIOS Name Resolution)</li>
<li>UDP 138 (NetBIOS Datagram Service)</li>
<li>TCP 636 (LDAP SSL)</li>
<li>UDP 636 (LDAP SSL)</li>
<li>TCP 3268 (Global Catalog)</li>
</ul>
<p>User can still change their password successfully even thought TCP 464 is blocked in Firewall</p>
<h3 id="firewall-rules-in-pfesense-firewall">Firewall Rules in pfesense Firewall</h3>
<p><img src="/img/include/AD-FW-03.png" alt="Firewall Rules"  class="zoomable" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install kubernetes with kubeadm]]></title>
            <link href="https://devopstales.github.io/home/k8s-install-containerd/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-kyverno-cosign/?utm_source=atom_feed" rel="related" type="text/html" title="Image Signature Verification with Kyverno" />
                <link href="https://devopstales.github.io/home/kyverno-image-mirror/?utm_source=atom_feed" rel="related" type="text/html" title="Automatically change registry in pod definition" />
                <link href="https://devopstales.github.io/home/migrate-docker-to-containerd/?utm_source=atom_feed" rel="related" type="text/html" title="Migrate Kubernetes from docker to containerd" />
                <link href="https://devopstales.github.io/home/k8s-connaisseur-v2/?utm_source=atom_feed" rel="related" type="text/html" title="Image Signature Verification Admission Controller V2" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller-v3/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V3" />
            
                <id>https://devopstales.github.io/home/k8s-install-containerd/</id>
            
            
            <published>2021-11-09T00:00:00+00:00</published>
            <updated>2021-11-09T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Kubeadm is a tool that helps you bootstrap a simple Kubernetes cluster and simplifies the deployment process.</p>
<H3>Parst of the Kubernetes series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
     <li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
     <li>Part1c: <a href="../../kubernetes/k8s-install-containerd/">Install K8S with containerd and kubeadm</a></li>
     <li>Part1d: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
     <li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a></li>
<!--      <li>Part3b: k8s-multiple-nginx-ingress -->
<!--      <li>Part3c: k8s-contour-ingress -->
     <li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a></li>
<!-- local folder (+ autoprovisioning ) -->
     <li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
     <li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
     <li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
     <li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
     <li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
     <li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a></li>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... -->
     <li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
     <li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
     <li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
     <li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
     <li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
     <li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
     <li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a></li>
<!-- istio-install.md -->
     <li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a></li>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog -->
</ul>


<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">192.168.1.41  kubernetes01 <span style="color:#75715e"># master node</span>
192.168.1.42  kubernetes02 <span style="color:#75715e"># frontend node</span>
192.168.1.43  kubernetes03 <span style="color:#75715e"># worker node</span>
192.168.1.44  kubernetes04 <span style="color:#75715e"># worker node</span>
192.168.1.45  kubernetes05 <span style="color:#75715e"># worker node</span>

<span style="color:#75715e"># hardware requirement</span>
<span style="color:#ae81ff">4</span> CPU
16G RAM
</code></pre></div><h3 id="install-docker">Install Docker</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">apt-get update
apt-get install apt-transport-https ca-certificates curl gnupg2 software-properties-common
curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add -
add-apt-repository <span style="color:#e6db74">&#34;deb [arch=amd64] https://download.docker.com/linux/debian </span><span style="color:#66d9ef">$(</span>lsb_release -cs<span style="color:#66d9ef">)</span><span style="color:#e6db74"> stable&#34;</span>

apt-get update
apt-get install containerd.io

<span style="color:#75715e">## Configure containerd</span>
sudo mkdir -p /etc/containerd
sudo containerd config default &gt; /etc/containerd/config.toml
</code></pre></div><p>To use the <code>systemd</code> cgroup driver in <code>/etc/containerd/config.toml</code> with <code>runc</code>, set</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano /etc/containerd/config.toml
...
          <span style="color:#f92672">[</span>plugins.<span style="color:#e6db74">&#34;io.containerd.grpc.v1.cri&#34;</span>.containerd.runtimes.runc.options<span style="color:#f92672">]</span>
            SystemdCgroup <span style="color:#f92672">=</span> true
</code></pre></div><h3 id="configuuration">Configuuration</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat <span style="color:#e6db74">&lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf
</span><span style="color:#e6db74">overlay
</span><span style="color:#e6db74">br_netfilter
</span><span style="color:#e6db74">kvm-intel
</span><span style="color:#e6db74">EOF</span>

sudo modprobe overlay
sudo modprobe br_netfilter

cat <span style="color:#e6db74">&lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
</span><span style="color:#e6db74">net.bridge.bridge-nf-call-iptables  = 1
</span><span style="color:#e6db74">net.ipv4.ip_forward                 = 1
</span><span style="color:#e6db74">net.bridge.bridge-nf-call-ip6tables = 1
</span><span style="color:#e6db74">EOF</span>

sysctl --system
</code></pre></div><h3 id="disable-swap">Disable swap</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">free -h
swapoff -a
swapoff -a
sed -i.bak -r <span style="color:#e6db74">&#39;s/(.+ swap .+)/#\1/&#39;</span> /etc/fstab
free -h
</code></pre></div><h3 id="install-kubeadm">Install kubeadm</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">echo <span style="color:#e6db74">&#34;runtime-endpoint: unix:///run/containerd/containerd.sock&#34;</span> &gt; /etc/crictl.yaml
crictl ps

<span style="color:#75715e"># Start containerd</span>
systemctl enable --now containerd
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">apt-get install ebtables ethtool apt-transport-https

curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
cat <span style="color:#e6db74">&lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list
</span><span style="color:#e6db74">deb http://apt.kubernetes.io/ kubernetes-xenial main
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><p>Change runtime in kubeadm config:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
<span style="color:#75715e"># add the following flags to KUBELET_KUBEADM_ARGS variable</span>
Environment<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;KUBELET_KUBEADM_ARGS=--node-ip=192.168.1.41 --container-runtime=remote --container-runtime-endpoint=/run/containerd/containerd.sock --cgroup-driver=systemd&#34;</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">apt-get update <span style="color:#f92672">&amp;&amp;</span> apt-get install -y kubelet kubeadm kubectl
apt-mark hold kubelet kubeadm kubect
systemctl enable kubelet <span style="color:#f92672">&amp;&amp;</span> systemctl start kubelet
kubeadm config images pul
</code></pre></div><h3 id="init-master">Init master</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubeadm init --pod-network-cidr<span style="color:#f92672">=</span>10.244.0.0/16 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--apiserver-advertise-address<span style="color:#f92672">=</span>192.168.1.41 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--cri-socket<span style="color:#f92672">=</span>unix:///run/containerd/containerd.sock


mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown <span style="color:#66d9ef">$(</span>id -u<span style="color:#66d9ef">)</span>:<span style="color:#66d9ef">$(</span>id -g<span style="color:#66d9ef">)</span> $HOME/.kube/config


kubectl apply -f https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml
</code></pre></div><h3 id="join-workers-to-cluster">Join workers to cluster</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubeadm join 192.168.1.41:6443 --token XXXXXXXX <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --discovery-token-ca-cert-hash sha256:XXXXXXXX
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Central authentication with oauth2-proxy]]></title>
            <link href="https://devopstales.github.io/home/k8s-central-oauth/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-gangway/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak and gangway" />
                <link href="https://devopstales.github.io/home/k8s-dasboard-auth/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak and gatekeeper" />
                <link href="https://devopstales.github.io/kubernetes/k8s-central-oauth/?utm_source=atom_feed" rel="related" type="text/html" title="Central authentication with oauth2-proxy" />
                <link href="https://devopstales.github.io/home/foreman-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Foreman openidc SSO with keycloak" />
                <link href="https://devopstales.github.io/home/mattermost-keycloak-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Free sso for Mattermost Teams Edition" />
            
                <id>https://devopstales.github.io/home/k8s-central-oauth/</id>
            
            
            <published>2021-11-05T00:00:00+00:00</published>
            <updated>2021-11-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how to use one central OAuth2 Proxy for multiple services inside your Kubernetes Cluster.</p>
<p>The default example on how to secure a service with Nginx and OAuth2 Proxy shows you how to secure only one service. With this setup you need to create one oauth2-proxy for every service. . Another problem of this setup is that it is not supported by most Helm charts.</p>
<p>First we need an oauth2-proxy to authenticate all of the requests:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: auth-proxy
  namespace: ingress-system
spec:
  repo: <span style="color:#e6db74">&#34;https://oauth2-proxy.github.io/manifests&#34;</span>
  chart: oauth2-proxy
  targetNamespace: ingress-system
  valuesContent: |-
    config:
      clientID: <span style="color:#e6db74">&#34;&lt;client-id&gt;&#34;</span>
      clientSecret: <span style="color:#e6db74">&#34;&lt;clinet-secret&gt;&#34;</span>
      cookieSecret: <span style="color:#e6db74">&#34;bkJnQW1ua2xGa2tCV2pGTlZDdHJWS0t4SWJ2MFFSOWY=&#34;</span>
    extraArgs:
      email-domain: <span style="color:#e6db74">&#39;*&#39;</span>
      provider: keycloak
      login-url: https://&lt;keycloak-url&gt;/auth/realms/&lt;keycloak-relm&gt;/protocol/openid-connect/auth
      redeem-url: https://&lt;keycloak-url&gt;/auth/realms/&lt;keycloak-relm&gt;/protocol/openid-connect/token
      profile-url: https://&lt;keycloak-url&gt;/auth/realms/&lt;keycloak-relm&gt;/protocol/openid-connect/userinfo
      validate-url: https://&lt;keycloak-url&gt;/auth/realms/&lt;keycloak-relm&gt;/protocol/openid-connect/userinfo
      scope: email
      skip-provider-button: <span style="color:#e6db74">&#39;true&#39;</span>
      force-https: <span style="color:#e6db74">&#39;false&#39;</span>
      cookie-secure: <span style="color:#e6db74">&#39;false&#39;</span>
      pass-authorization-header: <span style="color:#e6db74">&#39;true&#39;</span>
      pass-basic-auth: <span style="color:#e6db74">&#39;false&#39;</span>
      skip-jwt-bearer-tokens: <span style="color:#e6db74">&#39;true&#39;</span>
      whitelist-domain: .k8s.intra
      cookie-domain: .k8s.intra
      oidc-issuer-url: https://&lt;keycloak-url&gt;/auth/realms/&lt;keycloak-relm&gt;
    ingress:
      enabled: true
      path: /oauth2
      hosts:
        - oauth.k8s.intra
      annotations:
        kubernetes.io/ingress.class: nginx
        cert-manager.io/cluster-issuer: ca-issuer
        nginx.ingress.kubernetes.io/proxy-buffer-size: <span style="color:#e6db74">&#34;16k&#34;</span>
      tls:
      - secretName: oauth2-proxy-tls-cert
        hosts:
          - oauth.k8s.intra
    metrics:
      enabled: true
<span style="color:#75715e">#      servicemonitor:</span>
<span style="color:#75715e">#        enabled: true</span>
<span style="color:#75715e">#        namespace: &#34;ingress-system&#34;</span>
</code></pre></div><p>For the service you want to secure, add the below annotations to the Ingress:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: ca-issuer
    nginx.ingress.kubernetes.io/force-ssl-redirect: <span style="color:#e6db74">&#34;true&#34;</span>
    nginx.ingress.kubernetes.io/auth-url: <span style="color:#e6db74">&#34;http://auth-proxy-oauth2-proxy.ingress-system.svc.cluster.local/oauth2/auth&#34;</span>
    nginx.ingress.kubernetes.io/auth-signin: <span style="color:#e6db74">&#34;https://oauth.k8s.intra/oauth2/start?rd=https%3A%2F%2F</span>$host$request_uri<span style="color:#e6db74">&#34;</span>
  name: alertmanager
spec:
  rules:
  - host: alertmanager.example.com
    http:
      paths:
      - backend:
          serviceName: alertmanager
          servicePort: <span style="color:#ae81ff">9093</span>
        path: /
        pathType: ImplementationSpecific
  tls:
  - hosts:
    - alertmanager.example.com
    secretName: tls-cert
</code></pre></div><p>The <code>auth-sigin</code> redirects any needed login to the OAuth2 Proxy Ingress.
The <code>auth-url</code> annotation can access the OAuth2 Proxy internally via its service to verify a submitted token.</p>
<p>The OAuth2 Proxy will handle the authentication and later redirect you to the protected service again.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to create kubeconfig?]]></title>
            <link href="https://devopstales.github.io/home/k8s-rbac-gen/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-rbac-gen/?utm_source=atom_feed" rel="related" type="text/html" title="How to create kubeconfig?" />
                <link href="https://devopstales.github.io/home/k8s-gangway/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak and gangway" />
                <link href="https://devopstales.github.io/home/k8s-dasboard-auth/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak and gatekeeper" />
                <link href="https://devopstales.github.io/home/k8s-falco/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes audit logs and Falco" />
                <link href="https://devopstales.github.io/home/k8s-cisa-install/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Hardening Guide with CISA 1.6 Benchmark" />
            
                <id>https://devopstales.github.io/home/k8s-rbac-gen/</id>
            
            
            <published>2021-11-03T00:00:00+00:00</published>
            <updated>2021-11-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this blog, I will show you how to create a kubeconfig file with limited access to kubernetes cluster using service account, secret token and RBAC</p>
<p>Create namespace:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">export NAMESPACE<span style="color:#f92672">=</span>test-ns
export SERVICEACCOUNT<span style="color:#f92672">=</span>devopstales

kubectl create namespace $NAMESPACE
kubens $NAMESPACE
</code></pre></div><p>Create serviceaccount with RBAC:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat &lt;&lt;EOF | envsubst | kubectl create -f -</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ServiceAccount</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">$SERVICEACCOUNT</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">$NAMESPACE</span>
---
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Role</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">developer-access</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">$NAMESPACE</span>
<span style="color:#f92672">rules</span>:
- <span style="color:#f92672">apiGroups</span>: [<span style="color:#e6db74">&#34;&#34;</span>, <span style="color:#e6db74">&#34;extensions&#34;</span>, <span style="color:#e6db74">&#34;apps&#34;</span>]
  <span style="color:#f92672">resources</span>: [<span style="color:#e6db74">&#34;*&#34;</span>]
  <span style="color:#f92672">verbs</span>: [<span style="color:#e6db74">&#34;*&#34;</span>]
- <span style="color:#f92672">apiGroups</span>: [<span style="color:#e6db74">&#34;batch&#34;</span>]
  <span style="color:#f92672">resources</span>:
  - <span style="color:#ae81ff">jobs</span>
  - <span style="color:#ae81ff">cronjobs</span>
  <span style="color:#f92672">verbs</span>: [<span style="color:#e6db74">&#34;*&#34;</span>]
---
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">RoleBinding</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">$SERVICEACCOUNT</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">$NAMESPACE</span>
<span style="color:#f92672">subjects</span>:
- <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ServiceAccount</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">$SERVICEACCOUNT</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">$NAMESPACE</span>
<span style="color:#f92672">roleRef</span>:
  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Role</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">developer-access</span>
<span style="color:#ae81ff">EOF</span>
</code></pre></div><p>Create kubeconfig for serviceaccount:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">git clone https://github.com/devopstales/k8s_sec_lab.git
cd k8s_sec_lab/kubernetes-scripts
chmod +x create-kubeconfig.sh
./create-kubeconfig.sh $SERVICEACCOUNT &gt; kubeconfig-$NAMESPACE
</code></pre></div><p>Use kubeconfig:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl --kubeconfig<span style="color:#f92672">=</span>kubeconfig-$NAMESPACE get po
</code></pre></div><h3 id="permission-managger">Permission Managger</h3>
<p>Permission Manager is an application developed by SIGHUP that enables a super-easy and user-friendly RBAC management for Kubernetes.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl create namespace permission-manager
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano pm-secret.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Secret</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">permission-manager</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">permission-manager</span>
<span style="color:#f92672">type</span>: <span style="color:#ae81ff">Opaque</span>
<span style="color:#f92672">stringData</span>:
  <span style="color:#f92672">PORT</span>: <span style="color:#e6db74">&#34;4000&#34;</span> <span style="color:#75715e"># port where server is exposed</span>
  <span style="color:#f92672">CLUSTER_NAME</span>: <span style="color:#e6db74">&#34;my-cluster&#34;</span> <span style="color:#75715e"># name of the cluster to use in the generated kubeconfig file</span>
  <span style="color:#f92672">CONTROL_PLANE_ADDRESS</span>: <span style="color:#e6db74">&#34;https://172.17.0.3:6443&#34;</span> <span style="color:#75715e"># full address of the control plane to use in the generated kubeconfig file</span>
  <span style="color:#f92672">BASIC_AUTH_PASSWORD</span>: <span style="color:#e6db74">&#34;changeMe&#34;</span> <span style="color:#75715e"># password used by basic auth (username is `admin`)</span>
</code></pre></div><p>Deploy permission-manager:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f pm-secret.yaml
kubectl apply -f https://github.com/sighupio/permission-manager/releases/download/v1.7.1-rc1/crd.yml
kubectl apply -f https://github.com/sighupio/permission-manager/releases/download/v1.7.1-rc1/seed.yml
kubectl apply -f https://github.com/sighupio/permission-manager/releases/download/v1.7.1-rc1/deploy.yml

kubectl port-forward svc/permission-manager <span style="color:#ae81ff">4000</span> --namespace permission-manager
</code></pre></div><p>Connect on localhost:</p>
<p><img src="/img/include/permission-manager.gif" alt="permission-manager"  class="zoomable" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes audit logs and Falco]]></title>
            <link href="https://devopstales.github.io/home/k8s-falco/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-falco/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes audit logs and Falco" />
                <link href="https://devopstales.github.io/home/k8s-cisa-install/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Hardening Guide with CISA 1.6 Benchmark" />
                <link href="https://devopstales.github.io/home/k8s-seccomp/?utm_source=atom_feed" rel="related" type="text/html" title="Hardening Kubernetes with seccomp" />
                <link href="https://devopstales.github.io/kubernetes/k8s-cisa-install/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Hardening Guide with CISA 1.6 Benchmark" />
                <link href="https://devopstales.github.io/kubernetes/k8s-seccomp/?utm_source=atom_feed" rel="related" type="text/html" title="Hardening Kubernetes with seccomp" />
            
                <id>https://devopstales.github.io/home/k8s-falco/</id>
            
            
            <published>2021-11-02T00:00:00+00:00</published>
            <updated>2021-11-02T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this blog post I will show you how how you can use Kubernetes the audit logs and Falco for detecting suspicious activities in you cluster.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<p>In the <a href="../k8s-cisa-install">previous post</a> I configured CISA&rsquo;s best practices for the Kubernetes cluster. One of this best practice is to enable Kubernetes audit logging.  It’s a key feature in securing your Kubernetes cluster, as the audit logs capture events like creating a new deployment, deleting namespaces, starting a node port service, etc.</p>
<p>When a request, for example, creates a pod, it’s sent to the <code>kube-apiserver</code>. You can configure <code>kube-apiserver</code> to write all of this activities to a log file. Each request can be recorded with an associated stage. The defined stages are:</p>
<ul>
<li>RequestReceived: The event is generated as soon as the request is received by the audit handler without processing it.</li>
<li>ResponseStarted: Once the response headers are sent, but before the response body is sent. This stage is only generated for long-running requests (e.g., watch).</li>
<li>ResponseComplete: The event is generated when a response body is sent.</li>
<li>Panic: Event is generated when panic occurs.</li>
</ul>
<h3 id="enable-kubernetes-audit-policy">Enable Kubernetes audit policy</h3>
<p>You can enable this in the <code>kubeadm</code> conif as I did it in the previous post or edit the manifest of the running api server on the masters:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano /etc/kubernetes/manifests/kube-apiserver.yaml
spec:
  containers:
  - command:
    - kube-apiserver
...
    - --audit-log-path<span style="color:#f92672">=</span>/var/log/kube-audit/audit.log
    - --audit-policy-file<span style="color:#f92672">=</span>/etc/kubernetes/audit-policy.yaml
...
</code></pre></div><p>With security in mind, we’ll create a policy that filters requests related to pods, kube-proxy, secrets, configurations, and other key components.
Such a policy would look like:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">mkdir /var/log/kube-audit</span>
<span style="color:#ae81ff">nano /etc/kubernetes/audit-policy.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">audit.k8s.io/v1</span> <span style="color:#75715e"># This is required.</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Policy</span>
<span style="color:#75715e"># Don&#39;t generate audit events for all requests in RequestReceived stage.</span>
<span style="color:#f92672">omitStages</span>:
  - <span style="color:#e6db74">&#34;RequestReceived&#34;</span>
<span style="color:#f92672">rules</span>:
  <span style="color:#75715e"># Log pod changes at RequestResponse level</span>
  - <span style="color:#f92672">level</span>: <span style="color:#ae81ff">RequestResponse</span>
    <span style="color:#f92672">resources</span>:
    - <span style="color:#f92672">group</span>: <span style="color:#e6db74">&#34;&#34;</span>
      <span style="color:#75715e"># Resource &#34;pods&#34; doesn&#39;t match requests to any subresource of pods,</span>
      <span style="color:#75715e"># which is consistent with the RBAC policy.</span>
      <span style="color:#f92672">resources</span>: [<span style="color:#e6db74">&#34;pods&#34;</span>]
  <span style="color:#75715e"># Log &#34;pods/log&#34;, &#34;pods/status&#34; at Metadata level</span>
  - <span style="color:#f92672">level</span>: <span style="color:#ae81ff">Metadata</span>
    <span style="color:#f92672">resources</span>:
    - <span style="color:#f92672">group</span>: <span style="color:#e6db74">&#34;&#34;</span>
      <span style="color:#f92672">resources</span>: [<span style="color:#e6db74">&#34;pods/log&#34;</span>, <span style="color:#e6db74">&#34;pods/status&#34;</span>]

  <span style="color:#75715e"># Don&#39;t log requests to a configmap called &#34;controller-leader&#34;</span>
  - <span style="color:#f92672">level</span>: <span style="color:#ae81ff">None</span>
    <span style="color:#f92672">resources</span>:
    - <span style="color:#f92672">group</span>: <span style="color:#e6db74">&#34;&#34;</span>
      <span style="color:#f92672">resources</span>: [<span style="color:#e6db74">&#34;configmaps&#34;</span>]
      <span style="color:#f92672">resourceNames</span>: [<span style="color:#e6db74">&#34;controller-leader&#34;</span>]

  <span style="color:#75715e"># Don&#39;t log watch requests by the &#34;system:kube-proxy&#34; on endpoints or services</span>
  - <span style="color:#f92672">level</span>: <span style="color:#ae81ff">None</span>
    <span style="color:#f92672">users</span>: [<span style="color:#e6db74">&#34;system:kube-proxy&#34;</span>]
    <span style="color:#f92672">verbs</span>: [<span style="color:#e6db74">&#34;watch&#34;</span>]
    <span style="color:#f92672">resources</span>:
    - <span style="color:#f92672">group</span>: <span style="color:#e6db74">&#34;&#34;</span> <span style="color:#75715e"># core API group</span>
      <span style="color:#f92672">resources</span>: [<span style="color:#e6db74">&#34;endpoints&#34;</span>, <span style="color:#e6db74">&#34;services&#34;</span>]

  <span style="color:#75715e"># Don&#39;t log authenticated requests to certain non-resource URL paths.</span>
  - <span style="color:#f92672">level</span>: <span style="color:#ae81ff">None</span>
    <span style="color:#f92672">userGroups</span>: [<span style="color:#e6db74">&#34;system:authenticated&#34;</span>]
    <span style="color:#f92672">nonResourceURLs</span>:
    - <span style="color:#e6db74">&#34;/api*&#34;</span> <span style="color:#75715e"># Wildcard matching.</span>
    - <span style="color:#e6db74">&#34;/version&#34;</span>

  <span style="color:#75715e"># Log the request body of configmap changes in kube-system.</span>
  - <span style="color:#f92672">level</span>: <span style="color:#ae81ff">Request</span>
    <span style="color:#f92672">resources</span>:
    - <span style="color:#f92672">group</span>: <span style="color:#e6db74">&#34;&#34;</span> <span style="color:#75715e"># core API group</span>
      <span style="color:#f92672">resources</span>: [<span style="color:#e6db74">&#34;configmaps&#34;</span>]
    <span style="color:#75715e"># This rule only applies to resources in the &#34;kube-system&#34; namespace.</span>
    <span style="color:#75715e"># The empty string &#34;&#34; can be used to select non-namespaced resources.</span>
    <span style="color:#f92672">namespaces</span>: [<span style="color:#e6db74">&#34;kube-system&#34;</span>]

  <span style="color:#75715e"># Log configmap and secret changes in all other namespaces at the Metadata level.</span>
  <span style="color:#75715e">#- level: Metadata</span>
  - <span style="color:#f92672">level</span>: <span style="color:#ae81ff">Request</span>
    <span style="color:#f92672">resources</span>:
    - <span style="color:#f92672">group</span>: <span style="color:#e6db74">&#34;&#34;</span> <span style="color:#75715e"># core API group</span>
      <span style="color:#f92672">resources</span>: [<span style="color:#e6db74">&#34;secrets&#34;</span>, <span style="color:#e6db74">&#34;configmaps&#34;</span>]

  <span style="color:#75715e"># Log all other resources in core and extensions at the Request level.</span>
  - <span style="color:#f92672">level</span>: <span style="color:#ae81ff">Request</span>
    <span style="color:#f92672">resources</span>:
    - <span style="color:#f92672">group</span>: <span style="color:#e6db74">&#34;&#34;</span> <span style="color:#75715e"># core API group</span>
    - <span style="color:#f92672">group</span>: <span style="color:#e6db74">&#34;extensions&#34;</span> <span style="color:#75715e"># Version of group should NOT be included.</span>

  <span style="color:#75715e"># A catch-all rule to log all other requests at the Metadata level.</span>
  - <span style="color:#f92672">level</span>: <span style="color:#ae81ff">Metadata</span>
    <span style="color:#75715e"># Long-running requests like watches that fall under this rule will not</span>
    <span style="color:#75715e"># generate an audit event in RequestReceived.</span>
    <span style="color:#f92672">omitStages</span>:
      - <span style="color:#e6db74">&#34;RequestReceived&#34;</span>
</code></pre></div><p>Then restart the api server:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">systemctl restart kubelet
</code></pre></div><h3 id="what-is-falco">What is Falco?</h3>
<p>OK we hawe a log file at <code>/var/log/kube-audit/audit.log</code>, but what can we do with it? We need a tool to monitor and alert based on the events in the audit log. This tool is Falco. Falco makes it possible to monitor suspicious events directly inside the cluster. The events may include the following:</p>
<ul>
<li>Outgoing connections to specific IPs or domains</li>
<li>Use or mutation of sensitive files such as /etc/passwd</li>
<li>Execution of system binaries such as su</li>
<li>Privilege escalation or changes to the namespace</li>
<li>Modifications in certain folders such as /sbin</li>
</ul>
<h3 id="falco-architecture">Falco architecture</h3>
<p><img src="/img/include/falco01.png" alt="alco architecture"  class="zoomable" /></p>
<p>From a high-level view, Falco is comprised of the following components:</p>
<ul>
<li>Event sources (drivers, Kubernetes audit events)</li>
<li>A rule engine and a rule set</li>
<li>An output system integration</li>
</ul>
<p>Falco uses so-called drivers to monitor syscalls made by applications at the kernel level; it can therefore monitor everything that results in a syscall. As containers share a kernel, it is possible to monitor syscalls by all the containers on a host. This is not possible in the case of more isolated container engines like Kata Containers or Firecracker. Falco supports two types of drivers: kernel module, eBPF probe:</p>
<ul>
<li>Kernel module (the default): A kernel module that must be compiled for the kernel that Falco will run on.</li>
<li>eBPF probe: No need to load a kernel module, but requires a newer kernel that supports eBPF. Not supported on many managed services.</li>
</ul>
<h3 id="install-falco">Install falco</h3>
<p>First we need to install the devel kernel headers to allow falco to build the kernel mosul that Falco use to get syscalls.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">apt-get -y install linux-headers-<span style="color:#66d9ef">$(</span>uname -r<span style="color:#66d9ef">)</span>
<span style="color:#75715e"># or</span>
yum -y install kernel-devel-<span style="color:#66d9ef">$(</span>uname -r<span style="color:#66d9ef">)</span>
</code></pre></div><p>We can install falco client az a package:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -s https://falco.org/repo/falcosecurity-3672BA8F.asc | apt-key add -
echo <span style="color:#e6db74">&#34;deb https://download.falco.org/packages/deb stable main&#34;</span> | tee -a /etc/apt/sources.list.d/falcosecurity.list
apt-get update -y
apt-get install -y falco
<span style="color:#75715e"># or</span>
rpm --import https://falco.org/repo/falcosecurity-3672BA8F.asc
curl -s -o /etc/yum.repos.d/falcosecurity.repo https://falco.org/repo/falcosecurity-rpm.repo
yum -y install falco

falco-driver-loader

service falco start
journalctl -fu falco

</code></pre></div><p>Or install as docker container:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">helm repo add falcosecurity https://falcosecurity.github.io/charts
helm repo update

helm upgrade --install falco falcosecurity/falco --namespace falco <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set falcosidekick.enabled<span style="color:#f92672">=</span>true <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set falcosidekick.webui.enabled<span style="color:#f92672">=</span>true <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set auditLog.enabled<span style="color:#f92672">=</span>true

helm ls
kubectl get pods --namespace falco
NAME                                      READY   STATUS    RESTARTS   AGE
falco-falcosidekick-76f5885f7f-956vj      1/1     Running   <span style="color:#ae81ff">0</span>          4m27s
falco-falcosidekick-76f5885f7f-tmff6      1/1     Running   <span style="color:#ae81ff">0</span>          4m27s
falco-falcosidekick-ui-5b64749bc8-k8v4p   1/1     Running   <span style="color:#ae81ff">0</span>          4m27s
falco-h4qvx 
</code></pre></div><p>I prefer this solution because it is more elegant. For the easier installation I created a helmfile:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cd /opt
git clone https://github.com/devopstales/k8s_sec_lab
cd k8s_sec_lab/k8s-manifest

kubectl apply -f 120-falco-ns.yaml
kubectl apply -f 122-falco.yaml
</code></pre></div><h3 id="processing-falco-logs-with-a-logging-system">Processing Falco logs with a logging system</h3>
<p>Falco provides support for a variety of output channels for generated alerts. These can include stdout, gRPC, syslog, a file, and more. In my exaple I used loki and alertmanager.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat 122-falco.yaml
...
spec:
  chart: falco
  repo: <span style="color:#e6db74">&#34;https://falcosecurity.github.io/charts&#34;</span>
  targetNamespace: falco-system
  valuesContent: |-
...
      config:
        loki:
          hostport: http://logging-loki.logging-system:3100
        customfields: <span style="color:#e6db74">&#34;source:falco&#34;</span>
        alertmanager:
          hostport: http://monitoring-kube-prometheus-alertmanager.monitoring-system:9093
          minimumpriority: error
          mutualtls: false
          checkcert: false
</code></pre></div><h3 id="gathering-audit-logs-by-using-fluentbit">Gathering Audit Logs by using FluentBit</h3>
<p>In order to deploy FluentBit, I created a helmfile:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f 123-falco-fluentbit.yaml
</code></pre></div><p>I use FluentBit to send the Kubernetes Kubernetes audit logs to falco.</p>
<h3 id="test">Test</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl --namespace<span style="color:#f92672">=</span>falco-system port-forward svc/falco-falcosidekick-ui <span style="color:#ae81ff">2802</span>
Forwarding from 127.0.0.1:2802 -&gt; <span style="color:#ae81ff">2802</span>
Forwarding from <span style="color:#f92672">[</span>::1<span style="color:#f92672">]</span>:2802 -&gt; <span style="color:#ae81ff">2802</span>
</code></pre></div><p>On the master node edit the <code>/etc/hosts</code> file:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano /etc/hosts
...
<span style="color:#75715e"># test</span>
</code></pre></div><p><img src="/img/include/falco02.png" alt="alco architecture"  class="zoomable" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes Hardening Guide with CISA 1.6 Benchmark]]></title>
            <link href="https://devopstales.github.io/home/k8s-cisa-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-cisa-install/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Hardening Guide with CISA 1.6 Benchmark" />
                <link href="https://devopstales.github.io/home/k8s-seccomp/?utm_source=atom_feed" rel="related" type="text/html" title="Hardening Kubernetes with seccomp" />
                <link href="https://devopstales.github.io/kubernetes/k8s-seccomp/?utm_source=atom_feed" rel="related" type="text/html" title="Hardening Kubernetes with seccomp" />
                <link href="https://devopstales.github.io/home/migrate-docker-to-containerd/?utm_source=atom_feed" rel="related" type="text/html" title="Migrate Kubernetes from docker to containerd" />
                <link href="https://devopstales.github.io/home/lazyimage/?utm_source=atom_feed" rel="related" type="text/html" title="Speed up docker pull with lazypull" />
            
                <id>https://devopstales.github.io/home/k8s-cisa-install/</id>
            
            
            <published>2021-10-15T00:00:00+00:00</published>
            <updated>2021-10-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>On August 3rd, 2021 the National Security Agency (NSA) and the Cybersecurity and Infrastructure Security Agency (CISA) released, <a href="https://media.defense.gov/2021/Aug/03/2002820425/-1/-1/1/CTR_KUBERNETES%20HARDENING%20GUIDANCE.PDF">Kubernetes Hardening Guidance</a>, a cybersecurity technical report detailing the complexities of securely managing Kubernetes. This blog post will show you how you can harden your Kubernetes cluster based on CISA&rsquo;s best practices.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<h3 id="disable-swap">Disable swap</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">free -h
swapoff -a
swapoff -a
sed -i.bak -r <span style="color:#e6db74">&#39;s/(.+ swap .+)/#\1/&#39;</span> /etc/fstab
free -h
</code></pre></div><h3 id="project-longhorn-prerequisites">Project Longhorn Prerequisites</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">yum install -y iscsi-initiator-utils 
modprobe iscsi_tcp
echo <span style="color:#e6db74">&#34;iscsi_tcp&#34;</span> &gt;/etc/modules-load.d/iscsi-tcp.conf
systemctl enable iscsid
systemctl start iscsid 
</code></pre></div><h3 id="install-and-configure-containerd">Install and configure containerd</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">yum install -y yum-utils device-mapper-persistent-data lvm2 git nano wget iproute-tc vim-common
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo

yum install -y containerd.io

<span style="color:#75715e">## Configure containerd</span>
sudo mkdir -p /etc/containerd
sudo containerd config default &gt; /etc/containerd/config.toml

nano /etc/containerd/config.toml
...
          <span style="color:#f92672">[</span>plugins.<span style="color:#e6db74">&#34;io.containerd.grpc.v1.cri&#34;</span>.containerd.runtimes.runc.options<span style="color:#f92672">]</span>
            SystemdCgroup <span style="color:#f92672">=</span> true

systemctl enable --now containerd
systemctl status containerd

echo <span style="color:#e6db74">&#34;runtime-endpoint: unix:///run/containerd/containerd.sock&#34;</span> &gt; /etc/crictl.yaml

cd /tmp
wget https://github.com/containerd/nerdctl/releases/download/v0.12.0/nerdctl-0.12.0-linux-amd64.tar.gz

tar -xzf nerdctl-0.12.0-linux-amd64.tar.gz
mv nerdctl /usr/local/bin
nerdctl ps
</code></pre></div><h3 id="kubaedm-preconfiguuration">kubaedm preConfiguuration</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat <span style="color:#e6db74">&lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf
</span><span style="color:#e6db74">overlay
</span><span style="color:#e6db74">br_netfilter
</span><span style="color:#e6db74">EOF</span>

sudo modprobe overlay
sudo modprobe br_netfilter

cat <span style="color:#e6db74">&lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
</span><span style="color:#e6db74">net.bridge.bridge-nf-call-iptables  = 1
</span><span style="color:#e6db74">net.ipv4.ip_forward                 = 1
</span><span style="color:#e6db74">net.bridge.bridge-nf-call-ip6tables = 1
</span><span style="color:#e6db74">#
</span><span style="color:#e6db74"># protectKernelDefaults
</span><span style="color:#e6db74">#
</span><span style="color:#e6db74">kernel.keys.root_maxbytes           = 25000000
</span><span style="color:#e6db74">kernel.keys.root_maxkeys            = 1000000
</span><span style="color:#e6db74">kernel.panic                        = 10
</span><span style="color:#e6db74">kernel.panic_on_oops                = 1
</span><span style="color:#e6db74">vm.overcommit_memory                = 1
</span><span style="color:#e6db74">vm.panic_on_oom                     = 0
</span><span style="color:#e6db74">EOF</span>

sysctl --system
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cd /opt
git clone https://github.com/devopstales/k8s_sec_lab

mkdir /etc/kubernetes/

head -c <span style="color:#ae81ff">32</span> /dev/urandom | base64
nano /opt/k8s_sec_lab/k8s-manifest/002-etcd-encription.yaml

cp /opt/k8s_sec_lab/k8s-manifest/001-audit-policy.yaml /etc/kubernetes/audit-policy.yaml
cp /opt/k8s_sec_lab/k8s-manifest/002-etcd-encription.yaml /etc/kubernetes/etcd-encription.yaml
</code></pre></div><h3 id="install-kubeadm">Install kubeadm</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat <span style="color:#e6db74">&lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
</span><span style="color:#e6db74">[kubernetes]
</span><span style="color:#e6db74">name=Kubernetes
</span><span style="color:#e6db74">baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
</span><span style="color:#e6db74">enabled=1
</span><span style="color:#e6db74">gpgcheck=1
</span><span style="color:#e6db74">repo_gpgcheck=1
</span><span style="color:#e6db74">gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
</span><span style="color:#e6db74">       https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
</span><span style="color:#e6db74">EOF</span>


yum install epel-release

yum install -y kubeadm kubelet kubectl

echo <span style="color:#e6db74">&#39;KUBELET_KUBEADM_ARGS=&#34;--node-ip=172.17.13.10 --container-runtime=remote --container-runtime-endpoint=/run/containerd/containerd.sock&#34;&#39;</span> &gt; /etc/sysconfig/kubelet

kubeadm config images pull --config 010-kubeadm-conf-1-22-2.yaml

systemctl enable kubelet.service

nano 010-kubeadm-conf.yaml
<span style="color:#75715e"># add the ips, short hostnames, fqdns of the nodes and the ip of the loadbalancer as certSANs to the apiServer config.</span>

kubeadm init --skip-phases<span style="color:#f92672">=</span>addon/kube-proxy --config 010-kubeadm-conf-1-22-2.yaml

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown <span style="color:#66d9ef">$(</span>id -u<span style="color:#66d9ef">)</span>:<span style="color:#66d9ef">$(</span>id -g<span style="color:#66d9ef">)</span> $HOME/.kube/config

<span style="color:#75715e"># In my kubeadm config I forced the usage of PSP.</span>
<span style="color:#75715e"># At the beginning there is no psp deployed, so non of the pods can start.</span>
<span style="color:#75715e"># Thi is tru for the kube-apiserver too.</span>

kubectl apply -f 011-psp.yaml 
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get csr --all-namespaces

kubectl get csr -oname | xargs kubectl certificate approve

kubectl apply -f 012-k8s-clusterrole.yaml
</code></pre></div><h3 id="cilium">cilium</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">mount | grep /sys/fs/bpf
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">yum install -y https://harbottle.gitlab.io/harbottle-main/7/x86_64/harbottle-main-release.rpm
yum install -y kubectx

dnf copr enable cerenit/helm -y
yum install -y helm

helm repo add cilium https://helm.cilium.io/

kubectl taint nodes --all node-role.kubernetes.io/master-


helm upgrade --install cilium cilium/cilium <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --namespace kube-system <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  -f 031-cilium-helm-values.yaml

kubectl get pods -A

kubectl apply -f 013-k8s-cert-approver.yaml
</code></pre></div><h3 id="harden-kubernetes">harden kubernetes</h3>
<p>There is an opensource tool theat tests CISA&rsquo;s best best practices on your clsuter. We vill use this to test the resoults.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># kube-bench</span>
<span style="color:#75715e"># https://github.com/aquasecurity/kube-bench/releases/</span>
yum install -y https://github.com/aquasecurity/kube-bench/releases/download/v0.6.5/kube-bench_0.6.5_linux_amd64.rpm
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">useradd -r -c <span style="color:#e6db74">&#34;etcd user&#34;</span> -s /sbin/nologin -M etcd
chown etcd:etcd /var/lib/etcd
chmod <span style="color:#ae81ff">700</span> /var/lib/etcd

<span style="color:#75715e"># kube-bench</span>
kube-bench
kube-bench | grep <span style="color:#e6db74">&#34;\[FAIL\]&#34;</span>
</code></pre></div><p>There is no FAIL jusk WARNING. Jeee.</p>
<h3 id="join-nodes">join nodes</h3>
<p>Firs we need to get the join command from the master:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># master1</span>
kubeadm token create --print-join-command
kubeadm join 172.17.9.10:6443 --token c2t0rj.cofbfnwwrb387890 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span> --discovery-token-ca-cert-hash sha256:a52f4c16a6ce9ef72e3d6172611d17d9752dfb1c3870cf7c8ad4ce3bcb97547e
</code></pre></div><p>If the next node is a worke we can just use the command what we get. If a next node is a master we need to generate a certificate-key. You need a separate certificate-key for every new master.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># master1</span>
kubeadm init phase upload-certs --upload-certs
<span style="color:#f92672">[</span>upload-certs<span style="color:#f92672">]</span> Using certificate key:
29ab8a6013od73s8d3g4ba3a3b24679693e98acd796356eeb47df098c47f2773
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># master2</span>
kubeadm join 172.17.9.10:6443 --token c2t0rj.cofbfnwwrb387890 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--discovery-token-ca-cert-hash sha256:a52f4c16a6ce9ef72e3d6172611d17d9752dfb1c3870cf7c8ad4ce3bcb97547e <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--control-plane --certificate-key 29ab8a6013od73s8d3g4ba3a3b24679693e98acd796356eeb47df098c47f2773
</code></pre></div><p>In the end withevery new node we need to approve the certificate requests for the node.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get csr -oname | xargs kubectl certificate approve

useradd -r -c <span style="color:#e6db74">&#34;etcd user&#34;</span> -s /sbin/nologin -M etcd
chown etcd:etcd /var/lib/etcd
chmod <span style="color:#ae81ff">700</span> /var/lib/etcd
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[trivy-operator 1.0]]></title>
            <link href="https://devopstales.github.io/home/trivy-operator-1.0/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/trivy-operator-1.0/?utm_source=atom_feed" rel="related" type="text/html" title="trivy-operator 1.0" />
                <link href="https://devopstales.github.io/home/cilium-clustermesh/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Multicluster with Cilium Cluster Mesh" />
                <link href="https://devopstales.github.io/home/k8s-seccomp/?utm_source=atom_feed" rel="related" type="text/html" title="Hardening Kubernetes with seccomp" />
                <link href="https://devopstales.github.io/home/k8s-git-backup/?utm_source=atom_feed" rel="related" type="text/html" title="How to Backup Kubernetes to git?" />
                <link href="https://devopstales.github.io/home/firecracker-cri-o/?utm_source=atom_feed" rel="related" type="text/html" title="How to deploy CRI-O with Firecracker?" />
            
                <id>https://devopstales.github.io/home/trivy-operator-1.0/</id>
            
            
            <published>2021-10-09T00:00:00+00:00</published>
            <updated>2021-10-09T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Today I happy to announce the release of trivy-operator 1.0 and assign the first ever stable release number. This blog post focuses on the functionality provided by the trivy-operator 1.0 release.</p>
<h3 id="what-is-trivy-operator">What is trivy-operator?</h3>
<p>Trivy-operator is a Kubernetes Operator based on the open-source container vulnerability scanner Trivy. The goal of this project is to provide a vulnerability scanner that continuously scans containers deployed in a Kubernetes cluster. <a href="https://github.com/nolar/kopf">Built with Kubernetes Operator Pythonic Framework (Kopf)</a> There are a few solution for checking the images when you deploy them to the Kubernetes cluster, but fighting against vulnerabilities is a day to day task. Check once is not enough when every day is a new das for frats. That is why I created trivy-operator so you can create scheduled image scans on your running pods.</p>
<h3 id="scheduled-image-scans">Scheduled Image scans</h3>
<p>Default trivy-operator execute a scan script every 5 minutes. It will get images from all the namespaces with the label <code>trivy-scan=true</code>, and then check these images with trivy for vulnerabilities. You can modify the namespace selector. Finally we will get metrics on <code>http://[pod-ip]:9115/metrics</code></p>
<h2 id="usage">Usage</h2>
<p>To ease deployment I created a helm chart for trivy-operator.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">helm repo add devopstales https://devopstales.github.io/helm-charts
helm repo update
</code></pre></div><p>Create a value file for deploy:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat &lt;&lt;&#39;EOF&#39;&gt; values.yaml</span>
<span style="color:#f92672">image</span>:
  <span style="color:#f92672">repository</span>: <span style="color:#ae81ff">devopstales/trivy-operator</span>
  <span style="color:#f92672">pullPolicy</span>: <span style="color:#ae81ff">Always</span>
  <span style="color:#f92672">tag</span>: <span style="color:#e6db74">&#34;1.0&#34;</span>

<span style="color:#f92672">imagePullSecrets</span>: []
<span style="color:#f92672">podSecurityContext</span>:
  <span style="color:#f92672">fsGroup</span>: <span style="color:#ae81ff">10001</span>
  <span style="color:#f92672">fsGroupChangePolicy</span>: <span style="color:#e6db74">&#34;OnRootMismatch&#34;</span>

<span style="color:#f92672">serviceAccount</span>:
  <span style="color:#f92672">create</span>: <span style="color:#66d9ef">true</span>
  <span style="color:#f92672">annotations</span>: {}
  <span style="color:#f92672">name</span>: <span style="color:#e6db74">&#34;trivy-operator&#34;</span>

<span style="color:#f92672">monitoring</span>:
  <span style="color:#f92672">port</span>: <span style="color:#e6db74">&#34;9115&#34;</span>

<span style="color:#f92672">serviceMonitor</span>:
  <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">false</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#e6db74">&#34;kube-system&#34;</span>

<span style="color:#f92672">storage</span>:
  <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">true</span>
  <span style="color:#f92672">size</span>: <span style="color:#ae81ff">1Gi</span>

<span style="color:#f92672">NamespaceScanner</span>:
  <span style="color:#f92672">crontab</span>: <span style="color:#e6db74">&#34;*/5 * * * *&#34;</span>
  <span style="color:#f92672">namespaceSelector</span>: <span style="color:#e6db74">&#34;trivy-scan&#34;</span>

<span style="color:#f92672">registryAuth</span>:
  <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">false</span>
  <span style="color:#f92672">registry</span>:
  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">docker.io</span>
    <span style="color:#f92672">user</span>: <span style="color:#e6db74">&#34;user&#34;</span>
    <span style="color:#f92672">password</span>: <span style="color:#e6db74">&#34;password&#34;</span>

<span style="color:#f92672">githubToken</span>:
  <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">false</span>
  <span style="color:#f92672">token</span>: <span style="color:#e6db74">&#34;&#34;</span>
<span style="color:#ae81ff">EOF</span>
</code></pre></div><p>When the trivy in the container want to scan an image first download the vulnerability database from github. If you test many images you need a  <code>githubToken</code> overcome the github rate limit and dockerhub username and password for overcome the dockerhub rate limit. If your store you images in a private repository you need to add an username and password for authentication.</p>
<p>The following tables lists configurable parameters of the trivy-operator chart and their default values.</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>image.repository</td>
<td>image</td>
<td>devopstales/trivy-operator</td>
</tr>
<tr>
<td>image.pullPolicy</td>
<td>pullPolicy</td>
<td>Always</td>
</tr>
<tr>
<td>image.tag</td>
<td>image tag</td>
<td>1.0</td>
</tr>
<tr>
<td>imagePullSecrets</td>
<td>imagePullSecrets list</td>
<td>[]</td>
</tr>
<tr>
<td>podSecurityContext.fsGroup</td>
<td>mount id</td>
<td>10001</td>
</tr>
<tr>
<td>serviceAccount.create</td>
<td>create serviceAccount</td>
<td>true</td>
</tr>
<tr>
<td>serviceAccount.annotations</td>
<td>add annotation to serviceAccount</td>
<td>{}</td>
</tr>
<tr>
<td>serviceAccount.name</td>
<td>name of the serviceAccount</td>
<td>trivy-operator</td>
</tr>
<tr>
<td>monitoring.port</td>
<td>prometheus endpoint port</td>
<td>9115</td>
</tr>
<tr>
<td>serviceMonitor.enabled</td>
<td>enable serviceMonitor object creation</td>
<td>false</td>
</tr>
<tr>
<td>serviceMonitor.namespace</td>
<td>where to create serviceMonitor object</td>
<td>kube-system</td>
</tr>
<tr>
<td>storage.enabled</td>
<td>enable pv to store trivy database</td>
<td>true</td>
</tr>
<tr>
<td>storage.size</td>
<td>pv size</td>
<td>1Gi</td>
</tr>
<tr>
<td>NamespaceScanner.crontab</td>
<td>cronjob scheduler</td>
<td>&ldquo;*/5 * * * *&rdquo;</td>
</tr>
<tr>
<td>NamespaceScanner.namespaceSelector</td>
<td>Namespace Selector</td>
<td>&ldquo;trivy-scan&rdquo;</td>
</tr>
<tr>
<td>registryAuth.enabled</td>
<td>enable registry authentication in operator</td>
<td>false</td>
</tr>
<tr>
<td>registryAuth.registry</td>
<td>registry name for authentication</td>
<td></td>
</tr>
<tr>
<td>registryAuth.user</td>
<td>username for authentication</td>
<td></td>
</tr>
<tr>
<td>registryAuth.password</td>
<td>password for authentication</td>
<td></td>
</tr>
<tr>
<td>githubToken.enabled</td>
<td>Enable githubToken usage for trivy database update</td>
<td>false</td>
</tr>
<tr>
<td>githubToken.token</td>
<td>githubToken value</td>
<td>&quot;&quot;</td>
</tr>
</tbody>
</table>
<h3 id="monitoring">Monitoring</h3>
<p>Trivy-operatos has a prometheus endpoint op port <code>9115</code> and can be deployed wit <code>ServiceMonitor</code> for automated scrapping.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -s http://10.43.179.39:9115/metrics | grep so_vulnerabilities

so_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/library/nginx:1.18&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;UNKNOWN&#34;</span><span style="color:#f92672">}</span> <span style="color:#ae81ff">0</span>
so_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/library/nginx:1.18&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;LOW&#34;</span><span style="color:#f92672">}</span> <span style="color:#ae81ff">23</span>
so_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/library/nginx:1.18&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;MEDIUM&#34;</span><span style="color:#f92672">}</span> <span style="color:#ae81ff">93</span>
so_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/library/nginx:1.18&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;HIGH&#34;</span><span style="color:#f92672">}</span> <span style="color:#ae81ff">76</span>
so_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/library/nginx:1.18&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;CRITICAL&#34;</span><span style="color:#f92672">}</span> <span style="color:#ae81ff">25</span>
so_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/library/nginx:latest&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;UNKNOWN&#34;</span><span style="color:#f92672">}</span> <span style="color:#ae81ff">0</span>
so_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/library/nginx:latest&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;LOW&#34;</span><span style="color:#f92672">}</span> <span style="color:#ae81ff">23</span>
so_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/library/nginx:latest&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;MEDIUM&#34;</span><span style="color:#f92672">}</span> <span style="color:#ae81ff">88</span>
so_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/library/nginx:latest&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;HIGH&#34;</span><span style="color:#f92672">}</span> <span style="color:#ae81ff">60</span>
so_vulnerabilities<span style="color:#f92672">{</span>exported_namespace<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trivytest&#34;</span>,image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;docker.io/library/nginx:latest&#34;</span>,severity<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;CRITICAL&#34;</span><span style="color:#f92672">}</span> <span style="color:#ae81ff">8</span>

</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes Multicluster with Cilium Cluster Mesh]]></title>
            <link href="https://devopstales.github.io/home/cilium-clustermesh/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/cilium-clustermesh/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Multicluster with Cilium Cluster Mesh" />
                <link href="https://devopstales.github.io/home/cilium-opnsense-bgp/?utm_source=atom_feed" rel="related" type="text/html" title="Use Cilium BGP integration with OPNsense" />
                <link href="https://devopstales.github.io/home/k8s-metallb-bgp-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Self-hosted Load Balancer for bare metal Kubernetes" />
                <link href="https://devopstales.github.io/kubernetes/cilium-opnsense-bgp/?utm_source=atom_feed" rel="related" type="text/html" title="Use Cilium BGP integration with OPNsense" />
                <link href="https://devopstales.github.io/kubernetes/k8s-metallb-bgp-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Self-hosted Load Balancer for bare metal Kubernetes" />
            
                <id>https://devopstales.github.io/home/cilium-clustermesh/</id>
            
            
            <published>2021-10-06T00:00:00+00:00</published>
            <updated>2021-10-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this tutorial I will show you how to install Cilium on multiple Kubernetes clusters and connect those clusters with Cluster Mesh.</p>
<h3 id="what-is-cluster-mesh">What is Cluster Mesh</h3>
<p>Cluster mesh extends the networking datapath across multiple clusters. It allows endpoints in all connected clusters to communicate while providing full policy enforcement. Load-balancing is available via Kubernetes annotations.</p>
<h3 id="the-infrastructure">The infrastructure</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">k3s-cl1:
ip: 172.17.11.11

k3s-cl2:
ip: 172.17.11.12
etcd
kube-vip

k3s-cl3:
ip: 172.17.11.13
etcd
kube-vip
</code></pre></div><h2 id="installing-k3s-with-k3sup">Installing k3s with k3sup</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ssh-copy-id vagrant@172.17.11.11
ssh-copy-id vagrant@172.17.11.12
ssh-copy-id vagrant@172.17.11.13
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">tmux-cssh vagrant@172.17.11.11 vagrant@172.17.11.12 vagrant@172.17.11.13
sudo su -
curl -sLS https://get.k3sup.dev | sh
sudo install k3sup /usr/local/bin/
k3sup --help
</code></pre></div><h3 id="bootstrap-cl1">Bootstrap cl1</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ssh vagrant@172.17.11.11
sudo su -

k3sup install <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --local <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --ip<span style="color:#f92672">=</span>172.17.11.11 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --cluster <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-channel<span style="color:#f92672">=</span>stable <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-extra-args <span style="color:#e6db74">&#34;--flannel-backend=none --cluster-cidr=10.11.0.0/16 --disable-network-policy --no-deploy=traefik --no-deploy=servicelb --node-ip=172.17.11.11&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --merge <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --local-path $HOME/.kube/config <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --context<span style="color:#f92672">=</span>k3scl1

exit
exit
</code></pre></div><h3 id="bootstrap-cl2">Bootstrap cl2</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ssh vagrant@172.17.11.12
sudo su -

k3sup install <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --local <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --ip<span style="color:#f92672">=</span>172.17.11.12 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --cluster <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-channel<span style="color:#f92672">=</span>stable <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-extra-args <span style="color:#e6db74">&#34;--flannel-backend=none --cluster-cidr=10.12.0.0/16 --disable-network-policy --no-deploy=traefik --no-deploy=servicelb --node-ip=172.17.11.12&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --merge <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --local-path $HOME/.kube/config <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --context<span style="color:#f92672">=</span>k3scl2

exit
exit
</code></pre></div><h3 id="bootstrap-cl3">Bootstrap cl3</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ssh vagrant@172.17.11.13
sudo su -

k3sup install <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --local <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --ip<span style="color:#f92672">=</span>172.17.11.13 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --cluster <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-channel<span style="color:#f92672">=</span>stable <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-extra-args <span style="color:#e6db74">&#34;--flannel-backend=none --cluster-cidr=10.13.0.0/16 --disable-network-policy --no-deploy=traefik --no-deploy=servicelb --node-ip=172.17.11.13&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --merge <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --local-path $HOME/.kube/config <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --context<span style="color:#f92672">=</span>k3scl3

exit
exit
</code></pre></div><h2 id="deploy-cilium">Deploy cilium</h2>
<p>Each Kubernetes cluster maintains its own etcd cluster which contains the state of that cluster&rsquo;s cilium. State from multiple clusters is never mixed in etcd itself. Each cluster exposes its own etcd via a set of etcd proxies. In this demo I will use NodePort service. Cilium agents running in other clusters connect to the etcd proxies to watch for changes and replicate the multi-cluster relevant state into their own cluster. Use of etcd proxies ensures scalability of etcd watchers. Access is protected with TLS certificates.</p>
<p><img src="/img/include/clustermesh-arch.png" alt="Cilium clustermesh"  class="zoomable" /></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ssh vagrant@172.17.11.11
sudo su -

helm repo add cilium https://helm.cilium.io/
helm repo update
helm upgrade --install cilium cilium/cilium <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set operator.replicas<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set cluster.id<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set cluster.name<span style="color:#f92672">=</span>k3scl1 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set tunnel<span style="color:#f92672">=</span>vxlan <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set kubeProxyReplacement<span style="color:#f92672">=</span>strict <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set containerRuntime.integration<span style="color:#f92672">=</span>containerd <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set etcd.enabled<span style="color:#f92672">=</span>true <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set etcd.managed<span style="color:#f92672">=</span>true <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set k8sServiceHost<span style="color:#f92672">=</span>10.0.2.15 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set k8sServicePort<span style="color:#f92672">=</span><span style="color:#ae81ff">6443</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>-n kube-system

cat <span style="color:#e6db74">&lt;&lt;EOF | kubectl -n kube-system apply -f -
</span><span style="color:#e6db74">apiVersion: v1
</span><span style="color:#e6db74">kind: Service
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: cilium-etcd-external
</span><span style="color:#e6db74">spec:
</span><span style="color:#e6db74">  type: NodePort
</span><span style="color:#e6db74">  ports:
</span><span style="color:#e6db74">  - port: 2379
</span><span style="color:#e6db74">  selector:
</span><span style="color:#e6db74">    app: etcd
</span><span style="color:#e6db74">    etcd_cluster: cilium-etcd
</span><span style="color:#e6db74">    io.cilium/app: etcd-operator
</span><span style="color:#e6db74">EOF</span>

exit
exit
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ssh vagrant@172.17.11.12
sudo su -

helm repo add cilium https://helm.cilium.io/
helm repo update
helm upgrade --install cilium cilium/cilium <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set operator.replicas<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set cluster.id<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set cluster.name<span style="color:#f92672">=</span>k3scl2 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set tunnel<span style="color:#f92672">=</span>vxlan <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set containerRuntime.integration<span style="color:#f92672">=</span>containerd <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set etcd.enabled<span style="color:#f92672">=</span>true <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set etcd.managed<span style="color:#f92672">=</span>true <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set k8sServiceHost<span style="color:#f92672">=</span>10.0.2.15 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set k8sServicePort<span style="color:#f92672">=</span><span style="color:#ae81ff">6443</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>-n kube-system

cat <span style="color:#e6db74">&lt;&lt;EOF | kubectl -n kube-system apply -f -
</span><span style="color:#e6db74">apiVersion: v1
</span><span style="color:#e6db74">kind: Service
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: cilium-etcd-external
</span><span style="color:#e6db74">spec:
</span><span style="color:#e6db74">  type: NodePort
</span><span style="color:#e6db74">  ports:
</span><span style="color:#e6db74">  - port: 2379
</span><span style="color:#e6db74">  selector:
</span><span style="color:#e6db74">    app: etcd
</span><span style="color:#e6db74">    etcd_cluster: cilium-etcd
</span><span style="color:#e6db74">    io.cilium/app: etcd-operator
</span><span style="color:#e6db74">EOF</span>

exit
exit
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ssh vagrant@172.17.11.13
sudo su -

helm repo add cilium https://helm.cilium.io/
helm repo update
helm upgrade --install cilium cilium/cilium <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set operator.replicas<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set cluster.id<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set cluster.name<span style="color:#f92672">=</span>k3scl3 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set tunnel<span style="color:#f92672">=</span>vxlan <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set containerRuntime.integration<span style="color:#f92672">=</span>containerd <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set etcd.enabled<span style="color:#f92672">=</span>true <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set etcd.managed<span style="color:#f92672">=</span>true <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set k8sServiceHost<span style="color:#f92672">=</span>10.0.2.15 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set k8sServicePort<span style="color:#f92672">=</span><span style="color:#ae81ff">6443</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>-n kube-system

cat <span style="color:#e6db74">&lt;&lt;EOF | kubectl -n kube-system apply -f -
</span><span style="color:#e6db74">apiVersion: v1
</span><span style="color:#e6db74">kind: Service
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: cilium-etcd-external
</span><span style="color:#e6db74">spec:
</span><span style="color:#e6db74">  type: NodePort
</span><span style="color:#e6db74">  ports:
</span><span style="color:#e6db74">  - port: 2379
</span><span style="color:#e6db74">  selector:
</span><span style="color:#e6db74">    app: etcd
</span><span style="color:#e6db74">    etcd_cluster: cilium-etcd
</span><span style="color:#e6db74">    io.cilium/app: etcd-operator
</span><span style="color:#e6db74">EOF</span>

exit
exit
</code></pre></div><h3 id="configure-cluster-mesh">Configure cluster mesh</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ssh vagrant@172.17.11.11
sudo su -

cd /tmp
git clone https://github.com/cilium/clustermesh-tools.git
cd clustermesh-tools

./extract-etcd-secrets.sh 
Derived cluster-name k3scl1 from present ConfigMap
<span style="color:#f92672">====================================================</span>
 WARNING: The directory config contains private keys.
          Delete after use.
<span style="color:#f92672">====================================================</span>

exit
exit
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ssh vagrant@172.17.11.12
sudo su -

cd /tmp
git clone https://github.com/cilium/clustermesh-tools.git
cd clustermesh-tools

./extract-etcd-secrets.sh 
Derived cluster-name k3scl2 from present ConfigMap
<span style="color:#f92672">====================================================</span>
 WARNING: The directory config contains private keys.
          Delete after use.
<span style="color:#f92672">====================================================</span>

scp -r config/ 172.17.11.11:/tmp/clustermesh-tools/

exit
exit
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ssh vagrant@172.17.11.13
sudo su -

cd /tmp
git clone https://github.com/cilium/clustermesh-tools.git
cd clustermesh-tools

./extract-etcd-secrets.sh 
Derived cluster-name k3scl3 from present ConfigMap
<span style="color:#f92672">====================================================</span>
 WARNING: The directory config contains private keys.
          Delete after use.
<span style="color:#f92672">====================================================</span>

scp -r config/ 172.17.11.11:/tmp/clustermesh-tools/

exit
exit
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ssh vagrant@172.17.11.11
sudo su -

cd /tmp/clustermesh-tools/
./generate-secret-yaml.sh &gt; clustermesh.yaml
./generate-name-mapping.sh &gt; ds.patch

scp clustermesh.yaml ds.patch 172.17.11.12:
scp clustermesh.yaml ds.patch 172.17.11.13:

kubectl -n kube-system patch ds cilium -p <span style="color:#e6db74">&#34;</span><span style="color:#66d9ef">$(</span>cat ds.patch<span style="color:#66d9ef">)</span><span style="color:#e6db74">&#34;</span>
kubectl -n kube-system apply -f clustermesh.yaml
kubectl -n kube-system delete pod -l k8s-app<span style="color:#f92672">=</span>cilium
kubectl -n kube-system delete pod -l name<span style="color:#f92672">=</span>cilium-operator

ssh 172.17.11.12 <span style="color:#e6db74">&#39;
</span><span style="color:#e6db74">  kubectl -n kube-system patch ds cilium -p &#34;$(cat ds.patch)&#34;
</span><span style="color:#e6db74">  kubectl -n kube-system apply -f clustermesh.yaml
</span><span style="color:#e6db74">  kubectl -n kube-system delete pod -l k8s-app=cilium
</span><span style="color:#e6db74">  kubectl -n kube-system delete pod -l name=cilium-operator
</span><span style="color:#e6db74">&#39;</span>

ssh 172.17.11.13 <span style="color:#e6db74">&#39;
</span><span style="color:#e6db74">  kubectl -n kube-system patch ds cilium -p &#34;$(cat ds.patch)&#34;
</span><span style="color:#e6db74">  kubectl -n kube-system apply -f clustermesh.yaml
</span><span style="color:#e6db74">  kubectl -n kube-system delete pod -l k8s-app=cilium
</span><span style="color:#e6db74">  kubectl -n kube-system delete pod -l name=cilium-operator
</span><span style="color:#e6db74">&#39;</span>
</code></pre></div><p>Verify the cluster mesh by dumping the node list from any cilium. It should show all nodes in both the clusters.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get po -l k8s-app<span style="color:#f92672">=</span>cilium
NAME           READY   STATUS    RESTARTS   AGE
cilium-6z8zf   1/1     Running   <span style="color:#ae81ff">0</span>          3m54s

kubectl -n kube-system exec -ti cilium-6z8zf -- cilium node list
 
Defaulted container <span style="color:#e6db74">&#34;cilium-agent&#34;</span> out of: cilium-agent, mount-cgroup <span style="color:#f92672">(</span>init<span style="color:#f92672">)</span>, clean-cilium-state <span style="color:#f92672">(</span>init<span style="color:#f92672">)</span>
Name           IPv4 Address   Endpoint CIDR   IPv6 Address   Endpoint CIDR
k3scl1/k3s01   172.17.11.11   10.0.0.0/24                    
k3scl2/k3s02   172.17.11.12   10.0.0.0/24                    
k3scl3/k3s03   172.17.11.13   10.0.0.0/24      
</code></pre></div><h3 id="test-the-connection">Test the connection</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">ssh vagrant@172.17.11.11</span>
<span style="color:#ae81ff">sudo su -</span>

<span style="color:#ae81ff">kubectl create ns test</span>
<span style="color:#ae81ff">kubens test</span>

<span style="color:#ae81ff">cat &lt;&lt;EOF | kubectl apply -f -</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">apps/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Deployment</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">nginx</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">test</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">replicas</span>: <span style="color:#ae81ff">2</span>
  <span style="color:#f92672">selector</span>:
    <span style="color:#f92672">matchLabels</span>:
      <span style="color:#f92672">app</span>: <span style="color:#ae81ff">nginx</span>
  <span style="color:#f92672">template</span>:
    <span style="color:#f92672">metadata</span>:
      <span style="color:#f92672">labels</span>:
        <span style="color:#f92672">app</span>: <span style="color:#ae81ff">nginx</span>
    <span style="color:#f92672">spec</span>:
      <span style="color:#f92672">containers</span>:
      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">nginx</span>
        <span style="color:#f92672">image</span>: <span style="color:#ae81ff">nginx:1</span>
        <span style="color:#f92672">ports</span>:
        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">http</span>
          <span style="color:#f92672">containerPort</span>: <span style="color:#ae81ff">80</span>
<span style="color:#ae81ff">EOF</span>
</code></pre></div><p>The service discovery of Cilium&rsquo;s multi-cluster model is built using standard Kubernetes services and designed to be completely transparent to existing Kubernetes application deployments:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat &lt;&lt;EOF | kubectl apply -f -</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Service</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">nginx-global</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">test</span>
  <span style="color:#f92672">annotations</span>:
    <span style="color:#f92672">io.cilium/global-service</span>: <span style="color:#e6db74">&#34;true&#34;</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">ports</span>:
  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">http</span>
    <span style="color:#f92672">port</span>: <span style="color:#ae81ff">80</span>
    <span style="color:#f92672">protocol</span>: <span style="color:#ae81ff">TCP</span>
    <span style="color:#f92672">targetPort</span>: <span style="color:#ae81ff">80</span>
  <span style="color:#f92672">selector</span>:
    <span style="color:#f92672">app</span>: <span style="color:#ae81ff">nginx</span>
<span style="color:#ae81ff">EOF</span>
</code></pre></div><p>Cilium monitors Kubernetes services and endpoints and watches for services with an annotation <code>io.cilium/global-service: &quot;true&quot;</code>. For such services, all services with identical name and namespace information are automatically merged together and form a global service that is available across clusters.</p>
<h3 id="test-cl1">Test cl1</h3>
<p>After you deploy the global service to both of the clusters, you can test the connection:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl -n kube-system exec -ti cilium-6z8zf -- cilium service list 
Defaulted container <span style="color:#e6db74">&#34;cilium-agent&#34;</span> out of: cilium-agent, mount-cgroup <span style="color:#f92672">(</span>init<span style="color:#f92672">)</span>, clean-cilium-state <span style="color:#f92672">(</span>init<span style="color:#f92672">)</span>
ID   Frontend             Service Type   Backend                  
...
<span style="color:#ae81ff">5</span>    10.43.200.118:80     ClusterIP      1 <span style="color:#f92672">=</span>&gt; 10.0.0.145:80       
                                         2 <span style="color:#f92672">=</span>&gt; 10.0.0.69:80        
...

kubectl -n kube-system exec -ti cilium-6z8zf -- cilium bpf lb list 
Defaulted container <span style="color:#e6db74">&#34;cilium-agent&#34;</span> out of: cilium-agent, mount-cgroup <span style="color:#f92672">(</span>init<span style="color:#f92672">)</span>, clean-cilium-state <span style="color:#f92672">(</span>init<span style="color:#f92672">)</span>
SERVICE ADDRESS      BACKEND ADDRESS
...
10.43.200.118:80     10.0.0.69:80 <span style="color:#f92672">(</span>5<span style="color:#f92672">)</span>                          
                     10.0.0.145:80 <span style="color:#f92672">(</span>5<span style="color:#f92672">)</span>                         
...
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl run -it --rm --image<span style="color:#f92672">=</span>tianon/network-toolbox debian
curl nginx-global
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
html <span style="color:#f92672">{</span> color-scheme: light dark; <span style="color:#f92672">}</span>
body <span style="color:#f92672">{</span> width: 35em; margin: <span style="color:#ae81ff">0</span> auto;
font-family: Tahoma, Verdana, Arial, sans-serif; <span style="color:#f92672">}</span>
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;http://nginx.org/&#34;</span>&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;http://nginx.com/&#34;</span>&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you <span style="color:#66d9ef">for</span> using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre></div><h3 id="test-on-cl3">Test on cl3:</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ssh 172.17.11.12

kubectl create ns test
kubens test

cat <span style="color:#e6db74">&lt;&lt;EOF | kubectl apply -f -
</span><span style="color:#e6db74">apiVersion: v1
</span><span style="color:#e6db74">kind: Service
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: nginx-global
</span><span style="color:#e6db74">  namespace: test
</span><span style="color:#e6db74">  annotations:
</span><span style="color:#e6db74">    io.cilium/global-service: &#34;true&#34;
</span><span style="color:#e6db74">spec:
</span><span style="color:#e6db74">  ports:
</span><span style="color:#e6db74">  - name: http
</span><span style="color:#e6db74">    port: 80
</span><span style="color:#e6db74">    protocol: TCP
</span><span style="color:#e6db74">    targetPort: 80
</span><span style="color:#e6db74">  selector:
</span><span style="color:#e6db74">    app: nginx
</span><span style="color:#e6db74">EOF</span>

kubectl -n kube-system exec -ti cilium-8wzrc -- cilium service list
Defaulted container <span style="color:#e6db74">&#34;cilium-agent&#34;</span> out of: cilium-agent, mount-cgroup <span style="color:#f92672">(</span>init<span style="color:#f92672">)</span>, clean-cilium-state <span style="color:#f92672">(</span>init<span style="color:#f92672">)</span>
ID   Frontend            Service Type   Backend                  
...
<span style="color:#ae81ff">8</span>    10.43.189.53:80     ClusterIP      1 <span style="color:#f92672">=</span>&gt; 10.0.0.69:80        
                                        2 <span style="color:#f92672">=</span>&gt; 10.0.0.145:80
...

kubectl -n kube-system exec -ti cilium-8wzrc -- cilium bpf lb list
Defaulted container <span style="color:#e6db74">&#34;cilium-agent&#34;</span> out of: cilium-agent, mount-cgroup <span style="color:#f92672">(</span>init<span style="color:#f92672">)</span>, clean-cilium-state <span style="color:#f92672">(</span>init<span style="color:#f92672">)</span>
SERVICE ADDRESS     BACKEND ADDRESS
...
10.43.189.53:80     0.0.0.0:0 <span style="color:#f92672">(</span>8<span style="color:#f92672">)</span> <span style="color:#f92672">[</span>ClusterIP, non-routable<span style="color:#f92672">]</span>   
                    10.0.0.69:80 <span style="color:#f92672">(</span>8<span style="color:#f92672">)</span>                          
                    10.0.0.145:80 <span style="color:#f92672">(</span>8<span style="color:#f92672">)</span>                    
...

kubectl run -it --rm --image<span style="color:#f92672">=</span>tianon/network-toolbox debian
curl nginx-global
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
html <span style="color:#f92672">{</span> color-scheme: light dark; <span style="color:#f92672">}</span>
body <span style="color:#f92672">{</span> width: 35em; margin: <span style="color:#ae81ff">0</span> auto;
font-family: Tahoma, Verdana, Arial, sans-serif; <span style="color:#f92672">}</span>
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;http://nginx.org/&#34;</span>&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;http://nginx.com/&#34;</span>&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you <span style="color:#66d9ef">for</span> using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;

</code></pre></div><h3 id="multi-cluster-network-policy">Multi-cluster network policy</h3>
<p>It is possible to establish policies that apply to pod in particular clusters only. The cluster name is represented as a label on each pod by Cilium which allows to match on the cluster name in both the <code>endpointSelector</code> as well as the <code>matchLabels</code> for <code>toEndpoints</code> and fromEndpoints constructs:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#e6db74">&#34;cilium.io/v2&#34;</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">CiliumNetworkPolicy</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#e6db74">&#34;allow-cross-cluster&#34;</span>
  <span style="color:#f92672">description</span>: <span style="color:#e6db74">&#34;Allow x-wing in cluster1 to contact rebel-base in cluster2&#34;</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">endpointSelector</span>:
    <span style="color:#f92672">matchLabels</span>:
      <span style="color:#f92672">name</span>: <span style="color:#ae81ff">x-wing</span>
      <span style="color:#f92672">io.cilium.k8s.policy.cluster</span>: <span style="color:#ae81ff">cluster1</span>
  <span style="color:#f92672">egress</span>:
  - <span style="color:#f92672">toEndpoints</span>:
    - <span style="color:#f92672">matchLabels</span>:
        <span style="color:#f92672">name</span>: <span style="color:#ae81ff">rebel-base</span>
        <span style="color:#f92672">io.cilium.k8s.policy.cluster</span>: <span style="color:#ae81ff">cluster2</span>
</code></pre></div><p>The above example policy will allow <code>x-wing</code> in cluster1 to talk to <code>rebel-base</code> in cluster2. X-wings won&rsquo;t be able to talk to rebel bases in the local cluster unless additional policies exist that whitelist the communication.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Free Docker Desktop Alternative For Mac And Windows]]></title>
            <link href="https://devopstales.github.io/home/docker-desktop-alternatives/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/docker-desktop-alternatives/?utm_source=atom_feed" rel="related" type="text/html" title="Free Docker Desktop Alternative For Mac And Windows" />
                <link href="https://devopstales.github.io/home/k8s-seccomp/?utm_source=atom_feed" rel="related" type="text/html" title="Hardening Kubernetes with seccomp" />
                <link href="https://devopstales.github.io/home/migrate-docker-to-containerd/?utm_source=atom_feed" rel="related" type="text/html" title="Migrate Kubernetes from docker to containerd" />
                <link href="https://devopstales.github.io/home/kubernetes-deprecated-docker-containderd-docker/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes deprecated Docker? Containderd is the new Docker!!" />
                <link href="https://devopstales.github.io/home/docker-on-fedora31/?utm_source=atom_feed" rel="related" type="text/html" title="Install docker on fedora 31" />
            
                <id>https://devopstales.github.io/home/docker-desktop-alternatives/</id>
            
            
            <published>2021-09-20T00:00:00+00:00</published>
            <updated>2021-09-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>At Aug. 31, 2022 Docker announced a new subscription plan for Docker Desktop. So we will Check the best alternatives for docker desktop on Windows an MacOS.</p>
<p>Docker Desktop remain free for:</p>
<ul>
<li>Small businesses with fewer than 250 employees and less than $10 million in annual revenue.</li>
<li>Personal use.</li>
<li>Educational institutions.</li>
<li>Non-commercial open-source projects.</li>
</ul>
<p>For professional usage, Docker Desktop will require a paid subscription (either Pro, Team or Business), which starts at $5 per month and goes up from there.</p>
<h2 id="podman">podman</h2>
<p>Podman is a tool for running OCI containers and pods.</p>
<h3 id="macos">MacOS</h3>
<p>You can do this from a MacOS desktop as long as you have access to a linux box either running inside of a VM on the host, or available via the network. Podman includes a command, podman machine that automatically manages VM’s.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">brew install podman

podman machine init
podman machine start

podman info

podman run -it --rm -d -p 8080:80 --name web nginx
</code></pre></div><h3 id="windows">Windows</h3>
<p>Podman can be run in the Windows Subsystem for Linux system.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">wsl.exe --install

bash

sudo apt-get -y update
sudo apt-get -y install podman

podman info

podman run -it --rm -d -p 8080:80 --name web nginx
</code></pre></div><h2 id="lima-and-colima">lima and Colima</h2>
<p>Lima (Linux virtual machines, on macOS) launches Linux virtual machines with automatic file sharing, port forwarding, and containerd.</p>
<h3 id="macos-1">MacOS</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">brew install lima
limactl start
lima nerdctl run -it --rm alpine
</code></pre></div><blockquote>
<p>NOTE: ARM Mac requires installing a patched version of QEMU, see <a href="https://github.com/lima-vm/lima#does-lima-work-on-arm-mac">Lima documentation</a></p>
</blockquote>
<p>If you want to use Kubernetes on Lima there is a project called Colima.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">brew install lima docker kubectl
curl -LO https://raw.githubusercontent.com/abiosoft/colima/v0.1.10/colima <span style="color:#f92672">&amp;&amp;</span> sudo install colima /usr/local/bin/colima

colima version

colima start --with-kubernetes
<span style="color:#75715e"># configure vm</span>
colima stop
colima start --cpu <span style="color:#ae81ff">4</span> --memory <span style="color:#ae81ff">8</span>
</code></pre></div><p>Lima is already adopted by Rancher Desktop to run k3s on macOS.</p>
<h2 id="rancher-desktop">Rancher Desktop</h2>
<p>Rancher Desktop is an open-source project to bring Kubernetes and container management to the desktop. Windows and macOS versions of Rancher Desktop are available for download.</p>
<h3 id="macos-2">MacOS</h3>
<p>Rancher Desktop requires the following on macOS.</p>
<ul>
<li>macOS 10.10 or higher.</li>
<li>Intel CPU with VT-x.</li>
<li>Persistent internet connection.</li>
</ul>
<p>Apple Silion (M1) support is planned, but not currently implemented.</p>
<h3 id="windows-1">Windows</h3>
<p>Rancher Desktop requires the following on Windows:</p>
<ul>
<li>Windows 10, at least version 1903.</li>
<li>Running on a machine with virtualization capabilities.</li>
<li>Persistent internet connection.</li>
</ul>
<p>Rancher Desktop requires Windows Subsystem for Linux on Windows; this will automatically be installed as part of the Rancher Desktop setup. Manually downloading a distribution is not necessary.</p>
<h3 id="install">Install</h3>
<p>Download and install the newes version fro <a href="https://github.com/rancher-sandbox/rancher-desktop/releases">GitHub</a> Then Start it.</p>
<p><img src="/img/include/racherdesktop01.png" alt="Rancher Desktop Architecture"  class="zoomable" /></p>
<p>Rancher Desktop installs a new Linux VM in WSL2 that has a Kubernetes cluster based on k3s as well as installs various components in it such as KIM (for building docker images on the cluster), helm cli and the Traefik Ingress Controller</p>
<p><img src="/img/include/racherdesktop02.png" alt="Rancher Desktop GUI1"  class="zoomable" /></p>
<p><img src="/img/include/racherdesktop03.png" alt="Rancher Desktop GUI2"  class="zoomable" /></p>
<h2 id="mikrok8s">mikrok8s</h2>
<p>MicroK8s is the simplest production-grade upstream K8s distribution.</p>
<h3 id="macos-3">MacOS</h3>
<p>To run mikrok8s on macOS you need at least 8GB of RAM:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">brew install ubuntu/microk8s/microk8s
microk8s install

microk8s status --wait-ready
microk8s kubectl get nodes
microk8s kubectl get services

<span style="color:#75715e"># enable addons</span>
microk8s enable dns storage

<span style="color:#75715e"># manage vm</span>
microk8s stop
microk8s start
</code></pre></div><h3 id="windows-2">Windows</h3>
<p>To run mikrok8s on windows the requirements are:</p>
<ul>
<li>A Windows 10 machine with at least 8 GB of RAM and 40 GB storage</li>
<li>If you have Windows 10 Home edition, you will also need to install VirtualBox (Windows 10 Professional, Enterprise and Student editions include Hyper-v for virtualisation).</li>
</ul>
<p>MicroK8s has a Windows installer that will take care of setting up the software for you. <a href="https://github.com/ubuntu/microk8s/releases">Download the latest installer here.</a></p>
<p><img src="/img/include/mikrok8s01.png" alt="mikrok8s installer"  class="zoomable" /></p>
<p>The installer checks if Hyper-V is available and switched on. If you don’t have Hyper-v (e.g. on Windows 10 Home edition) it is possible to use VirtualBox as an alternative.</p>
<p>You can now configure MicroK8s - the minimum recommendations are already filled in.</p>
<p><img src="/img/include/mikrok8s02.png" alt="mikrok8s Config"  class="zoomable" /></p>
<p>Now you can tart an use micro k8s:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">microk8s status --wait-ready
microk8s kubectl get nodes
microk8s kubectl get services

<span style="color:#75715e"># enable addons</span>
microk8s enable dns storage

<span style="color:#75715e"># manage vm</span>
microk8s stop
microk8s start
</code></pre></div><h2 id="minikube">minikube</h2>
<p>minikube implements a local Kubernetes cluster on macOS, Linux, and Windows. minikube&rsquo;s primary goals are to be the best tool for local Kubernetes application development and to support all Kubernetes features that fit.</p>
<h3 id="macos-4">MacOS</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">brew install hyperkit minikube docker kubectl

<span style="color:#75715e"># configure vm</span>
minikube config set memory <span style="color:#ae81ff">16384</span>

<span style="color:#75715e"># set driver</span>
minikube start --driver hyperkit
minikube config set driver hyperkit

<span style="color:#75715e"># configure docker cli to connect minikube vm</span>
eval <span style="color:#66d9ef">$(</span>minikube -p minikube docker-env<span style="color:#66d9ef">)</span>
</code></pre></div><h3 id="windows-3">Windows</h3>
<p>Install with exe from powershell:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -Lo minikube.exe https://github.com/kubernetes/minikube/releases/latest/download/minikube-windows-amd64.exe
New-Item -Path <span style="color:#e6db74">&#34;c:\&#34; -Name &#34;</span>minikube<span style="color:#e6db74">&#34; -ItemType &#34;</span>directory<span style="color:#e6db74">&#34; -Force
</span><span style="color:#e6db74">Move-Item .\minikube.exe c:\minikube\minikube.exe -Force
</span><span style="color:#e6db74">
</span><span style="color:#e6db74"></span>$oldpath<span style="color:#e6db74">=[Environment]::GetEnvironmentVariable(&#34;</span>Path<span style="color:#e6db74">&#34;, [EnvironmentVariableTarget]::Machine)
</span><span style="color:#e6db74">if(</span>$oldpath<span style="color:#e6db74"> -notlike &#34;</span>*;C:<span style="color:#ae81ff">\m</span>inikube*<span style="color:#e6db74">&#34;){`
</span><span style="color:#e6db74">  [Environment]::SetEnvironmentVariable(&#34;</span>Path<span style="color:#e6db74">&#34;, </span>$oldpath<span style="color:#e6db74">+&#34;</span>;C:<span style="color:#ae81ff">\m</span>inikube<span style="color:#e6db74">&#34;, [EnvironmentVariableTarget]::Machine)`
</span><span style="color:#e6db74">}
</span></code></pre></div><p>Install wit Windows Package Manager:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">winget install minikube
</code></pre></div><p>using Chocolatey:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">choco install minikube
</code></pre></div><blockquote>
<p>VirtualBox and VMware Workstation (and VMware Player) are &ldquo;level 2 hypervisors.&rdquo; Hyper-V and VMware ESXi are &ldquo;level 1 hypervisors.&rdquo;
So wen Hyper-V is enabled your Windows 10 pc become a Virtualization Host. Hyper-V blocks all other Hyper Visors like VirtualBox from calling VT hardware.</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># to use with Hyper-V yo need to install</span>
Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V -All

<span style="color:#75715e"># configure vm</span>
minikube config set memory <span style="color:#ae81ff">16384</span>

<span style="color:#75715e"># set driver</span>
minikube start --driver hyperv
minikube config set driver hyperv
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[HA IPSec VPN with BGP Dynamic Routing between two pfSense Appliances]]></title>
            <link href="https://devopstales.github.io/home/pfsense-ha-ipsec/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/pfsense-ha-ipsec/?utm_source=atom_feed" rel="related" type="text/html" title="HA IPSec VPN with BGP Dynamic Routing between two pfSense Appliances" />
                <link href="https://devopstales.github.io/home/cilium-opnsense-bgp/?utm_source=atom_feed" rel="related" type="text/html" title="Use Cilium BGP integration with OPNsense" />
                <link href="https://devopstales.github.io/home/k8s-metallb-bgp-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Self-hosted Load Balancer for bare metal Kubernetes" />
                <link href="https://devopstales.github.io/home/pfsense-usg/?utm_source=atom_feed" rel="related" type="text/html" title="Pfsese USG S2S VPN" />
                <link href="https://devopstales.github.io/home/pfsense-telegraf/?utm_source=atom_feed" rel="related" type="text/html" title="Install telegraf on pfsense" />
            
                <id>https://devopstales.github.io/home/pfsense-ha-ipsec/</id>
            
            
            <published>2021-09-18T00:00:00+00:00</published>
            <updated>2021-09-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will setup an IPSec dynamic route-based vpn tunnel between two pfSense Appliances.</p>
<h3 id="the-architecture">The Architecture</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"> ------ WAN1 -----
 |               |
PF1 -- WAN2 -- PF2
 |               |
LAN             LAN

WAN1: 192.168.0.0/24       <span style="color:#f92672">(</span>Bridge<span style="color:#f92672">)</span>
WAN2: 192.168.0/24         <span style="color:#f92672">(</span>Host Only<span style="color:#f92672">)</span>
PF1 - LAN: 192.168.57.0/24 <span style="color:#f92672">(</span>Host Only<span style="color:#f92672">)</span>
PF2 - LAN: 192.168.58.0/24 <span style="color:#f92672">(</span>Host Only<span style="color:#f92672">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">pfsense1:
WAN1: 192.168.0.52
WAN2: 192.168.56.52
LAN:  192.168.57.52
IPSEC-VTI-IP1: 10.10.10.1
IPSEC-VTI-IP2: 10.10.10.5

pfsense2:
WAN1: 192.168.0.53  /24
WAN2: 192.168.56.53 /24
LAN:  192.168.58.53 /24
IPSEC-VTI-IP1: 10.10.10.2
IPSEC-VTI-IP2: 10.10.10.6
</code></pre></div><h3 id="firewall-rules-for-ipsec">Firewall rules For IPSEC</h3>
<p><a href="https://docs.netgate.com/pfsense/en/latest/book/ipsec/ipsec-and-firewall-rules.html">https://docs.netgate.com/pfsense/en/latest/book/ipsec/ipsec-and-firewall-rules.html</a></p>
<blockquote>
<p>When an IPsec tunnel is configured, pfSense® automatically adds hidden firewall rules to allow UDP
ports 500 and 4500, and the ESP protocol from the Remote gateway IP address destined to the Interface
IP address specified in the tunnel configuration.</p>
</blockquote>
<p>UDP Port 4500 is only required for NAT Traversal if the pfSense Applicance doesn’t have a public IP and is behind a NAT device.</p>
<h3 id="ipsec-s2s-vpn">IPSEC S2S VPN</h3>
<p>On the two WAN interfaces of the firewalls I will create two IPSEC S2S VPN with Routed IPsec (VTI). First we must configure on each site the <strong>PSec Phase 1</strong> for boat the VPNs.</p>
<p><strong>Site A</strong>
<img src="/img/include/pfsense_ha_ipsec_1.png" alt="Site A Phase 1"  class="zoomable" /></p>
<p><img src="/img/include/pfsense_ha_ipsec_5.png" alt="Site A Phase 1"  class="zoomable" /></p>
<p><strong>Site B</strong>
<img src="/img/include/pfsense_ha_ipsec_2.png" alt="Site B Phase 1"  class="zoomable" /></p>
<p><img src="/img/include/pfsense_ha_ipsec_6.png" alt="Site B Phase 1"  class="zoomable" /></p>
<p>Now after we finished configure the Phase 1 on each site we add an IPSec Phase 2 on each site. I will use <strong>Mode Routed (VTI).</strong> In this mode we must create a transit network <strong>with a subnet mask of /30</strong>.</p>
<p><strong>Site A</strong>
<img src="/img/include/pfsense_ha_ipsec_3.png" alt="Site A Phase 2"  class="zoomable" /></p>
<p><img src="/img/include/pfsense_ha_ipsec_7.png" alt="Site A Phase 2"  class="zoomable" /></p>
<p><strong>Site B</strong>
<img src="/img/include/pfsense_ha_ipsec_4.png" alt="Site B Phase 2"  class="zoomable" /></p>
<p><img src="/img/include/pfsense_ha_ipsec_8.png" alt="Site B Phase 2"  class="zoomable" /></p>
<h3 id="create-virtual-inteface-for-the-routed-vti">Create Virtual inteface for the Routed (VTI)</h3>
<p>Go to <strong>Interfaces -&gt; Assignement</strong> Assign and enable the new ipsec interfaces:</p>
<p><img src="/img/include/pfsense_ha_ipsec_9.png" alt="Add interace"  class="zoomable" /></p>
<p>Enable Interface but do not configure the ip. It will be automatically assigned by th VPN.</p>
<p><img src="/img/include/pfsense_ha_ipsec_10.png" alt="Enable interace"  class="zoomable" /></p>
<p>As you cans see the ip is automaticle assined to the interfaces:</p>
<p><img src="/img/include/pfsense_ha_ipsec_11.png" alt="Interface status"  class="zoomable" /></p>
<p>To see if the tunnel is up and running go to <strong>Status -&gt; IPSec</strong> in the menu. As you can see the connection between both peers is established.</p>
<p><img src="/img/include/pfsense_ha_ipsec_12.png" alt="IPSEC status"  class="zoomable" /></p>
<h3 id="add-firewall-rules-for-the-ipsec">Add firewall rules for the IPSEC</h3>
<p>One last thing we must configure on each site that traffic can flow from the remote site to the local site are the IPSec Firewall Rules .</p>
<p><img src="/img/include/pfsense_ha_ipsec_13.png" alt="IPSEC firewall rules"  class="zoomable" /></p>
<p><img src="/img/include/pfsense_ha_ipsec_14.png" alt="PING test"  class="zoomable" /></p>
<h3 id="install-the-frr-package">Install the FRR Package</h3>
<p>Go to <strong>System -&gt; Package Manager -&gt; Available Packages</strong> and search for <strong>bgp</strong> or <strong>frr</strong> to find and install the package.</p>
<p><img src="/img/include/pfsense_ha_ipsec_15.png" alt="Install BGP"  class="zoomable" /></p>
<p><img src="/img/include/pfsense_ha_ipsec_16.png" alt="Install BGP"  class="zoomable" /></p>
<p><img src="/img/include/pfsense_ha_ipsec_17.png" alt="Install BGP"  class="zoomable" /></p>
<blockquote>
<p>Install the package on tha master and the slave server too. The packagas dose not syncronise between cluster members.</p>
</blockquote>
<h3 id="configure-dynamic-routing">Configure dynamic routing</h3>
<p>But before traffic will be routed over the tunnel we first must configure the BGP Protocol on both sites at pfSense.</p>
<blockquote>
<p>zebra is an IP routing manager. It provides kernel routing table updates, interface lookups, and redistribution of routes between different routing protocols.</p>
</blockquote>
<blockquote>
<p>Packages do not synchronize with XMLRPC unless they implement their own XMLRPC synchronization settings, and FRR does not do that currently.
You will have to set it up on both nodes separately for the time being.</p>
</blockquote>
<p>First we click and go to the <strong>Services -&gt; FRR Global/Zebra</strong> configuration.</p>
<p><img src="/img/include/pfsense_ha_ipsec_18.png" alt="Configure Zebra"  class="zoomable" /></p>
<p>Before configuring BGP, add a route map to match any routes so it can be used by FRR to allow exchanging all routes with the peer. Navigate to <strong>Services &gt; FRR Global/Zebra, Route Maps</strong> tab</p>
<p><img src="/img/include/pfsense_ha_ipsec_19.png" alt="Policy"  class="zoomable" /></p>
<p><img src="/img/include/pfsense_ha_ipsec_20.png" alt="Policy"  class="zoomable" /></p>
<h3 id="configure-bgp">Configure BGP</h3>
<p>Now we can switch to the BGP configuration, therefore you can click in the Global Settings sections menu on BGP with the square brackets or from the main menu under <strong>Services -&gt; FRR BGP</strong>. Here you must configure the BGP Protocol for each site. As Router IP you must enter the internal IP from the LAN interface.</p>
<blockquote>
<p><a href="https://en.wikipedia.org/wiki/Autonomous_system_(Internet)">https://en.wikipedia.org/wiki/Autonomous_system_(Internet)</a></p>
<p>(64512 – 65534 Reserved for private use RFC1930, RFC6996)</p>
</blockquote>
<p><strong>Site A</strong>
<img src="/img/include/pfsense_ha_ipsec_21.png" alt="BGP Site A"  class="zoomable" /></p>
<p>Also we need to configure here in the Network Distribution section which networks we want to advertise to the other BGP peer.</p>
<p><img src="/img/include/pfsense_ha_ipsec_23.png" alt="BGP Site A"  class="zoomable" /></p>
<p><strong>Site B</strong>
<img src="/img/include/pfsense_ha_ipsec_22.png" alt="BGP Site B"  class="zoomable" /></p>
<p><img src="/img/include/pfsense_ha_ipsec_24.png" alt="BGP Site B"  class="zoomable" /></p>
<h3 id="configure-neighbors">Configure Neighbors</h3>
<p>Now we move on to the Neighbors menu where first I create a Neighbor callef <strong>BGP</strong>. When you add a name instead of the ip of the Neighbor the frr will interpreter this as a Peer Group. At the <strong>Route Map Filters</strong> eet both Inbound and Outbound to <strong>ALLOW-ALL</strong>.</p>
<p><img src="/img/include/pfsense_ha_ipsec_25.png" alt="Peer Groupe"  class="zoomable" /></p>
<p>Then create the Neighbors and use the Peer Group <strong>BGP</strong>.</p>
<p><strong>Site A</strong>
<img src="/img/include/pfsense_ha_ipsec_26.png" alt="Neighbors Site A"  class="zoomable" /></p>
<p><img src="/img/include/pfsense_ha_ipsec_27.png" alt="Neighbors Site A"  class="zoomable" /></p>
<p><img src="/img/include/pfsense_ha_ipsec_28.png" alt="Neighbors Site A"  class="zoomable" /></p>
<p><strong>Site B</strong>
<img src="/img/include/pfsense_ha_ipsec_29.png" alt="Neighbors Site B"  class="zoomable" /></p>
<h3 id="check-the-bgp-status">Check The BGP STatus</h3>
<p>The result for these settings you will see under <strong>Services -&gt; FRR -&gt; Status -&gt; BGP</strong> in the BGP table.</p>
<p><img src="/img/include/pfsense_ha_ipsec_30.png" alt="BGP Status"  class="zoomable" /></p>
<p><img src="/img/include/pfsense_ha_ipsec_31.png" alt="BGP Status"  class="zoomable" /></p>
<h3 id="test-the-results">Test the results</h3>
<p>Further we can see this advertised route from the other peer, under <strong>Diagnostics -&gt; Routes</strong></p>
<p><img src="/img/include/pfsense_ha_ipsec_32.png" alt="BGP Status"  class="zoomable" /></p>
<p><img src="/img/include/pfsense_ha_ipsec_33.png" alt="Ping test"  class="zoomable" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Hardening Kubernetes with seccomp]]></title>
            <link href="https://devopstales.github.io/home/k8s-seccomp/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-seccomp/?utm_source=atom_feed" rel="related" type="text/html" title="Hardening Kubernetes with seccomp" />
                <link href="https://devopstales.github.io/home/migrate-docker-to-containerd/?utm_source=atom_feed" rel="related" type="text/html" title="Migrate Kubernetes from docker to containerd" />
                <link href="https://devopstales.github.io/home/lazyimage/?utm_source=atom_feed" rel="related" type="text/html" title="Speed up docker pull with lazypull" />
                <link href="https://devopstales.github.io/home/firecracker-cri-o/?utm_source=atom_feed" rel="related" type="text/html" title="How to deploy CRI-O with Firecracker?" />
                <link href="https://devopstales.github.io/home/gvisor-cri-o/?utm_source=atom_feed" rel="related" type="text/html" title="How to deploy CRI-O with gVisor?" />
            
                <id>https://devopstales.github.io/home/k8s-seccomp/</id>
            
            
            <published>2021-09-03T00:00:00+00:00</published>
            <updated>2021-09-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will attempt to demystify the relationship of <code>seccomp</code> and Kubernetes This first part will look at containers and pods.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<p>With Kubernetes version v1.22 there is a new alpha feature that provides a way to use the <code>RuntimeDefault</code> as the defaut seccomp profile insted of <code>Unconfined</code>. By default, when Kubernetes makes a new container it creates with <code>Unconfined</code> seccomp profile. This means that seccomp filtering is disabled.</p>
<h3 id="wthat-is-seccomp-profile">Wthat is seccomp profile?</h3>
<p><a href="https://en.wikipedia.org/wiki/Seccomp">Seccomp (Secure Computing)</a> is a feature in the Linux kernel. It allow to create profiles to filter <a href="https://en.wikipedia.org/wiki/System_call">system calls</a>. Usage of seccomp profiles on containers reduces the chance that a Linux kernel vulnerability will be exploited. All container runtimes ship with a default seccomp profile. The problem come when we using Kubernetes, beasuse Kubernetes use <code>Unconfined</code> as default and disables seccomp filtering.</p>
<p>For example Docker&rsquo;s default seccomp profile disables approximately 44 system calls of the 300+ currently availble.</p>
<h3 id="test-seccomp-profile">Test Seccomp profile.</h3>
<p>For the test I will use <code>amicontained</code> to inspection tool. First test in a simple docker.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ocker run --rm -it r.j3ss.co/amicontained bash
Container Runtime: docker
Has Namespaces:
	pid: true
	user: false
AppArmor Profile: docker-default <span style="color:#f92672">(</span>enforce<span style="color:#f92672">)</span>
Capabilities:
	BOUNDING -&gt; chown dac_override fowner fsetid kill setgid setuid setpcap net_bind_service net_raw sys_chroot mknod audit_write setfcap
Seccomp: filtering
Blocked Syscalls <span style="color:#f92672">(</span>60<span style="color:#f92672">)</span>:
	SYSLOG SETPGID SETSID USELIB USTAT SYSFS VHANGUP PIVOT_ROOT _SYSCTL ACCT SETTIMEOFDAY MOUNT UMOUNT2 SWAPON SWAPOFF REBOOT SETHOSTNAME SETDOMAINNAME IOPL IOPERM CREATE_MODULE INIT_MODULE DELETE_MODULE GET_KERNEL_SYMS QUERY_MODULE QUOTACTL NFSSERVCTL GETPMSG PUTPMSG AFS_SYSCALL TUXCALL SECURITY LOOKUP_DCOOKIE CLOCK_SETTIME VSERVER MBIND SET_MEMPOLICY GET_MEMPOLICY KEXEC_LOAD ADD_KEY REQUEST_KEY KEYCTL MIGRATE_PAGES UNSHARE MOVE_PAGES PERF_EVENT_OPEN FANOTIFY_INIT NAME_TO_HANDLE_AT OPEN_BY_HANDLE_AT SETNS PROCESS_VM_READV PROCESS_VM_WRITEV KCMP FINIT_MODULE KEXEC_FILE_LOAD BPF USERFAULTFD PKEY_MPROTECT PKEY_ALLOC PKEY_FREE
Looking <span style="color:#66d9ef">for</span> Docker.sock
</code></pre></div><p>As you can see with the default Docker secom profile 60 Syscalls are being blocked. Now test wit default Kubernetes config on docker.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl run -it bash --image<span style="color:#f92672">=</span>r.j3ss.co/amicontained --restart<span style="color:#f92672">=</span>Never bash
Container Runtime: docker
Has Namespaces:
 pid: true
 user: false
AppArmor Profile: unconfined
Capabilities:
 BOUNDING -&gt; chown dac_override fowner fsetid kill setgid setuid setpcap net_bind_service net_raw sys_chroot mknod audit_write setfcap
Seccomp: disabled
Blocked Syscalls <span style="color:#f92672">(</span>21<span style="color:#f92672">)</span>:
 MSGRCV SYSLOG SETSID VHANGUP PIVOT_ROOT ACCT SETTIMEOFDAY SWAPON SWAPOFF REBOOT SETHOSTNAME SETDOMAINNAME INIT_MODULE DELETE_MODULE LOOKUP_DCOOKIE KEXEC_LOAD FANOTIFY_INIT OPEN_BY_HANDLE_AT FINIT_MODULE KEXEC_FILE_LOAD BPF
Looking <span style="color:#66d9ef">for</span> Docker.sock
</code></pre></div><p>In the output above you can see that seccomp is disabled and that 21 syscalls are being blocked. Now test wit default Kubernetes config on rke2 (containerd).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl run -it bash --image<span style="color:#f92672">=</span>r.j3ss.co/amicontained --restart<span style="color:#f92672">=</span>Never bash
Container Runtime: kube
Has Namespaces:
	pid: true
	user: false
AppArmor Profile: system_u:system_r:container_t:s0:c575,c847
Capabilities:
	BOUNDING -&gt; chown dac_override fowner fsetid kill setgid setuid setpcap net_bind_service net_raw sys_chroot mknod audit_write setfcap
Seccomp: disabled
Blocked Syscalls <span style="color:#f92672">(</span>22<span style="color:#f92672">)</span>:
	SYSLOG SETPGID SETSID VHANGUP PIVOT_ROOT ACCT SETTIMEOFDAY UMOUNT2 SWAPON SWAPOFF REBOOT SETHOSTNAME SETDOMAINNAME INIT_MODULE DELETE_MODULE LOOKUP_DCOOKIE KEXEC_LOAD FANOTIFY_INIT OPEN_BY_HANDLE_AT FINIT_MODULE KEXEC_FILE_LOAD BPF
Looking <span style="color:#66d9ef">for</span> Docker.sock
pod default/bash terminated <span style="color:#f92672">(</span>Error<span style="color:#f92672">)</span>
</code></pre></div><p>The <code>containerd</code> is similar then docker so lets test with <code>CRI-O</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl run -it bash --image<span style="color:#f92672">=</span>bash --restart<span style="color:#f92672">=</span>Never bash
<span style="color:#75715e"># apk add curl</span>
<span style="color:#75715e"># curl -LO k8s.work/amicontained</span>
<span style="color:#75715e"># chmod +x amicontained</span>
<span style="color:#75715e"># ./amicontained</span>
Container Runtime: kube
Has Namespaces:
	pid: true
	user: false
AppArmor Profile: system_u:system_r:spc_t:s0
Capabilities:
	BOUNDING -&gt; chown dac_override fowner fsetid kill setgid setuid setpcap net_bind_service
Seccomp: disabled
Blocked Syscalls <span style="color:#f92672">(</span>22<span style="color:#f92672">)</span>:
	MSGRCV SYSLOG SETSID VHANGUP PIVOT_ROOT ACCT SETTIMEOFDAY UMOUNT2 SWAPON SWAPOFF REBOOT SETHOSTNAME SETDOMAINNAME INIT_MODULE DELETE_MODULE LOOKUP_DCOOKIE KEXEC_LOAD FANOTIFY_INIT OPEN_BY_HANDLE_AT FINIT_MODULE KEXEC_FILE_LOAD BPF
Looking <span style="color:#66d9ef">for</span> Docker.sock
</code></pre></div><h3 id="enable-runtimedefault-seccomp-profile">Enable RuntimeDefault seccomp profile</h3>
<p>Enable in local kubelet config:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano /var/lib/kubelet/config.yaml
...
--feature-gates<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;...,SeccompDefault=true&#34;</span>
--seccomp-default RuntimeDefault

systemctl restart kubelet
</code></pre></div><p>Enable in running kubelet config:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl edit cm kubelet-config-1.22 -n kub-system
...
- --feature-gates<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;...,SeccompDefault=true&#34;</span>
- --seccomp-default RuntimeDefault
</code></pre></div><p>Then test:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl run -it bash --image<span style="color:#f92672">=</span>r.j3ss.co/amicontained --restart<span style="color:#f92672">=</span>Never bash
Container Runtime: docker
Has Namespaces:
 pid: true
 user: false
AppArmor Profile: unconfined
Capabilities:
 BOUNDING -&gt; chown dac_override fowner fsetid kill setgid setuid setpcap net_bind_service net_raw sys_chroot mknod audit_write setfcap
Seccomp: filtering
Blocked Syscalls <span style="color:#f92672">(</span>61<span style="color:#f92672">)</span>:
 MSGRCV PTRACE SYSLOG SETSID USELIB USTAT SYSFS VHANGUP PIVOT_ROOT _SYSCTL ACCT SETTIMEOFDAY MOUNT UMOUNT2 SWAPON SWAPOFF REBOOT SETHOSTNAME SETDOMAINNAME IOPL IOPERM CREATE_MODULE INIT_MODULE DELETE_MODULE GET_KERNEL_SYMS QUERY_MODULE QUOTACTL NFSSERVCTL GETPMSG PUTPMSG AFS_SYSCALL TUXCALL SECURITY LOOKUP_DCOOKIE CLOCK_SETTIME VSERVER MBIND SET_MEMPOLICY GET_MEMPOLICY KEXEC_LOAD ADD_KEY REQUEST_KEY KEYCTL MIGRATE_PAGES UNSHARE MOVE_PAGES PERF_EVENT_OPEN FANOTIFY_INIT NAME_TO_HANDLE_AT OPEN_BY_HANDLE_AT SETNS PROCESS_VM_READV PROCESS_VM_WRITEV KCMP FINIT_MODULE KEXEC_FILE_LOAD BPF USERFAULTFD PKEY_MPROTECT PKEY_ALLOC PKEY_FREE
Looking <span style="color:#66d9ef">for</span> Docker.sock
</code></pre></div><h3 id="customizing-a-profile">Customizing a Profile</h3>
<p>One way to write <code>seccomp</code> filter is to use Berkeley packet filter (BPF) language. Using this language isn&rsquo;t really simple or convenient. We can write JSON that is compiled into profile by <code>libseccomp</code>.</p>
<p>If you were to create a profile to allow a container to execute a ping against a website, you can use <code>strace</code> command to find the syscalls it makes:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">strace -fqc ping -c <span style="color:#ae81ff">20</span> www.google.com% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 29.55    0.000078           <span style="color:#ae81ff">4</span>        <span style="color:#ae81ff">20</span>        <span style="color:#ae81ff">11</span> openat
 14.02    0.000037           <span style="color:#ae81ff">9</span>         <span style="color:#ae81ff">4</span>         <span style="color:#ae81ff">4</span> socket
 11.74    0.000031           <span style="color:#ae81ff">3</span>        <span style="color:#ae81ff">12</span>           mprotect
  6.06    0.000016           <span style="color:#ae81ff">2</span>         <span style="color:#ae81ff">7</span>           read
  5.68    0.000015           <span style="color:#ae81ff">1</span>        <span style="color:#ae81ff">17</span>           mmap
  5.68    0.000015           <span style="color:#ae81ff">3</span>         <span style="color:#ae81ff">5</span>           capget
  4.92    0.000013          <span style="color:#ae81ff">13</span>         <span style="color:#ae81ff">1</span>           munmap
  3.79    0.000010           <span style="color:#ae81ff">1</span>         <span style="color:#ae81ff">9</span>           fstat
  3.41    0.000009           <span style="color:#ae81ff">9</span>         <span style="color:#ae81ff">1</span>           write
  3.41    0.000009           <span style="color:#ae81ff">1</span>         <span style="color:#ae81ff">9</span>           close
  2.65    0.000007           <span style="color:#ae81ff">2</span>         <span style="color:#ae81ff">3</span>           brk
  2.65    0.000007           <span style="color:#ae81ff">4</span>         <span style="color:#ae81ff">2</span>           prctl
  2.27    0.000006           <span style="color:#ae81ff">3</span>         <span style="color:#ae81ff">2</span>           getuid
  1.52    0.000004           <span style="color:#ae81ff">4</span>         <span style="color:#ae81ff">1</span>           setuid
  1.52    0.000004           <span style="color:#ae81ff">4</span>         <span style="color:#ae81ff">1</span>           capset
  1.14    0.000003           <span style="color:#ae81ff">3</span>         <span style="color:#ae81ff">1</span>           geteuid
  0.00    0.000000           <span style="color:#ae81ff">0</span>         <span style="color:#ae81ff">9</span>         <span style="color:#ae81ff">9</span> access
  0.00    0.000000           <span style="color:#ae81ff">0</span>         <span style="color:#ae81ff">1</span>           execve
  0.00    0.000000           <span style="color:#ae81ff">0</span>         <span style="color:#ae81ff">3</span>           fcntl
  0.00    0.000000           <span style="color:#ae81ff">0</span>         <span style="color:#ae81ff">1</span>           arch_prctl
------ ----------- ----------- --------- --------- ----------------
100.00    0.000264                   <span style="color:#ae81ff">109</span>        <span style="color:#ae81ff">24</span> total
</code></pre></div><p>A nothe solution is a tool called <a href="https://github.com/pjbgf/zaz">zaz</a> created by <a href="https://github.com/pjbgf">Paulo Gomes</a> That generate a seccomp prifile for you with the minimum system calls:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">zaz seccomp docker alpine <span style="color:#e6db74">&#34;ping -c5 8.8.8.8&#34;</span>
</code></pre></div><p>A basic seccomp has three key elements: the <code>defaultAction</code>, the <code>architectures</code> (or <code>archMap</code>) and the <code>syscalls</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#960050;background-color:#1e0010">mkdir</span> <span style="color:#960050;background-color:#1e0010">/var/lib/kubelet/seccomp</span>
<span style="color:#960050;background-color:#1e0010">nano</span> <span style="color:#960050;background-color:#1e0010">/var/lib/kubelet/seccomp/sample.json</span>
{
    <span style="color:#f92672">&#34;defaultAction&#34;</span>: <span style="color:#e6db74">&#34;SCMP_ACT_ERRNO&#34;</span>,
    <span style="color:#f92672">&#34;architectures&#34;</span>: [
        <span style="color:#e6db74">&#34;SCMP_ARCH_X86_64&#34;</span>,
        <span style="color:#e6db74">&#34;SCMP_ARCH_X86&#34;</span>,
        <span style="color:#e6db74">&#34;SCMP_ARCH_X32&#34;</span>
    ],
    <span style="color:#f92672">&#34;syscalls&#34;</span>: [
        {
            <span style="color:#f92672">&#34;names&#34;</span>: [
                <span style="color:#e6db74">&#34;arch_prctl&#34;</span>,
                <span style="color:#e6db74">&#34;sched_yield&#34;</span>,
                <span style="color:#e6db74">&#34;futex&#34;</span>,
                <span style="color:#e6db74">&#34;write&#34;</span>,
                <span style="color:#e6db74">&#34;mmap&#34;</span>,
                <span style="color:#e6db74">&#34;exit_group&#34;</span>,
                <span style="color:#e6db74">&#34;madvise&#34;</span>,
                <span style="color:#e6db74">&#34;rt_sigprocmask&#34;</span>,
                <span style="color:#e6db74">&#34;getpid&#34;</span>,
                <span style="color:#e6db74">&#34;gettid&#34;</span>,
                <span style="color:#e6db74">&#34;tgkill&#34;</span>,
                <span style="color:#e6db74">&#34;rt_sigaction&#34;</span>,
                <span style="color:#e6db74">&#34;read&#34;</span>,
                <span style="color:#e6db74">&#34;getpgrp&#34;</span>
            ],
            <span style="color:#f92672">&#34;action&#34;</span>: <span style="color:#e6db74">&#34;SCMP_ACT_ALLOW&#34;</span>,
   <span style="color:#f92672">&#34;args&#34;</span>: [],
   <span style="color:#f92672">&#34;comment&#34;</span>: <span style="color:#e6db74">&#34;&#34;</span>,
   <span style="color:#f92672">&#34;includes&#34;</span>: {},
   <span style="color:#f92672">&#34;excludes&#34;</span>: {}
        }
    ]
}
</code></pre></div><p>The <code>defaultAction</code> is <code>SCMP_ACT_ERRNO</code> which will block the execution of any system call. The we list the <code>syscalls</code> what we want to whitelist.</p>
<h3 id="the-different-types-of-actions">The different types of actions</h3>
<p>Below is a list of all the different types of actions and what they do:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">SCMP_ACT_KILL_THREAD <span style="color:#f92672">(</span>or SCMP_ACT_KILL<span style="color:#f92672">)</span>
Does not execute the syscall and terminate the thread that attempted making the call. Note that depending on the application being enforced <span style="color:#f92672">(</span>i.e. multi-threading<span style="color:#f92672">)</span> and its error handling, syscalls blocked using this action may <span style="color:#66d9ef">do</span> so silently which may result in side effects on the overall application.

SCMP_ACT_TRAP
Does not execute the syscall. The kernel will send a thread-directed SIGSYS signal to the thread that attempted making the call.

SCMP_ACT_ERRNO
Does not execute the syscall, returns error instead. Note that depending on the error handling of the application being enforced, syscalls blocked using this action may <span style="color:#66d9ef">do</span> so silently which may result in side effects on the overall application.

SCMP_ACT_TRACE
The decision on whether or not to execute the syscall will come from a tracer. If no tracer is present behaves like SECCOMP_RET_ERRNO.
This can be used to automate profile generation and also can be used to change the syscall being made. Not recommended when trying to enforce seccomp to line of business applications.

SCMP_ACT_ALLOW
Executes the syscall.

SCMP_ACT_LOG <span style="color:#f92672">(</span>since Linux 4.14<span style="color:#f92672">)</span>
Executes the syscall. Useful <span style="color:#66d9ef">for</span> running seccomp in <span style="color:#e6db74">&#34;complain-mode&#34;</span>, logging the syscalls that are mapped <span style="color:#f92672">(</span>or catch-all<span style="color:#f92672">)</span> and not blocking their execution. It can be used together with other action types to provide an allow and deny list approach.

SCMP_ACT_KILL_PROCESS <span style="color:#f92672">(</span>since Linux 4.14<span style="color:#f92672">)</span>
Does not execute the syscall and terminates the entire process with a core dump. Very useful when automating the profile generation.
</code></pre></div><h3 id="configure-on-a-pod">Configure on a pod</h3>
<p>With Kubernetes version v1.19 Seccomp Profile for a Container is GA. The sintax looks like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Pod</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">some-pod</span>
  <span style="color:#f92672">labels</span>:
    <span style="color:#f92672">app</span>: <span style="color:#ae81ff">some-pod</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">securityContext</span>:
    <span style="color:#f92672">seccompProfile</span>:
      <span style="color:#f92672">type</span>: <span style="color:#ae81ff">Localhost</span>
      <span style="color:#f92672">localhostProfile</span>: <span style="color:#ae81ff">sample.json</span>
  <span style="color:#f92672">containers</span>:
    <span style="color:#ae81ff">...</span>
</code></pre></div><p>Valid options for <code>type</code> include <code>RuntimeDefault</code>, <code>Unconfined</code>, and <code>Localhost</code>. Here is an example that sets the Seccomp profile to the node&rsquo;s container runtime default profile:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Pod</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">some-pod</span>
  <span style="color:#f92672">labels</span>:
    <span style="color:#f92672">app</span>: <span style="color:#ae81ff">some-pod</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">securityContext</span>:
    <span style="color:#f92672">seccompProfile</span>:
      <span style="color:#f92672">type</span>: <span style="color:#ae81ff">RuntimeDefault</span>
  <span style="color:#f92672">containers</span>:
    <span style="color:#ae81ff">...</span>
</code></pre></div><p>This Configuration is the same then setting <code>SeccompDefault=true</code> in kubelet config.</p>
<h3 id="add-audit-profile">Add audit profile</h3>
<p>Since linux kernel 4.14 it is now possible to define parts of your profile to run in audit mode, logging into syslog all the system calls you want without blocking them. To do that you can use the action SCMT_ACT_LOG:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#960050;background-color:#1e0010">nano</span> <span style="color:#960050;background-color:#1e0010">/var/lib/kubelet/seccomp/audit.json</span>
{
    <span style="color:#f92672">&#34;defaultAction&#34;</span>: <span style="color:#e6db74">&#34;SCMP_ACT_LOG&#34;</span>,
    <span style="color:#f92672">&#34;architectures&#34;</span>: [
        <span style="color:#e6db74">&#34;SCMP_ARCH_X86_64&#34;</span>,
        <span style="color:#e6db74">&#34;SCMP_ARCH_X86&#34;</span>,
        <span style="color:#e6db74">&#34;SCMP_ARCH_X32&#34;</span>
    ],
    <span style="color:#f92672">&#34;syscalls&#34;</span>: [
        {
            <span style="color:#f92672">&#34;names&#34;</span>: [
                <span style="color:#e6db74">&#34;arch_prctl&#34;</span>,
                <span style="color:#e6db74">&#34;sched_yield&#34;</span>,
                <span style="color:#e6db74">&#34;futex&#34;</span>,
                <span style="color:#e6db74">&#34;write&#34;</span>,
                <span style="color:#e6db74">&#34;mmap&#34;</span>,
                <span style="color:#e6db74">&#34;exit_group&#34;</span>,
                <span style="color:#e6db74">&#34;madvise&#34;</span>,
                <span style="color:#e6db74">&#34;rt_sigprocmask&#34;</span>,
                <span style="color:#e6db74">&#34;getpid&#34;</span>,
                <span style="color:#e6db74">&#34;gettid&#34;</span>,
                <span style="color:#e6db74">&#34;tgkill&#34;</span>,
                <span style="color:#e6db74">&#34;rt_sigaction&#34;</span>,
                <span style="color:#e6db74">&#34;read&#34;</span>,
                <span style="color:#e6db74">&#34;getpgrp&#34;</span>
            ],
            <span style="color:#f92672">&#34;action&#34;</span>: <span style="color:#e6db74">&#34;SCMP_ACT_ALLOW&#34;</span>
        },
        {
            <span style="color:#f92672">&#34;names&#34;</span>: [
                <span style="color:#e6db74">&#34;add_key&#34;</span>,
                <span style="color:#e6db74">&#34;keyctl&#34;</span>,
                <span style="color:#e6db74">&#34;ptrace&#34;</span>
            ],
            <span style="color:#f92672">&#34;action&#34;</span>: <span style="color:#e6db74">&#34;SCMP_ACT_ERRNO&#34;</span>
        }
    ]
}
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">~ $ tail /var/log/syslog

Nov <span style="color:#ae81ff">25</span> 19:38:18 kernel: <span style="color:#f92672">[</span>461698.749294<span style="color:#f92672">]</span> audit: ... syscall<span style="color:#f92672">=</span><span style="color:#ae81ff">21</span> compat<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span> ip<span style="color:#f92672">=</span>0x7ff8f8412d5b code<span style="color:#f92672">=</span>0x7ffc0000    <span style="color:#75715e"># access</span>
Nov <span style="color:#ae81ff">25</span> 19:38:18 kernel: <span style="color:#f92672">[</span>461698.749306<span style="color:#f92672">]</span> audit: ... syscall<span style="color:#f92672">=</span><span style="color:#ae81ff">257</span> compat<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span> ip<span style="color:#f92672">=</span>0x7ff8f8412ec8 code<span style="color:#f92672">=</span>0x7ffc0000   <span style="color:#75715e"># openat</span>
Nov <span style="color:#ae81ff">25</span> 19:38:18 kernel: <span style="color:#f92672">[</span>461698.749315<span style="color:#f92672">]</span> audit: ... syscall<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span> compat<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span> ip<span style="color:#f92672">=</span>0x7ff8f8412c99 code<span style="color:#f92672">=</span>0x7ffc0000     <span style="color:#75715e"># fstat</span>
Nov <span style="color:#ae81ff">25</span> 19:38:18 kernel: <span style="color:#f92672">[</span>461698.749317<span style="color:#f92672">]</span> audit: ... syscall<span style="color:#f92672">=</span><span style="color:#ae81ff">9</span> compat<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span> ip<span style="color:#f92672">=</span>0x7ff8f84130e6 code<span style="color:#f92672">=</span>0x7ffc0000     <span style="color:#75715e"># mmap</span>
Nov <span style="color:#ae81ff">25</span> 19:38:18 kernel: <span style="color:#f92672">[</span>461698.749323<span style="color:#f92672">]</span> audit: ... syscall<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span> compat<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span> ip<span style="color:#f92672">=</span>0x7ff8f8412d8b code<span style="color:#f92672">=</span>0x7ffc0000     <span style="color:#75715e"># close</span>
</code></pre></div><h3 id="set-capabilities-for-a-container">Set capabilities for a Container</h3>
<p>With version 1.22 you should be able to change the sysctl in the security context of your pod manifests, allowing containers that are running as unprivileged users to bind low ports.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">securityContext</span>:
     <span style="color:#f92672">sysctls</span>:
     - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">net.ipv4.ip_unprivileged_port_start</span>
          <span style="color:#f92672">value</span>: <span style="color:#e6db74">&#34;1&#34;</span>
</code></pre></div><h3 id="final-words">Final Words</h3>
<p>Whatever you define in your seccomp profile, the kernel will enforce it. Even if that is not what you want. For example, if you block access to calls such as exit or exit_group your container may not be able to exit and it could trap the container in an exit loop indefinitely. Leading to high CPU usage of your cluster.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Understanding kubernetes networking: owerlay networks]]></title>
            <link href="https://devopstales.github.io/home/kubernetes-networking-2/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/kubernetes-networking-1/?utm_source=atom_feed" rel="related" type="text/html" title="Understanding kubernetes networking: pods and services" />
                <link href="https://devopstales.github.io/kubernetes/kubernetes-networking-2/?utm_source=atom_feed" rel="related" type="text/html" title="Understanding kubernetes networking: owerlay networks" />
                <link href="https://devopstales.github.io/kubernetes/kubernetes-networking-1/?utm_source=atom_feed" rel="related" type="text/html" title="Understanding kubernetes networking: pods and services" />
                <link href="https://devopstales.github.io/home/rke2-cilium/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Install With cilium" />
                <link href="https://devopstales.github.io/home/k8s-security/?utm_source=atom_feed" rel="related" type="text/html" title="Best Practices to keeping Kubernetes Clusters Secure" />
            
                <id>https://devopstales.github.io/home/kubernetes-networking-2/</id>
            
            
            <published>2021-08-31T00:00:00+00:00</published>
            <updated>2021-08-31T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In my previous posts We check <a href="/kubernetes-networking-1/">how the pod networking</a> and <a href="/kubernetes-networking-1/">How Kubernetes use services for loadbalancing</a>. Now we check how the diferente Networking solutions works. How Theas solutins link the kubernetes nodes together.</p>
<p>Kubernetes was built to run distributed systems over a cluster of machines. This meanns networking is a central point of all Kubernetes clusters. Kubernetes dose not hawe an includid network solution, you haw to chouse one. Withouth this solutions all host whou be a closed separated ecosistem. Many kubernetes deployment guides provide instructions for deploying a kubernetes networking (CNI) to your cluster. The most polular solutions are Calico, Flannel, Weave, and Cilium. Most time you simpli deploy a yaml or a helm chart, but we didn&rsquo;t undestand how theas solutions works. Understanding the Kubernetes networking model will allow you to easely troubleshoot your applications running on Kubernetes.</p>
<h2 id="what-is-a-cni">What is a CNI?</h2>
<p>The CNI (Container Network Interface) project describes the specifications to provide a generic plugin-based networking solution for linux containers. A CNI plugin is an executable and it&rsquo;s config that follows the CNI spec and we’ll discuss some plugins in the post below.</p>
<h3 id="overlay-network">Overlay network</h3>
<p>The concept of overlay networking has been around for a while, but has made a return to the spotlight with the rise of Docker and Kubernetes. It is  basicly a virtual network of nodes and logical links, which are built on top of an existing network. To solve this chelange the CNI plugins use two main aprouch VXLAN (Virtual Extensible LAN) and simple layer3 routing. The aim of an overlay network is to enable a new service or function without having to reconfigure the entire network design.</p>
<h3 id="node-ipam-controller">Node IPAM Controller</h3>
<p>All pods are required to have an IP address. it’s important to ensure that all pods across the entire cluster have a unique IP address. <code>kube-controller-manager</code> allocates each node a dedicated subnet (podCIDR) from the cluster CIDR (IP range for the cluster network) For example if the overlay network is <code>10.244.0.0/16</code> all the kubernetes hosts gets a <code>/24</code> segment so the master01 is <code>10.244.0.0/24</code> the master02 is <code>10.244.1.0/24</code> and so on. podCIDR for a node can be listed using the following command:
cidr</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ kubectl get no &lt;nodeName&gt; -o json | jq <span style="color:#e6db74">&#39;.spec.podCIDR&#39;</span>
10.244.0.0/24
</code></pre></div><h3 id="flannel">Flannel</h3>
<p>Flannel runs a small, single binary agent called flanneld on each host, and is responsible for allocating a subnet lease to each host out of a larger, preconfigured address space. Flannel uses either the Kubernetes API or etcd directly to store the network configuration, the allocated subnets, and any auxiliary data (such as the host&rsquo;s public IP). Packets are forwarded using one of several backend mechanisms including VXLAN and various cloud integrations. (Refer to <a href="https://github.com/flannel-io/flannel">Flannel</a>)</p>
<p><img src="/img/include/flannel-01.png" alt="Flannel"  class="zoomable" /></p>
<h3 id="calico">Calico</h3>
<p>Calico originally used a pure 3-layer protocol to support multi-host network communication. On each host a vRouter propagates workload reachability information (routes) to the rest of the data center using BGP protocol. The pure Layer 3 approach avoids the packet encapsulation associated with the Layer 2 solution which simplifies diagnostics, reduces transport overhead and improves performance. Calico also implements VxLAN and the newer version use VxLAN as it&rsquo;s default soluton.</p>
<p><img src="/img/include/calicodd.png" alt="Calico"  class="zoomable" /></p>
<h3 id="weave-net">Weave Net</h3>
<p>Connectivity is set up by the weave-net binary by attaching pods to the weave Linux bridge. The bridge is, in turn, attached to the Open vSwitch’s kernel datapath which forwards the packets over the vxlan interface towards the target node.</p>
<h3 id="cilium">Cilium</h3>
<p>Cilium is one of the most advanced and powerful Kubernetes networking solutions. At its core, it utilizes the power of <a href="https://ebpf.io/">eBPF</a> to perform a wide range of functionality ranging from traffic filtering for NetworkPolicies all the way to CNI and <a href="https://k8s.networkop.co.uk/cni/cilium/">kube-proxy replacement</a>. BPF is basically the ability of an application developer to write a program, load that into the Linux kernel, then run it when certain events happen.</p>
<p><img src="/img/include/ciliumart2.png" alt="Cilium"  class="zoomable" /></p>
<table>
<thead>
<tr>
<th>Features/Capabilities</th>
<th>Calico</th>
<th>Flannel</th>
<th>Weave</th>
<th>Cilium</th>
</tr>
</thead>
<tbody>
<tr>
<td>Network Model</td>
<td>BGP or VxLAN</td>
<td>VxLAN or UDP Channel</td>
<td>VxLAN or UDP Channel</td>
<td>VXLAN or Geneve</td>
</tr>
<tr>
<td>Encryption Channel</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
</tr>
<tr>
<td>NetworkPolicies</td>
<td>✓</td>
<td>X</td>
<td>✓</td>
<td>✓</td>
</tr>
<tr>
<td>kube-proxy replacement</td>
<td>✓</td>
<td>X</td>
<td>X</td>
<td>✓</td>
</tr>
<tr>
<td>Egress Routing</td>
<td>X</td>
<td>X</td>
<td>X</td>
<td>✓</td>
</tr>
</tbody>
</table>
<h2 id="cluster-mesh">Cluster mesh</h2>
<p>Cluster mesh enables direct networking between Pods and Services in different Kubernetes clusters, either on-premises or in the cloud.</p>
<h3 id="cilium-clustermesh">Cilium ClusterMesh</h3>
<p>As Kubernetes gains adoption, teams are finding they must deploy and manage multiple clusters to facilitate features like geo-redundancy, scale, and fault isolation for their applications. Cilium’s multi-cluster implementation provides the following features:</p>
<ul>
<li>Inter-cluster pod-to-pod connectivity without gateways or proxies.</li>
<li>Transparent service discovery across clusters using standard Kubernetes services and CoreDNS.</li>
<li>Network policy enforcement across clusters.</li>
<li>Encryption in transit between nodes within a cluster as well as across cluster boundaries.</li>
</ul>
<p><img src="/img/include/clustermesh-arch.png" alt="Cilium clustermesh"  class="zoomable" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Understanding kubernetes networking: pods and services]]></title>
            <link href="https://devopstales.github.io/home/kubernetes-networking-1/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/kubernetes-networking-1/?utm_source=atom_feed" rel="related" type="text/html" title="Understanding kubernetes networking: pods and services" />
                <link href="https://devopstales.github.io/home/k8s-calico-ebpf/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with calico&#39;s eBPF mode" />
                <link href="https://devopstales.github.io/home/k8s-ipvs/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with IPVS mode" />
                <link href="https://devopstales.github.io/home/rke2-cilium/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Install With cilium" />
                <link href="https://devopstales.github.io/home/k8s-security/?utm_source=atom_feed" rel="related" type="text/html" title="Best Practices to keeping Kubernetes Clusters Secure" />
            
                <id>https://devopstales.github.io/home/kubernetes-networking-1/</id>
            
            
            <published>2021-08-30T00:00:00+00:00</published>
            <updated>2021-08-30T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this series I will attempt to demystify the Kubernetes networkiing layers. This first part will look at containers and pods.</p>
<h3 id="what-is-a-pod">What is a pod?</h3>
<p>A Pod is the atom of Kubernetes — the smallest deployable object for building applications.  A single Pod represents an applications in your cluster and encapsulates one or more containers. Containers that make up a pod are designed to be co-located and scheduled on the same machine. They share the same resources like ip, network and volumes. In Linux, each running process communicates within a <a href="https://en.wikipedia.org/wiki/Linux_namespaces">Linux namespace</a> that provides a logical networking stack. In essence, a pod is a representation of a Linux namespace that allow the containers to usa the same resources. Containers within a Pod all have the same IP address and port range. They can find each other via localhost since they reside in the same namespace. This means the containers in a pod can not us the same ports.</p>
<p><img src="/img/include/k8s-pod-ip-with-cni.png" alt="Pow With IP"  class="zoomable" /></p>
<p><img src="/img/include/kubernetes-network-1-1.png" alt="Pod Network"  class="zoomable" /></p>
<p>Kubernetes creates a special container for each pod whose purpose is to provide a network interface for the other containers. This is the &ldquo;pause&rdquo; container.</p>
<p><img src="/img/include/kubernetes-network-1-2.png" alt="Pod Network"  class="zoomable" /></p>
<h3 id="pod-to-pod-communication">Pod-to-Pod communication</h3>
<p>Every Pod has a real IP address and each Pod communicates with other Pods using that IP address. From the Pod’s perspective, it exists in its own Ethernet namespace that needs to communicate with other network namespaces on the same Node. Namespaces can be connected using a Linux Virtual Ethernet Device (veth) This setup can be replicated for as many Pods as we have on the machine. The default Gateway in this internal network is a Linux bridge. A bridge is a virtual Layer 2 networking device used to unite two or more network segments to connect networks together.</p>
<p><img src="/img/include/kubernetes-network-1-3.gif" alt="Pod to Pod"  class="zoomable" /></p>
<p>Bridges implement the ARP protocol to discover the link-layer MAC address associated with a given IP address.</p>
<h4 id="what-is-a-service">What is a Service?</h4>
<p>As we know containers are considered disposable. That means there is no guarantee that the pod&rsquo;s address won’t change the next time the pod is recreated. That is a common problem in cloud environments too, and it has a standard solution: run the traffic through a reverse-proxy. This proxy is represented by a Kubernetes resource type called a service.</p>
<p><img src="/img/include/kubernetes-network-1-4.gif" alt="Pod to Service"  class="zoomable" /></p>
<h3 id="service-types">Service Types</h3>
<h4 id="clusterip">ClusterIP</h4>
<p>ClusterIP is the default and most common service type. Kubernetes will assign a cluster-internal IP address to ClusterIP service. This makes the service only reachable within the cluster. You can use it for inter service communication within the cluster. For example, communication between the front-end and back-end components of your app.</p>
<p><img src="/img/include/k8s-cluster-ip-service.png" alt="ClusterIP"  class="zoomable" /></p>
<h4 id="nodeport">NodePort</h4>
<p>NodePort service is an extension of ClusterIP service. It exposes the service outside of the cluster by adding a cluster-wide port nat on top of ClusterIP. Each node proxies that port to there ip. So, external traffic has access to fixed port on each Node. ode port must be in the range of 30000–32767. Manually allocating a port to the service is optional. If it is undefined, Kubernetes will automatically assign one.</p>
<p><img src="/img/include/k8s-node-port.png" alt="NodePort"  class="zoomable" /></p>
<h4 id="loadbalancer">LoadBalancer</h4>
<p>LoadBalancer service is an extension of NodePort service. It integrates NodePort with cloud-based load balancers. It exposes the Service externally using a cloud provider’s load balancer. Each cloud provider (AWS, Azure, GCP, etc) has its own native load balancer implementation. The cloud provider will create a load balancer, which then automatically routes requests to your Kubernetes Service. Traffic from the external load balancer is directed at the backend Pods. The cloud provider decides how it is load balanced. Every time you want to expose a service to the outside world, you have to create a new LoadBalancer and get an IP address.</p>
<p><img src="/img/include/k8s-external-ip-service.png" alt="LoadBalancer"  class="zoomable" /></p>
<p><img src="/img/include/k8s-external-ip-service.png" alt="External IP"  class="zoomable" /></p>
<h4 id="externalname">ExternalName</h4>
<p>ExternalName map a Service to a DNS name, not to a typical selector. It maps the Service to the contents of the externalName field (e.g. foo.bar.example.com), by returning a CNAME record with its value. This is commonly used to create a service within Kubernetes to represent an external datastore like a database that runs externally to Kubernetes.</p>
<h3 id="pod-to-service-communication">Pod-to-Service communication</h3>
<p>When creating a new Kubernetes Service, a new virtual IP is created on your behalf. Anywhere within the cluster, traffic addressed to this virtual IP will be routed or load-balanced to the Pod or Pods associated with the Service. Kubernetes use a networking framework built in to Linux kernel called <code>netfilter</code>.</p>
<p><code>iptables</code> is a user-space utility program that allows a system administrator to configure the IP packet filter rules of the Linux kernel firewall, implemented as different <code>Netfilter</code> modules. In Kubernetes, <code>iptables</code> rules are configured by the <code>kube-proxy</code> controller that watches the Kubernetes API for changes. Creation of a service or change of the pod ip will trigger iptables rules update on the host. When a traffic destined for a Service’s virtual IP is detected the <code>kube-proxy</code> select a random pod ip from the set of available Pods and manipulate the <code>iptables</code> rules to change the destination ip in the package to it. This method is called destination nat. In the return path <code>iptables</code> again rewrites the IP header to replace the Pod IP with the Service’s IP. The path taken by a packet through the networking stack is depicted in the figure shown below:</p>
<p><img src="/img/include/services-userspace-overview.svg" alt="Pod to Service"  class="zoomable" /></p>
<p>In the iptables perspective:</p>
<p><img src="/img/include/kubernetes-network-1-5.png" alt="iptables structure"  class="zoomable" /></p>
<h3 id="ipvs">IPVS</h3>
<p>Since verion 1.11 Kubernetes includes a second option for load balancing. IPVS (IP Virtual Server) is also built on top of <code>netfilter</code> and implements load balancing as part of the Linux kernel. IPVS can direct requests for TCP- and UDP-based services to the real servers, and make services of the real servers appear as virtual services on a single IP address. When creating a Service load balanced with IPVS, three things happen: a dummy IPVS interface is created on the Node, the Service’s IP address is bound to the dummy IPVS interface, and IPVS servers are created for each Service IP address.</p>
<p><img src="/img/include/services-ipvs-overview.svg" alt="Pod to Service"  class="zoomable" /></p>
<h3 id="ebpf">eBPF</h3>
<p>Some Network plugin is Kubernetes can act as a replacement for <code>kube-proxy</code> like Calico and Cilium. They use eBPF as a solution to solve the load-balancing problem.</p>
<h3 id="what-is-ebpf">What is eBPF?</h3>
<p>eBPF is a virtual machine embedded within the Linux kernel. It allows small programs to be loaded into the kernel, and attached to hooks, which are triggered when some event occurs. This allows the behavior of the kernel to be (sometimes heavily) customized. While the eBPF virtual machine is the same for each type of hook, the capabilities of the hooks vary considerably. Since loading programs into the kernel could be dangerous; the kernel runs all programs through a very strict static verifier; the verifier sandboxes the program, ensuring it can only access allowed parts of memory and ensuring that it must terminate quickly.</p>
<p>eBPF dataplane attaches eBPF programs to hooks on each bridge interface as well as your data and tunnel interfaces. This allows Calico or Cilium to spot workload packets early and handle them through a fast-path that bypasses iptables and other packet processing that the kernel would normally do.</p>
<p><em><strong>Calico:</strong></em>
<img src="/img/include/kubernetes-network-1-5.svg" alt="Calico Architecture"  class="zoomable" /></p>
<p><em><strong>Cilium:</strong></em>
<img src="/img/include/kubernetes-network-1-6.png" alt="Cilium Architecture"  class="zoomable" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to Backup Kubernetes to git?]]></title>
            <link href="https://devopstales.github.io/home/k8s-git-backup/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-git-backup/?utm_source=atom_feed" rel="related" type="text/html" title="How to Backup Kubernetes to git?" />
                <link href="https://devopstales.github.io/home/cilium-opnsense-bgp/?utm_source=atom_feed" rel="related" type="text/html" title="Use Cilium BGP integration with OPNsense" />
                <link href="https://devopstales.github.io/home/lazyimage/?utm_source=atom_feed" rel="related" type="text/html" title="Speed up docker pull with lazypull" />
                <link href="https://devopstales.github.io/home/k8s-metallb-bgp-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Self-hosted Load Balancer for bare metal Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-longhorn/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes project longhorn" />
            
                <id>https://devopstales.github.io/home/k8s-git-backup/</id>
            
            
            <published>2021-08-28T00:00:00+00:00</published>
            <updated>2021-08-28T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this tutorial I will show you how you can backup the kubernetes object to git as yaml-s.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<p>Thanky to <a href="https://github.com/WoozyMasta">Maxim Levchenko</a> ther is a grate tool called <a href="https://github.com/WoozyMasta/kube-dump">kube-dump</a> that is dump all of the kubernetes objects to a git repository as yaml. We will use this tool to backup.</p>
<p>Key features:</p>
<ul>
<li>Saving is done only for those resources to which you have read access.</li>
<li>You can pass a list of namespaces as an input, otherwise all available for your context will be used.</li>
<li>Both namespace resources and global cluster resources are subject to persistence.</li>
<li>You can use the utility locally as a regular script or run it in a container or in a kubernetes cluster, for example, as a CronJob.</li>
<li>It can create archives and rotate them after itself.</li>
<li>Can commit state to git repository and push to remote repository.</li>
<li>You can specify a specific list of cluster resources for unloading.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl create ns kube-dump
kubectl -n kube-dump apply -f <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  https://raw.githubusercontent.com/WoozyMasta/kube-dump/master/deploy/cluster-role-view.yaml
</code></pre></div><h3 id="deploy-with-git-repository-oauth-token">Deploy with git repository oauth token</h3>
<blockquote>
<p>Project access tokens are supported for self-managed instances on Free and above. They are also supported on GitLab SaaS Premium and above. If you use GitLab SaaS on Free you can us Personal access token instead of Project Access Token.</p>
</blockquote>
<p>As an example, I will use authorization in GitLab using the Project Access Token, so we will create a secret with the repository address and an authorization token:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl -n kube-dump create secret generic kube-dump <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --from-literal<span style="color:#f92672">=</span>GIT_REMOTE_URL<span style="color:#f92672">=</span>https://oauth2:$TOKEN@corp-gitlab.com/devops/cluster-01.git
</code></pre></div><blockquote>
<p>Before Kubernetes 1.22 CronJob&rsquo;s timezone is always UTC. If you want to change this use <a href="https://github.com/hiddeco/cronjobber">cronjobber</a>
Since Kubernetes 1.22 you can add <a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/">timezon in cronjob</a> with <code>CRON_TZ</code> variable.</p>
</blockquote>
<p>Let’s set up a CronJob in which we indicate the frequency of the task launch:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">wget https://github.com/WoozyMasta/kube-dump/blob/master/deploy/cronjob-git-token.yaml

nano cronjob-git-token.yaml
...
spec:
  schedule: <span style="color:#e6db74">&#34;0 1 * * *&#34;</span>

kubectl apply -f cronjob-git-token.yaml -n kube-dump
</code></pre></div><h3 id="deploy-with-git-repository-write-allowed-ssh-key">Deploy with git repository write allowed ssh key</h3>
<p>Generate ssh key:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">mkdir -p ./.ssh
chmod <span style="color:#ae81ff">0700</span> ./.ssh
ssh-keygen -t ed25519 -C <span style="color:#e6db74">&#34;kube-dump&#34;</span> -f ./.ssh/kube-dump
cat ./.ssh/kube-dump.pub

kubectl -n kube-dump create secret generic kube-dump-key <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --from-file<span style="color:#f92672">=</span>./.ssh/kube-dump <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --from-file<span style="color:#f92672">=</span>./.ssh/kube-dump.pub
</code></pre></div><p>Create pvc for store data such as cache:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -n kube-dump -f deploy/pvc.yaml
</code></pre></div><p>And apply the cron job manifest, previously you could set up environment variables:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">wget https://github.com/WoozyMasta/kube-dump/blob/master/deploy/cronjob-git-key.yaml

nano cronjob-git-key.yaml
...
spec:
  schedule: <span style="color:#e6db74">&#34;0 1 * * *&#34;</span>
...
              env:
                - name: MODE
                  value: <span style="color:#e6db74">&#34;dump&#34;</span>
                - name: DESTINATION_DIR
                  value: <span style="color:#e6db74">&#34;/data/dump&#34;</span>
                - name: GIT_PUSH
                  value: <span style="color:#e6db74">&#34;true&#34;</span>
                - name: GIT_BRANCH
                  value: <span style="color:#e6db74">&#34;master&#34;</span>
                - name: GIT_COMMIT_USER
                  value: <span style="color:#e6db74">&#34;Kube Dump&#34;</span>
                - name: GIT_COMMIT_EMAIL
                  value: <span style="color:#e6db74">&#34;kube@dump.local&#34;</span>
                - name: GIT_REMOTE_URL
                  value: <span style="color:#e6db74">&#34;git@corp-gitlab.com:devops/cluster-bkp.git&#34;</span>

kubectl apply -f cronjob-git-key.yaml -n kube-dump
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to deploy CRI-O with Firecracker?]]></title>
            <link href="https://devopstales.github.io/home/firecracker-cri-o/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/firecracker-containerd/?utm_source=atom_feed" rel="related" type="text/html" title="How to deploy containerd with Firecracker?" />
                <link href="https://devopstales.github.io/kubernetes/firecracker-cri-o/?utm_source=atom_feed" rel="related" type="text/html" title="How to deploy CRI-O with Firecracker?" />
                <link href="https://devopstales.github.io/kubernetes/firecracker-containerd/?utm_source=atom_feed" rel="related" type="text/html" title="How to deploy containerd with Firecracker?" />
                <link href="https://devopstales.github.io/home/kata-container-containerd/?utm_source=atom_feed" rel="related" type="text/html" title="How to deploy containerd with kata containers?" />
                <link href="https://devopstales.github.io/home/gvisor-cri-o/?utm_source=atom_feed" rel="related" type="text/html" title="How to deploy CRI-O with gVisor?" />
            
                <id>https://devopstales.github.io/home/firecracker-cri-o/</id>
            
            
            <published>2021-08-23T00:00:00+00:00</published>
            <updated>2021-08-23T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can install and use kata-container with Firecracker engine in kubernetes.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<h3 id="what-is-kata-container-engine">What is Kata container engine</h3>
<p>Kata Containers is an open source community working to build a secure container runtime with lightweight virtual machines that feel and perform like containers, but provide stronger workload isolation using hardware virtualization technology as a second layer of defense. (Source: <a href="https://katacontainers.io/">Kata Containers Website</a> )</p>
<p><img src="/img/include/katacontainers.jpg" alt="Kata container engine"  class="zoomable" /></p>
<h3 id="why-should-you-use-firecracker">Why should you use Firecracker?</h3>
<p>Firecracker is a way to run virtual machines, but its primary goal is to be used as a container runtime interface, making it use very few resources by design.</p>
<h3 id="enable-qvemu">Enable qvemu</h3>
<p>I will use Vagrant and VirtualBox for running the AlmaLinux VM so first I need to enable then Nested virtualization on the VM:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">VBoxManage modifyvm alma8 --nested-hw-virt on
</code></pre></div><p>After the Linux is booted test the virtualization flag in the VM:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">egrep --color -i <span style="color:#e6db74">&#34;svm|vmx&#34;</span> /proc/cpuinfo
</code></pre></div><p>If you find one of this flags everything is ok. Now we need to enable the kvm kernel module.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo modprobe kvm-intel
sudo modprobe vhost_vsock
</code></pre></div><h3 id="disable-selinux">Disable selinux</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">setenforce <span style="color:#ae81ff">0</span>
sed -i <span style="color:#e6db74">&#39;s/=\(enforcing\|permissive\)/=disabled/g&#39;</span> /etc/sysconfig/selinux
sed -i <span style="color:#e6db74">&#39;s/=\(enforcing\|permissive\)/=disabled/g&#39;</span> /etc/selinux/config
</code></pre></div><h3 id="install-and-configure-cri-o">Install and configure CRI-O</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo dnf install epel-release nano wget -y

export VERSION<span style="color:#f92672">=</span>1.21
sudo curl -L -o /etc/yum.repos.d/devel_kubic_libcontainers_stable.repo https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable/CentOS_8/devel:kubic:libcontainers:stable.repo

sudo curl -L -o /etc/yum.repos.d/devel_kubic_libcontainers_stable_cri-o_<span style="color:#e6db74">${</span>VERSION<span style="color:#e6db74">}</span>.repo https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:<span style="color:#e6db74">${</span>VERSION<span style="color:#e6db74">}</span>/CentOS_8/devel:kubic:libcontainers:stable:cri-o:<span style="color:#e6db74">${</span>VERSION<span style="color:#e6db74">}</span>.repo

yum install cri-o
</code></pre></div><p>Devmapper is the only storage driver supported by Firecracker. The stable pubic verion of <code>cri-o</code> dose not contained the support for devicemapper. Thanx to <code>Sascha Grunert</code> ther is a new version built with libdevmapper. He answered my question on slack immediately and created a patch for this bug.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano /etc/containers/storage.conf
<span style="color:#f92672">[</span>storage<span style="color:#f92672">]</span>
driver <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;devicemapper&#34;</span>
...
<span style="color:#f92672">[</span>storage.options.thinpool<span style="color:#f92672">]</span>
autoextend_percent <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;20&#34;</span>
autoextend_threshold <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;80&#34;</span>
basesize <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;8G&#34;</span>
directlvm_device <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;/dev/sdb&#34;</span>
directlvm_device_force <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;True&#34;</span>
fs<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;xfs&#34;</span>

nano /etc/containers/registries.conf
registries <span style="color:#f92672">=</span> <span style="color:#f92672">[</span>
  <span style="color:#e6db74">&#34;quay.io&#34;</span>,
  <span style="color:#e6db74">&#34;docker.io&#34;</span>
<span style="color:#f92672">]</span>
unqualified-search-registries <span style="color:#f92672">=</span> <span style="color:#f92672">[</span>
  <span style="color:#e6db74">&#34;quay.io&#34;</span>,
  <span style="color:#e6db74">&#34;docker.io&#34;</span>
<span style="color:#f92672">]</span>

systemctl enable crio
systemctl restart crio
systemctl status crio

$ crio-status info
storage root: /var/lib/containers/storage
default GID mappings <span style="color:#f92672">(</span>format &lt;container&gt;:&lt;host&gt;:&lt;size&gt;<span style="color:#f92672">)</span>:
  0:0:4294967295
default UID mappings <span style="color:#f92672">(</span>format &lt;container&gt;:&lt;host&gt;:&lt;size&gt;<span style="color:#f92672">)</span>:
  0:0:4294967295
</code></pre></div><h3 id="install-tools">Install tools</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">yum install git -y

sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubectx /usr/local/sbin/kubectx
sudo ln -s /opt/kubectx/kubens /usr/local/sbin/kubens
</code></pre></div><h3 id="install-kubernetes">Install Kubernetes</h3>
<p>Configure Kernel parameters for Kubernetes.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">modprobe overlay
modprobe br_netfilter

cat <span style="color:#e6db74">&lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf
</span><span style="color:#e6db74">overlay
</span><span style="color:#e6db74">br_netfilter
</span><span style="color:#e6db74">kvm-intel
</span><span style="color:#e6db74">vhost_vsock
</span><span style="color:#e6db74">EOF</span>

cat <span style="color:#e6db74">&lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf
</span><span style="color:#e6db74">net.bridge.bridge-nf-call-ip6tables = 1
</span><span style="color:#e6db74">net.bridge.bridge-nf-call-iptables = 1
</span><span style="color:#e6db74">net.ipv6.conf.all.disable_ipv6 = 1
</span><span style="color:#e6db74">net.ipv6.conf.default.disable_ipv6 = 1
</span><span style="color:#e6db74">net.ipv4.ip_forward                 = 1
</span><span style="color:#e6db74">net.bridge.bridge-nf-call-ip6tables = 1
</span><span style="color:#e6db74">EOF</span>

sysctl --system
</code></pre></div><p>Disable swap for Kubernetes.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">free -h
swapoff -a
swapoff -a
sed -i.bak -r <span style="color:#e6db74">&#39;s/(.+ swap .+)/#\1/&#39;</span> /etc/fstab
free -h
</code></pre></div><p>The I will add the kubernetes repo and Install the packages.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat <span style="color:#e6db74">&lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
</span><span style="color:#e6db74">[kubernetes]
</span><span style="color:#e6db74">name=Kubernetes
</span><span style="color:#e6db74">baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
</span><span style="color:#e6db74">enabled=1
</span><span style="color:#e6db74">gpgcheck=1
</span><span style="color:#e6db74">repo_gpgcheck=1
</span><span style="color:#e6db74">gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
</span><span style="color:#e6db74">EOF</span>

CRIP_VERSION<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>crio --version | awk <span style="color:#e6db74">&#39;{print $3}&#39;</span><span style="color:#66d9ef">)</span>
yum install kubelet-$CRIP_VERSION kubeadm-$CRIP_VERSION kubectl-$CRIP_VERSION -y
</code></pre></div><p>You nee the same cgroup manager in cri-o and kubeadm. The default for kubeadm is cgroupfs and for cri-o the default is systemd. In this example I configured cri-o for cgroupfs.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano /etc/crio/crio.conf
<span style="color:#f92672">[</span>crio.runtime<span style="color:#f92672">]</span>
conmon_cgroup <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;pod&#34;</span>
cgroup_manager <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;cgroupfs&#34;</span>

systemctl restart crio
</code></pre></div><p>If you want to use systemd:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">echo <span style="color:#e6db74">&#34;KUBELET_EXTRA_ARGS=--cgroup-driver=systemd&#34;</span> | tee /etc/sysconfig/kubelet
</code></pre></div><p>Start Kubernetes with containerd engine.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">export IP<span style="color:#f92672">=</span>172.17.13.10

dnf install -y iproute-tc

systemctl enable kubelet.service

<span style="color:#75715e"># for multi interface configuration</span>
echo <span style="color:#e6db74">&#39;KUBELET_EXTRA_ARGS=&#34;--node-ip=&#39;</span>$IP<span style="color:#e6db74">&#39; --cgroup-driver=systemd&#34;&#39;</span> &gt; /etc/sysconfig/kubelet

kubeadm config images pull --cri-socket<span style="color:#f92672">=</span>unix:///var/run/crio/crio.sock --kubernetes-version<span style="color:#f92672">=</span>$CRIP_VERSION
kubeadm init --pod-network-cidr<span style="color:#f92672">=</span>10.244.0.0/16 --apiserver-advertise-address<span style="color:#f92672">=</span>$IP  --kubernetes-version<span style="color:#f92672">=</span>$CRIP_VERSION --cri-socket<span style="color:#f92672">=</span>unix:///var/run/crio/crio.sock
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown <span style="color:#66d9ef">$(</span>id -u<span style="color:#66d9ef">)</span>:<span style="color:#66d9ef">$(</span>id -g<span style="color:#66d9ef">)</span> $HOME/.kube/config

kubectl get no
crictl ps

kubectl taint nodes <span style="color:#66d9ef">$(</span>hostname<span style="color:#66d9ef">)</span> node-role.kubernetes.io/master:NoSchedule-
</code></pre></div><h3 id="inincialize-network">Inincialize network</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
kubectl aplly -f kube-flannel.yml
</code></pre></div><p>OR</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml
wget https://docs.projectcalico.org/manifests/custom-resources.yaml

nano custom-resources.yaml
...
      cidr: 10.244.0.0/16
...

kubectl apply -f custom-resources.yaml
</code></pre></div><hr>
<h3 id="install-kata-container-engine">Install Kata container engine</h3>
<p>If all the Nodes are redy deploy a Daemonset to build Kata containers with firecracker:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ kubectl get no

NAME    STATUS   ROLES                  AGE     VERSION
alma8   Ready    control-plane,master   2m31s   v1.22.1
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f https://raw.githubusercontent.com/kata-containers/kata-containers/b24ee4b11e771108d08eadc37938ac2ecf9a6929/tools/packaging/kata-deploy/kata-rbac/base/kata-rbac.yaml

kubectl apply -f https://raw.githubusercontent.com/kata-containers/kata-containers/b24ee4b11e771108d08eadc37938ac2ecf9a6929/tools/packaging/kata-deploy/kata-deploy/base/kata-deploy.yaml
</code></pre></div><p>Verify the pod:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubens kube-system

$ kubectl get DaemonSet
NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
kata-deploy   <span style="color:#ae81ff">1</span>         <span style="color:#ae81ff">1</span>         <span style="color:#ae81ff">1</span>       <span style="color:#ae81ff">1</span>            <span style="color:#ae81ff">1</span>           &lt;none&gt;                   3m43s
kube-proxy    <span style="color:#ae81ff">1</span>         <span style="color:#ae81ff">1</span>         <span style="color:#ae81ff">1</span>       <span style="color:#ae81ff">1</span>            <span style="color:#ae81ff">1</span>           kubernetes.io/os<span style="color:#f92672">=</span>linux   10m

$ kubectl get po kata-deploy-5zwmq
NAME                READY   STATUS    RESTARTS   AGE
kata-deploy-5zwmq   1/1     Running   <span style="color:#ae81ff">0</span>          4m24
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl logs kata-deploy-5zwmq
copying kata artifacts onto host
<span style="color:#75715e">#!/bin/bash</span>
KATA_CONF_FILE<span style="color:#f92672">=</span>/opt/kata/share/defaults/kata-containers/configuration-fc.toml /opt/kata/bin/containerd-shim-kata-v2 <span style="color:#e6db74">&#34;</span>$@<span style="color:#e6db74">&#34;</span>
<span style="color:#75715e">#!/bin/bash</span>
KATA_CONF_FILE<span style="color:#f92672">=</span>/opt/kata/share/defaults/kata-containers/configuration-qemu.toml /opt/kata/bin/containerd-shim-kata-v2 <span style="color:#e6db74">&#34;</span>$@<span style="color:#e6db74">&#34;</span>
<span style="color:#75715e">#!/bin/bash</span>
KATA_CONF_FILE<span style="color:#f92672">=</span>/opt/kata/share/defaults/kata-containers/configuration-clh.toml /opt/kata/bin/containerd-shim-kata-v2 <span style="color:#e6db74">&#34;</span>$@<span style="color:#e6db74">&#34;</span>
Add Kata Containers as a supported runtime <span style="color:#66d9ef">for</span> CRIO:

<span style="color:#75715e"># Path to the Kata Containers runtime binary that uses the fc</span>
<span style="color:#f92672">[</span>crio.runtime.runtimes.kata-fc<span style="color:#f92672">]</span>
	runtime_path <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;/usr/local/bin/containerd-shim-kata-fc-v2&#34;</span>
	runtime_type <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;vm&#34;</span>
	runtime_root <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;/run/vc&#34;</span>
	privileged_without_host_devices <span style="color:#f92672">=</span> true

<span style="color:#75715e"># Path to the Kata Containers runtime binary that uses the qemu</span>
<span style="color:#f92672">[</span>crio.runtime.runtimes.kata-qemu<span style="color:#f92672">]</span>
	runtime_path <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;/usr/local/bin/containerd-shim-kata-qemu-v2&#34;</span>
	runtime_type <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;vm&#34;</span>
	runtime_root <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;/run/vc&#34;</span>
	privileged_without_host_devices <span style="color:#f92672">=</span> true

<span style="color:#75715e"># Path to the Kata Containers runtime binary that uses the clh</span>
<span style="color:#f92672">[</span>crio.runtime.runtimes.kata-clh<span style="color:#f92672">]</span>
	runtime_path <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;/usr/local/bin/containerd-shim-kata-clh-v2&#34;</span>
	runtime_type <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;vm&#34;</span>
	runtime_root <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;/run/vc&#34;</span>
	privileged_without_host_devices <span style="color:#f92672">=</span> true
node/alma8 labeled
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ ll /opt/kata/bin/
total <span style="color:#ae81ff">157532</span>
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root  <span style="color:#ae81ff">4045032</span> Jul <span style="color:#ae81ff">19</span> 06:10 cloud-hypervisor
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root <span style="color:#ae81ff">42252997</span> Jul <span style="color:#ae81ff">19</span> 06:12 containerd-shim-kata-v2
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root  <span style="color:#ae81ff">3290472</span> Jul <span style="color:#ae81ff">19</span> 06:14 firecracker
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root  <span style="color:#ae81ff">2589888</span> Jul <span style="color:#ae81ff">19</span> 06:14 jailer
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root    <span style="color:#ae81ff">16686</span> Jul <span style="color:#ae81ff">19</span> 06:12 kata-collect-data.sh
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root <span style="color:#ae81ff">37429099</span> Jul <span style="color:#ae81ff">19</span> 06:12 kata-monitor
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root <span style="color:#ae81ff">54149384</span> Jul <span style="color:#ae81ff">19</span> 06:12 kata-runtime
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root <span style="color:#ae81ff">17521656</span> Jul <span style="color:#ae81ff">19</span> 06:18 qemu-system-x86_64
</code></pre></div><p>Restart containerd to enable the new config:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">systemctl restart containerd
</code></pre></div><hr>
<h3 id="start-deployment">Start Deployment</h3>
<p>First I create a <code>RuntimeClass</code> for kata-fc then start a pod with this <code>RuntimeClass</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubens default

kubectl apply -f - <span style="color:#e6db74">&lt;&lt;EOF
</span><span style="color:#e6db74">apiVersion: node.k8s.io/v1
</span><span style="color:#e6db74">kind: RuntimeClass
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: kata-fc
</span><span style="color:#e6db74">handler: kata-fc
</span><span style="color:#e6db74">EOF</span>

cat<span style="color:#e6db74">&lt;&lt;EOF | kubectl apply -f -
</span><span style="color:#e6db74">apiVersion: v1
</span><span style="color:#e6db74">kind: Pod
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  labels:
</span><span style="color:#e6db74">    app: untrusted
</span><span style="color:#e6db74">  name: www-kata-fc
</span><span style="color:#e6db74">spec:
</span><span style="color:#e6db74">  runtimeClassName: kata-fc
</span><span style="color:#e6db74">  containers:
</span><span style="color:#e6db74">  - image: nginx:1.18
</span><span style="color:#e6db74">    name: www
</span><span style="color:#e6db74">    ports:
</span><span style="color:#e6db74">    - containerPort: 80
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f - <span style="color:#e6db74">&lt;&lt;EOF
</span><span style="color:#e6db74">apiVersion: node.k8s.io/v1
</span><span style="color:#e6db74">kind: RuntimeClass
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: kata-qemu
</span><span style="color:#e6db74">handler: kata-qemu
</span><span style="color:#e6db74">EOF</span>

cat<span style="color:#e6db74">&lt;&lt;EOF | kubectl apply -f -
</span><span style="color:#e6db74">apiVersion: v1
</span><span style="color:#e6db74">kind: Pod
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  labels:
</span><span style="color:#e6db74">    app: untrusted
</span><span style="color:#e6db74">  name: www-kata-qemu
</span><span style="color:#e6db74">spec:
</span><span style="color:#e6db74">  runtimeClassName: kata-qemu
</span><span style="color:#e6db74">  containers:
</span><span style="color:#e6db74">  - image: nginx:1.18
</span><span style="color:#e6db74">    name: www
</span><span style="color:#e6db74">    ports:
</span><span style="color:#e6db74">    - containerPort: 80
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f - <span style="color:#e6db74">&lt;&lt;EOF
</span><span style="color:#e6db74">apiVersion: node.k8s.io/v1
</span><span style="color:#e6db74">kind: RuntimeClass
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: kata-clh
</span><span style="color:#e6db74">handler: kata-clh
</span><span style="color:#e6db74">EOF</span>

cat<span style="color:#e6db74">&lt;&lt;EOF | kubectl apply -f -
</span><span style="color:#e6db74">apiVersion: v1
</span><span style="color:#e6db74">kind: Pod
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  labels:
</span><span style="color:#e6db74">    app: untrusted
</span><span style="color:#e6db74">  name: www-kata-clh
</span><span style="color:#e6db74">spec:
</span><span style="color:#e6db74">  runtimeClassName: kata-clh
</span><span style="color:#e6db74">  containers:
</span><span style="color:#e6db74">  - image: nginx:1.18
</span><span style="color:#e6db74">    name: www
</span><span style="color:#e6db74">    ports:
</span><span style="color:#e6db74">    - containerPort: 80
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ kubectl get po
NAME            READY   STATUS    RESTARTS   AGE
www-kata-clh    1/1     Running   <span style="color:#ae81ff">0</span>          59s
www-kata-fc     1/1     Running   <span style="color:#ae81ff">0</span>          12s
www-kata-qemu   1/1     Running   <span style="color:#ae81ff">0</span>          69s
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to deploy CRI-O with gVisor?]]></title>
            <link href="https://devopstales.github.io/home/gvisor-cri-o/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/firecracker-cri-o/?utm_source=atom_feed" rel="related" type="text/html" title="How to deploy CRI-O with Firecracker?" />
                <link href="https://devopstales.github.io/home/gvisor-containerd/?utm_source=atom_feed" rel="related" type="text/html" title="How to deploy containerd with gVisor?" />
                <link href="https://devopstales.github.io/kubernetes/gvisor-cri-o/?utm_source=atom_feed" rel="related" type="text/html" title="How to deploy CRI-O with gVisor?" />
                <link href="https://devopstales.github.io/home/firecracker-containerd/?utm_source=atom_feed" rel="related" type="text/html" title="How to deploy containerd with Firecracker?" />
                <link href="https://devopstales.github.io/home/kata-container-containerd/?utm_source=atom_feed" rel="related" type="text/html" title="How to deploy containerd with kata containers?" />
            
                <id>https://devopstales.github.io/home/gvisor-cri-o/</id>
            
            
            <published>2021-08-23T00:00:00+00:00</published>
            <updated>2021-08-23T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can install and use gvisor engine in kubernetes.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<h3 id="what-is-gvisor">What is gvisor</h3>
<p>gVisor is an application kernel, written in Go, that implements a substantial portion of the Linux system call interface. It provides an additional layer of isolation between running applications and the host operating system.</p>
<p>gVisor includes an Open Container Initiative (OCI) runtime called <code>runsc</code> that makes it easy to work with existing container tooling. The <code>runsc</code> runtime integrates with Docker, CRI-O and Kubernetes, making it simple to run sandboxed containers.</p>
<p><img src="/img/include/gvisor2.png" alt="gvisor"  class="zoomable" />
<img src="/img/include/gvisor.png" alt="gvisor"  class="zoomable" /></p>
<h3 id="install-gvisor">Install gvisor</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo dnf install epel-release nano wget -y

nano gvisor.sh
<span style="color:#75715e">#!/bash</span>
<span style="color:#f92672">(</span>
  set -e
  ARCH<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>uname -m<span style="color:#66d9ef">)</span>
  URL<span style="color:#f92672">=</span>https://storage.googleapis.com/gvisor/releases/release/latest/<span style="color:#e6db74">${</span>ARCH<span style="color:#e6db74">}</span>
  wget <span style="color:#e6db74">${</span>URL<span style="color:#e6db74">}</span>/runsc <span style="color:#e6db74">${</span>URL<span style="color:#e6db74">}</span>/runsc.sha512 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    <span style="color:#e6db74">${</span>URL<span style="color:#e6db74">}</span>/containerd-shim-runsc-v1 <span style="color:#e6db74">${</span>URL<span style="color:#e6db74">}</span>/containerd-shim-runsc-v1.sha512
  sha512sum -c runsc.sha512 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    -c containerd-shim-runsc-v1.sha512
  rm -f *.sha512
  chmod a+rx runsc containerd-shim-runsc-v1
  sudo mv runsc containerd-shim-runsc-v1 /usr/local/bin
<span style="color:#f92672">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">bash gvisor.sh
...
runsc: OK
containerd-shim-runsc-v1: OK
</code></pre></div><h3 id="install-and-configure-cri-o">Install and configure CRI-O</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">export VERSION<span style="color:#f92672">=</span>1.21
sudo curl -L -o /etc/yum.repos.d/devel_kubic_libcontainers_stable.repo https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable/CentOS_8/devel:kubic:libcontainers:stable.repo

sudo curl -L -o /etc/yum.repos.d/devel_kubic_libcontainers_stable_cri-o_<span style="color:#e6db74">${</span>VERSION<span style="color:#e6db74">}</span>.repo https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:<span style="color:#e6db74">${</span>VERSION<span style="color:#e6db74">}</span>/CentOS_8/devel:kubic:libcontainers:stable:cri-o:<span style="color:#e6db74">${</span>VERSION<span style="color:#e6db74">}</span>.repo

yum install cri-o
</code></pre></div><p><code>runsc</code> implements cgroups using <code>cgroupfs</code> so I will use <code>cgroupfs</code> in <code>CRI-O</code> and Kubernets config.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano /etc/crio/crio.conf
<span style="color:#f92672">[</span>crio.runtime<span style="color:#f92672">]</span>
conmon_cgroup <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;pod&#34;</span>
cgroup_manager <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;cgroupfs&#34;</span>
selinux <span style="color:#f92672">=</span> false

nano /etc/containers/registries.conf
registries <span style="color:#f92672">=</span> <span style="color:#f92672">[</span>
  <span style="color:#e6db74">&#34;quay.io&#34;</span>,
  <span style="color:#e6db74">&#34;docker.io&#34;</span>
<span style="color:#f92672">]</span>
unqualified-search-registries <span style="color:#f92672">=</span> <span style="color:#f92672">[</span>
  <span style="color:#e6db74">&#34;quay.io&#34;</span>,
  <span style="color:#e6db74">&#34;docker.io&#34;</span>
<span style="color:#f92672">]</span>
</code></pre></div><p>Now I need to configure <code>CRI-O</code> to use <code>runsc</code> as low-level runetime egine.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">mkdir /etc/crio/crio.conf.d/
cat <span style="color:#e6db74">&lt;&lt;EOF &gt; /etc/crio/crio.conf.d/99-gvisor
</span><span style="color:#e6db74"># Path to the gVisor runtime binary that uses runsc
</span><span style="color:#e6db74">[crio.runtime.runtimes.runsc]
</span><span style="color:#e6db74">runtime_path = &#34;/usr/local/bin/runsc&#34;
</span><span style="color:#e6db74">EOF</span>

systemctl enable crio
systemctl restart crio
systemctl status crio
</code></pre></div><h3 id="install-tools">Install tools</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">yum install git -y

sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubectx /usr/local/sbin/kubectx
sudo ln -s /opt/kubectx/kubens /usr/local/sbin/kubens
</code></pre></div><h3 id="install-kubernetes">Install Kubernetes</h3>
<p>Configure Kernel parameters for Kubernetes.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat <span style="color:#e6db74">&lt;&lt;EOF | sudo tee /etc/modules-load.d/CRI-O.conf
</span><span style="color:#e6db74">overlay
</span><span style="color:#e6db74">br_netfilter
</span><span style="color:#e6db74">EOF</span>

sudo modprobe overlay
sudo modprobe br_netfilter

cat <span style="color:#e6db74">&lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
</span><span style="color:#e6db74">net.bridge.bridge-nf-call-iptables  = 1
</span><span style="color:#e6db74">net.ipv4.ip_forward                 = 1
</span><span style="color:#e6db74">net.bridge.bridge-nf-call-ip6tables = 1
</span><span style="color:#e6db74">EOF</span>

sysctl --system
</code></pre></div><p>Disable swap for Kubernetes.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">free -h
swapoff -a
swapoff -a
sed -i.bak -r <span style="color:#e6db74">&#39;s/(.+ swap .+)/#\1/&#39;</span> /etc/fstab
free -h
</code></pre></div><p>The I will add the kubernetes repo and Install the packages.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat <span style="color:#e6db74">&lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
</span><span style="color:#e6db74">[kubernetes]
</span><span style="color:#e6db74">name=Kubernetes
</span><span style="color:#e6db74">baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
</span><span style="color:#e6db74">enabled=1
</span><span style="color:#e6db74">gpgcheck=1
</span><span style="color:#e6db74">repo_gpgcheck=1
</span><span style="color:#e6db74">gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
</span><span style="color:#e6db74">EOF</span>

CRIP_VERSION<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>crio --version | awk <span style="color:#e6db74">&#39;{print $3}&#39;</span><span style="color:#66d9ef">)</span>
yum install kubelet-$CRIP_VERSION kubeadm-$CRIP_VERSION kubectl-$CRIP_VERSION -y
</code></pre></div><p>Start Kubernetes with CRI-O engine.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">export IP<span style="color:#f92672">=</span>172.17.13.10

dnf install -y iproute-tc

systemctl enable kubelet.service

<span style="color:#75715e"># for multi interface configuration</span>
echo <span style="color:#e6db74">&#39;KUBELET_EXTRA_ARGS=&#34;--node-ip=&#39;</span>$IP<span style="color:#e6db74">&#39; --cgroup-driver=cgroupfs&#34;&#39;</span> &gt; /etc/sysconfig/kubelet

kubeadm config images pull --cri-socket<span style="color:#f92672">=</span>unix:///var/run/crio/crio.sock --kubernetes-version<span style="color:#f92672">=</span>$CRIP_VERSION
kubeadm init --pod-network-cidr<span style="color:#f92672">=</span>10.244.0.0/16 --apiserver-advertise-address<span style="color:#f92672">=</span>$IP --kubernetes-version<span style="color:#f92672">=</span>$CRIP_VERSION --cri-socket<span style="color:#f92672">=</span>unix:///var/run/crio/crio.sock
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown <span style="color:#66d9ef">$(</span>id -u<span style="color:#66d9ef">)</span>:<span style="color:#66d9ef">$(</span>id -g<span style="color:#66d9ef">)</span> $HOME/.kube/config

kubectl get no
crictl ps

kubectl taint nodes <span style="color:#66d9ef">$(</span>hostname<span style="color:#66d9ef">)</span> node-role.kubernetes.io/master:NoSchedule-
</code></pre></div><h3 id="inincialize-network">Inincialize network</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
kubectl aplly -f kube-flannel.yml
</code></pre></div><p>OR</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml
wget https://docs.projectcalico.org/manifests/custom-resources.yaml

nano custom-resources.yaml
...
      cidr: 10.244.0.0/16
...

kubectl apply -f custom-resources.yaml
</code></pre></div><h3 id="start-deployment">Start Deployment</h3>
<p>First I create a <code>RuntimeClass</code> for gvisor then start a pod with this <code>RuntimeClass</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat<span style="color:#e6db74">&lt;&lt;EOF | kubectl apply -f -
</span><span style="color:#e6db74">apiVersion: node.k8s.io/v1
</span><span style="color:#e6db74">kind: RuntimeClass
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: gvisor
</span><span style="color:#e6db74">handler: runsc
</span><span style="color:#e6db74">EOF</span>

cat<span style="color:#e6db74">&lt;&lt;EOF | kubectl apply -f -
</span><span style="color:#e6db74">apiVersion: v1
</span><span style="color:#e6db74">kind: Pod
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  labels:
</span><span style="color:#e6db74">    app: untrusted
</span><span style="color:#e6db74">  name: www-gvisor2
</span><span style="color:#e6db74">spec:
</span><span style="color:#e6db74">  runtimeClassName: gvisor
</span><span style="color:#e6db74">  containers:
</span><span style="color:#e6db74">  - image: nginx:1.18
</span><span style="color:#e6db74">    name: www
</span><span style="color:#e6db74">    ports:
</span><span style="color:#e6db74">    - containerPort: 80
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ kubectl get po
NAME        READY   STATUS    RESTARTS   AGE
www-gvisor  1/1     Running   <span style="color:#ae81ff">0</span>          2m47s


$ kubectl describe po www-gvisor
...
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  2m42s  default-scheduler  Successfully assigned default/www-kata to alma8
  Normal  Pulled     2m13s  kubelet            Container image <span style="color:#e6db74">&#34;nginx:1.18&#34;</span> already present on machine
  Normal  Created    2m13s  kubelet            Created container www
  Normal  Started    2m11s  kubelet            Started container www
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to deploy containerd with Firecracker?]]></title>
            <link href="https://devopstales.github.io/home/firecracker-containerd/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/firecracker-containerd/?utm_source=atom_feed" rel="related" type="text/html" title="How to deploy containerd with Firecracker?" />
                <link href="https://devopstales.github.io/home/kata-container-containerd/?utm_source=atom_feed" rel="related" type="text/html" title="How to deploy containerd with kata containers?" />
                <link href="https://devopstales.github.io/home/gvisor-containerd/?utm_source=atom_feed" rel="related" type="text/html" title="How to deploy containerd with gVisor?" />
                <link href="https://devopstales.github.io/kubernetes/kata-container-containerd/?utm_source=atom_feed" rel="related" type="text/html" title="How to deploy containerd with kata containers?" />
                <link href="https://devopstales.github.io/kubernetes/gvisor-containerd/?utm_source=atom_feed" rel="related" type="text/html" title="How to deploy containerd with gVisor?" />
            
                <id>https://devopstales.github.io/home/firecracker-containerd/</id>
            
            
            <published>2021-08-22T00:00:00+00:00</published>
            <updated>2021-08-22T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can install and use kata-container with Firecracker engine in kubernetes.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<h3 id="what-is-kata-container-engine">What is Kata container engine</h3>
<p>Kata Containers is an open source community working to build a secure container runtime with lightweight virtual machines that feel and perform like containers, but provide stronger workload isolation using hardware virtualization technology as a second layer of defense. (Source: <a href="https://katacontainers.io/">Kata Containers Website</a> )</p>
<p><img src="/img/include/katacontainers.jpg" alt="Kata container engine"  class="zoomable" /></p>
<h3 id="why-should-you-use-firecracker">Why should you use Firecracker?</h3>
<p>Firecracker is a way to run virtual machines, but its primary goal is to be used as a container runtime interface, making it use very few resources by design.</p>
<h3 id="enable-qvemu">Enable qvemu</h3>
<p>I will use Vagrant and VirtualBox for running the AlmaLinux VM so first I need to enable then Nested virtualization on the VM:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">VBoxManage modifyvm alma8 --nested-hw-virt on
</code></pre></div><p>After the Linux is booted test the virtualization flag in the VM:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">egrep --color -i <span style="color:#e6db74">&#34;svm|vmx&#34;</span> /proc/cpuinfo
</code></pre></div><p>If you find one of this flags everything is ok. Now we need to enable the kvm kernel module.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo modprobe kvm-intel
sudo modprobe vhost_vsock
</code></pre></div><h3 id="install-and-configure-containerd">Install and configure containerd</h3>
<p>To work with firecracker the containerd must use devmapper for snapsoter plugin.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo dnf install epel-release nano wget -y

sudo dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
sudo dnf install -y containerd.io lvm2
</code></pre></div><p>First I installed the containerd. Devmapper is the only storage driver supported by Firecracker so now I create the thin-pool lvm for devmapper snapsoter:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo pvcreate /dev/sdb
sudo vgcreate containerd /dev/sdb
sudo lvcreate --wipesignatures y -n data containerd -l 95%VG
sudo lvcreate --wipesignatures y -n meta containerd -l 1%VG

sudo lvconvert -y <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--zero n <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>-c 512K <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--thinpool containerd/data <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--poolmetadata containerd/meta

nano /etc/lvm/profile/ontainerd-thinpool.profile
activation <span style="color:#f92672">{</span>
  thin_pool_autoextend_threshold<span style="color:#f92672">=</span><span style="color:#ae81ff">80</span>
  thin_pool_autoextend_percent<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>
<span style="color:#f92672">}</span>

sudo lvchange --metadataprofile ontainerd-thinpool containerd/data
sudo lvchange --monitor y containerd/data
</code></pre></div><p>And <code>dmsetup</code> will produce the following output:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">containerd-data	<span style="color:#f92672">(</span>253:2<span style="color:#f92672">)</span>
containerd-data_tdata	<span style="color:#f92672">(</span>253:1<span style="color:#f92672">)</span>
containerd-data_tmeta	<span style="color:#f92672">(</span>253:0<span style="color:#f92672">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo mkdir -p /etc/containerd
sudo containerd config default &gt; /etc/containerd/config.toml
nano /etc/containerd/config.toml
<span style="color:#f92672">[</span>plugins<span style="color:#f92672">]</span>
  ...
  <span style="color:#f92672">[</span>plugins.<span style="color:#e6db74">&#34;io.containerd.grpc.v1.cri&#34;</span>.containerd<span style="color:#f92672">]</span>
    snapshotter <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;devmapper&#34;</span>
  ...
  <span style="color:#f92672">[</span>plugins.<span style="color:#e6db74">&#34;io.containerd.snapshotter.v1.devmapper&#34;</span><span style="color:#f92672">]</span>
    pool_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;containerd-data&#34;</span>
    base_image_size <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;8GB&#34;</span>
    async_remove <span style="color:#f92672">=</span> false
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">echo <span style="color:#e6db74">&#34;runtime-endpoint: unix:///run/containerd/containerd.sock&#34;</span> &gt; /etc/crictl.yaml

<span style="color:#75715e"># Restart containerd</span>
sudo systemctl restart containerd
systemctl enable containerd.service
</code></pre></div><p>Test if working correctly:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">systemctl restart containerd
ctr images pull --snapshotter devmapper docker.io/library/hello-world:latest
ctr run --snapshotter devmapper docker.io/library/hello-world:latest test
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ ctr plugin ls
TYPE                            ID                       PLATFORMS      STATUS
io.containerd.content.v1        content                  -              ok
io.containerd.snapshotter.v1    aufs                     linux/amd64    error
io.containerd.snapshotter.v1    devmapper                linux/amd64    ok
io.containerd.snapshotter.v1    native                   linux/amd64    ok
io.containerd.snapshotter.v1    overlayfs                linux/amd64    ok
io.containerd.snapshotter.v1    zfs                      linux/amd64    error
io.containerd.metadata.v1       bolt                     -              ok
io.containerd.differ.v1         walking                  linux/amd64    ok
io.containerd.gc.v1             scheduler                -              ok
io.containerd.service.v1        introspection-service    -              ok
io.containerd.service.v1        containers-service       -              ok
io.containerd.service.v1        content-service          -              ok
io.containerd.service.v1        diff-service             -              ok
io.containerd.service.v1        images-service           -              ok
io.containerd.service.v1        leases-service           -              ok
io.containerd.service.v1        namespaces-service       -              ok
io.containerd.service.v1        snapshots-service        -              ok
io.containerd.runtime.v1        linux                    linux/amd64    ok
io.containerd.runtime.v2        task                     linux/amd64    ok
io.containerd.monitor.v1        cgroups                  linux/amd64    ok
io.containerd.service.v1        tasks-service            -              ok
io.containerd.internal.v1       restart                  -              ok
io.containerd.grpc.v1           containers               -              ok
io.containerd.grpc.v1           content                  -              ok
io.containerd.grpc.v1           diff                     -              ok
io.containerd.grpc.v1           events                   -              ok
io.containerd.grpc.v1           healthcheck              -              ok
io.containerd.grpc.v1           images                   -              ok
io.containerd.grpc.v1           leases                   -              ok
io.containerd.grpc.v1           namespaces               -              ok
io.containerd.internal.v1       opt                      -              ok
io.containerd.grpc.v1           snapshots                -              ok
io.containerd.grpc.v1           tasks                    -              ok
io.containerd.grpc.v1           version                  -              ok
io.containerd.grpc.v1           cri                      linux/amd64    ok
</code></pre></div><h3 id="install-nerdctl">Install nerdctl</h3>
<p>I like to use <code>nerdctl</code> instad of <code>ctr</code> or <code>crictl</code> cli so I will install it.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">wget https://github.com/containerd/nerdctl/releases/download/v0.11.0/nerdctl-0.11.0-linux-amd64.tar.gz

tar -xzf nerdctl-0.11.0-linux-amd64.tar.gz
mv nerdctl /usr/local/bin
nerdctl ps
</code></pre></div><h3 id="install-tools">Install tools</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">yum install git -y

sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubectx /usr/local/sbin/kubectx
sudo ln -s /opt/kubectx/kubens /usr/local/sbin/kubens
</code></pre></div><h3 id="install-kubernetes">Install Kubernetes</h3>
<p>Configure Kernel parameters for Kubernetes.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat <span style="color:#e6db74">&lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf
</span><span style="color:#e6db74">overlay
</span><span style="color:#e6db74">br_netfilter
</span><span style="color:#e6db74">kvm-intel
</span><span style="color:#e6db74">vhost_vsock
</span><span style="color:#e6db74">EOF</span>

sudo modprobe overlay
sudo modprobe br_netfilter

cat <span style="color:#e6db74">&lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
</span><span style="color:#e6db74">net.bridge.bridge-nf-call-iptables  = 1
</span><span style="color:#e6db74">net.ipv4.ip_forward                 = 1
</span><span style="color:#e6db74">net.bridge.bridge-nf-call-ip6tables = 1
</span><span style="color:#e6db74">EOF</span>

sysctl --system
</code></pre></div><p>Disable swap for Kubernetes.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">free -h
swapoff -a
swapoff -a
sed -i.bak -r <span style="color:#e6db74">&#39;s/(.+ swap .+)/#\1/&#39;</span> /etc/fstab
free -h
</code></pre></div><p>The I will add the kubernetes repo and Install the packages.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat <span style="color:#e6db74">&lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
</span><span style="color:#e6db74">[kubernetes]
</span><span style="color:#e6db74">name=Kubernetes
</span><span style="color:#e6db74">baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
</span><span style="color:#e6db74">enabled=1
</span><span style="color:#e6db74">gpgcheck=1
</span><span style="color:#e6db74">repo_gpgcheck=1
</span><span style="color:#e6db74">gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
</span><span style="color:#e6db74">EOF</span>

dnf install kubelet kubeadm kubectl -y
</code></pre></div><p>Start Kubernetes with containerd engine.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">export IP<span style="color:#f92672">=</span>172.17.13.10

dnf install -y iproute-tc

systemctl enable kubelet.service

echo <span style="color:#e6db74">&#34;KUBELET_EXTRA_ARGS=--cgroup-driver=systemd&#34;</span> | tee /etc/sysconfig/kubelet

kubeadm config images pull --cri-socket<span style="color:#f92672">=</span>unix:///run/containerd/containerd.sock
kubeadm init --pod-network-cidr<span style="color:#f92672">=</span>10.244.0.0/16 --apiserver-advertise-address<span style="color:#f92672">=</span>$IP --cri-socket<span style="color:#f92672">=</span>unix:///run/containerd/containerd.sock
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown <span style="color:#66d9ef">$(</span>id -u<span style="color:#66d9ef">)</span>:<span style="color:#66d9ef">$(</span>id -g<span style="color:#66d9ef">)</span> $HOME/.kube/config

kubectl get no
nerdctl -n k8s.io ps
crictl ps

kubectl taint nodes <span style="color:#66d9ef">$(</span>hostname<span style="color:#66d9ef">)</span> node-role.kubernetes.io/master:NoSchedule-
</code></pre></div><h3 id="inincialize-network">Inincialize network</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
kubectl aplly -f kube-flannel.yml
</code></pre></div><p>OR</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml
wget https://docs.projectcalico.org/manifests/custom-resources.yaml

nano custom-resources.yaml
...
      cidr: 10.244.0.0/16
...

kubectl apply -f custom-resources.yaml
</code></pre></div><hr>
<h3 id="install-kata-container-engine">Install Kata container engine</h3>
<p>If all the Nodes are redy deploy a Daemonset to build Kata containers with firecracker:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ kubectl get no

NAME    STATUS   ROLES                  AGE     VERSION
alma8   Ready    control-plane,master   2m31s   v1.22.1
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f https://raw.githubusercontent.com/kata-containers/kata-containers/b24ee4b11e771108d08eadc37938ac2ecf9a6929/tools/packaging/kata-deploy/kata-rbac/base/kata-rbac.yaml

kubectl apply -f https://raw.githubusercontent.com/kata-containers/kata-containers/b24ee4b11e771108d08eadc37938ac2ecf9a6929/tools/packaging/kata-deploy/kata-deploy/base/kata-deploy.yaml
</code></pre></div><p>Verify the pod:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubens kube-system

$ kubectl get DaemonSet
NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
kata-deploy   <span style="color:#ae81ff">1</span>         <span style="color:#ae81ff">1</span>         <span style="color:#ae81ff">1</span>       <span style="color:#ae81ff">1</span>            <span style="color:#ae81ff">1</span>           &lt;none&gt;                   3m43s
kube-proxy    <span style="color:#ae81ff">1</span>         <span style="color:#ae81ff">1</span>         <span style="color:#ae81ff">1</span>       <span style="color:#ae81ff">1</span>            <span style="color:#ae81ff">1</span>           kubernetes.io/os<span style="color:#f92672">=</span>linux   10m

$ kubectl get po kata-deploy-5zwmq
NAME                READY   STATUS    RESTARTS   AGE
kata-deploy-5zwmq   1/1     Running   <span style="color:#ae81ff">0</span>          4m24
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl logs kata-deploy-5zwmq
copying kata artifacts onto host
<span style="color:#75715e">#!/bin/bash</span>
KATA_CONF_FILE<span style="color:#f92672">=</span>/opt/kata/share/defaults/kata-containers/configuration-fc.toml /opt/kata/bin/containerd-shim-kata-v2 <span style="color:#e6db74">&#34;</span>$@<span style="color:#e6db74">&#34;</span>
<span style="color:#75715e">#!/bin/bash</span>
KATA_CONF_FILE<span style="color:#f92672">=</span>/opt/kata/share/defaults/kata-containers/configuration-qemu.toml /opt/kata/bin/containerd-shim-kata-v2 <span style="color:#e6db74">&#34;</span>$@<span style="color:#e6db74">&#34;</span>
<span style="color:#75715e">#!/bin/bash</span>
KATA_CONF_FILE<span style="color:#f92672">=</span>/opt/kata/share/defaults/kata-containers/configuration-clh.toml /opt/kata/bin/containerd-shim-kata-v2 <span style="color:#e6db74">&#34;</span>$@<span style="color:#e6db74">&#34;</span>
Add Kata Containers as a supported runtime <span style="color:#66d9ef">for</span> containerd
<span style="color:#f92672">[</span>plugins.<span style="color:#e6db74">&#34;io.containerd.grpc.v1.cri&#34;</span>.containerd.runtimes.kata<span style="color:#f92672">]</span>
  runtime_type <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;io.containerd.kata.v2&#34;</span>
  privileged_without_host_devices <span style="color:#f92672">=</span> true
  pod_annotations <span style="color:#f92672">=</span> <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;io.katacontainers.*&#34;</span><span style="color:#f92672">]</span>
  <span style="color:#f92672">[</span>plugins.<span style="color:#e6db74">&#34;io.containerd.grpc.v1.cri&#34;</span>.containerd.runtimes.kata.options<span style="color:#f92672">]</span>
    ConfigPath <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;/opt/kata/share/defaults/kata-containers/configuration.toml&#34;</span>
<span style="color:#f92672">[</span>plugins.<span style="color:#e6db74">&#34;io.containerd.grpc.v1.cri&#34;</span>.containerd.runtimes.kata-fc<span style="color:#f92672">]</span>
  runtime_type <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;io.containerd.kata-fc.v2&#34;</span>
  privileged_without_host_devices <span style="color:#f92672">=</span> true
  pod_annotations <span style="color:#f92672">=</span> <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;io.katacontainers.*&#34;</span><span style="color:#f92672">]</span>
  <span style="color:#f92672">[</span>plugins.<span style="color:#e6db74">&#34;io.containerd.grpc.v1.cri&#34;</span>.containerd.runtimes.kata-fc.options<span style="color:#f92672">]</span>
    ConfigPath <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;/opt/kata/share/defaults/kata-containers/configuration-fc.toml&#34;</span>
<span style="color:#f92672">[</span>plugins.<span style="color:#e6db74">&#34;io.containerd.grpc.v1.cri&#34;</span>.containerd.runtimes.kata-qemu<span style="color:#f92672">]</span>
  runtime_type <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;io.containerd.kata-qemu.v2&#34;</span>
  privileged_without_host_devices <span style="color:#f92672">=</span> true
  pod_annotations <span style="color:#f92672">=</span> <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;io.katacontainers.*&#34;</span><span style="color:#f92672">]</span>
  <span style="color:#f92672">[</span>plugins.<span style="color:#e6db74">&#34;io.containerd.grpc.v1.cri&#34;</span>.containerd.runtimes.kata-qemu.options<span style="color:#f92672">]</span>
    ConfigPath <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;/opt/kata/share/defaults/kata-containers/configuration-qemu.toml&#34;</span>
<span style="color:#f92672">[</span>plugins.<span style="color:#e6db74">&#34;io.containerd.grpc.v1.cri&#34;</span>.containerd.runtimes.kata-clh<span style="color:#f92672">]</span>
  runtime_type <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;io.containerd.kata-clh.v2&#34;</span>
  privileged_without_host_devices <span style="color:#f92672">=</span> true
  pod_annotations <span style="color:#f92672">=</span> <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;io.katacontainers.*&#34;</span><span style="color:#f92672">]</span>
  <span style="color:#f92672">[</span>plugins.<span style="color:#e6db74">&#34;io.containerd.grpc.v1.cri&#34;</span>.containerd.runtimes.kata-clh.options<span style="color:#f92672">]</span>
    ConfigPath <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;/opt/kata/share/defaults/kata-containers/configuration-clh.toml&#34;</span>
node/alma8 labeled
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ ll /opt/kata/bin/
total <span style="color:#ae81ff">157532</span>
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root  <span style="color:#ae81ff">4045032</span> Jul <span style="color:#ae81ff">19</span> 06:10 cloud-hypervisor
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root <span style="color:#ae81ff">42252997</span> Jul <span style="color:#ae81ff">19</span> 06:12 containerd-shim-kata-v2
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root  <span style="color:#ae81ff">3290472</span> Jul <span style="color:#ae81ff">19</span> 06:14 firecracker
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root  <span style="color:#ae81ff">2589888</span> Jul <span style="color:#ae81ff">19</span> 06:14 jailer
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root    <span style="color:#ae81ff">16686</span> Jul <span style="color:#ae81ff">19</span> 06:12 kata-collect-data.sh
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root <span style="color:#ae81ff">37429099</span> Jul <span style="color:#ae81ff">19</span> 06:12 kata-monitor
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root <span style="color:#ae81ff">54149384</span> Jul <span style="color:#ae81ff">19</span> 06:12 kata-runtime
-rwxr-xr-x. <span style="color:#ae81ff">1</span> root root <span style="color:#ae81ff">17521656</span> Jul <span style="color:#ae81ff">19</span> 06:18 qemu-system-x86_64
</code></pre></div><p>Restart containerd to enable the new config:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">systemctl restart containerd
</code></pre></div><p>Now I can start a Kata container from commadnline.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo ctr image pull docker.io/library/hello-world:latest
sudo ctr run --runtime io.containerd.run.kata-qemu.v2 -t --rm docker.io/library/hello-world:latest hello

sudo ctr run --runtime io.containerd.run.kata-clh.v2 -t --rm docker.io/library/hello-world:latest hello

ctr run --snapshotter devmapper --runtime io.containerd.run.kata-fc.v2 -t docker.io/library/hello-world:latest hello
</code></pre></div><hr>
<h3 id="start-deployment">Start Deployment</h3>
<p>First I create a <code>RuntimeClass</code> for kata-fc then start a pod with this <code>RuntimeClass</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubens default

kubectl apply -f - <span style="color:#e6db74">&lt;&lt;EOF
</span><span style="color:#e6db74">apiVersion: node.k8s.io/v1
</span><span style="color:#e6db74">kind: RuntimeClass
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: kata-fc
</span><span style="color:#e6db74">handler: kata-fc
</span><span style="color:#e6db74">EOF</span>

cat<span style="color:#e6db74">&lt;&lt;EOF | kubectl apply -f -
</span><span style="color:#e6db74">apiVersion: v1
</span><span style="color:#e6db74">kind: Pod
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  labels:
</span><span style="color:#e6db74">    app: untrusted
</span><span style="color:#e6db74">  name: www-kata-fc
</span><span style="color:#e6db74">spec:
</span><span style="color:#e6db74">  runtimeClassName: kata-fc
</span><span style="color:#e6db74">  containers:
</span><span style="color:#e6db74">  - image: nginx:1.18
</span><span style="color:#e6db74">    name: www
</span><span style="color:#e6db74">    ports:
</span><span style="color:#e6db74">    - containerPort: 80
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f - <span style="color:#e6db74">&lt;&lt;EOF
</span><span style="color:#e6db74">apiVersion: node.k8s.io/v1
</span><span style="color:#e6db74">kind: RuntimeClass
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: kata-qemu
</span><span style="color:#e6db74">handler: kata-qemu
</span><span style="color:#e6db74">EOF</span>

cat<span style="color:#e6db74">&lt;&lt;EOF | kubectl apply -f -
</span><span style="color:#e6db74">apiVersion: v1
</span><span style="color:#e6db74">kind: Pod
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  labels:
</span><span style="color:#e6db74">    app: untrusted
</span><span style="color:#e6db74">  name: www-kata-qemu
</span><span style="color:#e6db74">spec:
</span><span style="color:#e6db74">  runtimeClassName: kata-qemu
</span><span style="color:#e6db74">  containers:
</span><span style="color:#e6db74">  - image: nginx:1.18
</span><span style="color:#e6db74">    name: www
</span><span style="color:#e6db74">    ports:
</span><span style="color:#e6db74">    - containerPort: 80
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f - <span style="color:#e6db74">&lt;&lt;EOF
</span><span style="color:#e6db74">apiVersion: node.k8s.io/v1
</span><span style="color:#e6db74">kind: RuntimeClass
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: kata-clh
</span><span style="color:#e6db74">handler: kata-clh
</span><span style="color:#e6db74">EOF</span>

cat<span style="color:#e6db74">&lt;&lt;EOF | kubectl apply -f -
</span><span style="color:#e6db74">apiVersion: v1
</span><span style="color:#e6db74">kind: Pod
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  labels:
</span><span style="color:#e6db74">    app: untrusted
</span><span style="color:#e6db74">  name: www-kata-clh
</span><span style="color:#e6db74">spec:
</span><span style="color:#e6db74">  runtimeClassName: kata-clh
</span><span style="color:#e6db74">  containers:
</span><span style="color:#e6db74">  - image: nginx:1.18
</span><span style="color:#e6db74">    name: www
</span><span style="color:#e6db74">    ports:
</span><span style="color:#e6db74">    - containerPort: 80
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ kubectl get po
NAME            READY   STATUS    RESTARTS   AGE
www-kata-clh    1/1     Running   <span style="color:#ae81ff">0</span>          59s
www-kata-fc     1/1     Running   <span style="color:#ae81ff">0</span>          12s
www-kata-qemu   1/1     Running   <span style="color:#ae81ff">0</span>          69s
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to deploy containerd with gVisor?]]></title>
            <link href="https://devopstales.github.io/home/gvisor-containerd/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/firecracker-containerd/?utm_source=atom_feed" rel="related" type="text/html" title="How to deploy containerd with Firecracker?" />
                <link href="https://devopstales.github.io/home/kata-container-containerd/?utm_source=atom_feed" rel="related" type="text/html" title="How to deploy containerd with kata containers?" />
                <link href="https://devopstales.github.io/kubernetes/gvisor-containerd/?utm_source=atom_feed" rel="related" type="text/html" title="How to deploy containerd with gVisor?" />
                <link href="https://devopstales.github.io/kubernetes/firecracker-containerd/?utm_source=atom_feed" rel="related" type="text/html" title="How to deploy containerd with Firecracker?" />
                <link href="https://devopstales.github.io/kubernetes/kata-container-containerd/?utm_source=atom_feed" rel="related" type="text/html" title="How to deploy containerd with kata containers?" />
            
                <id>https://devopstales.github.io/home/gvisor-containerd/</id>
            
            
            <published>2021-08-22T00:00:00+00:00</published>
            <updated>2021-08-22T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can install and use gvisor engine in kubernetes.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<h3 id="what-is-gvisor">What is gvisor</h3>
<p>gVisor is an application kernel, written in Go, that implements a substantial portion of the Linux system call interface. It provides an additional layer of isolation between running applications and the host operating system.</p>
<p>gVisor includes an Open Container Initiative (OCI) runtime called <code>runsc</code> that makes it easy to work with existing container tooling. The <code>runsc</code> runtime integrates with Docker, containerd and Kubernetes, making it simple to run sandboxed containers.</p>
<p><img src="/img/include/gvisor2.png" alt="gvisor"  class="zoomable" />
<img src="/img/include/gvisor.png" alt="gvisor"  class="zoomable" /></p>
<h3 id="install-gvisor">Install gvisor</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo dnf install epel-release nano wget -y

nano gvisor.sh
<span style="color:#75715e">#!/bash</span>
<span style="color:#f92672">(</span>
  set -e
  URL<span style="color:#f92672">=</span>https://storage.googleapis.com/gvisor/releases/release/latest
  wget <span style="color:#e6db74">${</span>URL<span style="color:#e6db74">}</span>/runsc <span style="color:#e6db74">${</span>URL<span style="color:#e6db74">}</span>/runsc.sha512 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    <span style="color:#e6db74">${</span>URL<span style="color:#e6db74">}</span>/gvisor-containerd-shim <span style="color:#e6db74">${</span>URL<span style="color:#e6db74">}</span>/gvisor-containerd-shim.sha512 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    <span style="color:#e6db74">${</span>URL<span style="color:#e6db74">}</span>/containerd-shim-runsc-v1 <span style="color:#e6db74">${</span>URL<span style="color:#e6db74">}</span>/containerd-shim-runsc-v1.sha512
  sha512sum -c runsc.sha512 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    -c gvisor-containerd-shim.sha512 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    -c containerd-shim-runsc-v1.sha512
  rm -f *.sha512
  chmod a+rx runsc gvisor-containerd-shim containerd-shim-runsc-v1
  sudo mv runsc gvisor-containerd-shim containerd-shim-runsc-v1 /usr/local/bin
<span style="color:#f92672">)</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">bash gvisor.sh
...
runsc: OK
gvisor-containerd-shim: OK
containerd-shim-runsc-v1: OK
</code></pre></div><h3 id="install-and-configure-containerd">Install and configure containerd</h3>
<p>First I install containerd then I add Kata container as a containerd plugin to the config.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
sudo dnf install -y containerd.io

sudo mkdir -p /etc/containerd
sudo containerd config default &gt; /etc/containerd/config.toml

nano /etc/containerd/config.toml
...
      <span style="color:#f92672">[</span>plugins.<span style="color:#e6db74">&#34;io.containerd.grpc.v1.cri&#34;</span>.containerd.runtimes<span style="color:#f92672">]</span>
        <span style="color:#f92672">[</span>plugins.<span style="color:#e6db74">&#34;io.containerd.grpc.v1.cri&#34;</span>.containerd.runtimes.runc<span style="color:#f92672">]</span>
...
        <span style="color:#f92672">[</span>plugins.<span style="color:#e6db74">&#34;io.containerd.grpc.v1.cri&#34;</span>.containerd.runtimes.runsc<span style="color:#f92672">]</span>
          runtime_type <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;io.containerd.runsc.v1&#34;</span>

<span style="color:#75715e"># Restart containerd</span>
sudo systemctl restart containerd
systemctl enable containerd.service

echo <span style="color:#e6db74">&#34;runtime-endpoint: unix:///run/containerd/containerd.sock&#34;</span> &gt; /etc/crictl.yaml
crictl ps
</code></pre></div><p>Now I can start a Kata container from commadnline.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo ctr image pull docker.io/library/busybox:latest
sudo ctr run --runtime io.containerd.run.runsc.v1 -t --rm docker.io/library/busybox:latest hello sh
</code></pre></div><h3 id="install-nerdctl">Install nerdctl</h3>
<p>I like to use <code>nerdctl</code> instad of <code>ctr</code> or <code>crictl</code> cli so I will install it.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">wget https://github.com/containerd/nerdctl/releases/download/v0.11.0/nerdctl-0.11.0-linux-amd64.tar.gz

tar -xzf nerdctl-0.11.0-linux-amd64.tar.gz
mv nerdctl /usr/local/bin
nerdctl ps
</code></pre></div><h3 id="install-tools">Install tools</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">yum install git -y

sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubectx /usr/local/sbin/kubectx
sudo ln -s /opt/kubectx/kubens /usr/local/sbin/kubens
</code></pre></div><h3 id="install-kubernetes">Install Kubernetes</h3>
<p>Configure Kernel parameters for Kubernetes.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat <span style="color:#e6db74">&lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf
</span><span style="color:#e6db74">overlay
</span><span style="color:#e6db74">br_netfilter
</span><span style="color:#e6db74">EOF</span>

sudo modprobe overlay
sudo modprobe br_netfilter

cat <span style="color:#e6db74">&lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
</span><span style="color:#e6db74">net.bridge.bridge-nf-call-iptables  = 1
</span><span style="color:#e6db74">net.ipv4.ip_forward                 = 1
</span><span style="color:#e6db74">net.bridge.bridge-nf-call-ip6tables = 1
</span><span style="color:#e6db74">EOF</span>

sysctl --system
</code></pre></div><p>Disable swap for Kubernetes.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">free -h
swapoff -a
swapoff -a
sed -i.bak -r <span style="color:#e6db74">&#39;s/(.+ swap .+)/#\1/&#39;</span> /etc/fstab
free -h
</code></pre></div><p>The I will add the kubernetes repo and Install the packages.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat <span style="color:#e6db74">&lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
</span><span style="color:#e6db74">[kubernetes]
</span><span style="color:#e6db74">name=Kubernetes
</span><span style="color:#e6db74">baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
</span><span style="color:#e6db74">enabled=1
</span><span style="color:#e6db74">gpgcheck=1
</span><span style="color:#e6db74">repo_gpgcheck=1
</span><span style="color:#e6db74">gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
</span><span style="color:#e6db74">EOF</span>

dnf install kubelet kubeadm kubectl -y
</code></pre></div><p>Start Kubernetes with containerd engine.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">export IP<span style="color:#f92672">=</span>172.17.13.10

dnf install -y iproute-tc

systemctl enable kubelet.service

echo <span style="color:#e6db74">&#34;KUBELET_EXTRA_ARGS=--cgroup-driver=systemd&#34;</span> | tee /etc/sysconfig/kubelet

kubeadm config images pull --cri-socket<span style="color:#f92672">=</span>unix:///run/containerd/containerd.sock
kubeadm init --pod-network-cidr<span style="color:#f92672">=</span>10.244.0.0/16 --apiserver-advertise-address<span style="color:#f92672">=</span>$IP --cri-socket<span style="color:#f92672">=</span>unix:///run/containerd/containerd.sock
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown <span style="color:#66d9ef">$(</span>id -u<span style="color:#66d9ef">)</span>:<span style="color:#66d9ef">$(</span>id -g<span style="color:#66d9ef">)</span> $HOME/.kube/config

kubectl get no
nerdctl -n k8s.io ps
crictl ps

kubectl taint nodes <span style="color:#66d9ef">$(</span>hostname<span style="color:#66d9ef">)</span> node-role.kubernetes.io/master:NoSchedule-
</code></pre></div><h3 id="inincialize-network">Inincialize network</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
kubectl aplly -f kube-flannel.yml
</code></pre></div><p>OR</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml
wget https://docs.projectcalico.org/manifests/custom-resources.yaml

nano custom-resources.yaml
...
      cidr: 10.244.0.0/16
...

kubectl apply -f custom-resources.yaml
</code></pre></div><h3 id="start-deployment">Start Deployment</h3>
<p>First I create a <code>RuntimeClass</code> for gvisor then start a pod with this <code>RuntimeClass</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat<span style="color:#e6db74">&lt;&lt;EOF | kubectl apply -f -
</span><span style="color:#e6db74">apiVersion: node.k8s.io/v1
</span><span style="color:#e6db74">kind: RuntimeClass
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: gvisor
</span><span style="color:#e6db74">handler: runsc
</span><span style="color:#e6db74">EOF</span>

cat<span style="color:#e6db74">&lt;&lt;EOF | kubectl apply -f -
</span><span style="color:#e6db74">apiVersion: v1
</span><span style="color:#e6db74">kind: Pod
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  labels:
</span><span style="color:#e6db74">    app: untrusted
</span><span style="color:#e6db74">  name: www-gvisor
</span><span style="color:#e6db74">spec:
</span><span style="color:#e6db74">  runtimeClassName: gvisor
</span><span style="color:#e6db74">  containers:
</span><span style="color:#e6db74">  - image: nginx:1.18
</span><span style="color:#e6db74">    name: www
</span><span style="color:#e6db74">    ports:
</span><span style="color:#e6db74">    - containerPort: 80
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ kubectl get po
NAME        READY   STATUS    RESTARTS   AGE
www-gvisor  1/1     Running   <span style="color:#ae81ff">0</span>          2m47s


$ kubectl describe po www-gvisor
...
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  2m42s  default-scheduler  Successfully assigned default/www-kata to alma8
  Normal  Pulled     2m13s  kubelet            Container image <span style="color:#e6db74">&#34;nginx:1.18&#34;</span> already present on machine
  Normal  Created    2m13s  kubelet            Created container www
  Normal  Started    2m11s  kubelet            Started container www
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to deploy containerd with kata containers?]]></title>
            <link href="https://devopstales.github.io/home/kata-container-containerd/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/kata-container-containerd/?utm_source=atom_feed" rel="related" type="text/html" title="How to deploy containerd with kata containers?" />
                <link href="https://devopstales.github.io/home/lazyimage/?utm_source=atom_feed" rel="related" type="text/html" title="Speed up docker pull with lazypull" />
                <link href="https://devopstales.github.io/home/migrate-docker-to-containerd/?utm_source=atom_feed" rel="related" type="text/html" title="Migrate Kubernetes from docker to containerd" />
                <link href="https://devopstales.github.io/home/cilium-opnsense-bgp/?utm_source=atom_feed" rel="related" type="text/html" title="Use Cilium BGP integration with OPNsense" />
                <link href="https://devopstales.github.io/home/being-productive-with-kubectl/?utm_source=atom_feed" rel="related" type="text/html" title="Being Productive with K8S" />
            
                <id>https://devopstales.github.io/home/kata-container-containerd/</id>
            
            
            <published>2021-08-20T00:00:00+00:00</published>
            <updated>2021-08-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can install and use kata-container engine in kubernetes.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<h3 id="what-is-kata-container-engine">What is Kata container engine</h3>
<p>Kata Containers is an open source community working to build a secure container runtime with lightweight virtual machines that feel and perform like containers, but provide stronger workload isolation using hardware virtualization technology as a second layer of defense. (Source: <a href="https://katacontainers.io/">Kata Containers Website</a> )</p>
<p><img src="/img/include/katacontainers.jpg" alt="Kata container engine"  class="zoomable" /></p>
<h3 id="enable-qvemu">Enable qvemu</h3>
<p>I will use Vagrant and VirtualBox for running the AlmaLinux VM so first I need to enable then Nested virtualization on the VM:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">VBoxManage modifyvm alma8 --nested-hw-virt on
</code></pre></div><p>After the Linux is booted test the virtualization flag in the VM:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">egrep --color -i <span style="color:#e6db74">&#34;svm|vmx&#34;</span> /proc/cpuinfo
</code></pre></div><p>If you find one of this flags everything is ok. Now we need to enable the kvm kernel module.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo modprobe kvm-intel
</code></pre></div><h3 id="install-kata-container-engine">Install Kata container engine</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat <span style="color:#e6db74">&lt;&lt;EOF | sudo -E tee /etc/yum.repos.d/kata-containers.repo
</span><span style="color:#e6db74">[kata-containers]
</span><span style="color:#e6db74">name=Kata Containers
</span><span style="color:#e6db74">baseurl=http://mirror.centos.org/centos/$releasever/virt/$basearch/kata-containers
</span><span style="color:#e6db74">enabled=1
</span><span style="color:#e6db74">gpgcheck=0
</span><span style="color:#e6db74">skip_if_unavailable=1
</span><span style="color:#e6db74">EOF</span>

cat <span style="color:#e6db74">&lt;&lt;EOF | sudo -E tee /etc/yum.repos.d/advanced-virtualization.repo
</span><span style="color:#e6db74">[advanced-virtualization]  
</span><span style="color:#e6db74">name=Advanced Virtualization from CentOS $releasever
</span><span style="color:#e6db74">baseurl=http://mirror.centos.org/centos/$releasever/virt/$basearch/advanced-virtualization
</span><span style="color:#e6db74">enabled=1
</span><span style="color:#e6db74">gpgcheck=0
</span><span style="color:#e6db74">skip_if_unavailable=1
</span><span style="color:#e6db74">EOF</span>

sudo dnf install epel-release nano wget
sudo dnf module disable virt
sudo -E dnf install -y kata-containers
</code></pre></div><h3 id="install-and-configure-containerd">Install and configure containerd</h3>
<p>First I install containerd then I add Kata container as a containerd plugin to the config.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
sudo dnf install -y containerd.io

sudo mkdir -p /etc/containerd
sudo containerd config default &gt; /etc/containerd/config.toml

nano /etc/containerd/config.toml
...
      <span style="color:#f92672">[</span>plugins.<span style="color:#e6db74">&#34;io.containerd.grpc.v1.cri&#34;</span>.containerd.runtimes<span style="color:#f92672">]</span>
        <span style="color:#f92672">[</span>plugins.<span style="color:#e6db74">&#34;io.containerd.grpc.v1.cri&#34;</span>.containerd.runtimes.runc<span style="color:#f92672">]</span>
...
        <span style="color:#f92672">[</span>plugins.<span style="color:#e6db74">&#34;io.containerd.grpc.v1.cri&#34;</span>.containerd.runtimes.kata<span style="color:#f92672">]</span>
          runtime_type <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;io.containerd.kata.v2&#34;</span>

<span style="color:#75715e"># Restart containerd</span>
sudo systemctl restart containerd
systemctl enable containerd.service

echo <span style="color:#e6db74">&#34;runtime-endpoint: unix:///run/containerd/containerd.sock&#34;</span> &gt; /etc/crictl.yaml
crictl ps
</code></pre></div><p>Now I can start a Kata container from commadnline.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo ctr image pull docker.io/library/busybox:latest
sudo ctr run --runtime io.containerd.run.kata.v2 -t --rm docker.io/library/busybox:latest hello sh
</code></pre></div><h3 id="install-nerdctl">Install nerdctl</h3>
<p>I like to use <code>nerdctl</code> instad of <code>ctr</code> or <code>crictl</code> cli so I will install it.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">wget https://github.com/containerd/nerdctl/releases/download/v0.11.0/nerdctl-0.11.0-linux-amd64.tar.gz

tar -xzf nerdctl-0.11.0-linux-amd64.tar.gz
mv nerdctl /usr/local/bin
nerdctl ps
</code></pre></div><h3 id="install-kubernetes">Install Kubernetes</h3>
<p>Configure Kernel parameters for Kubernetes.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat <span style="color:#e6db74">&lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf
</span><span style="color:#e6db74">overlay
</span><span style="color:#e6db74">br_netfilter
</span><span style="color:#e6db74">kvm-intel
</span><span style="color:#e6db74">EOF</span>

sudo modprobe overlay
sudo modprobe br_netfilter

cat <span style="color:#e6db74">&lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
</span><span style="color:#e6db74">net.bridge.bridge-nf-call-iptables  = 1
</span><span style="color:#e6db74">net.ipv4.ip_forward                 = 1
</span><span style="color:#e6db74">net.bridge.bridge-nf-call-ip6tables = 1
</span><span style="color:#e6db74">EOF</span>

sysctl --system
</code></pre></div><p>Disable swap for Kubernetes.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">free -h
swapoff -a
swapoff -a
sed -i.bak -r <span style="color:#e6db74">&#39;s/(.+ swap .+)/#\1/&#39;</span> /etc/fstab
free -h
</code></pre></div><p>The I will add the kubernetes repo and Install the packages.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat <span style="color:#e6db74">&lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
</span><span style="color:#e6db74">[kubernetes]
</span><span style="color:#e6db74">name=Kubernetes
</span><span style="color:#e6db74">baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
</span><span style="color:#e6db74">enabled=1
</span><span style="color:#e6db74">gpgcheck=1
</span><span style="color:#e6db74">repo_gpgcheck=1
</span><span style="color:#e6db74">gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
</span><span style="color:#e6db74">EOF</span>

dnf install kubelet kubeadm kubectl -y
</code></pre></div><p>Start Kubernetes with containerd engine.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">export IP<span style="color:#f92672">=</span>172.17.13.10

dnf install -y iproute-tc

systemctl enable kubelet.service

echo <span style="color:#e6db74">&#34;KUBELET_EXTRA_ARGS=--cgroup-driver=systemd&#34;</span> | tee /etc/sysconfig/kubelet

kubeadm config images pull --cri-socket<span style="color:#f92672">=</span>unix:///run/containerd/containerd.sock
kubeadm init --pod-network-cidr<span style="color:#f92672">=</span>10.244.0.0/16 --apiserver-advertise-address<span style="color:#f92672">=</span>$IP --cri-socket<span style="color:#f92672">=</span>unix:///run/containerd/containerd.sock
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown <span style="color:#66d9ef">$(</span>id -u<span style="color:#66d9ef">)</span>:<span style="color:#66d9ef">$(</span>id -g<span style="color:#66d9ef">)</span> $HOME/.kube/config

kubectl get no
nerdctl -n k8s.io ps
crictl ps

kubectl taint nodes <span style="color:#66d9ef">$(</span>hostname<span style="color:#66d9ef">)</span> node-role.kubernetes.io/master:NoSchedule-
</code></pre></div><h3 id="inincialize-network">Inincialize network</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
kubectl aplly -f kube-flannel.yml
</code></pre></div><p>OR</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml
wget https://docs.projectcalico.org/manifests/custom-resources.yaml

nano custom-resources.yaml
...
      cidr: 10.244.0.0/16
...

kubectl apply -f custom-resources.yaml
</code></pre></div><h3 id="start-deployment">Start Deployment</h3>
<p>First I create a <code>RuntimeClass</code> for Kata then start a pod with this <code>RuntimeClass</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat<span style="color:#e6db74">&lt;&lt;EOF | kubectl apply -f -
</span><span style="color:#e6db74">kind: RuntimeClass
</span><span style="color:#e6db74">apiVersion: node.k8s.io/v1
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">    name: kata
</span><span style="color:#e6db74">handler: kata
</span><span style="color:#e6db74">EOF</span>

cat<span style="color:#e6db74">&lt;&lt;EOF | kubectl apply -f -
</span><span style="color:#e6db74">apiVersion: v1
</span><span style="color:#e6db74">kind: Pod
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  labels:
</span><span style="color:#e6db74">    app: untrusted
</span><span style="color:#e6db74">  name: www-kata
</span><span style="color:#e6db74">spec:
</span><span style="color:#e6db74">  runtimeClassName: kata
</span><span style="color:#e6db74">  containers:
</span><span style="color:#e6db74">  - image: nginx:1.18
</span><span style="color:#e6db74">    name: www
</span><span style="color:#e6db74">    ports:
</span><span style="color:#e6db74">    - containerPort: 80
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ kubectl get po
NAME       READY   STATUS    RESTARTS   AGE
www-kata   1/1     Running   <span style="color:#ae81ff">0</span>          2m47s


$ kubectl describe po www-kata
...
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  2m42s  default-scheduler  Successfully assigned default/www-kata to alma8
  Normal  Pulled     2m13s  kubelet            Container image <span style="color:#e6db74">&#34;nginx:1.18&#34;</span> already present on machine
  Normal  Created    2m13s  kubelet            Created container www
  Normal  Started    2m11s  kubelet            Started container www
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Image Signature Verification with Kyverno]]></title>
            <link href="https://devopstales.github.io/home/k8s-kyverno-cosign/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/kubernetes-policy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Policy" />
                <link href="https://devopstales.github.io/home/kyverno-image-mirror/?utm_source=atom_feed" rel="related" type="text/html" title="Automatically change registry in pod definition" />
                <link href="https://devopstales.github.io/kubernetes/k8s-kyverno-cosign/?utm_source=atom_feed" rel="related" type="text/html" title="Image Signature Verification with Kyverno" />
                <link href="https://devopstales.github.io/kubernetes/kubernetes-policy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Policy" />
                <link href="https://devopstales.github.io/kubernetes/kyverno-image-mirror/?utm_source=atom_feed" rel="related" type="text/html" title="Automatically change registry in pod definition" />
            
                <id>https://devopstales.github.io/home/k8s-kyverno-cosign/</id>
            
            
            <published>2021-08-18T00:00:00+00:00</published>
            <updated>2021-08-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can use Kyverno and Cosign for Image Signature Verification in a Kubernetes cluster.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<h3 id="wat-is-cosign">Wat is Cosign?</h3>
<p>Cosign is a new open-source tool to manage the process of signing and verifying container images. Developed by Googele in collaboration with Linux Foundation’s sigstore project. The motivation for cosign is &ldquo;to make signatures invisible infrastructure.&rdquo; With Images signed by Cosign you didn&rsquo;t neet to change your infrastructure to store the public signing key, like Notary. (With Notary you need a Notary server connected to your registry to store the keys) With Cosign, the signatures directly appear as tags of the image linked to the associated image via the digest:</p>
<p><img src="/img/include/Cosign.png" alt="Notice how the signature tag below corresponds to the sha256 digest of the image tag ‘latest’ above."  class="zoomable" /></p>
<p>Key Management options:</p>
<ul>
<li>fixed, text-based keys generated using <code>cosign generate-key-pair</code></li>
<li>cloud KMS-based keys generated using <code>cosign generate-key-pair -kms</code></li>
<li>keys generated on hardware tokens using the PIV interface using <code>cosign piv-tool</code></li>
<li>Kubernetes-secret based keys generated using <code>cosign generate-key-pair -k8s</code></li>
</ul>
<h3 id="installing-cosign">Installing Cosign</h3>
<p>It’s a golang project, so it’s fairly easy to get started, there’s a single binary available from their <a href="https://github.com/sigstore/cosign/releases">release pag</a> and it has been signed by them.</p>
<p>Generate key pair with:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ cosign generate-key-pair
Enter password <span style="color:#66d9ef">for</span> private key:
Enter again:
Private key written to cosign.key
Public key written to cosign.pub
</code></pre></div><p>Sign an image with cosign:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">docker pull alpine:edge
docker tag alpine:edge devopstales/testimage:unsigned
docker push devopstales/testimage:unsigned


docker pull alpine:latest
docker tag alpine:latest devopstales/testimage:cosign
docker push devopstales/testimage:cosign


cosign sign -key ~/data/cosign.key devopstales/testimage:cosign
Enter password <span style="color:#66d9ef">for</span> private key: 
Pushing signature to: index.docker.io/devopstales/testimage:sha256-4661fb57f7890b9145907a1fe2555091d333ff3d28db86c3bb906f6a2be93c87.sig
</code></pre></div><p>Verify a container against a public key:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ cosign verify -key ~/data/cosign.pub devopstales/testimage:cosign

Verification <span style="color:#66d9ef">for</span> devopstales/testimage:cosign --
The following checks were performed on each of these signatures:
  - The cosign claims were validated
  - The signatures were verified against the specified public key
  - Any certificates were verified against the Fulcio roots.
<span style="color:#f92672">{</span><span style="color:#e6db74">&#34;critical&#34;</span>:<span style="color:#f92672">{</span><span style="color:#e6db74">&#34;identity&#34;</span>:<span style="color:#f92672">{</span><span style="color:#e6db74">&#34;docker-reference&#34;</span>:<span style="color:#e6db74">&#34;index.docker.io/devopstales/testimage&#34;</span><span style="color:#f92672">}</span>,<span style="color:#e6db74">&#34;image&#34;</span>:<span style="color:#f92672">{</span><span style="color:#e6db74">&#34;docker-manifest-digest&#34;</span>:<span style="color:#e6db74">&#34;sha256:4661fb57f7890b9145907a1fe2555091d333ff3d28db86c3bb906f6a2be93c87&#34;</span><span style="color:#f92672">}</span>,<span style="color:#e6db74">&#34;type&#34;</span>:<span style="color:#e6db74">&#34;cosign container image signature&#34;</span><span style="color:#f92672">}</span>,<span style="color:#e6db74">&#34;optional&#34;</span>:null<span style="color:#f92672">}</span>
</code></pre></div><h3 id="image-signature-verification-tools">Image Signature Verification tools</h3>
<p>In a <a href="">previous post</a> I used Connaisseur to Image Signature Verification. I could youse Connaisseur with Cosign too but with the new release of Kyverno we didn&rsquo;t need to deploy a separate tool for Image Signature Verification. We can use Kyverno&rsquo;s <code>verifyImages</code> rule.</p>
<p>It validate signatures for matching images using Cosign and mutates image references with the digest returned by Cosign. Using an image digest guarantees immutability of images and hence improves security.</p>
<p>Install the latest version of Kyverno:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl create -f https://raw.githubusercontent.com/kyverno/kyverno/main/definitions/release/install.yaml
</code></pre></div><p>Patch the Kyverno webhook, to allow time for calling the OCI registry:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl patch mutatingwebhookconfigurations kyverno-resource-mutating-webhook-cfg <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--type json <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>-p<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;[{&#34;op&#34;: &#34;replace&#34;, &#34;path&#34;: &#34;/webhooks/0/failurePolicy&#34;, &#34;value&#34;: &#34;Ignore&#34;},{&#34;op&#34;: &#34;replace&#34;, &#34;path&#34;: &#34;/webhooks/0/timeoutSeconds&#34;, &#34;value&#34;: 15}]&#39;</span>
</code></pre></div><p>Here is a policy that verifies all images from a specific repository:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">kyverno.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterPolicy</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">check-image</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">validationFailureAction</span>: <span style="color:#ae81ff">enforce</span>
  <span style="color:#f92672">background</span>: <span style="color:#66d9ef">false</span>
  <span style="color:#f92672">rules</span>:
    - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">check-image</span>
      <span style="color:#f92672">match</span>:
        <span style="color:#f92672">resources</span>:
          <span style="color:#f92672">kinds</span>:
            - <span style="color:#ae81ff">Pod</span>
      <span style="color:#f92672">verifyImages</span>:
      - <span style="color:#f92672">image</span>: <span style="color:#e6db74">&#34;docker.io/devopstales/testimage:*&#34;</span>
        <span style="color:#f92672">key</span>: |-<span style="color:#e6db74">
</span><span style="color:#e6db74">          -----BEGIN PUBLIC KEY-----
</span><span style="color:#e6db74">          MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEL53O1V5FP2Vaa60BTwRjrOxhuu5C
</span><span style="color:#e6db74">          iB/mODf/V2eiGw+WbA689ZZRjWwXCf+4jwzfRSrik0YvTCMqvl3BDaPG2A==
</span><span style="color:#e6db74">          -----END PUBLIC KEY-----</span>          
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl create deployment signed-my <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--image<span style="color:#f92672">=</span>devopstales/testimage:cosign
</code></pre></div><p>Try running an unsigned image that matches the configured rule:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl create deployment unsigned-my <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--image<span style="color:#f92672">=</span>docker.io/devopstales/testimage:unsigned
</code></pre></div><p>This will be blocked:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">error: failed to create deployment: admission webhook <span style="color:#e6db74">&#34;mutate.kyverno.svc&#34;</span> denied the request: 

resource Deployment/kyverno-system/unsigned-my was blocked due to the following policies

verify-image:
  autogen-verify-image: <span style="color:#e6db74">&#39;image verification failed for docker.io/devopstales/testimage:unsigned:
</span><span style="color:#e6db74">    failed to verify image: fetching signatures: getting signature manifest: GET https://index.docker.io/v2/devopstales/testimage/manifests/sha256-0119f88f395766eb52f9b817c3d23576bf31935dc8e94abe14bae9a083ce4639.sig:
</span><span style="color:#e6db74">    MANIFEST_UNKNOWN: manifest unknown; map[Tag:sha256-0119f88f395766eb52f9b817c3d23576bf31935dc8e94abe14bae9a083ce4639.sig]&#39;</span>
</code></pre></div><hr>
<ul>
<li><a href="https://nirmata.com/2021/08/12/kubernetes-supply-chain-policy-management-with-cosign-and-kyverno/">Kyverno Blog post</a></li>
</ul>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Automatically change registry in pod definition]]></title>
            <link href="https://devopstales.github.io/home/kyverno-image-mirror/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/kubernetes-policy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Policy" />
                <link href="https://devopstales.github.io/kubernetes/kyverno-image-mirror/?utm_source=atom_feed" rel="related" type="text/html" title="Automatically change registry in pod definition" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller-v3/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V3" />
                <link href="https://devopstales.github.io/home/continuous-image-security/?utm_source=atom_feed" rel="related" type="text/html" title="Continuous Image Security" />
                <link href="https://devopstales.github.io/home/k8s-vault-v2/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes integration with external Vault" />
            
                <id>https://devopstales.github.io/home/kyverno-image-mirror/</id>
            
            
            <published>2021-08-16T00:00:00+00:00</published>
            <updated>2021-08-16T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can automatically change the registry part in deployed pods in Kubernetes.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<h3 id="imageswap-mutating-admission-controller-for-kubernetes">ImageSwap Mutating Admission Controller for Kubernetes</h3>
<p>The ImageSwap webhook enables you to define one or more mappings to automatically swap image definitions within Kubernetes Pods with a different registry.</p>
<p>Install ImageSwap:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ kubectl apply -f https://raw.githubusercontent.com/phenixblue/imageswap-webhook/v1.4.2/deploy/install.yaml
</code></pre></div><p>For swapping configuration ImageSwap use a configmap to define what image should change to what:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">data</span>:
  <span style="color:#f92672">maps</span>: |<span style="color:#e6db74">
</span><span style="color:#e6db74">    default:registry.example.com
</span><span style="color:#e6db74">    #gcr.io: # This is a comment
</span><span style="color:#e6db74">    gitlab.com:registry.example.com/gitlab
</span><span style="color:#e6db74">    noswap_wildcards:example.com</span>    
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ConfigMap</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">creationTimestamp</span>: <span style="color:#66d9ef">null</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">imageswap-maps</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">imageswap-system</span>
</code></pre></div><p>Example MAPS Configs:</p>
<p>Disable image swapping for all registries EXCEPT <code>gcr.io</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">default:
gcr.io:harbor.internal.example.com
</code></pre></div><p>Enable image swapping for all registries except <code>gcr.io</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"></code></pre></div><p>With this, all images will be swapped except those that already match the <code>harbor.internal.example.com</code> pattern</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">default:harbor.internal.example.com
noswap_wildcards:harbor.internal.example.com
</code></pre></div><h3 id="test-imageswap">Test ImageSwap</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ kubectl create ns test1
$ kubectl label ns test1 k8s.twr.io/imageswap<span style="color:#f92672">=</span>enabled
</code></pre></div><p>Then deploy a pod:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl create deployment unsigned-my <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--image<span style="color:#f92672">=</span>docker.io/devopstales/testimage:unsigned
</code></pre></div><p>ImageSwap can be disabled on a per workload level by adding the <code>k8s.twr.io/imageswap</code> label with a value of <code>disabled</code> to the pod template.</p>
<h3 id="kyverno">Kyverno</h3>
<p>Here is an example of a Kyverno policy that validates that images are only pulled from an allowed list of image registries (based on wildcard patterns):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion </span>: <span style="color:#ae81ff">kyverno.io/v1alpha1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Policy</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">check-registries</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">rules</span>:
  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">check-registries</span>
    <span style="color:#f92672">resource</span>:
      <span style="color:#f92672">kinds</span>:
      - <span style="color:#ae81ff">Deployment</span>
      - <span style="color:#ae81ff">StatefulSet</span>
    <span style="color:#f92672">validate</span>:
      <span style="color:#f92672">message</span>: <span style="color:#e6db74">&#34;Registry is not allowed&#34;</span>
      <span style="color:#f92672">pattern</span>:
        <span style="color:#f92672">spec</span>:
          <span style="color:#f92672">template</span>:
            <span style="color:#f92672">spec</span>:
              <span style="color:#f92672">containers</span>:
              - <span style="color:#f92672">name</span>: <span style="color:#e6db74">&#34;*&#34;</span>
                <span style="color:#75715e"># Check allowed registries</span>
                <span style="color:#f92672">image</span>: <span style="color:#e6db74">&#34;*/nirmata/* | https://private.registry.io/*&#34;</span>
</code></pre></div><p>Rather than blocking Pods which come from outside registries, it is also possible to mutate them so the pulls are directed to approved registries.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">kyverno.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterPolicy</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">replace-image-registry</span>
  <span style="color:#f92672">annotations</span>:
    <span style="color:#f92672">policies.kyverno.io/title</span>: <span style="color:#ae81ff">Replace Image Registry</span>
    <span style="color:#f92672">policies.kyverno.io/category</span>: <span style="color:#ae81ff">Sample</span>
    <span style="color:#f92672">policies.kyverno.io/severity</span>: <span style="color:#ae81ff">medium</span>
    <span style="color:#f92672">policies.kyverno.io/subject</span>: <span style="color:#ae81ff">Pod</span>
    <span style="color:#f92672">policies.kyverno.io/minversion</span>: <span style="color:#ae81ff">1.3.6</span>
    <span style="color:#f92672">policies.kyverno.io/description</span>: &gt;-<span style="color:#e6db74">
</span><span style="color:#e6db74">      Rather than blocking Pods which come from outside registries,
</span><span style="color:#e6db74">      it is also possible to mutate them so the pulls are directed to
</span><span style="color:#e6db74">      approved registries. In some cases, those registries may function as
</span><span style="color:#e6db74">      pull-through proxies and can fetch the image if not cached.
</span><span style="color:#e6db74">      This policy policy mutates all images either
</span><span style="color:#e6db74">      in the form &#39;image:tag&#39; or &#39;registry.corp.com/image:tag&#39; to be prefaced
</span><span style="color:#e6db74">      with `myregistry.corp.com/`.      </span>      
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">background</span>: <span style="color:#66d9ef">false</span>
  <span style="color:#f92672">rules</span>:
    - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">replace-image-registry</span>
      <span style="color:#f92672">match</span>:
        <span style="color:#f92672">resources</span>:
          <span style="color:#f92672">kinds</span>:
          - <span style="color:#ae81ff">Pod</span>
      <span style="color:#f92672">mutate</span>:
        <span style="color:#f92672">patchStrategicMerge</span>:
          <span style="color:#f92672">spec</span>:
            <span style="color:#f92672">containers</span>:
            - <span style="color:#f92672">(name)</span>: <span style="color:#e6db74">&#34;*&#34;</span>
              <span style="color:#f92672">image</span>: |-<span style="color:#e6db74">
</span><span style="color:#e6db74">                                </span>                                {{ <span style="color:#ae81ff">regex_replace_all(&#39;^[^/]+&#39;, &#39;{{@}}&#39;, &#39;myregistry.corp.com&#39;) }}</span>
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Migrate Kubernetes from docker to containerd]]></title>
            <link href="https://devopstales.github.io/home/migrate-docker-to-containerd/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/migrate-docker-to-containerd/?utm_source=atom_feed" rel="related" type="text/html" title="Migrate Kubernetes from docker to containerd" />
                <link href="https://devopstales.github.io/home/lazyimage/?utm_source=atom_feed" rel="related" type="text/html" title="Speed up docker pull with lazypull" />
                <link href="https://devopstales.github.io/home/k8s-connaisseur-v2/?utm_source=atom_feed" rel="related" type="text/html" title="Image Signature Verification Admission Controller V2" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller-v3/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V3" />
                <link href="https://devopstales.github.io/home/continuous-image-security/?utm_source=atom_feed" rel="related" type="text/html" title="Continuous Image Security" />
            
                <id>https://devopstales.github.io/home/migrate-docker-to-containerd/</id>
            
            
            <published>2021-08-08T00:00:00+00:00</published>
            <updated>2021-08-08T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can migrate your kubernetes cluster from docker to containerd.</p>
<h3 id="how-to-migrate">How to migrate</h3>
<p>You have to be careful if you are on a single master node configuration. The cluster will be unavailable under the upgrade.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get nodes -o wide
NAME    STATUS   ROLES                  AGE     VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION                             CONTAINER-RUNTIME
k8s01   Ready    control-plane,master   78m     v1.20.4   10.65.79.164   &lt;none&gt;        CentOS Linux <span style="color:#ae81ff">8</span>       4.18.0-240.15.1.el8_3.centos.plus.x86_64   docker://20.10.5
k8s02   Ready    control-plane,master   64m     v1.20.4   10.65.79.131   &lt;none&gt;        CentOS Linux <span style="color:#ae81ff">8</span>       4.18.0-240.15.1.el8_3.centos.plus.x86_64   docker://20.10.5
k8s03   Ready    control-plane,master   4m16s   v1.20.4   10.65.79.244   &lt;none&gt;        CentOS Linux <span style="color:#ae81ff">8</span>       4.18.0-240.15.1.el8_3.centos.plus.x86_64   docker://20.10.5
</code></pre></div><p>First we will cordon and drain the node:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl cordon k8s01
kubectl drain k8s01 --ignore-daemonsets
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get nodes
NAME    STATUS                      ROLES                  AGE    VERSION
k8s01   Ready,SchedulingDisabled    control-plane,master   83m    v1.20.4
k8s02   Ready                       control-plane,master   69m    v1.20.4
k8s03   Ready                       control-plane,master   9m30s  v1.20.4
</code></pre></div><p>Stop the kubelet sevice and remove docker:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo systemctl stop kubelet
sudo systemctl status kubelet
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">apt purge docker-ce docker-ce-cli
OR
yum remove docker-ce docker-ce-cli
</code></pre></div><p>Install and configure containerd:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e">## Install containerd</span>
sudo yum update -y <span style="color:#f92672">&amp;&amp;</span> sudo yum install -y containerd.io

<span style="color:#75715e">## Configure containerd</span>
sudo mkdir -p /etc/containerd
sudo containerd config default &gt; /etc/containerd/config.toml
</code></pre></div><p>To use the <code>systemd</code> cgroup driver in <code>/etc/containerd/config.toml</code> with <code>runc</code>, set</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano /etc/containerd/config.toml
...
          <span style="color:#f92672">[</span>plugins.<span style="color:#e6db74">&#34;io.containerd.grpc.v1.cri&#34;</span>.containerd.runtimes.runc.options<span style="color:#f92672">]</span>
            SystemdCgroup <span style="color:#f92672">=</span> true
</code></pre></div><p>Prepare the system for containerd:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat <span style="color:#e6db74">&lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf
</span><span style="color:#e6db74">overlay
</span><span style="color:#e6db74">br_netfilter
</span><span style="color:#e6db74">kvm-intel
</span><span style="color:#e6db74">EOF</span>

sudo modprobe overlay
sudo modprobe br_netfilter

cat <span style="color:#e6db74">&lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
</span><span style="color:#e6db74">net.bridge.bridge-nf-call-iptables  = 1
</span><span style="color:#e6db74">net.ipv4.ip_forward                 = 1
</span><span style="color:#e6db74">net.bridge.bridge-nf-call-ip6tables = 1
</span><span style="color:#e6db74">EOF</span>

sysctl --system
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">echo <span style="color:#e6db74">&#34;runtime-endpoint: unix:///run/containerd/containerd.sock&#34;</span> &gt; /etc/crictl.yaml
crictl ps

<span style="color:#75715e"># Start containerd</span>
systemctl enable --now containerd
</code></pre></div><p>Change runtime in kubeadm config:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano /etc/sysconfig/kubelet
<span style="color:#75715e"># add the following flags to KUBELET_KUBEADM_ARGS variable</span>
KUBELET_KUBEADM_ARGS<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;... --container-runtime=remote --container-runtime-endpoint=/run/containerd/containerd.sock&#34;</span>
</code></pre></div><p>Start kubelet:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo systemctl start kubelet
</code></pre></div><p>Check if the new runtime on the node:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl describe node k8s01
System Info:
  Machine ID:                 21a5dd31f86c4
  System UUID:                4227EF55-BA3BCCB57BCE
  Boot ID:                    77229747-9ea581ec6773
  Kernel Version:             4.18.0-240.15.1.el8_3.centos.plus.x86_64
  OS Image:                   CentOS Linux <span style="color:#ae81ff">8</span>
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.4.3
  Kubelet Version:            v1.20.4
  Kube-Proxy Version:         v1.20.4
</code></pre></div><p>Uncordon the node to mark it schedulable agen:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl uncordon k8s01
</code></pre></div><p>Once you changed the runetime on all the nodes you are done.</p>
<h3 id="debugging-tipps">Debugging tipps</h3>
<p>Here are some usefull conmmands to debug:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">journalctl -u kubelet

journalctl -u containerd

crictl --runtime-endpoint /run/containerd/containerd.sock ps

kubectl describe node &lt;master_node_name&gt;
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Use Cilium BGP integration with OPNsense]]></title>
            <link href="https://devopstales.github.io/home/cilium-opnsense-bgp/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/cilium-opnsense-bgp/?utm_source=atom_feed" rel="related" type="text/html" title="Use Cilium BGP integration with OPNsense" />
                <link href="https://devopstales.github.io/home/k8s-metallb-bgp-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Self-hosted Load Balancer for bare metal Kubernetes" />
                <link href="https://devopstales.github.io/kubernetes/k8s-metallb-bgp-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Self-hosted Load Balancer for bare metal Kubernetes" />
                <link href="https://devopstales.github.io/home/lazyimage/?utm_source=atom_feed" rel="related" type="text/html" title="Speed up docker pull with lazypull" />
                <link href="https://devopstales.github.io/home/k3s-cilium/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with k3sup and Cilium" />
            
                <id>https://devopstales.github.io/home/cilium-opnsense-bgp/</id>
            
            
            <published>2021-08-05T00:00:00+00:00</published>
            <updated>2021-08-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this tutorial I will show you how to install Cilium with BGP integration for Kubernetes.</p>
<p>Cilium recently announced the release of 1.10 which allo to advertise routes to Service IPs via BGP, so we didn&rsquo;t need to install MetalLB.</p>
<h3 id="how-does-the-full-setup-look-like">How does the full setup look like?</h3>
<p>For this Demo I will use a pfsense in virtualbox and tree vm for kubernetes in the same host-only network.</p>
<table>
<thead>
<tr>
<th>vm</th>
<th>nic</th>
<th>ip</th>
<th>mode</th>
</tr>
</thead>
<tbody>
<tr>
<td>opnsense01</td>
<td>em1</td>
<td>192.168.0.200</td>
<td>bridged</td>
</tr>
<tr>
<td>opnsense01</td>
<td>em2</td>
<td>172.17.9.200</td>
<td>host-only</td>
</tr>
<tr>
<td>k8sm01</td>
<td>enp0s8</td>
<td>172.17.9.10</td>
<td>host-only</td>
</tr>
<tr>
<td>k8sm02</td>
<td>enp0s8</td>
<td>172.17.9.11</td>
<td>host-only</td>
</tr>
<tr>
<td>k8sm02</td>
<td>enp0s8</td>
<td>172.17.9.12</td>
<td>host-only</td>
</tr>
</tbody>
</table>
<h3 id="install-gbp-to-opnsense">Install GBP to OPNsense</h3>
<p>Go to <code>System &gt; Firmware &gt; Plugins</code> and install <code>os-frr</code></p>
<h3 id="configure-gbp-on-opnsense">Configure GBP on OPNsense</h3>
<p>Go tp <code>Routing &gt; General</code> and enable enable the plugin. Next go to <code>Routing &gt; GBP</code> and enble, then add AS Number.</p>
<p><img src="/img/include/cilium-opnsense-bgp1.jpg" alt="Enable BGP"  class="zoomable" /></p>
<h3 id="configure-neighbor">Configure Neighbor</h3>
<p>Go to  <code>Routing &gt; GBP</code> the switch to the <code>Neighbor</code> tab and add the following three neighbors.</p>
<p><img src="/img/include/cilium-opnsense-bgp2.jpg" alt="Neighbor"  class="zoomable" /></p>
<p><img src="/img/include/cilium-opnsense-bgp3.jpg" alt="Neighbor"  class="zoomable" /></p>
<p><img src="/img/include/cilium-opnsense-bgp4.PNG" alt="Neighbor"  class="zoomable" /></p>
<h3 id="cilium-configuration">Cilium configuration</h3>
<p>BGP support is enabled by providing the BGP configuration via a ConfigMap and by setting a few Helm values. Otherwise, BGP is disabled by default.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">helm install cilium cilium/cilium <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--version 1.10.3 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set bgp.enabled<span style="color:#f92672">=</span>true <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set bgp.announce.loadbalancerIP<span style="color:#f92672">=</span>true
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ConfigMap</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">bgp-config</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">kube-system</span>
<span style="color:#f92672">data</span>:
  <span style="color:#f92672">config.yaml</span>: |<span style="color:#e6db74">
</span><span style="color:#e6db74">    peers:
</span><span style="color:#e6db74">    - peer-address: 172.17.9.200
</span><span style="color:#e6db74">      peer-asn: 64512
</span><span style="color:#e6db74">      my-asn: 64513
</span><span style="color:#e6db74">    address-pools:
</span><span style="color:#e6db74">    - name: default
</span><span style="color:#e6db74">      protocol: bgp
</span><span style="color:#e6db74">      addresses:
</span><span style="color:#e6db74">      - 10.25.0.10-10.25.3.250</span>    
</code></pre></div><h3 id="demo-time">Demo Time</h3>
<p>Let’s create a demo application for testing.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano test.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">apps/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Deployment</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">test-nginx</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">default</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">selector</span>:
    <span style="color:#f92672">matchLabels</span>:
      <span style="color:#f92672">run</span>: <span style="color:#ae81ff">test-nginx</span>
  <span style="color:#f92672">replicas</span>: <span style="color:#ae81ff">3</span>
  <span style="color:#f92672">template</span>:
    <span style="color:#f92672">metadata</span>:
      <span style="color:#f92672">labels</span>:
        <span style="color:#f92672">run</span>: <span style="color:#ae81ff">test-nginx</span>
    <span style="color:#f92672">spec</span>:
      <span style="color:#f92672">containers</span>:
      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">test-nginx</span>
        <span style="color:#f92672">image</span>: <span style="color:#ae81ff">nginx</span>
        <span style="color:#f92672">ports</span>:
        - <span style="color:#f92672">containerPort</span>: <span style="color:#ae81ff">80</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Service</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">test-nginx</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">default</span>
  <span style="color:#f92672">labels</span>:
    <span style="color:#f92672">run</span>: <span style="color:#ae81ff">test-nginx</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">type</span>: <span style="color:#ae81ff">LoadBalancer</span>
  <span style="color:#f92672">ports</span>:
  - <span style="color:#f92672">port</span>: <span style="color:#ae81ff">80</span>
    <span style="color:#f92672">protocol</span>: <span style="color:#ae81ff">TCP</span>
  <span style="color:#f92672">selector</span>:
    <span style="color:#f92672">run</span>: <span style="color:#ae81ff">test-nginx</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f bgpconfig.yaml
kubectl apply -f test.yaml
</code></pre></div><p>After a few moments, you can run this command to get the IP address:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ kubectl describe service test-nginx | grep <span style="color:#e6db74">&#34;LoadBalancer Ingress&#34;</span>
LoadBalancer Ingress:     10.25.0.11
</code></pre></div><p>Let&rsquo;s check the address in a browser. If pfSense is you default gateway it will work perfectly, but in my demo enviroment I need to create a route to pfSense for this network on my host machine:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo route add -net 10.25.0.0/22 gw 172.17.9.200
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         192.168.0.1     0.0.0.0         UG    <span style="color:#ae81ff">600</span>    <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> wlan0
10.25.0.0       172.17.9.200    255.255.252.0   UG    <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> vboxnet7
172.17.9.0      0.0.0.0         255.255.255.0   U     <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> vboxnet7
</code></pre></div><p><img src="/img/include/pfsense-bgp-kubernetes.png" alt="Demo"  class="zoomable" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes Multi-Tenancy With vCluster]]></title>
            <link href="https://devopstales.github.io/home/vcluster/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/vcluster/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Multi-Tenancy With vCluster" />
                <link href="https://devopstales.github.io/home/k3sup-calico/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with k3sup and Calico" />
                <link href="https://devopstales.github.io/home/k3s-cilium/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with k3sup and Cilium" />
                <link href="https://devopstales.github.io/home/k3s-etcd-kube-vip/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with k3sup and kube-vip" />
                <link href="https://devopstales.github.io/home/k3s-helm-controller/?utm_source=atom_feed" rel="related" type="text/html" title="Install and use rancher helm-controller" />
            
                <id>https://devopstales.github.io/home/vcluster/</id>
            
            
            <published>2021-08-03T00:00:00+00:00</published>
            <updated>2021-08-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will use vCluster to run virtual Kubernetes clusters inside a Kubernetes cluster.</p>
<H3>Parst of the K3S series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/k3s-etcd-kube-vip/">Install K3S with k3sup and kube-vip</a></li>
     <li>Part1b: <a href="../../kubernetes/k3s-crio/">Install K3S with CRI-O</a></li>
     <li>Part1c: <a href="../../kubernetes/k3s-fcos/">Install K3S on Fedora CoreOS</a></li>
<!-- K3S falnnel wiregard - https://medium.com/@bestpractices/k3s-with-flannel-wireguard-backend-fa433065a401 -->
     <li>Part2b: <a href="../../kubernetes/k3sup-calico/">Install K3S with k3sup and Calico</a></li>
     <li>Part2c: <a href="../../kubernetes/k3s-cilium/">Install K3S with k3sup and Cilium</a></li>
     <li>Part3: <a href="../../kubernetes/k3s-helm-controller/">K3S helm CR</a></li>
<!-- K3D -->
     <li>Part5: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
     <li>Part6: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
</ul>


<h3 id="what-is-vcluster">What is vCluster</h3>
<p>We all know about <code>k3d</code> is not, it is a lightweight wrapper to run <code>k3s</code> n docker. In that scenario a Kubernetes node is a container running on docker. Probably <code>k3sd</code> was the inspiration for <code>vCluster</code> but they take the idea to the next level. With <code>vCluster</code> you can run a <code>k3s</code> cluster in a single namespace This solution is similar the <a href="https://github.com/kcp-dev/kcp">kcp</a> but they used the tools that already exists.</p>
<p><img src="/img/include/vcluster-architecture.svg" alt="vcluster - Architecture"  class="zoomable" /></p>
<p><code>vCluster</code> runs a component called <code>syncert</code> that is synchronize the Low-Level component from the <code>k3s</code> cluster like pods, services, ingress. So in reality the parent cluster will run these objects.</p>
<h3 id="getting-started">Getting Started</h3>
<p><code>vCluster</code> give you a cli to ease the task of installation:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -s -L <span style="color:#e6db74">&#34;https://github.com/loft-sh/vcluster/releases/latest&#34;</span> | sed -nE <span style="color:#e6db74">&#39;s!.*&#34;([^&#34;]*vcluster-linux-amd64)&#34;.*!https://github.com\1!p&#39;</span> | xargs -n <span style="color:#ae81ff">1</span> curl -L -o vcluster <span style="color:#f92672">&amp;&amp;</span> chmod +x vcluster;
sudo mv vcluster /usr/local/bin;
</code></pre></div><p>In the background <code>vCluster CLI</code> use <code>helm</code> and <code>kubectl</code> to do the magic. So you can easily modify the configuration in the <code>values.yaml</code> file if you want.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl create ns vcluster-1

vcluster create vcluster-1 -n vcluster-1
</code></pre></div><h3 id="exposing-vcluster">Exposing vcluster</h3>
<p>By default, vcluster is not reachable. To directly access vcluste you need to use one of the following methods:</p>
<ul>
<li>port-forwarding</li>
<li>LoadBalancer service</li>
<li>NodePort service</li>
<li>Ingress</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># Use --expose to create a vcluster with an LoadBalancer Service</span>
vcluster create vcluster-1 -n vcluster-1 --expose 
</code></pre></div><p>I have a preinstalled <code>MetalLB</code> so the <code>LoadBalancer Service</code> will work perfectly for me, but if you want to use ingress you can do just fine.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">extensions/v1beta1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Ingress</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">annotations</span>:
    <span style="color:#f92672">kubernetes.io/ingress.class</span>: <span style="color:#ae81ff">nginx</span>
    <span style="color:#f92672">nginx.ingress.kubernetes.io/backend-protocol</span>: <span style="color:#ae81ff">HTTPS</span>
    <span style="color:#f92672">nginx.ingress.kubernetes.io/ssl-passthrough</span>: <span style="color:#e6db74">&#34;true&#34;</span>
    <span style="color:#f92672">nginx.ingress.kubernetes.io/ssl-redirect</span>: <span style="color:#e6db74">&#34;true&#34;</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">vcluster-1</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">vcluster-1</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">rules</span>:
  - <span style="color:#f92672">host</span>: <span style="color:#ae81ff">vcluster-1.example.com</span>
    <span style="color:#f92672">http</span>:
      <span style="color:#f92672">paths</span>:
      - <span style="color:#f92672">backend</span>:
          <span style="color:#f92672">serviceName</span>: <span style="color:#ae81ff">vcluster-1</span>
          <span style="color:#f92672">servicePort</span>: <span style="color:#ae81ff">443</span>
        <span style="color:#f92672">path</span>: <span style="color:#ae81ff">/</span>
</code></pre></div><p>When you manually add an ingress or service you mast add the ip or the hostname to the certificate of the <code>k3s</code> cluster</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">syncer</span>:
  <span style="color:#f92672">extraArgs</span>:
  - --<span style="color:#ae81ff">tls-san=my-vcluster.example.com</span>
<span style="color:#75715e"># or</span>
<span style="color:#f92672">syncer</span>:
  <span style="color:#f92672">extraArgs</span>:
  - --<span style="color:#ae81ff">tls-san=10.10.10.5</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">vcluster create vcluster-1 -n vcluster-1 -f values.yaml
</code></pre></div><h3 id="external-datastorage">External Datastorage</h3>
<p>In the default scenario <code>k3s</code> in the namespace will use SQLite as the Datastorage but <code>vCluster</code> can run <code>k3s</code> with all the other supported Datastorage:</p>
<ul>
<li>Embedded SQLite (default)</li>
<li>PostgreSQL</li>
<li>MySQL</li>
<li>MariaDB</li>
<li>etcd</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">vcluster</span>:
  <span style="color:#f92672">env</span>:
  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">K3S_DATASTORE_ENDPOINT</span>
    <span style="color:#f92672">value</span>: <span style="color:#ae81ff">https://etcd-host-1:2379,https://etcd-host-2:2379,https://etcd-host-3:2379</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">vcluster</span>:
  <span style="color:#f92672">env</span>:
  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">K3S_DATASTORE_ENDPOINT</span>
    <span style="color:#f92672">value</span>: <span style="color:#ae81ff">postgres://username:password@hostname:5432/k3s</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">vcluster</span>:
  <span style="color:#f92672">env</span>:
  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">K3S_DATASTORE_ENDPOINT</span>
    <span style="color:#f92672">value</span>: <span style="color:#e6db74">&#39;mysql://username:password@tcp(hostname:3306)/k3s&#39;</span>
  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">K3S_DATASTORE_CERTFILE</span>
    <span style="color:#f92672">value</span>: <span style="color:#e6db74">&#39;/path/to/client.crt&#39;</span> 
  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">K3S_DATASTORE_KEYFILE</span>
    <span style="color:#f92672">value</span>: <span style="color:#e6db74">&#39;/path/to/client.key&#39;</span> 
  <span style="color:#f92672">volumeMounts</span>:
    - <span style="color:#f92672">mountPath</span>: <span style="color:#ae81ff">/data</span>
      <span style="color:#f92672">name</span>: <span style="color:#ae81ff">data</span>
    - <span style="color:#f92672">mountPath</span>: <span style="color:#ae81ff">/path/to</span>
      <span style="color:#f92672">name</span>: <span style="color:#ae81ff">datastore-tls</span>
<span style="color:#f92672">volumes</span>:
- <span style="color:#f92672">name</span>: <span style="color:#ae81ff">datastore-tls</span>
  <span style="color:#f92672">secret</span>:
    <span style="color:#f92672">secretName</span>: <span style="color:#ae81ff">my-datastore-secret</span>
    <span style="color:#f92672">items</span>:
    - <span style="color:#f92672">key</span>: <span style="color:#ae81ff">tls.key</span>
      <span style="color:#f92672">path</span>: <span style="color:#ae81ff">client.key</span>
    - <span style="color:#f92672">key</span>: <span style="color:#ae81ff">tls.crt</span>
      <span style="color:#f92672">path</span>: <span style="color:#ae81ff">client.crt</span>
</code></pre></div><p>For this demo I will use the default Datastorage.</p>
<h3 id="access-the-vcluster">Access the vcluster</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">vcluster connect vcluster-1 -n vcluster-1

export KUBECONFIG<span style="color:#f92672">=</span>./kubeconfig.yaml

kubectl get ns
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Speed up docker pull with lazypull]]></title>
            <link href="https://devopstales.github.io/home/lazyimage/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/lazyimage/?utm_source=atom_feed" rel="related" type="text/html" title="Speed up docker pull with lazypull" />
                <link href="https://devopstales.github.io/home/k8s-metallb-bgp-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Self-hosted Load Balancer for bare metal Kubernetes" />
                <link href="https://devopstales.github.io/home/k3s-helm-controller/?utm_source=atom_feed" rel="related" type="text/html" title="Install and use rancher helm-controller" />
                <link href="https://devopstales.github.io/home/k8s-longhorn/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes project longhorn" />
                <link href="https://devopstales.github.io/home/k8s-connaisseur-v2/?utm_source=atom_feed" rel="related" type="text/html" title="Image Signature Verification Admission Controller V2" />
            
                <id>https://devopstales.github.io/home/lazyimage/</id>
            
            
            <published>2021-08-02T00:00:00+00:00</published>
            <updated>2021-08-02T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you the solutions to speed up the container downloads.</p>
<p>According to Googles&rsquo;s analisys &ldquo;pulling packages accounts for 76% of container start time, but only 6.4% of that data is read&rdquo; This of course affecting various kinds of workloads on our Kubernetes clusters like cold-start of containers, serverless functions, build and CI/CD. In the community, workarounds are known but they still have unavoidable drawbacks. Let&rsquo;s check this workarounds:</p>
<h3 id="background">Background</h3>
<p>When you run a container the command will pull images from a repository if they are not already available locally. The image is downloaded but not in one big file then multiple smaller one. The runtime engin downloads this files simultaneously to speed up this process. In the background all part is a tar-file downloaded with wget the extracted.</p>
<p>The AUFS storage driver is a common default in container runtime. The AUFS driver takes advantage of the AUFS file system’s layering and copy-on-write (COW) capabilities while also accessing the file system underlying AUFS directly. The driver creates a new directory in the underlying file system for each layer it stores. As a union file system it does not store data directly on disk, but instead uses another file system (e.g. ext4) as underlying storage. A union mount point provides a view of multiple directories in the underlying file system.</p>
<p>Using a HelloBench tool that they wrote, the authors analyse 57 different container images pulled from the Docker Hub. Across these images there are 550 nodes and and 19 roots. They realized that the average uncompressed image is 15 x larger that the amount of image data needed for container startup.</p>
<p>For solving this problem several solution have been proposed including <a href="https://cernvm.cern.ch/fs/">CernVM-FS</a>, <a href="https://stevelasker.blog/2019/10/29/azure-container-registry-teleportation/">Microsoft Teleportation</a>, <a href="https://github.com/google/crfs">Google CRFS</a> and <a href="https://d7y.io/en-us/">Dragonfly</a></p>
<h3 id="standard-compatible-solution">Standard compatible solution</h3>
<p>Containerd started <a href="https://github.com/containerd/stargz-snapshotter">Stargz Snapshotter</a> as a plugin to improve the pull performance. It enables to lazy pull container images leveraging <a href="https://github.com/google/crfs">stargz image format from Google</a>.</p>
<p><img src="/img/include/lazypull1.png" alt="lazypull"  class="zoomable" /></p>
<p>The lazy pull here means containerd doesn’t download the entire image on pull operation but fetches necessary contents on-demand. This shortens the container startup latency from tens of seconds into a few seconds at the best.</p>
<p><img src="/img/include/lazypull2.png" alt="Benchmarking result from the project repository."  class="zoomable" /></p>
<h3 id="quick-start">Quick start</h3>
<p>Containerd supports lazy pulling since version 1.4. Stargz Snapshotter is the plugin that enables containerd to handle eStargz.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano /etc/containerd/config.toml
...
<span style="color:#f92672">[</span>proxy_plugins<span style="color:#f92672">]</span>
  <span style="color:#f92672">[</span>proxy_plugins.stargz<span style="color:#f92672">]</span>
    type <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;snapshot&#34;</span>
    address <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;/run/containerd-stargz-grpc/containerd-stargz-grpc.sock&#34;</span>

<span style="color:#75715e"># Use stargz snapshotter through CRI</span>
<span style="color:#f92672">[</span>plugins.<span style="color:#e6db74">&#34;io.containerd.grpc.v1.cri&#34;</span>.containerd<span style="color:#f92672">]</span>
  snapshotter <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;stargz&#34;</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nerdctl --snapshotter<span style="color:#f92672">=</span>stargz run -it --rm docker.io/stargz-containers/fedora:30-esgz
</code></pre></div><p>CRI-O experimentally supports lazy pulling. The plugin that enables this is called <a href="https://github.com/containerd/stargz-snapshotter/blob/main/docs/INSTALL.md#whats-stargz-snapshotter-and-stargz-store">Additional Layer Store</a>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano /etc/containers/storage.conf
<span style="color:#75715e"># Additional Layer Store is supported only by overlay driver as of now </span>
<span style="color:#f92672">[</span>storage<span style="color:#f92672">]</span>
driver <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;overlay&#34;</span>
graphroot <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;/var/lib/containers/storage&#34;</span>
runroot <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;/run/containers/storage&#34;</span>

<span style="color:#75715e"># Interact with Additional Layer Store over a directory</span>
<span style="color:#f92672">[</span>storage.options<span style="color:#f92672">]</span>
additionallayerstores <span style="color:#f92672">=</span> <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;/path/to/additional/layer/store:ref&#34;</span><span style="color:#f92672">]</span>
</code></pre></div><p><img src="/img/include/lazypull3.png" alt="eStargz in container workflow"  class="zoomable" /></p>
<h3 id="build-images">Build images</h3>
<p>You can create an eStarge image using eStargz-aware image builders (e.g. <a href="https://github.com/GoogleContainerTools/kaniko">Kaniko</a>) or image converters (e.g. ctr-remote and nerdctl).</p>
<p><a href="https://github.com/containerd/stargz-snapshotter/blob/main/docs/ctr-remote.md">ctr-remote</a> is a CLI for converting an OCI/Docker image into eStargz.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ctr-remote image pull docker.io/library/ubuntu:21.04
ctr-remote image optimize --oci <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    docker.io/library/ubuntu:21.04 docker.io/devopstales/ubuntu:21.04-esgz
ctr-remote image push docker.io/devopstales/ubuntu:21.04-esgz
</code></pre></div><p>nerdctl supports creating eStargz images</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nerdctl build -t docker.io/devopstales/foo:1 .
nerdctl image convert --estargz --oci <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    docker.io/devopstales/foo:1 docker.io/devopstales/foo:1-esgz
nerdctl push docker.io/devopstales/foo:1-esgz
</code></pre></div><p><a href="https://github.com/GoogleContainerTools/kaniko">Kaniko</a> is an image builder runnable in containers and Kubernetes. Since v1.5.0, it experimentally supports building eStargz. 7</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">docker run --rm -e GGCR_EXPERIMENT_ESTARGZ<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    -v /tmp/context:/workspace <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    -v ~/.docker/config.json:/kaniko/.docker/config.json:ro <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    gcr.io/kaniko-project/executor:v1.6.0 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>      --destination <span style="color:#e6db74">&#34;docker.io/devopstales/sample:esgz&#34;</span>
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Image Signature Verification Admission Controller V2]]></title>
            <link href="https://devopstales.github.io/home/k8s-connaisseur-v2/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-connaisseur/?utm_source=atom_feed" rel="related" type="text/html" title="Image Signature Verification Admission Controller" />
                <link href="https://devopstales.github.io/kubernetes/k8s-connaisseur-v2/?utm_source=atom_feed" rel="related" type="text/html" title="Image Signature Verification Admission Controller V2" />
                <link href="https://devopstales.github.io/home/kubernetes-policy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Policy" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller-v3/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V3" />
                <link href="https://devopstales.github.io/home/continuous-image-security/?utm_source=atom_feed" rel="related" type="text/html" title="Continuous Image Security" />
            
                <id>https://devopstales.github.io/home/k8s-connaisseur-v2/</id>
            
            
            <published>2021-08-01T00:00:00+00:00</published>
            <updated>2021-08-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can deploy Connaisseur 2.0 to Image Signature Verification into a Kubernetes cluster.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<h3 id="what-is-connaisseur">What is Connaisseur?</h3>
<p>Connaisseur is an admission controller for Kubernetes that integrates Image Signature Verification into a cluster, as a means to ensure that only valid images are being deployed.</p>
<h3 id="notary">Notary</h3>
<p>Notary is an open source signing solution for containers based on The Update Framework Notary uses TUFs’ roles and key hierarchy for signing of the images. There are five keys to sign the metadata files which lists all filenames in the collection, their sizes and respective hashes.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">apt install notary
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">docker pull alpine
docker tag alpine:latest devopstales/testimage:unsigned
docker push devopstales/testimage:unsigned
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">notary -s https://notary.docker.io -d ~/.docker/trust init -p docker.io/devopstales/testimage     
Root key found, using: 31579f2a034add499da6e799bc9260d08a15ab1804298218f05f78d97a669f77
Enter passphrase <span style="color:#66d9ef">for</span> root key with ID 31579f2: 
Enter passphrase <span style="color:#66d9ef">for</span> new targets key with ID 42e49c6: 
Repeat passphrase <span style="color:#66d9ef">for</span> new targets key with ID 42e49c6: 
Enter passphrase <span style="color:#66d9ef">for</span> new snapshot key with ID 399243c: 
Repeat passphrase <span style="color:#66d9ef">for</span> new snapshot key with ID 399243c: 
Enter username: devopstales
Enter password: 
Auto-publishing changes to docker.io/devopstales/testimage
Enter username: devopstales
Enter password: 
Successfully published changes <span style="color:#66d9ef">for</span> repository docker.io/devopstales/testimage


export DOCKER_CONTENT_TRUST<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>
export DOCKER_CONTENT_TRUST_SERVER<span style="color:#f92672">=</span>https://notary.docker.io
docker tag alpine:latest devopstales/testimage:signed
docker push devopstales/testimage:signed
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ find ~/.docker/trust/ | head
/home/devopstales/.docker/trust/
/home/devopstales/.docker/trust/private
/home/devopstales/.docker/trust/private/1f4a9a0922605b3bc19c97e180d962d530721288f4fd0845ad0aa37ba4a6f95d.key
/home/devopstales/.docker/trust/private/fe30e72f5976b2ae7d0d365f28dacfae9c71f11ad854065603ccc806900e84fa.key
/home/devopstales/.docker/trust/private/3da0d27e2d3b964d238d1d184c7578b5f2737b918ec5b8265474e22b07b2ea22.key
/home/devopstales/.docker/trust/private/root-priv.key
/home/devopstales/.docker/trust/private/root-pub.pem
/home/devopstales/.docker/trust/tuf
/home/devopstales/.docker/trust/tuf/docker.io
/home/devopstales/.docker/trust/tuf/docker.io/devopstales
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">notary -s https://notary.docker.io -d ~/.docker/trust list docker.io/devopstales/testimage
NAME     DIGEST                                                              SIZE <span style="color:#f92672">(</span>BYTES<span style="color:#f92672">)</span>    ROLE
----     ------                                                              ------------    ----
signed    4661fb57f7890b9145907a1fe2555091d333ff3d28db86c3bb906f6a2be93c87    <span style="color:#ae81ff">528</span>             targets/devopstales
</code></pre></div><h3 id="install-connaisseur">Install Connaisseur</h3>
<pre tabindex="0"><code># The installer use yq so we need to install it

wget https://github.com/mikefarah/yq/releases/download/v4.2.0/yq_linux_amd64 -O /usr/bin/yq &amp;&amp;\
    chmod +x /usr/bin/yq
</code></pre><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># generate the public root cert</span>

cd ~/.docker/trust/private
sed <span style="color:#e6db74">&#39;/^role:\sroot$/d&#39;</span> <span style="color:#66d9ef">$(</span>grep -iRl <span style="color:#e6db74">&#34;role: root&#34;</span> .<span style="color:#66d9ef">)</span> &gt; root-priv.key
openssl ec -in root-priv.key -pubout -out root-pub.pem
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">git clone https://github.com/sse-secure-systems/connaisseur.git</span>
<span style="color:#ae81ff">cd connaisseur</span>
<span style="color:#ae81ff">nano helm/values.yaml</span>
...
<span style="color:#f92672">validators</span>:
...
<span style="color:#75715e"># static validator that allows each image</span>
- <span style="color:#f92672">name</span>: <span style="color:#ae81ff">allow</span>
  <span style="color:#f92672">type</span>: <span style="color:#ae81ff">static</span>
  <span style="color:#f92672">approve</span>: <span style="color:#66d9ef">true</span>
<span style="color:#75715e"># pre-configured nv1 validator for public notary from Docker Hub</span>
- <span style="color:#f92672">name</span>: <span style="color:#ae81ff">dockerhub_basics</span>
  <span style="color:#f92672">type</span>: <span style="color:#ae81ff">notaryv1</span>
  <span style="color:#f92672">host</span>: <span style="color:#ae81ff">notary.docker.io</span>
  <span style="color:#f92672">trust_roots</span>:
    <span style="color:#75715e"># public key for official docker images (https://hub.docker.com/search?q=&amp;type=image&amp;image_filter=official)</span>
    <span style="color:#75715e"># !if not needed feel free to remove the key!</span>
  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">docker_official</span>
    <span style="color:#f92672">key</span>: |<span style="color:#e6db74">
</span><span style="color:#e6db74">      -----BEGIN PUBLIC KEY-----
</span><span style="color:#e6db74">      MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEOXYta5TgdCwXTCnLU09W5T4M4r9f
</span><span style="color:#e6db74">      QQrqJuADP6U7g5r9ICgPSmZuRHP/1AYUfOQW3baveKsT969EfELKj1lfCA==
</span><span style="color:#e6db74">      -----END PUBLIC KEY-----</span>      
  <span style="color:#75715e"># public key securesystemsengineering repo including Connaisseur images</span>
  <span style="color:#75715e"># !this key is critical for Connaisseur!</span>
  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">securesystemsengineering_official</span>
    <span style="color:#f92672">key</span>: |<span style="color:#e6db74">
</span><span style="color:#e6db74">      -----BEGIN PUBLIC KEY-----
</span><span style="color:#e6db74">      MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEsx28WV7BsQfnHF1kZmpdCTTLJaWe
</span><span style="color:#e6db74">      d0CA+JOi8H4REuBaWSZ5zPDe468WuOJ6f71E7WFg3CVEVYHuoZt2UYbN/Q==
</span><span style="color:#e6db74">      -----END PUBLIC KEY-----</span>      
    <span style="color:#75715e"># public key securesystemsengineering repo including devopstales images</span>
  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">devopstales_official</span>
    <span style="color:#f92672">key</span>: |<span style="color:#e6db74">
</span><span style="color:#e6db74">      -----BEGIN PUBLIC KEY-----
</span><span style="color:#e6db74">      MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE9m6WfwViwT8lYjLF6jAs1bvd1hPp
</span><span style="color:#e6db74">      cRUmONP49JszW1X/6Q22DygylIJGyC8IXeb3zBWVMoYDxauiqrFomHUOEA==
</span><span style="color:#e6db74">      -----END PUBLIC KEY-----</span>      

<span style="color:#f92672">policy</span>:
- <span style="color:#f92672">pattern</span>: <span style="color:#e6db74">&#34;*:*&#34;</span>
- <span style="color:#f92672">pattern</span>: <span style="color:#e6db74">&#34;docker.io/library/*:*&#34;</span>
  <span style="color:#f92672">validator</span>: <span style="color:#ae81ff">dockerhub_basics</span>
  <span style="color:#f92672">with</span>:
    <span style="color:#f92672">trust_root</span>: <span style="color:#ae81ff">docker_official</span>
- <span style="color:#f92672">pattern</span>: <span style="color:#e6db74">&#34;k8s.gcr.io/*:*&#34;</span>
  <span style="color:#f92672">validator</span>: <span style="color:#ae81ff">allow</span>
- <span style="color:#f92672">pattern</span>: <span style="color:#e6db74">&#34;docker.io/securesystemsengineering/*:*&#34;</span>
  <span style="color:#f92672">validator</span>: <span style="color:#ae81ff">dockerhub_basics</span>
  <span style="color:#f92672">with</span>:
    <span style="color:#f92672">trust_root</span>: <span style="color:#ae81ff">securesystemsengineering_official</span>
- <span style="color:#f92672">pattern</span>: <span style="color:#e6db74">&#34;docker.io/devopstales/*:*&#34;</span>
  <span style="color:#f92672">validator</span>: <span style="color:#ae81ff">dockerhub_basics</span>
  <span style="color:#f92672">with</span>:
    <span style="color:#f92672">trust_root</span>: <span style="color:#ae81ff">devopstales_official</span>

</code></pre></div><ul>
<li>the <code>default</code> validator is used if no validator is specified in image policy</li>
<li>type: supported validators (e.g. &ldquo;cosign&rdquo; or &ldquo;notaryv1&rdquo;) notaryv2 is not yet supported</li>
<li>host: url of the notary server</li>
<li>key: the public part of the root key, for verifying notary&rsquo;s signatures</li>
</ul>
<p>Then deploy the helm chart. This can take a few minutes.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">helm install connaisseur helm --atomic --create-namespace --namespace connaisseur
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get all -n connaisseur
NAME                                          READY   STATUS    RESTARTS   AGE
pod/connaisseur-deployment-565d45bb74-ktbmb   1/1     Running   <span style="color:#ae81ff">0</span>          71s
pod/connaisseur-deployment-565d45bb74-pfghx   1/1     Running   <span style="color:#ae81ff">0</span>          71s
pod/connaisseur-deployment-565d45bb74-rcj44   1/1     Running   <span style="color:#ae81ff">0</span>          71s

NAME                      TYPE        CLUSTER-IP    EXTERNAL-IP   PORT<span style="color:#f92672">(</span>S<span style="color:#f92672">)</span>   AGE
service/connaisseur-svc   ClusterIP   10.43.196.6   &lt;none&gt;        443/TCP   71s

NAME                                     READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/connaisseur-deployment   3/3     <span style="color:#ae81ff">3</span>            <span style="color:#ae81ff">3</span>           71s

NAME                                                DESIRED   CURRENT   READY   AGE
replicaset.apps/connaisseur-deployment-565d45bb74   <span style="color:#ae81ff">3</span>         <span style="color:#ae81ff">3</span>         <span style="color:#ae81ff">3</span>       71s
</code></pre></div><h3 id="test-the-image-signature-verification">Test the Image Signature Verification</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubens default

kubectl run unsigned --image<span style="color:#f92672">=</span>docker.io/devopstales/testimage:unsigned
Error from server: admission webhook <span style="color:#e6db74">&#34;connaisseur-svc.connaisseur.svc&#34;</span> denied the request: Unable to find signed digest <span style="color:#66d9ef">for</span> image docker.io/devopstales/testimage:unsigned.

kubectl run signed --image<span style="color:#f92672">=</span>docker.io/devopstales/testimage:signed
pod/signed created

kubectl get po
</code></pre></div><h3 id="final-words">Final words</h3>
<p>Connaisseur is a grate tool and with the 2.0 it solved all of the 1.0&rsquo;s shortcomings:</p>
<ul>
<li>There is no option to whitelist images in a specific namespace.</li>
<li>Connaisseur supports only one Notary server</li>
<li>Connaisseur supports only one public key</li>
</ul>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Create a Helm reposirory with GitHub Pages]]></title>
            <link href="https://devopstales.github.io/home/helm-repositoty/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/helm-repositoty/?utm_source=atom_feed" rel="related" type="text/html" title="Create a Helm reposirory with GitHub Pages" />
                <link href="https://devopstales.github.io/home/helm3-loki/?utm_source=atom_feed" rel="related" type="text/html" title="Install Grafana Loki with Helm3" />
                <link href="https://devopstales.github.io/home/k8s-error-at-kubectl-logs/?utm_source=atom_feed" rel="related" type="text/html" title="K8s ERROR at kubectl logs" />
                <link href="https://devopstales.github.io/home/k8s-kuberos/?utm_source=atom_feed" rel="related" type="text/html" title="Kubectl authentication with Kuberos" />
                <link href="https://devopstales.github.io/home/k8s-ceph-storage-with-csi-driver/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD volume with CSI driver" />
            
                <id>https://devopstales.github.io/home/helm-repositoty/</id>
            
            
            <published>2021-07-25T00:00:00+00:00</published>
            <updated>2021-07-25T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can host your own Helm repository with GitHub Pages.</p>
<h2 id="create-a-new-github-repository">Create a new GitHub Repository</h2>
<p>Log into GitHub and create a <a href="https://github.com/new">new repository</a> called helm-charts. I chose to hav a README file and an Apache2 licence in mye repository.</p>
<p>Clone the repository to start working.</p>
<pre tabindex="0"><code>git clone git@github.com:devopstales/helm-charts.git
cd helm-charts

tree
.
├── LICENSE
└── README.md
</code></pre><p>Create a hem chart in the repository:</p>
<pre tabindex="0"><code>mkdir charts
helm create charts/chart1
helm create charts/chart2

tree
.
├── charts
│   ├── chart1
│   │   ├── charts
│   │   ├── Chart.yaml
│   │   ├── templates
│   │   │   ├── deployment.yaml
│   │   │   ├── _helpers.tpl
│   │   │   ├── ingress.yaml
│   │   │   ├── NOTES.txt
│   │   │   ├── service.yaml
│   │   │   └── tests
│   │   │       └── test-connection.yaml
│   │   └── values.yaml
│   └── chart2
│       ├── charts
│       ├── Chart.yaml
│       ├── templates
│       │   ├── deployment.yaml
│       │   ├── _helpers.tpl
│       │   ├── ingress.yaml
│       │   ├── NOTES.txt
│       │   ├── service.yaml
│       │   └── tests
│       │       └── test-connection.yaml
│       └── values.yaml
├── LICENSE
└── README.md
</code></pre><h3 id="push-to-github">Push to GitHub:</h3>
<pre tabindex="0"><code>echo &quot;.deploy&quot; &gt;&gt; .gitignore
git add . --all
git commit -m 'Initial Commit'
git push origin main
</code></pre><p>Create brach for GitHub Pages and release:</p>
<pre tabindex="0"><code>git checkout --orphan gh-pages
Switched to a new branch 'gh-pages'

rm -rf charts
git add . --all
git commit -m 'initial gh-pages'
git push origin gh-pages
git checkout main
</code></pre><p>Next enable GitHub Pages i the repository settings. After a few minutes you should have a default rendering on your README.md at the provided URL.</p>
<h2 id="use-chart-releaser">Use chart-releaser</h2>
<p>Yo can create a chart Helm repository by usin the <code>helm package</code> and <code>helm repo</code> commands but you can simplify your life by using <code>chart-releaser</code>.</p>
<h3 id="install-for-lnux">Install for Lnux:</h3>
<pre tabindex="0"><code>cd /tmp
curl -sSL https://github.com/helm/chart-releaser/releases/download/v1.2.1/chart-releaser_1.2.1_linux_amd64.tar.gz | tar xzf -
mv cr ~/bin/cr
cr help
</code></pre><h2 id="install-for-mac-osx">Install for Mac osX:</h2>
<pre tabindex="0"><code>$ brew tap helm/tap
$ brew install chart-releaser
</code></pre><h3 id="usage">Usage:</h3>
<p>The <code>cr index</code> will create the appropriate <code>index.yaml</code> and <code>cr upload</code> will upload the packages to GitHub Releases. Fot theat you need a GitHub Token.
In your browser go to your <a href="https://github.com/settings/tokens">github developer settings</a> and create a new personal access token and add full access to the repo.</p>
<p>Create an environment variable for the token:</p>
<pre tabindex="0"><code>export CH_TOKEN=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sdf

helm package charts/{chart1,chart2} --destination .deploy

cr upload -o devopstales -r helm-charts -p .deploy
git checkout gh-pages
cr index -i ./index.yaml -p .deploy -o devopstales -r helm-charts -c https://devopstales.github.io/helm-charts/

git add index.yaml
git commit -m 'release 0.1.0'
git push origin gh-pages
</code></pre><h3 id="update-the-readmemd-with-instructions-to-usage">Update the README.md with instructions to usage</h3>
<pre tabindex="0"><code>nano README.md
git add README.md
git commit -m 'update readme with instructions'
git push origin gh-pages
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Add a Custom Host to Kubernetes]]></title>
            <link href="https://devopstales.github.io/home/k8s-custom-host/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/image-security-admission-controller-v3/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V3" />
                <link href="https://devopstales.github.io/home/continuous-image-security/?utm_source=atom_feed" rel="related" type="text/html" title="Continuous Image Security" />
                <link href="https://devopstales.github.io/home/k8s-prometheus-stack/?utm_source=atom_feed" rel="related" type="text/html" title="K8S Logging And Monitoring" />
                <link href="https://devopstales.github.io/home/k8s-vault-v2/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes integration with external Vault" />
                <link href="https://devopstales.github.io/home/rke2-calico/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Install With Calico" />
            
                <id>https://devopstales.github.io/home/k8s-custom-host/</id>
            
            
            <published>2021-07-22T00:00:00+00:00</published>
            <updated>2021-07-22T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I&rsquo;will show you how to add custom hosts to kubernetes.</p>
<H3>Parst of the Kubernetes series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
     <li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
     <li>Part1c: <a href="../../kubernetes/k8s-install-containerd/">Install K8S with containerd and kubeadm</a></li>
     <li>Part1d: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
     <li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a></li>
<!--      <li>Part3b: k8s-multiple-nginx-ingress -->
<!--      <li>Part3c: k8s-contour-ingress -->
     <li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a></li>
<!-- local folder (+ autoprovisioning ) -->
     <li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
     <li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
     <li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
     <li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
     <li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
     <li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a></li>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... -->
     <li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
     <li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
     <li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
     <li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
     <li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
     <li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
     <li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a></li>
<!-- istio-install.md -->
     <li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a></li>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog -->
</ul>


<p>CoreDNS is the DNS server in kubernetes. I some situation I need to add custom hosts to be resolvable in the kubernetes netwok.</p>
<p>First, edit the ConfigMap of the coredns using the following command:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl edit cm -n kube-system coredns
<span style="color:#75715e"># or</span>
kubectl edit cm -n kube-system rke2-coredns-rke2-coredns
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ConfigMap</span>
<span style="color:#f92672">data</span>:
  <span style="color:#f92672">Corefile</span>: |<span style="color:#e6db74">
</span><span style="color:#e6db74">    .:53 {
</span><span style="color:#e6db74">        errors
</span><span style="color:#e6db74">        health {
</span><span style="color:#e6db74">          lameduck 5s
</span><span style="color:#e6db74">        }
</span><span style="color:#e6db74">        ready
</span><span style="color:#e6db74">        kubernetes cluster.local in-addr.arpa ip6.arpa {
</span><span style="color:#e6db74">          pods insecure
</span><span style="color:#e6db74">          fallthrough in-addr.arpa ip6.arpa
</span><span style="color:#e6db74">        }
</span><span style="color:#e6db74">        prometheus :9153
</span><span style="color:#e6db74">        forward . 8.8.8.8 8.8.4.4
</span><span style="color:#e6db74">        cache 30
</span><span style="color:#e6db74">        loop
</span><span style="color:#e6db74">        reload
</span><span style="color:#e6db74">        loadbalance
</span><span style="color:#e6db74">        hosts /etc/coredns/customdomains.db k8s.intra {
</span><span style="color:#e6db74">          172.17.14.10 rancher.k8s.intra
</span><span style="color:#e6db74">          172.17.14.10 hubble.k8s.intra
</span><span style="color:#e6db74">          172.17.14.10 grafana.k8s.intra
</span><span style="color:#e6db74">          172.17.14.10 alertmanager.k8s.intra
</span><span style="color:#e6db74">          172.17.14.10 prometheus.k8s.intra
</span><span style="color:#e6db74">          172.17.14.10 sso.k8s.intra
</span><span style="color:#e6db74">          fallthrough
</span><span style="color:#e6db74">        }
</span><span style="color:#e6db74">    }</span>    
</code></pre></div><p>Delete coredens pods:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get pod -n kube-system | grep dns
kubectl delete pod -n kube-system core-dns-#########
</code></pre></div><p>It’s done! You can now reach that custom host from inside any pod on the cluster.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Subject Alternative Name in Active Dyrectory LDAPS Cerificate]]></title>
            <link href="https://devopstales.github.io/home/msad-ldaps-subject-alternative-mame/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/msad-ldaps-subject-alternative-mame/</id>
            
            
            <published>2021-07-22T00:00:00+00:00</published>
            <updated>2021-07-22T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can configure custom Subject Alternative Name in Active Directory LDAPS certificate.</p>
<h3 id="open-mmc">Open mmc</h3>
<ul>
<li><code>windows + r</code></li>
<li>run <code>mmc</code></li>
</ul>
<p><img src="/img/include/ldapssan1.PNG" alt="Example image"  class="zoomable" /></p>
<ul>
<li>Click <code>File / Add/Remove Snap-in..</code> or <code>ctrl + m</code></li>
</ul>
<p><img src="/img/include/ldapssan2.PNG" alt="Example image"  class="zoomable" />
<img src="/img/include/ldapssan3.PNG" alt="Example image"  class="zoomable" /></p>
<ul>
<li>Add certificates</li>
</ul>
<p><img src="/img/include/ldapssan4.PNG" alt="Example image"  class="zoomable" />
<img src="/img/include/ldapssan5.PNG" alt="Example image"  class="zoomable" /></p>
<ul>
<li>Add a nother certificates for service</li>
</ul>
<p><img src="/img/include/ldapssan6.PNG" alt="Example image"  class="zoomable" />
<img src="/img/include/ldapssan5.PNG" alt="Example image"  class="zoomable" />
<img src="/img/include/ldapssan7.PNG" alt="Example image"  class="zoomable" /></p>
<ul>
<li>Add Certificate Authoraty</li>
</ul>
<p><img src="/img/include/ldapssan5.PNG" alt="Example image"  class="zoomable" />
<img src="/img/include/ldapssan8.PNG" alt="Example image"  class="zoomable" /></p>
<h3 id="clone-template">Clone Template</h3>
<ul>
<li><code>Certificate Authoraty / Domain Controller / Certificate Template</code></li>
</ul>
<p><img src="/img/include/ldapssan9.PNG" alt="Example image"  class="zoomable" />
<img src="/img/include/ldapssan10.PNG" alt="Example image"  class="zoomable" /></p>
<ul>
<li>Select <code>Domain Controller Template</code></li>
<li>Right Click and <code>Duplicate template</code></li>
</ul>
<p><img src="/img/include/ldapssan11.PNG" alt="Example image"  class="zoomable" />
<img src="/img/include/ldapssan12.PNG" alt="Example image"  class="zoomable" />
<img src="/img/include/ldapssan22.PNG" alt="Example image"  class="zoomable" /></p>
<ul>
<li>the click <code>OK</code> and cluse the <code>Certificate Teplate Console</code></li>
</ul>
<h3 id="add-template-to-certificate-template-list">Add template to Certificate Template list</h3>
<ul>
<li>At <code>Certificate Authoraty / Domain Controller / Certificate Template</code></li>
</ul>
<p><img src="/img/include/ldapssan9.PNG" alt="Example image"  class="zoomable" /></p>
<ul>
<li>Rght click and select <code>Certificate Template to Issue</code> Add the new Template</li>
</ul>
<p><img src="/img/include/ldapssan13.PNG" alt="Example image"  class="zoomable" /></p>
<h3 id="generate-certificate">Generate Certificate</h3>
<ul>
<li>Right click on <code>Certificates (Local Computer) / Personal / Certificate</code> and select <code>All Tasks / Request New Certificate</code></li>
</ul>
<p><img src="/img/include/ldapssan16.PNG" alt="Example image"  class="zoomable" />
<img src="/img/include/ldapssan17.PNG" alt="Example image"  class="zoomable" />
<img src="/img/include/ldapssan18.PNG" alt="Example image"  class="zoomable" />
<img src="/img/include/ldapssan19.PNG" alt="Example image"  class="zoomable" />
<img src="/img/include/ldapssan20.PNG" alt="Example image"  class="zoomable" /></p>
<ul>
<li>enroll</li>
</ul>
<p><img src="/img/include/ldapssan21.PNG" alt="Example image"  class="zoomable" /></p>
<h4 id="change-certificate">Change Certificate</h4>
<ul>
<li>To activate the new certificate you need to restart the Domain Controller</li>
</ul>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[GKE cluster’s egress traffic via Cloud NAT]]></title>
            <link href="https://devopstales.github.io/home/gke-egress/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/gke-egress/?utm_source=atom_feed" rel="related" type="text/html" title="GKE cluster’s egress traffic via Cloud NAT" />
                <link href="https://devopstales.github.io/home/gcp-vm-export/?utm_source=atom_feed" rel="related" type="text/html" title="Export GCP VM to S3" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller-v3/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V3" />
                <link href="https://devopstales.github.io/home/continuous-image-security/?utm_source=atom_feed" rel="related" type="text/html" title="Continuous Image Security" />
                <link href="https://devopstales.github.io/home/k8s-prometheus-stack/?utm_source=atom_feed" rel="related" type="text/html" title="K8S Logging And Monitoring" />
            
                <id>https://devopstales.github.io/home/gke-egress/</id>
            
            
            <published>2021-07-01T00:00:00+00:00</published>
            <updated>2021-07-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can can reroute the GKE egress traffic via cloud NAT.</p>
<p>In Public GKE cluster wach node has it&rsquo;s own external IP address and the nodes route all egress traffic through there external IP. This external IPs can change over time. In the case of a private GKE cluster, all the nodes will have an internal ip address and you can define a cloud NAT for all your egress traffic from the cluster. So public cluster is not a ideal solutinon if you need a static ip list for source ip whtelistink, but here is a solution.</p>
<h3 id="create-a-cloud-nat-gateway">Create a cloud NAT gateway</h3>
<p>We will use a daemon set in GKE , that will rewrite the ip-table rules in the GKE Nodes to masquerade the outbound traffic.</p>
<p>Select the VPC in which you have deployed your public GKE cluster and create a new cloud router. Create it manualli to configure the NAT gateway’s ip. This will be the ip-address that you will give to your third party vendor for whitelisting your incoming connection.</p>
<p>Create the config map and the daemon-set:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano config.yaml</span>
---
<span style="color:#f92672">nonMasqueradeCIDRs</span>:
  - <span style="color:#ae81ff">0.0.0.0</span><span style="color:#ae81ff">/0</span>
<span style="color:#f92672">masqLinkLocal</span>: <span style="color:#66d9ef">true</span>
<span style="color:#f92672">resyncInterval</span>: <span style="color:#ae81ff">60s</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl create configmap ip-masq-agent --from-file config.yaml --namespace kube-system
</code></pre></div><p>Deploy the masq-agent:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano ip-masq-agent.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">apps/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">DaemonSet</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">ip-masq-agent</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">kube-system</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">selector</span>:
    <span style="color:#f92672">matchLabels</span>:
      <span style="color:#f92672">k8s-app</span>: <span style="color:#ae81ff">ip-masq-agent</span>
  <span style="color:#f92672">template</span>:
    <span style="color:#f92672">metadata</span>:
      <span style="color:#f92672">labels</span>:
        <span style="color:#f92672">k8s-app</span>: <span style="color:#ae81ff">ip-masq-agent</span>
    <span style="color:#f92672">spec</span>:
      <span style="color:#f92672">hostNetwork</span>: <span style="color:#66d9ef">true</span>
      <span style="color:#f92672">containers</span>:
      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">ip-masq-agent</span>
        <span style="color:#f92672">image</span>: <span style="color:#ae81ff">gcr.io/google-containers/ip-masq-agent-amd64:v2.4.1</span>
        <span style="color:#f92672">args</span>:
            - --<span style="color:#ae81ff">masq-chain=IP-MASQ</span>
            <span style="color:#75715e"># To non-masquerade reserved IP ranges by default, uncomment the line below.</span>
            <span style="color:#75715e"># - --nomasq-all-reserved-ranges</span>
        <span style="color:#f92672">securityContext</span>:
          <span style="color:#f92672">privileged</span>: <span style="color:#66d9ef">true</span>
        <span style="color:#f92672">volumeMounts</span>:
          - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">config</span>
            <span style="color:#f92672">mountPath</span>: <span style="color:#ae81ff">/etc/config</span>
      <span style="color:#f92672">volumes</span>:
        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">config</span>
          <span style="color:#f92672">configMap</span>:
            <span style="color:#75715e"># Note this ConfigMap must be created in the same namespace as the</span>
            <span style="color:#75715e"># daemon pods - this spec uses kube-system</span>
            <span style="color:#f92672">name</span>: <span style="color:#ae81ff">ip-masq-agent</span>
            <span style="color:#f92672">optional</span>: <span style="color:#66d9ef">true</span>
            <span style="color:#f92672">items</span>:
              <span style="color:#75715e"># The daemon looks for its config in a YAML file at /etc/config/ip-masq-agent</span>
              - <span style="color:#f92672">key</span>: <span style="color:#ae81ff">config</span>
                <span style="color:#f92672">path</span>: <span style="color:#ae81ff">ip-masq-agent</span>
      <span style="color:#f92672">tolerations</span>:
      - <span style="color:#f92672">effect</span>: <span style="color:#ae81ff">NoSchedule</span>
        <span style="color:#f92672">operator</span>: <span style="color:#ae81ff">Exists</span>
      - <span style="color:#f92672">effect</span>: <span style="color:#ae81ff">NoExecute</span>
        <span style="color:#f92672">operator</span>: <span style="color:#ae81ff">Exists</span>
      - <span style="color:#f92672">key</span>: <span style="color:#e6db74">&#34;CriticalAddonsOnly&#34;</span>
        <span style="color:#f92672">operator</span>: <span style="color:#e6db74">&#34;Exists&#34;</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f ip-masq-agent.yaml
</code></pre></div><p>After the creation ogthe ip-masq-agent check the firewall rules of the GKE nodes:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo iptables -t NAT -L IP-MASQ

Chain IP-MASQ <span style="color:#f92672">(</span><span style="color:#ae81ff">2</span> references<span style="color:#f92672">)</span>
target     prot opt cource      destination
RETURN     all  --  anywhere    anywhere      /* ip-masq-agent: local traffic is not subject to MASQUERADE */
MASQUERADE  all  --  anywhere    anywhere      /* ip-masq-agent: outbound traffic is subject to MASQUERADE <span style="color:#f92672">(</span>must be last in chain<span style="color:#f92672">)</span> */
</code></pre></div><p>So the egress traffic from GKE to internet will go via the cloud NAT’s gateway ip address.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Active Directory Configure secure LDAPS]]></title>
            <link href="https://devopstales.github.io/home/msad-ldaps/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/msad-ldaps/</id>
            
            
            <published>2021-06-22T00:00:00+00:00</published>
            <updated>2021-06-22T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can configure LDAPS in Active Directory.</p>
<h3 id="install-certificate-authority">Install Certificate Authority</h3>
<ul>
<li>
<p>On your Windows Server Machine, click on Start -&gt; Server Manager -&gt; Add Roles and Features.
adldaps1.png</p>
</li>
<li>
<p>After selecting Add Roles and Features and Click on Next.
<img src="/img/include/adldaps2.png" alt="Example image"  class="zoomable" /></p>
</li>
<li>
<p>Choose Role-based or feature-based installation option and Click on Next button.
<img src="/img/include/adldaps3.png" alt="Example image"  class="zoomable" /></p>
</li>
<li>
<p>Choose Select a server from the server pool option &amp; Select ldap server from the server pool and click on Next button.
<img src="/img/include/adldaps4.png" alt="Example image"  class="zoomable" /></p>
</li>
<li>
<p>Choose Active Directory Certificate Services option from the list of roles and click on Next button.
<img src="/img/include/adldaps5.png" alt="Example image"  class="zoomable" /></p>
</li>
<li>
<p>Choose nothing from the list of features and click on Next button.</p>
</li>
<li>
<p>In Active Directory Certificate Services (AD CS) choose nothing and Click on Next button.
<img src="/img/include/adldaps6.png" alt="Example image"  class="zoomable" /></p>
</li>
<li>
<p>Mark Certification Authority from the list of roles and Click on Next button.
<img src="/img/include/adldaps7.png" alt="Example image"  class="zoomable" /></p>
</li>
<li>
<p>Click on Install button to confirm installation.</p>
</li>
<li>
<p>Now, click on Configure Active Directory Certificate Services on Destination Server option and click on Close button.
<img src="/img/include/adldaps8.png" alt="Example image"  class="zoomable" /></p>
</li>
<li>
<p>We can use the currently logged on user to configure role services since it belongs to the local Administrators group. Click on Next button.</p>
</li>
<li>
<p>Mark Certification Authority from the list of roles and Click on Next button.
<img src="/img/include/adldaps9.png" alt="Example image"  class="zoomable" /></p>
</li>
<li>
<p>Choose Enterprise CA option and Click on Next.
<img src="/img/include/adldaps10.png" alt="Example image"  class="zoomable" /></p>
</li>
<li>
<p>Choose Root CA option and Click on Next button.
<img src="/img/include/adldaps11.png" alt="Example image"  class="zoomable" /></p>
</li>
<li>
<p>Choose Create a new private key option and Click on Next button.
<img src="/img/include/adldaps12.png" alt="Example image"  class="zoomable" /></p>
</li>
<li>
<p>Choose SHA256 as the hash algorithm and Click on Next. UPDATE : Recommended to select the most recent hashing algorithm.
<img src="/img/include/adldaps13.png" alt="Example image"  class="zoomable" /></p>
</li>
<li>
<p>Click on Next button.
<img src="/img/include/adldaps14.png" alt="Example image"  class="zoomable" /></p>
</li>
<li>
<p>Specify the validity of the certificate choosing Default 5 years and Click on Next button.
<img src="/img/include/adldaps15.png" alt="Example image"  class="zoomable" /></p>
</li>
<li>
<p>Select the default database location and Click on Next.
<img src="/img/include/adldaps16.png" alt="Example image"  class="zoomable" /></p>
</li>
<li>
<p>Click on Configure button to confirm.</p>
</li>
<li>
<p>Once the configuration succeeded and click on Close button.
<img src="/img/include/adldaps17.png" alt="Example image"  class="zoomable" /></p>
</li>
</ul>
<h3 id="configuring-secure-ldap">Configuring secure LDAP:</h3>
<ul>
<li>At restart the Domain Controller Will generate a new Certificate fos self.</li>
</ul>
<h3 id="test-ldaps">Test LDAPS</h3>
<ul>
<li>windows + R</li>
<li>Run: ldp</li>
</ul>
<p><img src="/img/include/adldaps18.png" alt="Example image"  class="zoomable" /></p>
<ul>
<li>Select connect menu in top right</li>
<li>Add the name of the serfer for server</li>
<li>port: 636</li>
<li>selset SSL</li>
<li>click OK</li>
</ul>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[RKE2 Image security Admission Controller V3]]></title>
            <link href="https://devopstales.github.io/home/image-security-admission-controller-v3/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/continuous-image-security/?utm_source=atom_feed" rel="related" type="text/html" title="Continuous Image Security" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller-v2/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V2" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller" />
                <link href="https://devopstales.github.io/kubernetes/image-security-admission-controller-v3/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V3" />
                <link href="https://devopstales.github.io/kubernetes/continuous-image-security/?utm_source=atom_feed" rel="related" type="text/html" title="Continuous Image Security" />
            
                <id>https://devopstales.github.io/home/image-security-admission-controller-v3/</id>
            
            
            <published>2021-06-21T00:00:00+00:00</published>
            <updated>2021-06-21T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In a previous posts we talked about <a href="https://devopstales.github.io/home/image-security-admission-controller/">Banzaicloud&rsquo;s anchore-image-validator</a> and <a href="https://devopstales.github.io/home/image-security-admission-controller-v2/">Anchore&rsquo;s own admission-controller</a>. In this post I will show you my own admission-controller for image scanning.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<p>I found multiple solution for Anchore Engine but only one for Trivy. The <a href="https://github.com/aquasecurity/trivy-enforcer">trivy-enforcer</a> that is an experimental project and use OPA for enforce the policy. So I decide to create mey own dmission-controller.</p>
<h3 id="how-an-admission-controller-works">How an admission controller works</h3>
<p>An Admission Controller Webhook is triggered when a Kubernetes object is created. It sends a JSON formatted HTTP request to a specific Kubernetes Service in a namespace which returns a JSON response. If you whoud like to now more aboute admission controllers you can read about it in my previous post <a href="https://devopstales.github.io/kubernetes/admission-controllers/">Using Admission Controllers</a></p>
<h3 id="writing-a-validating-admission-controller">Writing a Validating Admission Controller</h3>
<p>I want to walidate the Pod object to check how many vulnerability has the image in this pod. So I wrote a <a href="https://github.com/devopstales/trivy-image-validator/blob/master/trivy-scanner.py">python script</a> that will pars the JSON request for the pods image, rin a trivy scan on it and sen back the answer. Then I build it to a docker image called <code>devopstales/trivy-scanner-admission:1.0.1</code>. I run it as a deployment:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">--- 
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">PersistentVolumeClaim</span>
<span style="color:#f92672">metadata</span>: 
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">trivy-cache</span>
<span style="color:#f92672">spec</span>: 
  <span style="color:#f92672">accessModes</span>: 
    - <span style="color:#ae81ff">ReadWriteOnce</span>
  <span style="color:#f92672">resources</span>: 
    <span style="color:#f92672">requests</span>: 
      <span style="color:#f92672">storage</span>: <span style="color:#ae81ff">1G</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">apps/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Deployment</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">trivy-scanner</span>
  <span style="color:#f92672">labels</span>:
    <span style="color:#f92672">app</span>: <span style="color:#ae81ff">trivy-scanner</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">validation</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">replicas</span>: <span style="color:#ae81ff">1</span>
  <span style="color:#f92672">selector</span>:
    <span style="color:#f92672">matchLabels</span>:
      <span style="color:#f92672">app</span>: <span style="color:#ae81ff">trivy-scanner</span>
  <span style="color:#f92672">template</span>:
    <span style="color:#f92672">metadata</span>:
      <span style="color:#f92672">labels</span>:
        <span style="color:#f92672">app</span>: <span style="color:#ae81ff">trivy-scanner</span>
    <span style="color:#f92672">spec</span>:
      <span style="color:#f92672">securityContext</span>:
        <span style="color:#f92672">fsGroup</span>: <span style="color:#ae81ff">10001</span>
      <span style="color:#f92672">containers</span>:
        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">trivy-scanner</span>
          <span style="color:#f92672">image</span>: <span style="color:#ae81ff">devopstales/trivy-scanner-admission:1.0.1</span>
          <span style="color:#f92672">imagePullPolicy</span>: <span style="color:#ae81ff">Always</span>
          <span style="color:#f92672">volumeMounts</span>:
          - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cache</span>
            <span style="color:#f92672">mountPath</span>: <span style="color:#e6db74">&#34;/home/kube-trivy-admission/.cache/trivy&#34;</span>
<span style="color:#75715e">#          - name: config-json</span>
<span style="color:#75715e">#            mountPath: &#34;/home/kube-trivy-admission/.docker&#34;</span>
      <span style="color:#f92672">volumes</span>:
      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cache</span>
        <span style="color:#f92672">persistentVolumeClaim</span>:
            <span style="color:#f92672">claimName</span>: <span style="color:#e6db74">&#34;trivy-cache&#34;</span>
<span style="color:#75715e">#      - name: config-json</span>
<span style="color:#75715e">#        secret:</span>
<span style="color:#75715e">#          secretName: config-json</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Service</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">trivy-scanner</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">validation</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">selector</span>:
    <span style="color:#f92672">app</span>: <span style="color:#ae81ff">trivy-scanner</span>
  <span style="color:#f92672">ports</span>:
  - <span style="color:#f92672">port</span>: <span style="color:#ae81ff">443</span>
    <span style="color:#f92672">targetPort</span>: <span style="color:#ae81ff">5000</span>
</code></pre></div><p>The Service must be an HTTPS port on 443 for the Admission Webhook so I created a self-signed certificate and placed in the docker container.</p>
<h3 id="create-the-admission-webhook">Create the Admission Webhook</h3>
<p>The abow Admission Webhook will send teh HTTP request to my <code>trivy-scanner</code> service:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">admissionregistration.k8s.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ValidatingWebhookConfiguration</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">trivy-scanner</span>
<span style="color:#f92672">webhooks</span>:
  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">trivy-scanner.devopstales.intra</span>
    <span style="color:#f92672">sideEffects</span>: <span style="color:#e6db74">&#34;None&#34;</span>
    <span style="color:#f92672">admissionReviewVersions</span>: [<span style="color:#ae81ff">v1beta1, v1]</span>
    <span style="color:#f92672">clientConfig</span>:
      <span style="color:#f92672">service</span>:
        <span style="color:#f92672">name</span>: <span style="color:#ae81ff">trivy-scanner</span>
        <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">validation</span>
        <span style="color:#f92672">path</span>: <span style="color:#e6db74">&#34;/validate&#34;</span>
      <span style="color:#f92672">caBundle</span>: <span style="color:#e6db74">&#34;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&#34;</span>
    <span style="color:#f92672">rules</span>:
      - <span style="color:#f92672">apiGroups</span>: [<span style="color:#e6db74">&#34;apps&#34;</span>, <span style="color:#e6db74">&#34;&#34;</span>]
        <span style="color:#f92672">resources</span>:
          - <span style="color:#e6db74">&#34;pods&#34;</span>
        <span style="color:#f92672">apiVersions</span>:
          - <span style="color:#e6db74">&#34;*&#34;</span>
        <span style="color:#f92672">operations</span>:
          - <span style="color:#ae81ff">CREATE</span>
</code></pre></div><p>I placed the root CA of my self-signed certificate in this Validating Webhook Configuration to make my Service&rsquo;s certiface valid for the Kubernetes api.</p>
<h3 id="policy">Policy</h3>
<p>Now If I create a Deployment, Pod &hellip; It scan the Image with tryvi. To block an Image I need to add the limits of the maximum numer of vulnerability for the severities.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">apps/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Deployment</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">labels</span>:
    <span style="color:#f92672">app</span>: <span style="color:#ae81ff">nginx</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">nginx</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">selector</span>:
    <span style="color:#f92672">matchLabels</span>:
      <span style="color:#f92672">app</span>: <span style="color:#ae81ff">nginx</span>
  <span style="color:#f92672">template</span>:
    <span style="color:#f92672">metadata</span>:
      <span style="color:#f92672">annotations</span>:
        <span style="color:#f92672">trivy.security.devopstales.io/medium</span>: <span style="color:#e6db74">&#34;5&#34;</span>
        <span style="color:#f92672">trivy.security.devopstales.io/low</span>: <span style="color:#e6db74">&#34;10&#34;</span>
        <span style="color:#f92672">trivy.security.devopstales.io/critical</span>: <span style="color:#e6db74">&#34;2&#34;</span>
      <span style="color:#f92672">labels</span>:
        <span style="color:#f92672">app</span>: <span style="color:#ae81ff">nginx</span>
    <span style="color:#f92672">spec</span>:
      <span style="color:#f92672">containers</span>:
      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">nginx</span>
        <span style="color:#f92672">image</span>: <span style="color:#ae81ff">nginx:latest</span>
        <span style="color:#f92672">ports</span>:
        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">http</span>
          <span style="color:#f92672">containerPort</span>: <span style="color:#ae81ff">80</span>
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Continuous Image Security]]></title>
            <link href="https://devopstales.github.io/home/continuous-image-security/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/image-security-admission-controller-v2/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V2" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller" />
                <link href="https://devopstales.github.io/kubernetes/continuous-image-security/?utm_source=atom_feed" rel="related" type="text/html" title="Continuous Image Security" />
                <link href="https://devopstales.github.io/home/k8s-vault-v2/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes integration with external Vault" />
                <link href="https://devopstales.github.io/home/rke2-calico/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Install With Calico" />
            
                <id>https://devopstales.github.io/home/continuous-image-security/</id>
            
            
            <published>2021-06-15T00:00:00+00:00</published>
            <updated>2021-06-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you my tool to Continuously scann deployed images in your Kubernetes cluster.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<p>In a previous posts we talked about admission-controllers that scnas the image at deploy. Like <a href="https://devopstales.github.io/home/image-security-admission-controller/">Banzaicloud&rsquo;s anchore-image-validator</a> and <a href="https://devopstales.github.io/home/image-security-admission-controller-v2/">Anchore&rsquo;s own admission-controller</a>. But what if you run your image for a long time. Last weak I realised I run containers wit imagest older the a year. I this time period many new vulnerability came up.</p>
<p>I find a tool called <a href="https://github.com/fleeto/trivy-scanner">trivy-scanner</a> that do almast what I want. It scans the docker images in all namespaces with the label <code>trivy=true</code> and get the resoults to a prometheus endpoint. It based on <a href="https://github.com/flant/shell-operator">Shell Operator</a> that runs a small python script. I made my own version from it:</p>
<h3 id="deploy-the-app">Deploy the app</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">git clone https://github.com/devopstales/trivy-scanner

nano trivy-scanner/deploy/kubernetes/kustomization.yaml
namespace: trivy-scanner
...

kubectl create ns trivy-scanner
kubectl aplly -k trivy-scanner/deploy/kubernetes/
</code></pre></div><h3 id="demo">Demo</h3>
<p>Test the <code>guestbook-demo</code> namespace:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl label namespaces guestbook-demo trivy<span style="color:#f92672">=</span>true

kubectl get service -n trivy-scanner
NAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT<span style="color:#f92672">(</span>S<span style="color:#f92672">)</span>  AGE
trivy-scanner   ClusterIP   10.43.179.39   &lt;none&gt;        9115/TCP   15m

curl -s http://10.43.179.39:9115/metrics | grep so_vulnerabilities
</code></pre></div><p>Now you need to add the <code>trivy-scanner</code> <code>Service</code> as target for your prometheus. I created a <code>ServiceMonitor</code> object for that:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">monitoring.coreos.com/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ServiceMonitor</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">labels</span>:
    <span style="color:#f92672">serviceapp</span>: <span style="color:#ae81ff">trivy-exporter-servicemonitor</span>
    <span style="color:#f92672">release</span>: <span style="color:#ae81ff">prometheus</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">trivy-exporter-servicemonitor</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">selector</span>:
    <span style="color:#f92672">matchLabels</span>:
      <span style="color:#f92672">app</span>: <span style="color:#ae81ff">trivy-scanner</span>
  <span style="color:#f92672">endpoints</span>:
  - <span style="color:#f92672">port</span>: <span style="color:#ae81ff">metrics</span>
</code></pre></div><p>If you use my grafana dasgboard from the repo you can see someting like this:</p>
<p><img src="/img/include/trivy-exporter.png" alt="image"  class="zoomable" /> <!-- raw HTML omitted --></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[K8S Logging And Monitoring]]></title>
            <link href="https://devopstales.github.io/home/k8s-prometheus-stack/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-prometheus-stack/?utm_source=atom_feed" rel="related" type="text/html" title="K8S Logging And Monitoring" />
                <link href="https://devopstales.github.io/home/continuous-image-security/?utm_source=atom_feed" rel="related" type="text/html" title="Continuous Image Security" />
                <link href="https://devopstales.github.io/home/k8s-vault-v2/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes integration with external Vault" />
                <link href="https://devopstales.github.io/home/rke2-calico/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Install With Calico" />
                <link href="https://devopstales.github.io/home/rke2-cilium/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Install With cilium" />
            
                <id>https://devopstales.github.io/home/k8s-prometheus-stack/</id>
            
            
            <published>2021-06-15T00:00:00+00:00</published>
            <updated>2021-06-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this tutorial I will show you how to install a prometheus operator to monotor kubernetes and loki to gether logs.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<p>Prometheus is an open-source monitoring system with a built-in noSQL time-series database. It offers a multi-dimensional data model, a flexible query language, and diverse visualization possibilities. Prometheus collects metrics from http nedpoint. Most service dind&rsquo;t have this endpoint so you need optional programs that generate additional metrics cald exporters.</p>
<h3 id="monitoring">Monitoring</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano values.yaml</span>
---
<span style="color:#f92672">global</span>:
  <span style="color:#f92672">rbac</span>:
    <span style="color:#f92672">create</span>: <span style="color:#66d9ef">true</span>
    <span style="color:#f92672">pspEnabled</span>: <span style="color:#66d9ef">true</span>

<span style="color:#f92672">alertmanager</span>:
  <span style="color:#f92672">alertmanagerSpec</span>:
    <span style="color:#f92672">storage</span>:
      <span style="color:#f92672">volumeClaimTemplate</span>:
        <span style="color:#f92672">spec</span>:
          <span style="color:#f92672">resources</span>:
            <span style="color:#f92672">requests</span>:
              <span style="color:#f92672">storage</span>: <span style="color:#ae81ff">10Gi</span>
  <span style="color:#f92672">ingress</span>:
    <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">true</span>
    <span style="color:#f92672">annotations</span>:
      <span style="color:#f92672">cert-manager.io/cluster-issuer</span>: <span style="color:#ae81ff">ca-issuer</span>
    <span style="color:#f92672">hosts</span>:
      - <span style="color:#ae81ff">alertmanager.k8s.intra</span>
    <span style="color:#f92672">paths</span>:
    - <span style="color:#ae81ff">/</span>
    <span style="color:#f92672">pathType</span>: <span style="color:#ae81ff">ImplementationSpecific</span>
    <span style="color:#f92672">tls</span>:
    - <span style="color:#f92672">secretName</span>: <span style="color:#ae81ff">tls-alertmanager-cert</span>
      <span style="color:#f92672">hosts</span>:
      - <span style="color:#ae81ff">alertmanager.k8s.intra</span>

<span style="color:#f92672">grafana</span>:
  <span style="color:#f92672">rbac</span>:
    <span style="color:#f92672">enable</span>: <span style="color:#66d9ef">true</span>
    <span style="color:#f92672">pspEnabled</span>: <span style="color:#66d9ef">true</span>
    <span style="color:#f92672">pspUseAppArmor</span>: <span style="color:#66d9ef">false</span>
  <span style="color:#f92672">initChownData</span>:
    <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">false</span>
  <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">true</span>
  <span style="color:#f92672">adminPassword</span>: <span style="color:#ae81ff">Password1</span>
  <span style="color:#f92672">plugins</span>:
  - <span style="color:#ae81ff">grafana-piechart-panel</span>
  <span style="color:#f92672">persistence</span>:
    <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">true</span>
    <span style="color:#f92672">size</span>: <span style="color:#ae81ff">10Gi</span>
  <span style="color:#f92672">ingress</span>:
    <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">true</span>
    <span style="color:#f92672">annotations</span>:
      <span style="color:#f92672">cert-manager.io/cluster-issuer</span>: <span style="color:#ae81ff">ca-issuer</span>
    <span style="color:#f92672">hosts</span>:
      - <span style="color:#ae81ff">grafana.k8s.intra</span>
    <span style="color:#f92672">paths</span>:
    - <span style="color:#ae81ff">/</span>
    <span style="color:#f92672">pathType</span>: <span style="color:#ae81ff">ImplementationSpecific</span>
    <span style="color:#f92672">tls</span>:
    - <span style="color:#f92672">secretName</span>: <span style="color:#ae81ff">tls-grafana-cert</span>
      <span style="color:#f92672">hosts</span>:
      - <span style="color:#ae81ff">grafana.k8s.intra</span>


<span style="color:#f92672">prometheus</span>:
  <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">true</span>
  <span style="color:#f92672">prometheusSpec</span>:
    <span style="color:#f92672">podMonitorSelectorNilUsesHelmValues</span>: <span style="color:#66d9ef">false</span>
    <span style="color:#f92672">serviceMonitorSelectorNilUsesHelmValues</span>: <span style="color:#66d9ef">false</span>
    <span style="color:#f92672">secrets</span>: [<span style="color:#e6db74">&#39;etcd-client-cert&#39;</span>]
    <span style="color:#f92672">storageSpec</span>:
      <span style="color:#f92672">volumeClaimTemplate</span>:
        <span style="color:#f92672">spec</span>:
          <span style="color:#f92672">resources</span>:
            <span style="color:#f92672">requests</span>:
              <span style="color:#f92672">storage</span>: <span style="color:#ae81ff">10Gi</span>
  <span style="color:#f92672">ingress</span>:
    <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">true</span>
    <span style="color:#f92672">annotations</span>:
      <span style="color:#f92672">cert-manager.io/cluster-issuer</span>: <span style="color:#ae81ff">ca-issuer</span>
    <span style="color:#f92672">hosts</span>:
      - <span style="color:#ae81ff">prometheus.k8s.intra</span>
    <span style="color:#f92672">paths</span>:
    - <span style="color:#ae81ff">/</span>
    <span style="color:#f92672">pathType</span>: <span style="color:#ae81ff">ImplementationSpecific</span>
    <span style="color:#f92672">tls</span>:
    - <span style="color:#f92672">secretName</span>: <span style="color:#ae81ff">tls-prometheus-cert</span>
      <span style="color:#f92672">hosts</span>:
      - <span style="color:#ae81ff">prometheus.k8s.intra</span>
</code></pre></div><p>There is a bug in the Grafana helm chart so it didn&rsquo;t sreate the psp correcly for the init container: <a href="https://github.com/grafana/helm-charts/issues/427">https://github.com/grafana/helm-charts/issues/427</a></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># solution</span>
kubectl edit psp prometheus-grafana
...
  runAsUser:
    rule: RunAsAny
...

kubectl get rs
NAME                                             DESIRED   CURRENT   READY   AGE
prometheus-grafana-74b5d957bc                    <span style="color:#ae81ff">1</span>         <span style="color:#ae81ff">0</span>         <span style="color:#ae81ff">0</span>       12m
...

kubectl delete rs prometheus-grafana-74b5d957bc
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e">#### grafana dashboards</span>
<span style="color:#75715e">## RKE2</span>
<span style="color:#75715e"># 14243</span>
<span style="color:#75715e">## NGINX Ingress controller</span>
<span style="color:#75715e"># 9614</span>
<span style="color:#75715e">## cert-manager</span>
<span style="color:#75715e"># 11001</span>
<span style="color:#75715e">## longhorn</span>
<span style="color:#75715e"># 13032</span>
<span style="color:#75715e">### kyverno</span>
<span style="color:#75715e"># https://raw.githubusercontent.com/kyverno/grafana-dashboard/master/grafana/dashboard.json</span>
<span style="color:#75715e">### calico</span>
<span style="color:#75715e"># 12175</span>
<span style="color:#75715e"># 3244</span>
<span style="color:#75715e">### cilium</span>
<span style="color:#75715e"># 6658</span>
<span style="color:#75715e"># 14500</span>
<span style="color:#75715e"># 14502</span>
<span style="color:#75715e"># 14501</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">helm repo add prometheus-community https://prometheus-community.github.io/helm-charts

helm install prometheus prometheus-community/kube-prometheus-stack -n monitoring -f values.yaml
</code></pre></div><p>For the proxy down status:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl edit cm/kube-proxy -n kube-system
...
kind: KubeProxyConfiguration
metricsBindAddress: 0.0.0.0:10249
...
</code></pre></div><p>If you use rke2 you can configure this from the helm chart before first start:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat <span style="color:#e6db74">&lt;&lt; EOF &gt; /var/lib/rancher/rke2/server/manifests/rke2-kube-proxy-config.yaml
</span><span style="color:#e6db74">apiVersion: helm.cattle.io/v1
</span><span style="color:#e6db74">kind: HelmChartConfig
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: rke2-kube-proxy
</span><span style="color:#e6db74">  namespace: kube-system
</span><span style="color:#e6db74">spec:
</span><span style="color:#e6db74">  valuesContent: |-
</span><span style="color:#e6db74">    metricsBindAddress: 0.0.0.0:10249
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><p>For the controller-manager down status:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano /etc/kubernetes/manifests/kube-controller-manager.yaml
<span style="color:#75715e"># OR</span>
nano /var/lib/rancher/rke2/agent/pod-manifests/kube-controller-manager.yaml
apiVersion: v1
kind: Pod
metadata:
  ...
spec:
  containers:
  - command:
    - kube-controller-manager
    ...
    - --address<span style="color:#f92672">=</span>0.0.0.0
    ...
    - --bind-address<span style="color:#f92672">=</span>&lt;your control-plane IP or 0.0.0.0&gt;
    ...
    livenessProbe:
      failureThreshold: <span style="color:#ae81ff">8</span>
      httpGet:
       	host: 0.0.0.0
    ...
</code></pre></div><p>For the kube-scheduler down status:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano /etc/kubernetes/manifests/kube-scheduler.yaml
<span style="color:#75715e"># OR</span>
nano /var/lib/rancher/rke2/agent/pod-manifests/kube-scheduler.yaml
apiVersion: v1
kind: Pod
metadata:
  ...
spec:
  containers:
  - command:
    - kube-scheduler
    - --address<span style="color:#f92672">=</span>0.0.0.0
    - --bind-address<span style="color:#f92672">=</span>0.0.0.0
    ...
    livenessProbe:
      failureThreshold: <span style="color:#ae81ff">8</span>
      httpGet:
       	host: 0.0.0.0
    ...
</code></pre></div><p>For the etcd down status firs we need to create a secret to authenticate for the etcd:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># kubeadm</span>
kubectl -n monitoring create secret generic etcd-client-cert <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--from-file<span style="color:#f92672">=</span>/etc/kubernetes/pki/etcd/ca.crt <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--from-file<span style="color:#f92672">=</span>/etc/kubernetes/pki/etcd/healthcheck-client.crt <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--from-file<span style="color:#f92672">=</span>/etc/kubernetes/pki/etcd/healthcheck-client.key

<span style="color:#75715e"># rancher</span>
kubectl -n monitoring create secret generic etcd-client-cert <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--from-file<span style="color:#f92672">=</span>/var/lib/rancher/rke2/server/tls/etcd/server-ca.crt <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--from-file<span style="color:#f92672">=</span>/var/lib/rancher/rke2/server/tls/etcd/server-client.crt <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--from-file<span style="color:#f92672">=</span>/var/lib/rancher/rke2/server/tls/etcd/server-client.key
</code></pre></div><p>Then we configure the prometheus to use it:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano values.yaml</span>
---
...
<span style="color:#f92672">prometheus</span>:
  <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">true</span>
  <span style="color:#f92672">prometheusSpec</span>:
    <span style="color:#f92672">secrets</span>: [<span style="color:#e6db74">&#39;etcd-client-cert&#39;</span>]
...
<span style="color:#f92672">kubeEtcd</span>:
  <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">true</span>
  <span style="color:#f92672">service</span>:
    <span style="color:#f92672">port</span>: <span style="color:#ae81ff">2379</span>
    <span style="color:#f92672">targetPort</span>: <span style="color:#ae81ff">2379</span>
    <span style="color:#f92672">selector</span>:
      <span style="color:#f92672">component</span>: <span style="color:#ae81ff">etcd</span>
  <span style="color:#f92672">serviceMonitor</span>:
    <span style="color:#f92672">interval</span>: <span style="color:#e6db74">&#34;&#34;</span>
    <span style="color:#f92672">scheme</span>: <span style="color:#ae81ff">https</span>
    <span style="color:#f92672">insecureSkipVerify</span>: <span style="color:#66d9ef">true</span>
    <span style="color:#f92672">serverName</span>: <span style="color:#e6db74">&#34;&#34;</span>
    <span style="color:#f92672">metricRelabelings</span>: []
    <span style="color:#f92672">relabelings</span>: []
    <span style="color:#f92672">caFile</span>: <span style="color:#ae81ff">/etc/prometheus/secrets/etcd-client-cert/server-ca.crt</span>
    <span style="color:#f92672">certFile</span>: <span style="color:#ae81ff">/etc/prometheus/secrets/etcd-client-cert/server-client.crt</span>
    <span style="color:#f92672">keyFile</span>: <span style="color:#ae81ff">/etc/prometheus/secrets/etcd-client-cert/server-client.key</span>

<span style="color:#75715e"># for kubeadm</span>
<span style="color:#75715e">#    caFile: /etc/prometheus/secrets/etcd-client-cert/ca.crt</span>
<span style="color:#75715e">#    certFile: /etc/prometheus/secrets/etcd-client-cert/healthcheck-client.crt</span>
<span style="color:#75715e">#    keyFile: /etc/prometheus/secrets/etcd-client-cert/healthcheck-client.key</span>

</code></pre></div><h3 id="monitoring-nginx">Monitoring Nginx</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat &lt;&lt; EOF &gt; /var/lib/rancher/rke2/server/manifests/rke2-ingress-nginx-config.yaml</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">helm.cattle.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">HelmChartConfig</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">rke2-ingress-nginx</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">kube-system</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">valuesContent</span>: |-<span style="color:#e6db74">
</span><span style="color:#e6db74">    controller:
</span><span style="color:#e6db74">      metrics:</span>    
	      <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">true</span>
        <span style="color:#f92672">service</span>:
          <span style="color:#f92672">annotations</span>:
            <span style="color:#f92672">prometheus.io/scrape</span>: <span style="color:#e6db74">&#34;true&#34;</span>
            <span style="color:#f92672">prometheus.io/port</span>: <span style="color:#e6db74">&#34;10254&#34;</span>
        <span style="color:#f92672">serviceMonitor</span>:
          <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">true</span>
          <span style="color:#f92672">namespace</span>: <span style="color:#e6db74">&#34;monitoring&#34;</span>
<span style="color:#ae81ff">EOF</span>

<span style="color:#ae81ff">kaf /var/lib/rancher/rke2/server/manifests/rke2-ingress-nginx-config.yaml</span>
</code></pre></div><h3 id="monitoring-core-dns">Monitoring Core-DNS</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat &lt;&lt; EOF &gt; default-network-dns-policy.yaml</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">networking.k8s.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">NetworkPolicy</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">default-network-dns-policy</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">kube-system</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">ingress</span>:
  - <span style="color:#f92672">ports</span>:
    - <span style="color:#f92672">port</span>: <span style="color:#ae81ff">53</span>
      <span style="color:#f92672">protocol</span>: <span style="color:#ae81ff">TCP</span>
    - <span style="color:#f92672">port</span>: <span style="color:#ae81ff">53</span>
      <span style="color:#f92672">protocol</span>: <span style="color:#ae81ff">UDP</span>
    - <span style="color:#f92672">port</span>: <span style="color:#ae81ff">9153</span>
      <span style="color:#f92672">protocol</span>: <span style="color:#ae81ff">TCP</span>
  <span style="color:#f92672">podSelector</span>:
    <span style="color:#f92672">matchLabels</span>:
      <span style="color:#f92672">k8s-app</span>: <span style="color:#ae81ff">kube-dns</span>
  <span style="color:#f92672">policyTypes</span>:
  - <span style="color:#ae81ff">Ingress</span>
<span style="color:#ae81ff">EOF</span>

<span style="color:#ae81ff">kaf default-network-dns-policy.yaml</span>
</code></pre></div><h3 id="monitor-cert-manager">Monitor cert-manager</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano 01-cert-managger.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Namespace</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">ingress-system</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">helm.cattle.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">HelmChart</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cert-manager</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">ingress-system</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">repo</span>: <span style="color:#e6db74">&#34;https://charts.jetstack.io&#34;</span>
  <span style="color:#f92672">chart</span>: <span style="color:#ae81ff">cert-manager</span>
  <span style="color:#f92672">targetNamespace</span>: <span style="color:#ae81ff">ingress-system</span>
  <span style="color:#f92672">valuesContent</span>: |-<span style="color:#e6db74">
</span><span style="color:#e6db74">    installCRDs: true
</span><span style="color:#e6db74">    clusterResourceNamespace: &#34;ingress-system&#34;
</span><span style="color:#e6db74">    prometheus:
</span><span style="color:#e6db74">      enabled: true
</span><span style="color:#e6db74">      servicemonitor:
</span><span style="color:#e6db74">        enabled: true
</span><span style="color:#e6db74">        namespace: &#34;monitoring&#34;</span>    

<span style="color:#ae81ff">kubectl apply -f 01-cert-managger.yaml</span>
</code></pre></div><h3 id="longhorn">Longhorn</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">monitoring.coreos.com/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ServiceMonitor</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">longhorn-prometheus-servicemonitor</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">monitoring</span>
  <span style="color:#f92672">labels</span>:
    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">longhorn-prometheus-servicemonitor</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">selector</span>:
    <span style="color:#f92672">matchLabels</span>:
      <span style="color:#f92672">app</span>: <span style="color:#ae81ff">longhorn-manager</span>
  <span style="color:#f92672">namespaceSelector</span>:
    <span style="color:#f92672">matchNames</span>:
    - <span style="color:#ae81ff">longhorn-system</span>
  <span style="color:#f92672">endpoints</span>:
  - <span style="color:#f92672">port</span>: <span style="color:#ae81ff">manager</span>
</code></pre></div><h3 id="logging">Logging</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">helm repo add grafana https://grafana.github.io/helm-charts
 
helm repo update
 
helm search repo loki
 
helm upgrade --install loki-stack grafana/loki-stack <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--create-namespace <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--namespace loki-stack <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set promtail.enabled<span style="color:#f92672">=</span>true,loki.persistence.enabled<span style="color:#f92672">=</span>true,loki.persistence.size<span style="color:#f92672">=</span>10Gi
</code></pre></div><p>Promtail dose not working with enabled <code>selinux</code>, because this promtail deployment store som files on the host filesystem and <code>selinux</code> dose not allow to write it, so you need ti use fluent-bit.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">helm upgrade --install loki-stack grafana/loki-stack <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--create-namespace <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--namespace loki-stack <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set fluent-bit.enabled<span style="color:#f92672">=</span>true,promtail.enabled<span style="color:#f92672">=</span>false <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--set loki.persistence.enabled<span style="color:#f92672">=</span>true,loki.persistence.size<span style="color:#f92672">=</span>10Gi

</code></pre></div><p>Add datasource to grafana:</p>
<pre tabindex="0"><code>type: loki
name: Loki
url: http://loki-stack.loki-stack:3100
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes integration with external Vault]]></title>
            <link href="https://devopstales.github.io/home/k8s-vault-v2/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-vault/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes and Vault integration" />
                <link href="https://devopstales.github.io/kubernetes/k8s-vault-v2/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes integration with external Vault" />
                <link href="https://devopstales.github.io/home/rke2-calico/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Install With Calico" />
                <link href="https://devopstales.github.io/home/rke2-cilium/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Install With cilium" />
                <link href="https://devopstales.github.io/home/k3s-gvisor/?utm_source=atom_feed" rel="related" type="text/html" title="Secure k3s with gVisor" />
            
                <id>https://devopstales.github.io/home/k8s-vault-v2/</id>
            
            
            <published>2021-06-05T00:00:00+00:00</published>
            <updated>2021-06-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can integrate an external HashiCorp Vault to Kubernetes.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<h3 id="vhat-is-hashicorp-vault">Vhat is Hashicorp Vault</h3>
<p>HashiCorp Vault is a secrets management solution that brokers access for both humans and machines, through programmatic access, to systems. Secrets can be stored, dynamically generated, and in the case of encryption, keys can be consumed as a service without the need to expose the underlying key materials.</p>
<p><img src="/img/include/vault-k8s-auth-workflow.png" alt="Example image"  class="zoomable" /></p>
<h3 id="k3s-install">K3s install</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ssh-copy-id vagrant@172.17.8.101

k3sup install <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --ip<span style="color:#f92672">=</span>172.17.8.101 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --user<span style="color:#f92672">=</span>vagrant <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --sudo <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --tls-san<span style="color:#f92672">=</span>172.17.8.100 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --cluster <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-channel<span style="color:#f92672">=</span>stable <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-extra-args <span style="color:#e6db74">&#34;--no-deploy=traefik --flannel-iface=enp0s8 --node-ip=172.17.8.101&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --merge <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --local-path $HOME/.kube/config <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --context<span style="color:#f92672">=</span>k3s-ha
</code></pre></div><h3 id="install-vault">Install vault</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo dnf install -y dnf-plugins-core nano jq

cp /etc/rancher/k3s/k3s.yaml ~/.kube/config

sudo dnf config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo
sudo dnf install -y vault
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano /etc/vault.d/vault.hcl
<span style="color:#75715e"># HTTP listener</span>
listener <span style="color:#e6db74">&#34;tcp&#34;</span> <span style="color:#f92672">{</span>
  address <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;0.0.0.0:8200&#34;</span>
  tls_disable <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
<span style="color:#f92672">}</span>

<span style="color:#75715e"># HTTPS listener</span>
<span style="color:#75715e">#listener &#34;tcp&#34; {</span>
<span style="color:#75715e">#  address       = &#34;0.0.0.0:8200&#34;</span>
<span style="color:#75715e">#  tls_cert_file = &#34;/opt/vault/tls/tls.crt&#34;</span>
<span style="color:#75715e">#  tls_key_file  = &#34;/opt/vault/tls/tls.key&#34;</span>
<span style="color:#75715e">#}</span>

api_addr         <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;http://0.0.0.0:8200&#34;</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">systemctl start vault
systemctl enable vault

vault -autocomplete-install
complete -C /usr/bin/vault vault

export VAULT_ADDR<span style="color:#f92672">=</span>http://127.0.0.1:8200
echo <span style="color:#e6db74">&#34;export VAULT_ADDR=http://127.0.0.1:8200&#34;</span> &gt;&gt; ~/.bashrc

vault status

vault operator init | tee /opt/vault/init.txt
Unseal Key 1: t4PsGsw8cj25l9tSpvh2Avr5647HhdaI27aAzSiYJz0<span style="color:#f92672">=</span>

Initial Root Token: s.sPKauYvv9iFKliclTIaMgbU1
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">export VAULT_TOKEN<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;s.sPKauYvv9iFKliclTIaMgbU1&#34;</span>

vault operator unseal t4PsGsw8cj25l9tSpvh2Avr5647HhdaI27aAzSiYJz0<span style="color:#f92672">=</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">vault auth enable userpass
vault write auth/userpass/users/devopstales <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    password<span style="color:#f92672">=</span>Password1 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    policies<span style="color:#f92672">=</span>admins
</code></pre></div><h3 id="integrate-a-kubernetes-cluster-with-an-external-vault">Integrate a Kubernetes Cluster with an External Vault</h3>
<p><a href="https://3p8owy1gdkoh452nrc36wbnp-wpengine.netdna-ssl.com/wp-content/uploads/2018/12/nirmata-vault-7-1024x623.png">https://3p8owy1gdkoh452nrc36wbnp-wpengine.netdna-ssl.com/wp-content/uploads/2018/12/nirmata-vault-7-1024x623.png</a></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">vault secrets enable kv

vault kv put kv/secret/devwebapp/config username<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;giraffe&#39;</span> password<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;salsa&#39;</span>

vault kv get -format<span style="color:#f92672">=</span>json kv/secret/devwebapp/config | jq <span style="color:#e6db74">&#34;.data&#34;</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># create policy</span>
vault policy write devwebapp-kv-ro - <span style="color:#e6db74">&lt;&lt;EOF
</span><span style="color:#e6db74">path &#34;kv/secret/devwebapp/*&#34; {
</span><span style="color:#e6db74">    capabilities = [&#34;read&#34;, &#34;list&#34;]
</span><span style="color:#e6db74">}
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># create Kubernetes ServiceAccount</span>
cat &gt; internal-app.yaml <span style="color:#e6db74">&lt;&lt;EOF
</span><span style="color:#e6db74">apiVersion: v1
</span><span style="color:#e6db74">kind: ServiceAccount
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: devwebapp
</span><span style="color:#e6db74">---
</span><span style="color:#e6db74">  apiVersion: rbac.authorization.k8s.io/v1
</span><span style="color:#e6db74">  kind: ClusterRoleBinding
</span><span style="color:#e6db74">  metadata:
</span><span style="color:#e6db74">    name: role-tokenreview-binding
</span><span style="color:#e6db74">    namespace: default
</span><span style="color:#e6db74">  roleRef:
</span><span style="color:#e6db74">    apiGroup: rbac.authorization.k8s.io
</span><span style="color:#e6db74">    kind: ClusterRole
</span><span style="color:#e6db74">    name: system:auth-delegator
</span><span style="color:#e6db74">  subjects:
</span><span style="color:#e6db74">  - kind: ServiceAccount
</span><span style="color:#e6db74">    name: devwebapp
</span><span style="color:#e6db74">    namespace: default
</span><span style="color:#e6db74">EOF</span>

kubectl apply --filename internal-app.yaml
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">export EXTERNAL_VAULT_ADDR<span style="color:#f92672">=</span>172.17.8.101
export K8S_HOST<span style="color:#f92672">=</span>172.17.8.101

export VAULT_SA_NAME<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>kubectl get sa devwebapp <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    -o jsonpath<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;{.secrets[*][&#39;name&#39;]}&#34;</span><span style="color:#66d9ef">)</span>

export SA_JWT_TOKEN<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>kubectl get secret $VAULT_SA_NAME <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    -o jsonpath<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;{.data.token}&#34;</span> | base64 --decode; echo<span style="color:#66d9ef">)</span>

export SA_CA_CRT<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>kubectl get secret $VAULT_SA_NAME <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    -o jsonpath<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;{.data[&#39;ca\.crt&#39;]}&#34;</span> | base64 --decode; echo<span style="color:#66d9ef">)</span>

vault auth enable kubernetes

vault write auth/kubernetes/config <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  issuer<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;https://kubernetes.default.svc.cluster.local&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  token_reviewer_jwt<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;</span>$SA_JWT_TOKEN<span style="color:#e6db74">&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  kubernetes_host<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;https://</span>$K8S_HOST<span style="color:#e6db74">:6443&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  kubernetes_ca_cert<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;</span>$SA_CA_CRT<span style="color:#e6db74">&#34;</span>

vault write auth/kubernetes/role/devwebapp <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>        bound_service_account_names<span style="color:#f92672">=</span>devwebapp <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>        bound_service_account_namespaces<span style="color:#f92672">=</span>default <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>        policies<span style="color:#f92672">=</span>devwebapp-kv-ro <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>        ttl<span style="color:#f92672">=</span>24h
</code></pre></div><h3 id="install-vault-agent-injector">INstall Vault Agent Injector</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">dnf copr enable cerenit/helm -y
dnf install helm -y

helm repo add hashicorp https://helm.releases.hashicorp.com
helm repo update

<span style="color:#75715e">### változók???</span>
helm install vault hashicorp/vault <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --set <span style="color:#e6db74">&#34;injector.externalVaultAddr=http://</span>$EXTERNAL_VAULT_ADDR<span style="color:#e6db74">:8200&#34;</span>
</code></pre></div><h3 id="demo">Demo</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat &gt; devwebapp.yaml <span style="color:#e6db74">&lt;&lt;EOF
</span><span style="color:#e6db74">---
</span><span style="color:#e6db74">apiVersion: apps/v1
</span><span style="color:#e6db74">kind: Deployment
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: orgchart
</span><span style="color:#e6db74">  labels:
</span><span style="color:#e6db74">    app: orgchart
</span><span style="color:#e6db74">spec:
</span><span style="color:#e6db74">  selector:
</span><span style="color:#e6db74">    matchLabels:
</span><span style="color:#e6db74">      app: orgchart
</span><span style="color:#e6db74">  replicas: 1
</span><span style="color:#e6db74">  template:
</span><span style="color:#e6db74">    metadata:
</span><span style="color:#e6db74">      annotations:
</span><span style="color:#e6db74">        vault.hashicorp.com/agent-inject: &#34;true&#34;
</span><span style="color:#e6db74">        vault.hashicorp.com/role: &#34;devwebapp&#34;
</span><span style="color:#e6db74">        vault.hashicorp.com/agent-inject-secret-config.txt: &#34;kv/secret/devwebapp/config&#34;
</span><span style="color:#e6db74">      labels:
</span><span style="color:#e6db74">        app: orgchart
</span><span style="color:#e6db74">    spec:
</span><span style="color:#e6db74">      serviceAccountName: devwebapp
</span><span style="color:#e6db74">      containers:
</span><span style="color:#e6db74">        - name: orgchart
</span><span style="color:#e6db74">          image: jweissig/app:0.0.1
</span><span style="color:#e6db74">EOF</span>

kubectl apply -f devwebapp.yaml

kubectl exec <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    <span style="color:#66d9ef">$(</span>kubectl get pod -l app<span style="color:#f92672">=</span>orgchart -o jsonpath<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;{.items[0].metadata.name}&#34;</span><span style="color:#66d9ef">)</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --container orgchart -- cat /vault/secrets/config.txt
</code></pre></div><hr>
<ul>
<li><a href="https://tansanrao.com/hashicorp-vault-sidecar/">https://tansanrao.com/hashicorp-vault-sidecar/</a></li>
<li><a href="https://learn.hashicorp.com/tutorials/vault/kubernetes-external-vault?in=vault/kubernetes">https://learn.hashicorp.com/tutorials/vault/kubernetes-external-vault?in=vault/kubernetes</a></li>
<li><a href="https://learn.hashicorp.com/tutorials/vault/kubernetes-secret-store-driver?in=vault/kubernetes">https://learn.hashicorp.com/tutorials/vault/kubernetes-secret-store-driver?in=vault/kubernetes</a></li>
<li><a href="https://www.vaultproject.io/docs/platform/k8s/injector/examples#environment-variable-example">https://www.vaultproject.io/docs/platform/k8s/injector/examples#environment-variable-example</a>
<ul>
<li>mount to /etc/profile.d/secret in export format</li>
</ul>
</li>
</ul>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[RKE2 Install With Calico]]></title>
            <link href="https://devopstales.github.io/home/rke2-calico/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/rke2-cilium/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Install With cilium" />
                <link href="https://devopstales.github.io/kubernetes/rke2-calico/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Install With Calico" />
                <link href="https://devopstales.github.io/home/k8s-vault/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes and Vault integration" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller-v2/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V2" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller" />
            
                <id>https://devopstales.github.io/home/rke2-calico/</id>
            
            
            <published>2021-05-25T00:00:00+00:00</published>
            <updated>2021-05-25T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can install a RKE2 in with Calico&rsquo;s encripted VXLAN.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<h2 id="rke2-setup">RKE2 Setup</h2>
<h3 id="project-longhorn-prerequisites">Project Longhorn Prerequisites</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">yum install -y epel-release
yum install -y nano curl wget git tmux jq
yum install -y iscsi-initiator-utils 
modprobe iscsi_tcp
echo <span style="color:#e6db74">&#34;iscsi_tcp&#34;</span> &gt;/etc/modules-load.d/iscsi-tcp.conf
systemctl enable iscsid
systemctl start iscsid 
</code></pre></div><p>Ensure the eBFP filesystem is mounted (which should already be the case on RHEL 8.3):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">mount | grep /sys/fs/bpf
<span style="color:#75715e"># if present should output, e.g. &#34;none on /sys/fs/bpf type bpf&#34;...</span>
</code></pre></div><p>If that&rsquo;s not the case, mount it using the commands down here:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo mount bpffs -t bpf /sys/fs/bpf
sudo bash -c <span style="color:#e6db74">&#39;cat &lt;&lt;EOF &gt;&gt; /etc/fstab
</span><span style="color:#e6db74">none /sys/fs/bpf bpf rw,relatime 0 0
</span><span style="color:#e6db74">EOF&#39;</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat <span style="color:#e6db74">&lt;&lt;EOF&gt;&gt; /etc/NetworkManager/conf.d/rke2-canal.conf
</span><span style="color:#e6db74">[keyfile]
</span><span style="color:#e6db74">unmanaged-devices=interface-name:cali*;interface-name:flannel*
</span><span style="color:#e6db74">EOF</span>
systemctl reload NetworkManager
</code></pre></div><h3 id="rke2-rpm-install">RKE2 rpm Install</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat <span style="color:#e6db74">&lt;&lt; EOF &gt; /etc/yum.repos.d/rancher-rke2-1-20-latest.repo
</span><span style="color:#e6db74">[rancher-rke2-common-latest]
</span><span style="color:#e6db74">name=Rancher RKE2 Common Latest
</span><span style="color:#e6db74">baseurl=https://rpm.rancher.io/rke2/latest/common/centos/8/noarch
</span><span style="color:#e6db74">enabled=1
</span><span style="color:#e6db74">gpgcheck=1
</span><span style="color:#e6db74">gpgkey=https://rpm.rancher.io/public.key
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">[rancher-rke2-1-20-latest]
</span><span style="color:#e6db74">name=Rancher RKE2 1.20 Latest
</span><span style="color:#e6db74">baseurl=https://rpm.rancher.io/rke2/latest/1.20/centos/8/x86_64
</span><span style="color:#e6db74">enabled=1
</span><span style="color:#e6db74">gpgcheck=1
</span><span style="color:#e6db74">gpgkey=https://rpm.rancher.io/public.key
</span><span style="color:#e6db74">EOF</span>

yum -y install rke2-server
</code></pre></div><h3 id="kubectl-helm--rke2">Kubectl, Helm &amp; RKE2</h3>
<p>Install <code>kubectl</code>, <code>helm</code> and RKE2 to the host system:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx
sudo ln -s /opt/kubectx/kubens /usr/local/bin/kubens
echo <span style="color:#e6db74">&#39;PATH=$PATH:/usr/local/bin&#39;</span> &gt;&gt; /etc/profile
echo <span style="color:#e6db74">&#39;PATH=$PATH:/var/lib/rancher/rke2/bin&#39;</span> &gt;&gt; /etc/profile
source /etc/profile

sudo dnf copr -y enable cerenit/helm
sudo dnf install -y helm
</code></pre></div><h3 id="rke2-specific-ports">RKE2 specific ports</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo firewall-cmd --add-port<span style="color:#f92672">=</span>9345/tcp --permanent
sudo firewall-cmd --add-port<span style="color:#f92672">=</span>6443/tcp --permanent
sudo firewall-cmd --add-port<span style="color:#f92672">=</span>10250Air-Gap/tcp --permanent
sudo firewall-cmd --add-port<span style="color:#f92672">=</span>2379/tcp --permanent
sudo firewall-cmd --add-port<span style="color:#f92672">=</span>2380/tcp --permanent
sudo firewall-cmd --add-port<span style="color:#f92672">=</span>30000-32767/tcp --permanent
<span style="color:#75715e"># Used for the Rancher Monitoring</span>
sudo firewall-cmd --add-port<span style="color:#f92672">=</span>9796/tcp --permanent
sudo firewall-cmd --add-port<span style="color:#f92672">=</span>19090/tcp --permanent
sudo firewall-cmd --add-port<span style="color:#f92672">=</span>6942/tcp --permanent
sudo firewall-cmd --add-port<span style="color:#f92672">=</span>9091/tcp --permanent
<span style="color:#75715e">### CNI specific ports</span>
<span style="color:#75715e"># 4244/TCP is required when the Hubble Relay is enabled and therefore needs to connect to all agents to collect the flows</span>
sudo firewall-cmd --add-port<span style="color:#f92672">=</span>4244/tcp --permanent
<span style="color:#75715e"># Cilium healthcheck related permits:</span>
sudo firewall-cmd --add-port<span style="color:#f92672">=</span>4240/tcp --permanent
sudo firewall-cmd --remove-icmp-block<span style="color:#f92672">=</span>echo-request --permanent
sudo firewall-cmd --remove-icmp-block<span style="color:#f92672">=</span>echo-reply --permanent
<span style="color:#75715e"># Since we are using Cilium with GENEVE as overlay, we need the following port too:</span>
sudo firewall-cmd --add-port<span style="color:#f92672">=</span>6081/udp --permanent
<span style="color:#75715e">### Ingress Controller specific ports</span>
sudo firewall-cmd --add-port<span style="color:#f92672">=</span>80/tcp --permanent
sudo firewall-cmd --add-port<span style="color:#f92672">=</span>443/tcp --permanent
<span style="color:#75715e">### To get DNS resolution working, simply enable Masquerading.</span>
sudo firewall-cmd --zone<span style="color:#f92672">=</span>public  --add-masquerade --permanent

<span style="color:#75715e">### Finally apply all the firewall changes</span>
sudo firewall-cmd --reload
</code></pre></div><p>Verification:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo firewall-cmd --list-all
public <span style="color:#f92672">(</span>active<span style="color:#f92672">)</span>
  target: default
  icmp-block-inversion: no
  interfaces: eno1
  sources: 
  services: cockpit dhcpv6-client ssh wireguard
  ports: 9345/tcp 6443/tcp 10250/tcp 2379/tcp 2380/tcp 30000-32767/tcp 4240/tcp 6081/udp 80/tcp 443/tcp 4244/tcp 9796/tcp 19090/tcp 6942/tcp 9091/tcp
  protocols: 
  masquerade: yes
  forward-ports: 
  source-ports: 
  icmp-blocks: 
  rich rules: 
</code></pre></div><h3 id="basic-configuration">Basic Configuration</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">mkdir -p /etc/rancher/rke2
cat <span style="color:#e6db74">&lt;&lt; EOF &gt;  /etc/rancher/rke2/config.yaml
</span><span style="color:#e6db74">write-kubeconfig-mode: &#34;0644&#34;
</span><span style="color:#e6db74">profile: &#34;cis-1.5&#34;
</span><span style="color:#e6db74">selinux: true
</span><span style="color:#e6db74"># add ips/hostname of hosts and loadbalancer
</span><span style="color:#e6db74">tls-san:
</span><span style="color:#e6db74">  - &#34;k8s.mydomain.intra&#34;
</span><span style="color:#e6db74">  - &#34;172.17.9.10&#34;
</span><span style="color:#e6db74"># Make a etcd snapshot every 6 hours
</span><span style="color:#e6db74">etcd-snapshot-schedule-cron: &#34; */6 * * *&#34;
</span><span style="color:#e6db74"># Keep 56 etcd snapshorts (equals to 2 weeks with 6 a day)
</span><span style="color:#e6db74">etcd-snapshot-retention: 56
</span><span style="color:#e6db74">cni:
</span><span style="color:#e6db74">  - calico
</span><span style="color:#e6db74">disable:
</span><span style="color:#e6db74">  - rke2-canal
</span><span style="color:#e6db74">  - rke2-kube-proxy
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><p><strong>Note:</strong> I disabled <code>rke2-canal</code> and <code>rke2-kube-proxy</code> since I plan to install Canal as CNI in <a href="https://docs.projectcalico.org/maintenance/ebpf/enabling-bpf">&ldquo;kube-proxy less mode&rdquo;</a>. Do not disable <code>rke2-kube-proxy</code> if you use another CNI - it will not work afterwards!</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo cp -f /usr/share/rke2/rke2-cis-sysctl.conf /etc/sysctl.d/60-rke2-cis.conf
sysctl -p /etc/sysctl.d/60-rke2-cis.conf

useradd -r -c <span style="color:#e6db74">&#34;etcd user&#34;</span> -s /sbin/nologin -M etcd

mkdir -p /var/lib/rancher/rke2/server/manifests/
cat <span style="color:#e6db74">&lt;&lt; EOF &gt; /var/lib/rancher/rke2/server/manifests/rke2-ingress-nginx-config.yaml
</span><span style="color:#e6db74">apiVersion: helm.cattle.io/v1
</span><span style="color:#e6db74">kind: HelmChartConfig
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: rke2-ingress-nginx
</span><span style="color:#e6db74">  namespace: kube-system
</span><span style="color:#e6db74">spec:
</span><span style="color:#e6db74">  valuesContent: |-
</span><span style="color:#e6db74">    controller:
</span><span style="color:#e6db74">      metrics:
</span><span style="color:#e6db74">        service:
</span><span style="color:#e6db74">          annotations:
</span><span style="color:#e6db74">            prometheus.io/scrape: &#34;true&#34;
</span><span style="color:#e6db74">            prometheus.io/port: &#34;10254&#34;
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><p>!!!!!!!!!!!!!!!!!!!!!</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get endpoints kubernetes -o wide
NAME         ENDPOINTS        AGE
kubernetes   10.0.2.15:6443   86m
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat &lt;&lt; EOF &gt;  /var/lib/rancher/rke2/server/manifests/rke2-cilium.yaml</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">helm.cattle.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">HelmChartConfig</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">rke2-cilium</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">kube-system</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">valuesContent</span>: |-<span style="color:#e6db74">
</span><span style="color:#e6db74">    cilium:
</span><span style="color:#e6db74">      k8sServiceHost: 10.0.2.15
</span><span style="color:#e6db74">      k8sServicePort: 6443
</span><span style="color:#e6db74">      operator:
</span><span style="color:#e6db74">        replicas: 1
</span><span style="color:#e6db74">      global:
</span><span style="color:#e6db74">        encryption:
</span><span style="color:#e6db74">          enabled: true
</span><span style="color:#e6db74">          nodeEncryption: true
</span><span style="color:#e6db74">      hubble:
</span><span style="color:#e6db74">        metrics:
</span><span style="color:#e6db74">          enabled:
</span><span style="color:#e6db74">          - dns:query;ignoreAAAA
</span><span style="color:#e6db74">          - drop
</span><span style="color:#e6db74">          - tcp
</span><span style="color:#e6db74">          - flow
</span><span style="color:#e6db74">          - icmp
</span><span style="color:#e6db74">          - http
</span><span style="color:#e6db74">        relay:
</span><span style="color:#e6db74">          enabled: true
</span><span style="color:#e6db74">        ui:
</span><span style="color:#e6db74">          enabled: true
</span><span style="color:#e6db74">          replicas: 1
</span><span style="color:#e6db74">          ingress:
</span><span style="color:#e6db74">            enabled: true
</span><span style="color:#e6db74">            hosts:
</span><span style="color:#e6db74">              - hubble.k8s.intra
</span><span style="color:#e6db74">            annotations:
</span><span style="color:#e6db74">              cert-manager.io/cluster-issuer: ca-issuer
</span><span style="color:#e6db74">            tls:
</span><span style="color:#e6db74">            - secretName: ingress-hubble-ui
</span><span style="color:#e6db74">              hosts:
</span><span style="color:#e6db74">              - hubble.k8s.intra
</span><span style="color:#e6db74">      prometheus:
</span><span style="color:#e6db74">        enabled: true
</span><span style="color:#e6db74">        # Default port value (9090) needs to be changed since the RHEL cockpit also listens on this port.
</span><span style="color:#e6db74">        port: 19090
</span><span style="color:#e6db74">        # Configure this serviceMonitor section AFTER Rancher Monitoring is enabled!
</span><span style="color:#e6db74">        #serviceMonitor:
</span><span style="color:#e6db74">        #  enabled: true</span>    
<span style="color:#ae81ff">EOF</span>
</code></pre></div><h3 id="prevent-rke2-package-updates">Prevent RKE2 Package Updates</h3>
<p>In order to provide more stability, I chose to DNF/YUM &ldquo;mark/hold&rdquo; the RKE2 related packages so a <code>dnf update</code>/<code>yum update</code> does not mess around with them.</p>
<p>Add the following line to <code>/etc/dnf/dnf.conf</code> and/or <code>/etc/yum.conf</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">exclude<span style="color:#f92672">=</span>rke2-*
</code></pre></div><h2 id="starting-rke2">Starting RKE2</h2>
<p>Enable the <code>rke2-server</code> service and start it:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo systemctl enable rke2-server --now
</code></pre></div><p>Verification:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo systemctl status rke2-server
sudo journalctl -u rke2-server -f
</code></pre></div><h2 id="configure-kubectl-on-rke2-host">Configure Kubectl (on RKE2 Host)</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">mkdir ~/.kube
ln -s /etc/rancher/rke2/rke2.yaml ~/.kube/config
chmod <span style="color:#ae81ff">600</span> /root/.kube/config
ln -s /var/lib/rancher/rke2/agent/etc/crictl.yaml /etc/crictl.yaml

kubectl get node
crictl ps
crictl images
</code></pre></div><p>Verification:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get nodes
NAME                    STATUS   ROLES         AGE     VERSION
k8s.mydomain.intra   Ready   etcd,master   2m4s   v1.18.16+rke2r1
</code></pre></div><h3 id="deploy-demo-app">Deploy demo app</h3>
<pre tabindex="0"><code>kubens default
kubectl apply -f https://raw.githubusercontent.com/cilium/cilium/v1.9/examples/minikube/http-sw-app.yaml
kubectl apply -f k8s_sec_lab/manifest/cilium_demo_rb.yaml

kubectl exec xwing -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing
kubectl exec tiefighter -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[RKE2 Install With cilium]]></title>
            <link href="https://devopstales.github.io/home/rke2-cilium/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/rke2-cilium/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Install With cilium" />
                <link href="https://devopstales.github.io/home/k8s-vault/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes and Vault integration" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller-v2/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V2" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller" />
                <link href="https://devopstales.github.io/home/rke2-pod-security-policy/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Pod Security Policy" />
            
                <id>https://devopstales.github.io/home/rke2-cilium/</id>
            
            
            <published>2021-05-24T00:00:00+00:00</published>
            <updated>2021-05-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can install a RKE2 in with cilium&rsquo;s encripted VXLAN.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<h3 id="what-is-cilium">What is Cilium?</h3>
<p>Cilium is open source software for transparently securing the network connectivity between application services deployed using Linux container management platforms like Docker and Kubernetes.</p>
<p>At the foundation of Cilium is a new Linux kernel technology called eBPF, which enables the dynamic insertion of powerful security visibility and control logic within Linux itself. Because eBPF runs inside the Linux kernel, Cilium security policies can be applied and updated without any changes to the application code or container configuration. (Source: <a href="https://docs.cilium.io/en/v1.9/intro/">cilium.io</a> )</p>
<h3 id="what-is-hubble">What is Hubble?</h3>
<p>Hubble is a fully distributed networking and security observability platform. It is built on top of Cilium and eBPF to enable deep visibility into the communication and behavior of services as well as the networking infrastructure in a completely transparent manner.</p>
<p>By building on top of Cilium, Hubble can leverage eBPF for visibility. By relying on eBPF, all visibility is programmable and allows for a dynamic approach that minimizes overhead while providing deep and detailed visibility as required by users. Hubble has been created and specifically designed to make best use of these new eBPF powers. (Source: <a href="https://docs.cilium.io/en/v1.9/intro/">cilium.io</a> )</p>
<h2 id="rke2-setup">RKE2 Setup</h2>
<h3 id="project-longhorn-prerequisites">Project Longhorn Prerequisites</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">yum install -y epel-release
yum install -y nano curl wget git tmux jq vim-common
yum install -y iscsi-initiator-utils 
modprobe iscsi_tcp
echo <span style="color:#e6db74">&#34;iscsi_tcp&#34;</span> &gt;/etc/modules-load.d/iscsi-tcp.conf
systemctl enable iscsid
systemctl start iscsid 
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat <span style="color:#e6db74">&lt;&lt;EOF&gt;&gt; /etc/NetworkManager/conf.d/rke2-canal.conf
</span><span style="color:#e6db74">[keyfile]
</span><span style="color:#e6db74">unmanaged-devices=interface-name:cali*;interface-name:flannel*
</span><span style="color:#e6db74">EOF</span>
systemctl reload NetworkManager
</code></pre></div><h3 id="rke2-rpm-install">RKE2 rpm Install</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat <span style="color:#e6db74">&lt;&lt; EOF &gt; /etc/yum.repos.d/rancher-rke2-1-20-latest.repo
</span><span style="color:#e6db74">[rancher-rke2-common-latest]
</span><span style="color:#e6db74">name=Rancher RKE2 Common Latest
</span><span style="color:#e6db74">baseurl=https://rpm.rancher.io/rke2/latest/common/centos/8/noarch
</span><span style="color:#e6db74">enabled=1
</span><span style="color:#e6db74">gpgcheck=1
</span><span style="color:#e6db74">gpgkey=https://rpm.rancher.io/public.key
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">[rancher-rke2-1-20-latest]
</span><span style="color:#e6db74">name=Rancher RKE2 1.20 Latest
</span><span style="color:#e6db74">baseurl=https://rpm.rancher.io/rke2/latest/1.20/centos/8/x86_64
</span><span style="color:#e6db74">enabled=1
</span><span style="color:#e6db74">gpgcheck=1
</span><span style="color:#e6db74">gpgkey=https://rpm.rancher.io/public.key
</span><span style="color:#e6db74">EOF</span>

yum -y install rke2-server
</code></pre></div><h3 id="kubectl-helm--rke2">Kubectl, Helm &amp; RKE2</h3>
<p>Install <code>kubectl</code>, <code>helm</code> and RKE2 to the host system:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx
sudo ln -s /opt/kubectx/kubens /usr/local/bin/kubens
echo <span style="color:#e6db74">&#39;PATH=$PATH:/usr/local/bin&#39;</span> &gt;&gt; /etc/profile
echo <span style="color:#e6db74">&#39;PATH=$PATH:/var/lib/rancher/rke2/bin&#39;</span> &gt;&gt; /etc/profile
source /etc/profile

sudo dnf copr -y enable cerenit/helm
sudo dnf install -y helm
</code></pre></div><h3 id="rke2-specific-ports">RKE2 specific ports</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo firewall-cmd --add-port<span style="color:#f92672">=</span>9345/tcp --permanent
sudo firewall-cmd --add-port<span style="color:#f92672">=</span>6443/tcp --permanent
sudo firewall-cmd --add-port<span style="color:#f92672">=</span>10250Air-Gap/tcp --permanent
sudo firewall-cmd --add-port<span style="color:#f92672">=</span>2379/tcp --permanent
sudo firewall-cmd --add-port<span style="color:#f92672">=</span>2380/tcp --permanent
sudo firewall-cmd --add-port<span style="color:#f92672">=</span>30000-32767/tcp --permanent
<span style="color:#75715e"># Used for the Rancher Monitoring</span>
sudo firewall-cmd --add-port<span style="color:#f92672">=</span>9796/tcp --permanent
sudo firewall-cmd --add-port<span style="color:#f92672">=</span>19090/tcp --permanent
sudo firewall-cmd --add-port<span style="color:#f92672">=</span>6942/tcp --permanent
sudo firewall-cmd --add-port<span style="color:#f92672">=</span>9091/tcp --permanent
<span style="color:#75715e">### CNI specific ports</span>
<span style="color:#75715e"># 4244/TCP is required when the Hubble Relay is enabled and therefore needs to connect to all agents to collect the flows</span>
sudo firewall-cmd --add-port<span style="color:#f92672">=</span>4244/tcp --permanent
<span style="color:#75715e"># Cilium healthcheck related permits:</span>
sudo firewall-cmd --add-port<span style="color:#f92672">=</span>4240/tcp --permanent
sudo firewall-cmd --remove-icmp-block<span style="color:#f92672">=</span>echo-request --permanent
sudo firewall-cmd --remove-icmp-block<span style="color:#f92672">=</span>echo-reply --permanent
<span style="color:#75715e"># Since we are using Cilium with GENEVE as overlay, we need the following port too:</span>
sudo firewall-cmd --add-port<span style="color:#f92672">=</span>6081/udp --permanent
<span style="color:#75715e">### Ingress Controller specific ports</span>
sudo firewall-cmd --add-port<span style="color:#f92672">=</span>80/tcp --permanent
sudo firewall-cmd --add-port<span style="color:#f92672">=</span>443/tcp --permanent
<span style="color:#75715e">### To get DNS resolution working, simply enable Masquerading.</span>
sudo firewall-cmd --zone<span style="color:#f92672">=</span>public  --add-masquerade --permanent

<span style="color:#75715e">### Finally apply all the firewall changes</span>
sudo firewall-cmd --reload
</code></pre></div><p>Verification:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo firewall-cmd --list-all
public <span style="color:#f92672">(</span>active<span style="color:#f92672">)</span>
  target: default
  icmp-block-inversion: no
  interfaces: eno1
  sources: 
  services: cockpit dhcpv6-client ssh wireguard
  ports: 9345/tcp 6443/tcp 10250/tcp 2379/tcp 2380/tcp 30000-32767/tcp 4240/tcp 6081/udp 80/tcp 443/tcp 4244/tcp 9796/tcp 19090/tcp 6942/tcp 9091/tcp
  protocols: 
  masquerade: yes
  forward-ports: 
  source-ports: 
  icmp-blocks: 
  rich rules: 
</code></pre></div><h3 id="basic-configuration">Basic Configuration</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">mkdir -p /etc/rancher/rke2
cat <span style="color:#e6db74">&lt;&lt; EOF &gt;  /etc/rancher/rke2/config.yaml
</span><span style="color:#e6db74">write-kubeconfig-mode: &#34;0644&#34;
</span><span style="color:#e6db74">profile: &#34;cis-1.5&#34;
</span><span style="color:#e6db74">selinux: true
</span><span style="color:#e6db74"># add ips/hostname of hosts and loadbalancer
</span><span style="color:#e6db74">tls-san:
</span><span style="color:#e6db74">  - &#34;k8s.mydomain.intra&#34;
</span><span style="color:#e6db74">  - &#34;172.17.9.10&#34;
</span><span style="color:#e6db74"># Make a etcd snapshot every 6 hours
</span><span style="color:#e6db74">etcd-snapshot-schedule-cron: &#34; */6 * * *&#34;
</span><span style="color:#e6db74"># Keep 56 etcd snapshorts (equals to 2 weeks with 6 a day)
</span><span style="color:#e6db74">etcd-snapshot-retention: 56
</span><span style="color:#e6db74">cni:
</span><span style="color:#e6db74">  - cilium
</span><span style="color:#e6db74">disable:
</span><span style="color:#e6db74">  - rke2-canal
</span><span style="color:#e6db74">  - rke2-kube-proxy
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><p><strong>Note:</strong> I disabled <code>rke2-canal</code> and <code>rke2-kube-proxy</code> since I plan to install Cilium as CNI in <a href="https://docs.cilium.io/en/v1.9/gettingstarted/kubeproxy-free/">&ldquo;kube-proxy less mode&rdquo;</a> (<code>kubeProxyReplacement: &quot;strict&quot;</code>). Do not disable <code>rke2-kube-proxy</code> if you use another CNI - it will not work afterwards!</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo cp -f /usr/share/rke2/rke2-cis-sysctl.conf /etc/sysctl.d/60-rke2-cis.conf
sysctl -p /etc/sysctl.d/60-rke2-cis.conf

useradd -r -c <span style="color:#e6db74">&#34;etcd user&#34;</span> -s /sbin/nologin -M etcd

mkdir -p /var/lib/rancher/rke2/server/manifests/
cat <span style="color:#e6db74">&lt;&lt; EOF &gt; /var/lib/rancher/rke2/server/manifests/rke2-ingress-nginx-config.yaml
</span><span style="color:#e6db74">apiVersion: helm.cattle.io/v1
</span><span style="color:#e6db74">kind: HelmChartConfig
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: rke2-ingress-nginx
</span><span style="color:#e6db74">  namespace: kube-system
</span><span style="color:#e6db74">spec:
</span><span style="color:#e6db74">  valuesContent: |-
</span><span style="color:#e6db74">    controller:
</span><span style="color:#e6db74">      metrics:
</span><span style="color:#e6db74">        service:
</span><span style="color:#e6db74">          annotations:
</span><span style="color:#e6db74">            prometheus.io/scrape: &#34;true&#34;
</span><span style="color:#e6db74">            prometheus.io/port: &#34;10254&#34;
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><h3 id="prevent-rke2-package-updates">Prevent RKE2 Package Updates</h3>
<p>In order to provide more stability, I chose to DNF/YUM &ldquo;mark/hold&rdquo; the RKE2 related packages so a <code>dnf update</code>/<code>yum update</code> does not mess around with them.</p>
<p>Add the following line to <code>/etc/dnf/dnf.conf</code> and/or <code>/etc/yum.conf</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">exclude<span style="color:#f92672">=</span>rke2-*
</code></pre></div><h3 id="cilium-prerequisites">Cilium Prerequisites</h3>
<p>Ensure the eBFP filesystem is mounted (which should already be the case on RHEL 8.3):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">mount | grep /sys/fs/bpf
<span style="color:#75715e"># if present should output, e.g. &#34;none on /sys/fs/bpf type bpf&#34;...</span>
</code></pre></div><p>If that&rsquo;s not the case, mount it using the commands down here:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo mount bpffs -t bpf /sys/fs/bpf
sudo bash -c <span style="color:#e6db74">&#39;cat &lt;&lt;EOF &gt;&gt; /etc/fstab
</span><span style="color:#e6db74">none /sys/fs/bpf bpf rw,relatime 0 0
</span><span style="color:#e6db74">EOF&#39;</span>
</code></pre></div><h3 id="deploy-cilium">Deploy Cilium</h3>
<p>Cilium’s eBPF kube-proxy replacement currently cannot be used with Transparent Encryption.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat &lt;&lt; EOF &gt;  /var/lib/rancher/rke2/server/manifests/rke2-cilium-config.yaml</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">helm.cattle.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">HelmChartConfig</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">rke2-cilium</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">kube-system</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">valuesContent</span>: |-<span style="color:#e6db74">
</span><span style="color:#e6db74">    cilium:
</span><span style="color:#e6db74">      kubeProxyReplacement: &#34;strict&#34;
</span><span style="color:#e6db74">      k8sServiceHost: 10.0.2.15
</span><span style="color:#e6db74">      k8sServicePort: 6443
</span><span style="color:#e6db74">      operator:
</span><span style="color:#e6db74">        replicas: 1
</span><span style="color:#e6db74">      encryption:
</span><span style="color:#e6db74">        enabled: false
</span><span style="color:#e6db74">        type: wireguard
</span><span style="color:#e6db74">      l7Proxy: false
</span><span style="color:#e6db74">      hubble:
</span><span style="color:#e6db74">        metrics:
</span><span style="color:#e6db74">          enabled:
</span><span style="color:#e6db74">          - dns:query;ignoreAAAA
</span><span style="color:#e6db74">          - drop
</span><span style="color:#e6db74">          - tcp
</span><span style="color:#e6db74">          - flow
</span><span style="color:#e6db74">          - icmp
</span><span style="color:#e6db74">          - http
</span><span style="color:#e6db74">        relay:
</span><span style="color:#e6db74">          enabled: true
</span><span style="color:#e6db74">        ui:
</span><span style="color:#e6db74">          enabled: true
</span><span style="color:#e6db74">          replicas: 1
</span><span style="color:#e6db74">          ingress:
</span><span style="color:#e6db74">            enabled: true
</span><span style="color:#e6db74">            hosts:
</span><span style="color:#e6db74">              - hubble.k8s.intra
</span><span style="color:#e6db74">            annotations:
</span><span style="color:#e6db74">              cert-manager.io/cluster-issuer: ca-issuer
</span><span style="color:#e6db74">            tls:
</span><span style="color:#e6db74">            - secretName: ingress-hubble-ui-tls
</span><span style="color:#e6db74">              hosts:
</span><span style="color:#e6db74">              - hubble.k8s.intra
</span><span style="color:#e6db74">      prometheus:
</span><span style="color:#e6db74">        enabled: true
</span><span style="color:#e6db74">        # Default port value (9090) needs to be changed since the RHEL cockpit also listens on this port.
</span><span style="color:#e6db74">        port: 19090
</span><span style="color:#e6db74">        # Configure this serviceMonitor section AFTER Rancher Monitoring is enabled!
</span><span style="color:#e6db74">        #serviceMonitor:
</span><span style="color:#e6db74">        #  enabled: true</span>    
<span style="color:#ae81ff">EOF</span>
</code></pre></div><h2 id="starting-rke2">Starting RKE2</h2>
<p>Enable the <code>rke2-server</code> service and start it:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo systemctl enable rke2-server --now
</code></pre></div><p>Verification:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo systemctl status rke2-server
sudo journalctl -u rke2-server -f
</code></pre></div><h2 id="configure-kubectl-on-rke2-host">Configure Kubectl (on RKE2 Host)</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">mkdir ~/.kube
ln -s /etc/rancher/rke2/rke2.yaml ~/.kube/config
chmod <span style="color:#ae81ff">600</span> /root/.kube/config
ln -s /var/lib/rancher/rke2/agent/etc/crictl.yaml /etc/crictl.yaml

kubectl get node
crictl ps
crictl images
</code></pre></div><p>Verification:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get nodes
NAME                    STATUS   ROLES         AGE     VERSION
k8s.mydomain.intra   NotReady   etcd,master   2m4s   v1.18.16+rke2r1
</code></pre></div><h3 id="deploy-demo-app">Deploy demo app</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubens default
kubectl apply -f https://raw.githubusercontent.com/cilium/cilium/v1.9/examples/minikube/http-sw-app.yaml
kubectl apply -f k8s_sec_lab/manifest/cilium_demo_rb.yaml

kubectl exec xwing -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing
kubectl exec tiefighter -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Flux2 and Mozilla SOPS to encrypt secrets]]></title>
            <link href="https://devopstales.github.io/home/gitops-flux2-sops/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/gitops-flux2-kubeseal/?utm_source=atom_feed" rel="related" type="text/html" title="Flux2 and kubeseal to encrypt secrets" />
                <link href="https://devopstales.github.io/kubernetes/gitops-flux2-sops/?utm_source=atom_feed" rel="related" type="text/html" title="Flux2 and Mozilla SOPS to encrypt secrets" />
                <link href="https://devopstales.github.io/home/gitops-flux2/?utm_source=atom_feed" rel="related" type="text/html" title="Flux2 Install and Usage" />
                <link href="https://devopstales.github.io/kubernetes/gitops-flux2-kubeseal/?utm_source=atom_feed" rel="related" type="text/html" title="Flux2 and kubeseal to encrypt secrets" />
                <link href="https://devopstales.github.io/home/argocd-image-updater/?utm_source=atom_feed" rel="related" type="text/html" title="Argo CD Image Updater for automate image update" />
            
                <id>https://devopstales.github.io/home/gitops-flux2-sops/</id>
            
            
            <published>2021-05-08T00:00:00+00:00</published>
            <updated>2021-05-08T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can use Mozilla SOPS with Flux2 to protect secrets.</p>
<H3>Parst of the K8S Gitops series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-gitops/">GitOps solutions for Kubernetes</a></li>
     <li>Part2: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part3: <a href="../../kubernetes/argocd-image-updater/">Argo CD Image Updater for automate image update</a></li>
<!-- ArgoCD notify
https://blog.argoproj.io/notifications-for-argo-bb7338231604
-->
<!----------------------------------------------->
     <li>Part4: <a href="../../kubernetes/gitops-flux2/">Flux2 Install and Usage</a></li>
     <li>Part5: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part6: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- Flux CD auto image update
https://particule.io/en/blog/flux-auto-image-update/
-->
<!----------------------------------------------->
<!-- Fleet -->
<!----------------------------------------------->
<!-- Canary Deployment:
Canary Basics - https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#canary-deployments
Canary with helm:
  https://itnext.io/safe-and-automation-friendly-canary-deployments-with-helm-669394d2c48a
  https://github.com/stoehdoi/canary-demo/tree/master/deploy
ArgoCD + Argo Rollouts for canary deploy:
-->
     <li>Part7: <a href="../../kubernetes/flagger-nginx-canary-deployments/">Flagger NGINX Canary Deployments</a></li>
</ul>



<p>First you need to bootstrap the fluxcomponent as I showd in the <a href="../gitops-flux2/">previous post</a>.</p>
<h2 id="install-ops-cli">Install OPS CLI</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">VERSION<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>curl --silent <span style="color:#e6db74">&#34;https://api.github.com/repos/mozilla/sops/releases/latest&#34;</span> | grep <span style="color:#e6db74">&#39;&#34;tag_name&#34;&#39;</span> | sed -E <span style="color:#e6db74">&#39;s/.*&#34;([^&#34;]+)&#34;.*/\1/&#39;</span><span style="color:#66d9ef">)</span>
VERSION<span style="color:#f92672">=</span><span style="color:#e6db74">${</span>VERSION:1<span style="color:#e6db74">}</span>

wget https://github.com/mozilla/sops/releases/download/v$VERSION/sops-<span style="color:#e6db74">&#34;</span>$VERSION<span style="color:#e6db74">&#34;</span>-1.x86_64.rpm
yum install -y sops-<span style="color:#e6db74">&#34;</span>$VERSION<span style="color:#e6db74">&#34;</span>-1.x86_64.rpm
rm -f sops-<span style="color:#e6db74">&#34;</span>$VERSION<span style="color:#e6db74">&#34;</span>-1.x86_64.rpm
</code></pre></div><h3 id="generate-a-gpg-key">Generate a GPG key</h3>
<p>Generate a GPG/OpenPGP key:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">export KEY_NAME<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;cl1.mydomain.intra&#34;</span>
export KEY_COMMENT<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;flux secrets&#34;</span>

gpg --batch --full-generate-key <span style="color:#e6db74">&lt;&lt;EOF
</span><span style="color:#e6db74">%no-protection
</span><span style="color:#e6db74">Key-Type: 1
</span><span style="color:#e6db74">Key-Length: 4096
</span><span style="color:#e6db74">Subkey-Type: 1
</span><span style="color:#e6db74">Subkey-Length: 4096
</span><span style="color:#e6db74">Expire-Date: 0
</span><span style="color:#e6db74">Name-Comment: ${KEY_COMMENT}
</span><span style="color:#e6db74">Name-Real: ${KEY_NAME}
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><p>The above configuration creates an rsa4096 key that does not expire. Retrieve the GPG key fingerprint:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">gpg --list-secret-keys <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">${</span>KEY_NAME<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>

sec   rsa4096 2020-09-06 <span style="color:#f92672">[</span>SC<span style="color:#f92672">]</span>
      1F3D1CED2F865F5E59CA564553241F147E7C5FA4

<span style="color:#75715e"># Store the key fingerprint as an environment variable:</span>
export KEY_FP<span style="color:#f92672">=</span>1F3D1CED2F865F5E59CA564553241F147E7C5FA4
</code></pre></div><p>Export the public and private key from your local GPG keyring and create a Kubernetes secret named sops-gpg in the flux-system namespace:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">gpg --export-secret-keys --armor <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">${</span>KEY_FP<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span> |
kubectl create secret generic sops-gpg <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--namespace<span style="color:#f92672">=</span>flux-system <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--from-file<span style="color:#f92672">=</span>sops.asc<span style="color:#f92672">=</span>/dev/stdin

mkdir ./02_flux2/03_SOPS_demo/
gpg --export-secret-keys --armor <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">${</span>KEY_FP<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span> &gt; ./02_flux2/03_SOPS_demo/sops.pub.asc
</code></pre></div><p>It’s a good idea to back up this secret-key/K8s-Secret with a password manager or offline storage. Also consider deleting the secret decryption key from you machine:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">gpg --delete-secret-keys <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">${</span>KEY_FP<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">git add -A
git commit -m <span style="color:#e6db74">&#34;add sops config&#34;</span>
git push
</code></pre></div><h3 id="configure-in-cluster-secrets-decryption">Configure in-cluster secrets decryption</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano 02_flux2/flux-system/gotk-sync.yaml
---
apiVersion: source.toolkit.fluxcd.io/v1beta1
kind: GitRepository
metadata:
  name: flux-system
  namespace: flux-system
spec:
  interval: 1m0s
  ref:
    branch: main
  secretRef:
    name: flux-system
  url: ssh://git@github.com/devopstales/gitops-repo
---
apiVersion: kustomize.toolkit.fluxcd.io/v1beta1
kind: Kustomization
metadata:
  name: flux-system
  namespace: flux-system
spec:
  decryption:
    provider: sops
    secretRef:
      name: sops-gpg
  interval: 10m0s
  path: ./01_flux2
  prune: true
  sourceRef:
    kind: GitRepository
    name: flux-system
  validation: client
</code></pre></div><p>Create a secret you want to encrypt:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cp 01_flux2/02_secret/*.yaml 02_flux2/03_SOPS_demo/</span>
<span style="color:#ae81ff">rm -f 02_flux2/03_SOPS_demo/sealedsecret.yaml</span>

<span style="color:#ae81ff">nano 02_flux2/03_SOPS_demo/secret.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">data</span>:
  <span style="color:#f92672">secret</span>: <span style="color:#ae81ff">UzNDUjNUCg==</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Secret</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">creationTimestamp</span>: <span style="color:#66d9ef">null</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">mysecret</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">demo-app</span>
</code></pre></div><p>ncrypt the secret with SOPS using your GPG key:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sops --encrypt --encrypted-regex <span style="color:#e6db74">&#39;^(data|stringData)$&#39;</span> --pgp <span style="color:#e6db74">${</span>KEY_FP<span style="color:#e6db74">}</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--in-place 02_flux2/03_SOPS_demo/secret.yaml
</code></pre></div><p>Check the result:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat 02_flux2/03_SOPS_demo/secret.yaml </span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">data</span>:
    <span style="color:#f92672">secret</span>: <span style="color:#ae81ff">ENC[AES256_GCM,data:RxdPIf8i5G+yjiT0,iv:A8iYsJ4hdd1MNZDAKQyD4L/b6Caa1TDn+MNKsFku3nc=,tag:QLN3Y6B/S87qzPYh2PATaQ==,type:str]</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Secret</span>
<span style="color:#f92672">metadata</span>:
    <span style="color:#f92672">creationTimestamp</span>: <span style="color:#66d9ef">null</span>
    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">mysecret</span>
    <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">demo-app</span>
<span style="color:#f92672">sops</span>:
    <span style="color:#f92672">kms</span>: []
    <span style="color:#f92672">gcp_kms</span>: []
    <span style="color:#f92672">azure_kv</span>: []
    <span style="color:#f92672">hc_vault</span>: []
    <span style="color:#f92672">age</span>: []
    <span style="color:#f92672">lastmodified</span>: <span style="color:#e6db74">&#34;2021-05-09T08:59:30Z&#34;</span>
    <span style="color:#f92672">mac</span>: <span style="color:#ae81ff">ENC[AES256_GCM,data:2SwDXqI5R880Y/uf4yW6o3rraJ7WYQ5aIfKwIPEpss/evD15dfLulUUahN0bNmrwtSYuad0aXHopGfFsKEvxSVMx9UkPaAd0xVZHSj7aJ5Fi5D2De3Tw1yi8tuWjU8OMG81nIZkx6GdIa4yZlm+LEangYVIpzluWk6C7In8GNc8=,iv:9E0R+R6QjqCXBnzPlAzri/fYxEr0HPZ0FzQX5oddMZk=,tag:FEJknbbaqp402biU/hxUaA==,type:str]</span>
    <span style="color:#f92672">pgp</span>:
        - <span style="color:#f92672">created_at</span>: <span style="color:#e6db74">&#34;2021-05-09T08:59:30Z&#34;</span>
          <span style="color:#f92672">enc</span>: |<span style="color:#e6db74">
</span><span style="color:#e6db74">            -----BEGIN PGP MESSAGE-----
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">            hQIMA1gFjmLlSkpZAQ//ZwU9ZEL2jDtmg8ZvJ4Wrnaa66Rjvnr/Uz9VOUxKZk1WJ
</span><span style="color:#e6db74">            V6+Wa1Od80tODzr9gfmjHor0ZCbdJmPxf96z4MhcBNbo9oBr43GX2Wm67ijwIEdo
</span><span style="color:#e6db74">            Wup2252ANsn1stZIk5krdlZVkRTW+GeAwEDHnW4zSOSVfc9Ad1SHGy1vgXM/i7Je
</span><span style="color:#e6db74">            ttx3s/PZhPPZLUQ6SKQjEaf6Xod9nnLyqSb1SdB5idhBy/wV3nt08p/LJ8QVItzh
</span><span style="color:#e6db74">            iFclOBRXeEQuYEt2O57Bo/kRGTaBjq6YE0KCbu7PMkm8gerZOyW2Od1maF4Bsjlz
</span><span style="color:#e6db74">            c+qOaLjFOP4K/8OIDtzTOw9tXbQREWC9tl1ReadGHReTbdCs55msmMWscPJtq8wi
</span><span style="color:#e6db74">            a3eMdLDwvJHhAERaJwvAa5Le6uIwr+lOEVzetj2ucK6LgVlTjgs9IPTMul4ASyji
</span><span style="color:#e6db74">            tOtTUzXoEHu/wfGKP7QFDbROFBWalNBkSegdOQx+/GSLebfWY/HmTy+V7isRhoa3
</span><span style="color:#e6db74">            iH4/hapCDUQVqwQSyvjpVUzoAsp9g7XaYITKGbSkIuUA3TI5aTp3SSF7sbNHdG8g
</span><span style="color:#e6db74">            6i3jh4FxS9yzFgM7fGnlbHDta/DzyBQB5Z77cI8pJW9mri1/U6R63zJuDqSUvFlw
</span><span style="color:#e6db74">            zf1zJwEV3xgwMog/7nx4aAItPBeqsT0pSYs5pQpciKgTJcCOTG8r6+3+cjckS/vS
</span><span style="color:#e6db74">            XgH6CQtPyzD1JAVDz94n0RkZKC+TXUfLnRF0yLwaAeOJ9h6vFVnEggOIXiWF4Iy7
</span><span style="color:#e6db74">            Ds1zIaq79+H/gWo0fGk2srKIHdcPZkQc7zAOqbO5X94fo0TtNx5y/QT6FUwUH4k=
</span><span style="color:#e6db74">            =GMAU
</span><span style="color:#e6db74">            -----END PGP MESSAGE-----</span>            
          <span style="color:#f92672">fp</span>: <span style="color:#ae81ff">FCF6E84DC263BDDB9A35D3C6DF8C27FF0C09F771</span>
    <span style="color:#f92672">encrypted_regex</span>: <span style="color:#ae81ff">^(data|stringData)$</span>
    <span style="color:#f92672">version</span>: <span style="color:#ae81ff">3.7.1</span>
</code></pre></div><p>Upload files and test:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">git add -A
git commit -m <span style="color:#e6db74">&#34;sops demo&#34;</span>
git push

kubectl logs -n demo-app2 demo-app
S3CR3T
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Flux2 and kubeseal to encrypt secrets]]></title>
            <link href="https://devopstales.github.io/home/gitops-flux2-kubeseal/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/gitops-flux2-kubeseal/?utm_source=atom_feed" rel="related" type="text/html" title="Flux2 and kubeseal to encrypt secrets" />
                <link href="https://devopstales.github.io/home/gitops-flux2/?utm_source=atom_feed" rel="related" type="text/html" title="Flux2 Install and Usage" />
                <link href="https://devopstales.github.io/home/argocd-kubeseal/?utm_source=atom_feed" rel="related" type="text/html" title="ArgoCD and kubeseal to encrypt secrets" />
                <link href="https://devopstales.github.io/home/argocd-image-updater/?utm_source=atom_feed" rel="related" type="text/html" title="Argo CD Image Updater for automate image update" />
                <link href="https://devopstales.github.io/home/k8s-gitops/?utm_source=atom_feed" rel="related" type="text/html" title="GitOps solutions for Kubernetes" />
            
                <id>https://devopstales.github.io/home/gitops-flux2-kubeseal/</id>
            
            
            <published>2021-05-07T00:00:00+00:00</published>
            <updated>2021-05-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can use kubeseal and Mozilla SOPS with Flux2 to protect secrets.</p>
<H3>Parst of the K8S Gitops series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-gitops/">GitOps solutions for Kubernetes</a></li>
     <li>Part2: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part3: <a href="../../kubernetes/argocd-image-updater/">Argo CD Image Updater for automate image update</a></li>
<!-- ArgoCD notify
https://blog.argoproj.io/notifications-for-argo-bb7338231604
-->
<!----------------------------------------------->
     <li>Part4: <a href="../../kubernetes/gitops-flux2/">Flux2 Install and Usage</a></li>
     <li>Part5: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part6: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- Flux CD auto image update
https://particule.io/en/blog/flux-auto-image-update/
-->
<!----------------------------------------------->
<!-- Fleet -->
<!----------------------------------------------->
<!-- Canary Deployment:
Canary Basics - https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#canary-deployments
Canary with helm:
  https://itnext.io/safe-and-automation-friendly-canary-deployments-with-helm-669394d2c48a
  https://github.com/stoehdoi/canary-demo/tree/master/deploy
ArgoCD + Argo Rollouts for canary deploy:
-->
     <li>Part7: <a href="../../kubernetes/flagger-nginx-canary-deployments/">Flagger NGINX Canary Deployments</a></li>
</ul>



<h2 id="install-kubeseal-cli">Install kubeseal cli</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">VERSION<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>curl --silent <span style="color:#e6db74">&#34;https://api.github.com/repos/bitnami-labs/sealed-secrets/releases/latest&#34;</span> | grep <span style="color:#e6db74">&#39;&#34;tag_name&#34;&#39;</span> | sed -E <span style="color:#e6db74">&#39;s/.*&#34;([^&#34;]+)&#34;.*/\1/&#39;</span><span style="color:#66d9ef">)</span>

wget https://github.com/bitnami-labs/sealed-secrets/releases/download/$VERSION/kubeseal-linux-amd64 -O /usr/local/bin/kubeseal
chmod <span style="color:#ae81ff">755</span> /usr/local/bin/kubeseal
kubeseal --version
</code></pre></div><h3 id="deploy-sealed-secrets-with-a-helmrelease">Deploy sealed-secrets with a HelmRelease</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">flux create source helm sealed-secrets <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--interval<span style="color:#f92672">=</span>1h <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--url<span style="color:#f92672">=</span>https://bitnami-labs.github.io/sealed-secrets

flux create helmrelease sealed-secrets <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--interval<span style="color:#f92672">=</span>1h <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--release-name<span style="color:#f92672">=</span>sealed-secrets <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--target-namespace<span style="color:#f92672">=</span>flux-system <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--source<span style="color:#f92672">=</span>HelmRepository/sealed-secrets <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--chart<span style="color:#f92672">=</span>sealed-secrets <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--chart-version<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;&gt;=1.15.0-0&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--crds<span style="color:#f92672">=</span>CreateReplace
</code></pre></div><p>At startup, the sealed-secrets controller generates a 4096-bit RSA key pair and persists the private and public keys as Kubernetes secrets in the flux-system namespace.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubeseal --fetch-cert <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--controller-name<span style="color:#f92672">=</span>sealed-secrets <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--controller-namespace<span style="color:#f92672">=</span>flux-system <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>&gt; ../pub-sealed-secrets.pem
</code></pre></div><p>Generate a Kubernetes secret manifest with kubectl:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl -n default create secret generic basic-auth <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--from-literal<span style="color:#f92672">=</span>user<span style="color:#f92672">=</span>admin <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--from-literal<span style="color:#f92672">=</span>password<span style="color:#f92672">=</span>change-me <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--dry-run<span style="color:#f92672">=</span>client <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>-o yaml &gt; ../basic-auth.yaml
</code></pre></div><h3 id="create-a-sealed-secret">Create a sealed secret</h3>
<p>Create a secret you want to encrypt:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">data</span>:
  <span style="color:#f92672">secret</span>: <span style="color:#ae81ff">UzNDUjNUCg==</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Secret</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">creationTimestamp</span>: <span style="color:#66d9ef">null</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">mysecret</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">demo-app</span>
</code></pre></div><p>A secret in Kubernetes cluster is encoded in base64 but not encrypted! Theses data are &ldquo;only&rdquo; encoded so if a user have access to your secrets, he can simply base64 decode to see your sensitive data:</p>
<pre tabindex="0"><code class="language-base" data-lang="base">echo &quot;UzNDUjNUCg==&quot; | base64 -d
S3CR3T
</code></pre><p>Encrypt the secret with kubeseal:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">mkdir ./01_flux2/02_secret/

kubeseal --format yaml --cert<span style="color:#f92672">=</span>../../../pub-sealed-secrets.pem <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>&lt; secret.yaml &gt;sealedsecret.yaml
rm -f secret.yaml

git add -A
git commit -m <span style="color:#e6db74">&#34;kubeseal&#34;</span>
git push

kubectl logs -n demo-app demo-app
S3CR3T
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Flux2 Install and Usage]]></title>
            <link href="https://devopstales.github.io/home/gitops-flux2/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/gitops-flux2-kubeseal/?utm_source=atom_feed" rel="related" type="text/html" title="Flux2 and kubeseal to encrypt secrets" />
                <link href="https://devopstales.github.io/home/argocd-image-updater/?utm_source=atom_feed" rel="related" type="text/html" title="Argo CD Image Updater for automate image update" />
                <link href="https://devopstales.github.io/home/argocd-kubeseal/?utm_source=atom_feed" rel="related" type="text/html" title="ArgoCD and kubeseal to encrypt secrets" />
                <link href="https://devopstales.github.io/home/k8s-gitops/?utm_source=atom_feed" rel="related" type="text/html" title="GitOps solutions for Kubernetes" />
                <link href="https://devopstales.github.io/kubernetes/gitops-flux2/?utm_source=atom_feed" rel="related" type="text/html" title="Flux2 Install and Usage" />
            
                <id>https://devopstales.github.io/home/gitops-flux2/</id>
            
            
            <published>2021-05-07T00:00:00+00:00</published>
            <updated>2021-05-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can use Install and Use the GitOps Tool Flux2.</p>
<H3>Parst of the K8S Gitops series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-gitops/">GitOps solutions for Kubernetes</a></li>
     <li>Part2: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part3: <a href="../../kubernetes/argocd-image-updater/">Argo CD Image Updater for automate image update</a></li>
<!-- ArgoCD notify
https://blog.argoproj.io/notifications-for-argo-bb7338231604
-->
<!----------------------------------------------->
     <li>Part4: <a href="../../kubernetes/gitops-flux2/">Flux2 Install and Usage</a></li>
     <li>Part5: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part6: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- Flux CD auto image update
https://particule.io/en/blog/flux-auto-image-update/
-->
<!----------------------------------------------->
<!-- Fleet -->
<!----------------------------------------------->
<!-- Canary Deployment:
Canary Basics - https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#canary-deployments
Canary with helm:
  https://itnext.io/safe-and-automation-friendly-canary-deployments-with-helm-669394d2c48a
  https://github.com/stoehdoi/canary-demo/tree/master/deploy
ArgoCD + Argo Rollouts for canary deploy:
-->
     <li>Part7: <a href="../../kubernetes/flagger-nginx-canary-deployments/">Flagger NGINX Canary Deployments</a></li>
</ul>



<h3 id="install-flux2-cli">Install Flux2 cli</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -s https://fluxcd.io/install.sh | sudo bash
</code></pre></div><h3 id="bootstrap-flux2-server-components">Bootstrap Flux2 Server components</h3>
<p>Flux is installed in a GitOps way and its manifest will be pushed to the repository, so you will also need a GitHub account and a <a href="https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token">personal access token</a> that can create repositories (check all permissions under <code>repo</code>) to enable Flux do this.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">export GITHUB_TOKEN<span style="color:#f92672">=</span>&lt;token&gt;
export GITHUB_USER<span style="color:#f92672">=</span>devopstales

flux check --pre

flux bootstrap github <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --owner<span style="color:#f92672">=</span>$GITHUB_USER <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --repository<span style="color:#f92672">=</span>gitops-repo <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --branch<span style="color:#f92672">=</span>main <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --path<span style="color:#f92672">=</span>./01_flux2/ <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --personal
</code></pre></div><p>If you try to install in a secure Kubernetes cluster with runAsNonRoot psp the notification-controller and the source-controller can&rsquo;t start because it runs as root.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano rb.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">RoleBinding</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">psp-rolebinding-flux-system</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">flux-system</span>
<span style="color:#f92672">roleRef</span>:
  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRole</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">system-unrestricted-psp-role</span>
<span style="color:#f92672">subjects</span>:
- <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Group</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">system:serviceaccounts</span>

<span style="color:#ae81ff">kubectl apply -f rb.yaml</span>
</code></pre></div><p>With <code>--path</code> you can configure the directory which will be used to reconcile the target cluster.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">./01_flux2/
└── flux-system <span style="color:#75715e"># &lt;- namespace dir generated by bootstrap</span>
    ├── gotk-components.yaml
    ├── gotk-sync.yaml
    ├── rb.yaml <span style="color:#75715e"># &lt;- RoleBinding for psp created by me</span>
    └── kustomization.yaml
</code></pre></div><h3 id="deploy-application">Deploy application</h3>
<p>Add an application to the cluster and upload to the git repository:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">./01_flux2/
├── 00_guestbook <span style="color:#75715e"># &lt;- guestbook application</span>
│   ├── 00_ns.yaml
│   ├── 01_rb.yaml
│   ├── 02_guestbook-ui-svc.yaml
│   └── 03_guestbook-ui-deployment.yaml
└── flux-system <span style="color:#75715e"># &lt;- namespace dir generated by bootstrap</span>
    ├── gotk-components.yaml
    ├── gotk-sync.yaml
    ├── rb.yaml <span style="color:#75715e"># &lt;- RoleBinding for psp created by me</span>
    └── kustomization.yaml
</code></pre></div><h3 id="add-another-git-repository">Add another Git repository</h3>
<p>We will be using a public repository github.com/stefanprodan/podinfo, podinfo is a tiny web application made with Go. Create a GitRepository manifest pointing to podinfo repository’s master branch:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">mkdir ./01_flux2/01_podinfo

flux create source git podinfo <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --url<span style="color:#f92672">=</span>https://github.com/stefanprodan/podinfo <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --branch<span style="color:#f92672">=</span>master <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --interval<span style="color:#f92672">=</span>30s <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --export &gt; ./01_flux2/01_podinfo/podinfo-source.yaml
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat 01_flux2/01_podinfo/podinfo-source.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">source.toolkit.fluxcd.io/v1beta1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">GitRepository</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">podinfo</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">flux-system</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">interval</span>: <span style="color:#ae81ff">30s</span>
  <span style="color:#f92672">ref</span>:
    <span style="color:#f92672">branch</span>: <span style="color:#ae81ff">master</span>
  <span style="color:#f92672">url</span>: <span style="color:#ae81ff">https://github.com/stefanprodan/podinfo</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">./01_flux2/
├── 00_guestbook <span style="color:#75715e"># &lt;- guestbook application</span>
│   ├── 00_ns.yaml
│   ├── 01_rb.yaml
│   ├── 02_guestbook-ui-svc.yaml
│   └── 03_guestbook-ui-deployment.yaml
├── 01_podinfo
│   └── podinfo-source.yaml
└── flux-system <span style="color:#75715e"># &lt;- namespace dir generated by bootstrap</span>
    ├── gotk-components.yaml
    ├── gotk-sync.yaml
    ├── rb.yaml <span style="color:#75715e"># &lt;- RoleBinding for psp created by me</span>
    └── kustomization.yaml
</code></pre></div><h3 id="kustomization">Kustomization</h3>
<p>We will create a Flux Kustomization manifest for podinfo. This configures Flux to apply the kustomize directory located in the podinfo repository.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">flux create kustomization podinfo <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --source<span style="color:#f92672">=</span>podinfo <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --path<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;./kustomize&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --prune<span style="color:#f92672">=</span>true <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --validation<span style="color:#f92672">=</span>client <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --interval<span style="color:#f92672">=</span>5m <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --export &gt; ./01_flux2/01_podinfo/podinfo-kustomization.yaml
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat ./01_flux2/01_podinfo/podinfo-kustomization.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">kustomize.toolkit.fluxcd.io/v1beta1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Kustomization</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">podinfo</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">flux-system</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">interval</span>: <span style="color:#ae81ff">5m0s</span>
  <span style="color:#f92672">path</span>: <span style="color:#ae81ff">./kustomize</span>
  <span style="color:#f92672">prune</span>: <span style="color:#66d9ef">true</span>
  <span style="color:#f92672">sourceRef</span>:
    <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">GitRepository</span>
    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">podinfo</span>
  <span style="color:#f92672">validation</span>: <span style="color:#ae81ff">client</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">./01_flux2/
├── 00_guestbook <span style="color:#75715e"># &lt;- guestbook application</span>
│   ├── 00_ns.yaml
│   ├── 01_rb.yaml
│   ├── 02_guestbook-ui-svc.yaml
│   └── 03_guestbook-ui-deployment.yaml
├── 01_podinfo
│   ├── podinfo-kustomization.yaml
│   └── podinfo-source.yaml
└── flux-system <span style="color:#75715e"># &lt;- namespace dir generated by bootstrap</span>
    ├── gotk-components.yaml
    ├── gotk-sync.yaml
    ├── rb.yaml <span style="color:#75715e"># &lt;- RoleBinding for psp created by me</span>
    └── kustomization.yaml
</code></pre></div><h3 id="manage-helm-releases">Manage Helm Releases</h3>
<p>I usually use Ransher&rsquo;s helm operator but Flux has it&rsquo;s own. It has two part the <code>HelmRepository</code> and the <code>HelmRelease</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">source.toolkit.fluxcd.io/v1beta1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">HelmRepository</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">chartmuseum</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">flux-system</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">url</span>: <span style="color:#ae81ff">https://chartmuseum.github.io/charts</span>
  <span style="color:#f92672">interval</span>: <span style="color:#ae81ff">10m</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">helm.toolkit.fluxcd.io/v2beta1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">HelmRelease</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">chartmuseum</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">flux-system</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">interval</span>: <span style="color:#ae81ff">5m</span>
  <span style="color:#f92672">chart</span>:
    <span style="color:#f92672">spec</span>:
      <span style="color:#f92672">chart</span>: <span style="color:#ae81ff">chartmuseum</span>
      <span style="color:#f92672">version</span>: <span style="color:#e6db74">&#34;2.14.2&#34;</span>
      <span style="color:#f92672">sourceRef</span>:
        <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">HelmRepository</span>
        <span style="color:#f92672">name</span>: <span style="color:#ae81ff">chartmuseum</span>
        <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">flux-system</span>
      <span style="color:#f92672">interval</span>: <span style="color:#ae81ff">1m</span>
  <span style="color:#f92672">values</span>:
    <span style="color:#f92672">env</span>:
      <span style="color:#f92672">open</span>:
        <span style="color:#f92672">AWS_SDK_LOAD_CONFIG</span>: <span style="color:#66d9ef">true</span>
        <span style="color:#f92672">STORAGE</span>: <span style="color:#ae81ff">amazon</span>
        <span style="color:#f92672">STORAGE_AMAZON_BUCKET</span>: <span style="color:#e6db74">&#34;bucket-name&#34;</span>
        <span style="color:#f92672">STORAGE_AMAZON_PREFIX</span>: <span style="color:#e6db74">&#34;&#34;</span>
        <span style="color:#f92672">STORAGE_AMAZON_REGION</span>: <span style="color:#e6db74">&#34;region-name&#34;</span>
    <span style="color:#f92672">serviceAccount</span>:
      <span style="color:#f92672">create</span>: <span style="color:#66d9ef">true</span>
      <span style="color:#f92672">annotations</span>:
        <span style="color:#f92672">eks.amazonaws.com/role-arn</span>: <span style="color:#e6db74">&#34;role-arn&#34;</span>
    <span style="color:#f92672">securityContext</span>:
      <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">true</span>
      <span style="color:#f92672">fsGroup</span>: <span style="color:#ae81ff">65534</span>
</code></pre></div><p>It is possible to define a list of ConfigMap and Secret resources from which to take values.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">spec</span>:
  <span style="color:#f92672">valuesFrom</span>:
  - <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ConfigMap</span>
    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">prod-env-values</span>
    <span style="color:#f92672">valuesKey</span>: <span style="color:#ae81ff">values-prod.yaml</span>
  - <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Secret</span>
    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">prod-tls-values</span>
    <span style="color:#f92672">valuesKey</span>: <span style="color:#ae81ff">crt</span>
    <span style="color:#f92672">targetPath</span>: <span style="color:#ae81ff">tls.crt</span>
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes Certificate Rotation]]></title>
            <link href="https://devopstales.github.io/home/k8s-cert/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k3s-gvisor/?utm_source=atom_feed" rel="related" type="text/html" title="Secure k3s with gVisor" />
                <link href="https://devopstales.github.io/home/k8s-vault/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes and Vault integration" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller-v2/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V2" />
                <link href="https://devopstales.github.io/home/kubernetes-policy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Policy" />
                <link href="https://devopstales.github.io/home/k8s-networkpolicy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Network Policy" />
            
                <id>https://devopstales.github.io/home/k8s-cert/</id>
            
            
            <published>2021-05-01T00:00:00+00:00</published>
            <updated>2021-05-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can rotate your Kubernetes Engine Certificates.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<p>By default, kubeadm generates all the certificates needed for a cluster to run. Client certificates generated by kubeadm expire after 1 year. The base concept is that you probably update for the next kubernetes version in a year.</p>
<h3 id="check-certificate-expiration">Check certificate expiration</h3>
<p>You can use the <code>check-expiration</code> subcommand to check when certificates expire:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubeadm certs check-expiration
<span style="color:#f92672">[</span>check-expiration<span style="color:#f92672">]</span> Reading configuration from the cluster...
<span style="color:#f92672">[</span>check-expiration<span style="color:#f92672">]</span> FYI: You can look at this config file with <span style="color:#e6db74">&#39;kubectl -n kube-system get cm kubeadm-config -o yaml&#39;</span>

CERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED
admin.conf                 Apr 19, <span style="color:#ae81ff">2022</span> 16:34 UTC   364d                                    no
apiserver                  Apr 19, <span style="color:#ae81ff">2022</span> 16:34 UTC   364d            ca                      no
apiserver-etcd-client      Apr 19, <span style="color:#ae81ff">2022</span> 16:34 UTC   364d            etcd-ca                 no
apiserver-kubelet-client   Apr 19, <span style="color:#ae81ff">2022</span> 16:34 UTC   364d            ca                      no
controller-manager.conf    Apr 19, <span style="color:#ae81ff">2022</span> 16:34 UTC   364d                                    no
etcd-healthcheck-client    Apr 19, <span style="color:#ae81ff">2022</span> 16:34 UTC   364d            etcd-ca                 no
etcd-peer                  Apr 19, <span style="color:#ae81ff">2022</span> 16:34 UTC   364d            etcd-ca                 no
etcd-server                Apr 19, <span style="color:#ae81ff">2022</span> 16:34 UTC   364d            etcd-ca                 no
front-proxy-client         Apr 19, <span style="color:#ae81ff">2022</span> 16:34 UTC   364d            front-proxy-ca          no
scheduler.conf             Apr 19, <span style="color:#ae81ff">2022</span> 16:34 UTC   364d                                    no

CERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED
ca                      Apr 17, <span style="color:#ae81ff">2031</span> 16:34 UTC   9y              no
etcd-ca                 Apr 17, <span style="color:#ae81ff">2031</span> 16:34 UTC   9y              no
front-proxy-ca          Apr 17, <span style="color:#ae81ff">2031</span> 16:34 UTC   9y              no
</code></pre></div><p>The kubernetes certificates are located under <code>/etc/kubernetes/pki/</code> folder. You can check the certificates manulally:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">find /etc/kubernetes/pki/ -type f -name <span style="color:#e6db74">&#34;*.crt&#34;</span> -print |xargs -L <span style="color:#ae81ff">1</span> -t  -i bash -c <span style="color:#e6db74">&#39;openssl x509  -noout -text -in {}|grep Not&#39;</span>
</code></pre></div><p>The kubele certificate is not checkd by the abow command. It is located under the <code>/var/lib/kubelet/pki/</code> folder.</p>
<h3 id="automatic-certificate-renewal">Automatic certificate renewal</h3>
<p><code>kubeadm</code> renews all the certificates during control plane upgrade. It is a best practice to upgrade your cluster frequently in order to stay secure. Kubernetes v1.8 and higher kubelet implements features for enabling rotation of its client and/or serving certificates.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl create clusterrolebinding kubelet-bootstrap <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --clusterrole<span style="color:#f92672">=</span>system:node-bootstrapper <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --user<span style="color:#f92672">=</span>kubelet-bootstrap

kubectl create clusterrolebinding node-client-auto-approve-csr <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --clusterrole<span style="color:#f92672">=</span>system:certificates.k8s.io:certificatesigningrequests:nodeclient <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --group<span style="color:#f92672">=</span>system:node-bootstrappers

kubectl create clusterrolebinding node-client-auto-renew-crt <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --clusterrole<span style="color:#f92672">=</span>system:certificates.k8s.io:certificatesigningrequests:selfnodeclient <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --group<span style="color:#f92672">=</span>system:nodes
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano /etc/systemd/system/kubelet.env
<span style="color:#75715e"># or</span>
nano /var/lib/kubelet/kubeadm-flags.env
...
KUBELET_EXTRA_ARGS<span style="color:#f92672">==</span><span style="color:#e6db74">&#34;--rotate-certificates=true --rotate-server-certificates=true&#34;</span>

systemctl restart kubelet

<span style="color:#75715e"># OR</span>
nano /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
<span style="color:#75715e"># OR</span>
nano /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf
Environment<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;KUBELET_EXTRA_ARGS=--rotate-certificates=true --rotate-server-certificates=true&#34;</span>

systemctl daemon-reload
systemctl restart kubelet

ps -ef | grep kubelet | grep <span style="color:#e6db74">&#34;rotate-certificates&#34;</span>
root      <span style="color:#ae81ff">14105</span>      <span style="color:#ae81ff">1</span>  <span style="color:#ae81ff">0</span> 16:56 pts/0    00:00:00 bash -c <span style="color:#66d9ef">while</span> true ; <span style="color:#66d9ef">do</span> /usr/bin/kubelet  --bootstrap-kubeconfig<span style="color:#f92672">=</span>/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig<span style="color:#f92672">=</span>/etc/kubernetes/kubelet.conf --pod-manifest-path<span style="color:#f92672">=</span>/etc/kubernetes/manifests --pod-infra-container-image<span style="color:#f92672">=</span>k8s.gcr.io/pause:3.2 --network-plugin<span style="color:#f92672">=</span>cni --cni-conf-dir<span style="color:#f92672">=</span>/etc/cni/net.d --cni-bin-dir<span style="color:#f92672">=</span>/opt/cni/bin --cluster-dns<span style="color:#f92672">=</span>10.96.0.10 --cluster-domain<span style="color:#f92672">=</span>cluster.local --authorization-mode<span style="color:#f92672">=</span>Webhook --client-ca-file<span style="color:#f92672">=</span>/etc/kubernetes/pki/ca.crt --cgroup-driver<span style="color:#f92672">=</span>cgroupfs --fail-swap-on<span style="color:#f92672">=</span>false --resolv-conf<span style="color:#f92672">=</span>/etc/resolv.conf.override --rotate-certificates<span style="color:#f92672">=</span>true --rotate-server-certificates<span style="color:#f92672">=</span>true; sleep 5; <span style="color:#66d9ef">done</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get csr
NAME                                                   AGE   SIGNERNAME                                    REQUESTOR                 CONDITION
csr-pswns                                              27m   kubernetes.io/kube-apiserver-client-kubelet   system:node:node1         Approved,Issued
node-csr-cQYdjcH2F3kl-ysnzq2TlZOuUDCPgYU8cfKV1V0kqlE   47m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:lzhuxv   Approved,Issued
node-csr-f2J5HT9hg4CIKP0-0BtsEffBzg28VlUbesKJ4p_2mi0   47m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:lzhuxv   Approved,Issued
</code></pre></div><h3 id="manual-certificate-renewal">Manual certificate renewal</h3>
<p>You can renew your certificates manually at any time with the <code>kubeadm certs renew</code> command. This command performs the renewal using CA (or front-proxy-CA) certificate and key stored in <code>/etc/kubernetes/pki</code> If you are running an HA cluster, this command needs to be executed on all the control-plane nodes.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo kubeadm alpha certs renew all
</code></pre></div><h3 id="rke2-and-k3s">RKE2 and K3S</h3>
<p>By default, certificates in RKE2 and K3S expire in 12 months. If the certificates are expired or have fewer than 90 days remaining before they expire, the certificates are rotated when RKE2 is restarted.  It is expected that you would be taking your hosts down periodically for patching and upgrading every few months. With regular updates the reboots should happen - but reality has shown that many of us do not patch / reboot for more than 3 months.. so the best practice is monitoring the certificate expiration.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Secure k3s with gVisor]]></title>
            <link href="https://devopstales.github.io/home/k3s-gvisor/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k3s-gvisor/?utm_source=atom_feed" rel="related" type="text/html" title="Secure k3s with gVisor" />
                <link href="https://devopstales.github.io/home/k8s-vault/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes and Vault integration" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller-v2/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V2" />
                <link href="https://devopstales.github.io/home/kubernetes-policy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Policy" />
                <link href="https://devopstales.github.io/home/k8s-networkpolicy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Network Policy" />
            
                <id>https://devopstales.github.io/home/k3s-gvisor/</id>
            
            
            <published>2021-04-30T00:00:00+00:00</published>
            <updated>2021-04-30T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can secure k3s with gVisor.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<p>In  <a href="https://devopstales.github.io/kubernetes/k3s-etcd-kube-vip/">previous pos</a> I showd you how to install a k3s Cluster. Now we modify the configuration of the containerd to use different low level container runtime.</p>
<h3 id="what-is-gvisor">What is gvisor</h3>
<p>gVisor is an application kernel, written in Go, that implements a substantial portion of the Linux system call interface. It provides an additional layer of isolation between running applications and the host operating system.</p>
<p>gVisor includes an Open Container Initiative (OCI) runtime called <code>runsc</code> that makes it easy to work with existing container tooling. The <code>runsc</code> runtime integrates with Docker, containerd and Kubernetes, making it simple to run sandboxed containers.</p>
<p><img src="/img/include/gvisor2.png" alt="gvisor"  class="zoomable" />
<img src="/img/include/gvisor.png" alt="gvisor"  class="zoomable" /></p>
<h3 id="bootstrap-the-k3s-cluster">Bootstrap the k3s cluster</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">k3sup install <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --ip<span style="color:#f92672">=</span>172.17.8.101 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --user<span style="color:#f92672">=</span>vagrant <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --sudo <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --cluster <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-channel<span style="color:#f92672">=</span>stable <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-extra-args <span style="color:#e6db74">&#34;--no-deploy=traefik --no-deploy=servicelb --flannel-iface=enp0s8 --node-ip=172.17.8.101&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --merge <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --local-path $HOME/.kube/config <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --context<span style="color:#f92672">=</span>k3s-ha

k3sup join <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --ip 172.17.8.102 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --user vagrant <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --sudo <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-channel stable <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --server <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --server-ip 172.17.8.101 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --server-user vagrant <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --sudo <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-extra-args <span style="color:#e6db74">&#34;--no-deploy=traefik --no-deploy=servicelb --flannel-iface=enp0s8 --node-ip=172.17.8.102&#34;</span>
  
k3sup join <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --ip 172.17.8.103 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --user vagrant <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --sudo <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-channel stable <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --server <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --server-ip 172.17.8.101 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --server-user vagrant <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --sudo <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-extra-args <span style="color:#e6db74">&#34;--no-deploy=traefik --no-deploy=servicelb --flannel-iface=enp0s8 --node-ip=172.17.8.103&#34;</span>
</code></pre></div><h3 id="what-is-gvisor-1">What is gVisor</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">tmux-cssh -u vagrant 172.17.8.101 172.17.8.102 172.17.8.103

sudo su -
yum install nano wget -y

nano gvisor.sh
<span style="color:#75715e">#!/bash</span>
<span style="color:#f92672">(</span>
  set -e
  URL<span style="color:#f92672">=</span>https://storage.googleapis.com/gvisor/releases/release/latest
  wget <span style="color:#e6db74">${</span>URL<span style="color:#e6db74">}</span>/runsc <span style="color:#e6db74">${</span>URL<span style="color:#e6db74">}</span>/runsc.sha512 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    <span style="color:#e6db74">${</span>URL<span style="color:#e6db74">}</span>/gvisor-containerd-shim <span style="color:#e6db74">${</span>URL<span style="color:#e6db74">}</span>/gvisor-containerd-shim.sha512 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    <span style="color:#e6db74">${</span>URL<span style="color:#e6db74">}</span>/containerd-shim-runsc-v1 <span style="color:#e6db74">${</span>URL<span style="color:#e6db74">}</span>/containerd-shim-runsc-v1.sha512
  sha512sum -c runsc.sha512 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    -c gvisor-containerd-shim.sha512 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    -c containerd-shim-runsc-v1.sha512
  rm -f *.sha512
  chmod a+rx runsc gvisor-containerd-shim containerd-shim-runsc-v1
  sudo mv runsc gvisor-containerd-shim containerd-shim-runsc-v1 /usr/local/bin
<span style="color:#f92672">)</span>

bash gvisor.sh

cp /var/lib/rancher/k3s/agent/etc/containerd/config.toml <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>/var/lib/rancher/k3s/agent/etc/containerd/config.toml.back

cp /var/lib/rancher/k3s/agent/etc/containerd/config.toml.back <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>/var/lib/rancher/k3s/agent/etc/containerd/config.toml.tmpl

nano /var/lib/rancher/k3s/agent/etc/containerd/config.toml.tmpl
...
<span style="color:#f92672">[</span>plugins.cri.containerd<span style="color:#f92672">]</span>
  disable_snapshot_annotations <span style="color:#f92672">=</span> true
  snapshotter <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;overlayfs&#34;</span>

disabled_plugins <span style="color:#f92672">=</span> <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;restart&#34;</span><span style="color:#f92672">]</span>

<span style="color:#f92672">[</span>plugins.linux<span style="color:#f92672">]</span>
  shim_debug <span style="color:#f92672">=</span> true

<span style="color:#f92672">[</span>plugins.cri.containerd.runtimes.runsc<span style="color:#f92672">]</span>
  runtime_type <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;io.containerd.runsc.v1&#34;</span>

<span style="color:#f92672">[</span>plugins.cri.cni<span style="color:#f92672">]</span>
...

systemcl restart k3s

exit
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat<span style="color:#e6db74">&lt;&lt;EOF | kubectl apply -f -
</span><span style="color:#e6db74">apiVersion: node.k8s.io/v1beta1
</span><span style="color:#e6db74">kind: RuntimeClass
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: gvisor
</span><span style="color:#e6db74">handler: runsc
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat <span style="color:#e6db74">&lt;&lt;EOF | kubectl apply -f -
</span><span style="color:#e6db74">apiVersion: v1
</span><span style="color:#e6db74">kind: Pod
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: www-runc
</span><span style="color:#e6db74">spec:
</span><span style="color:#e6db74">  containers:
</span><span style="color:#e6db74">  - image: nginx:1.18
</span><span style="color:#e6db74">    name: www
</span><span style="color:#e6db74">    ports:
</span><span style="color:#e6db74">    - containerPort: 80
</span><span style="color:#e6db74">EOF</span>

cat<span style="color:#e6db74">&lt;&lt;EOF | kubectl apply -f -
</span><span style="color:#e6db74">apiVersion: v1
</span><span style="color:#e6db74">kind: Pod
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  labels:
</span><span style="color:#e6db74">    app: untrusted
</span><span style="color:#e6db74">  name: www-gvisor
</span><span style="color:#e6db74">spec:
</span><span style="color:#e6db74">  runtimeClassName: gvisor
</span><span style="color:#e6db74">  containers:
</span><span style="color:#e6db74">  - image: nginx:1.18
</span><span style="color:#e6db74">    name: www
</span><span style="color:#e6db74">    ports:
</span><span style="color:#e6db74">    - containerPort: 80
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get po
NAME         READY   STATUS    RESTARTS   AGE
www-gvisor   1/1     Running   <span style="color:#ae81ff">0</span>          9s
www-runc     1/1     Running   <span style="color:#ae81ff">0</span>          1m
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install K3S with k3sup and Calico]]></title>
            <link href="https://devopstales.github.io/home/k3sup-calico/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k3s-cilium/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with k3sup and Cilium" />
                <link href="https://devopstales.github.io/home/k3s-etcd-kube-vip/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with k3sup and kube-vip" />
                <link href="https://devopstales.github.io/kubernetes/k3sup-calico/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with k3sup and Calico" />
                <link href="https://devopstales.github.io/home/k3s-helm-controller/?utm_source=atom_feed" rel="related" type="text/html" title="Install and use rancher helm-controller" />
                <link href="https://devopstales.github.io/home/k3s-crio/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with CRI-O and kadalu" />
            
                <id>https://devopstales.github.io/home/k3sup-calico/</id>
            
            
            <published>2021-04-18T00:00:00+00:00</published>
            <updated>2021-04-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how to install K3S with k3sup and use Calico as networking.</p>
<H3>Parst of the K3S series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/k3s-etcd-kube-vip/">Install K3S with k3sup and kube-vip</a></li>
     <li>Part1b: <a href="../../kubernetes/k3s-crio/">Install K3S with CRI-O</a></li>
     <li>Part1c: <a href="../../kubernetes/k3s-fcos/">Install K3S on Fedora CoreOS</a></li>
<!-- K3S falnnel wiregard - https://medium.com/@bestpractices/k3s-with-flannel-wireguard-backend-fa433065a401 -->
     <li>Part2b: <a href="../../kubernetes/k3sup-calico/">Install K3S with k3sup and Calico</a></li>
     <li>Part2c: <a href="../../kubernetes/k3s-cilium/">Install K3S with k3sup and Cilium</a></li>
     <li>Part3: <a href="../../kubernetes/k3s-helm-controller/">K3S helm CR</a></li>
<!-- K3D -->
     <li>Part5: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
     <li>Part6: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
</ul>


<h3 id="installing-k3sup">Installing k3sup</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -sLS https://get.k3sup.dev | sh
sudo install k3sup /usr/local/bin/
k3sup --help
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ssh-copy-id vagrant@172.17.8.101
ssh-copy-id vagrant@172.17.8.102
ssh-copy-id vagrant@172.17.8.103
</code></pre></div><h3 id="bootstrap-the-first-k3s-node">Bootstrap the first k3s node</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">k3sup install <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --ip<span style="color:#f92672">=</span>172.17.8.101 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --user<span style="color:#f92672">=</span>vagrant <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --sudo <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --cluster <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-channel<span style="color:#f92672">=</span>stable <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-extra-args <span style="color:#e6db74">&#34;--flannel-backend=none --cluster-cidr=10.10.0.0/16 --disable-network-policy --no-deploy=traefik --no-deploy=servicelb --node-ip=172.17.8.101&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --merge <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --local-path $HOME/.kube/config <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --context<span style="color:#f92672">=</span>k3s-ha
</code></pre></div><h3 id="install-calico-for-networking">Install calico for networking</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectx k3s-ha

kubectl get no
NAME        STATUS     ROLES                       AGE   VERSION
k3s-node1   NotReady   control-plane,etcd,master   15m   v1.20.5+k3s1

kubectl get po -A -o wide
NAMESPACE     NAME                                      READY   STATUS    RESTARTS   AGE
kube-system   coredns-854c77959c-zbgkt                  0/1     Pending   <span style="color:#ae81ff">0</span>          16m
kube-system   local-path-provisioner-5ff76fc89d-btmx6   0/1     Pending   <span style="color:#ae81ff">0</span>          16m
kube-system   metrics-server-86cbb8457f-n99rp           0/1     Pending   <span style="color:#ae81ff">0</span>          16m
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml
kubectl create -f https://docs.projectcalico.org/manifests/custom-resources.yaml
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get po -A   
NAMESPACE         NAME                                       READY   STATUS    RESTARTS   AGE
calico-system     calico-kube-controllers-77c7dbc6d6-srss8   1/1     Running   <span style="color:#ae81ff">0</span>          90s
calico-system     calico-node-zd7rg                          1/1     Running   <span style="color:#ae81ff">0</span>          90s
calico-system     calico-typha-7b4c95fcd4-lw4wx              1/1     Running   <span style="color:#ae81ff">0</span>          90s
kube-system       coredns-854c77959c-zbgkt                   1/1     Running   <span style="color:#ae81ff">0</span>          27m
kube-system       local-path-provisioner-5ff76fc89d-btmx6    1/1     Running   <span style="color:#ae81ff">0</span>          27m
kube-system       metrics-server-86cbb8457f-n99rp            1/1     Running   <span style="color:#ae81ff">0</span>          27m
tigera-operator   tigera-operator-675ccbb69c-fv894           1/1     Running   <span style="color:#ae81ff">0</span>          10m
</code></pre></div><h3 id="bootstrap-the-other-k3s-nodes">Bootstrap the other k3s nodes</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">k3sup join <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --ip 172.17.8.102 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --user vagrant <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --sudo <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-channel stable <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --server <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --server-ip 172.17.8.101 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --server-user vagrant <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --sudo <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-extra-args <span style="color:#e6db74">&#34;--flannel-backend=none --cluster-cidr=10.10.0.0/16 --disable-network-policy --no-deploy=traefik --no-deploy=servicelb --node-ip=172.17.8.102&#34;</span>
  
k3sup join <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --ip 172.17.8.103 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --user vagrant <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --sudo <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-channel stable <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --server <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --server-ip 172.17.8.101 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --server-user vagrant <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --sudo <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-extra-args <span style="color:#e6db74">&#34;--flannel-backend=none --cluster-cidr=10.10.0.0/16 --disable-network-policy --no-deploy=traefik --no-deploy=servicelb --node-ip=172.17.8.103&#34;</span>
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install K3S with k3sup and Cilium]]></title>
            <link href="https://devopstales.github.io/home/k3s-cilium/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k3s-etcd-kube-vip/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with k3sup and kube-vip" />
                <link href="https://devopstales.github.io/kubernetes/k3s-cilium/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with k3sup and Cilium" />
                <link href="https://devopstales.github.io/home/k3s-helm-controller/?utm_source=atom_feed" rel="related" type="text/html" title="Install and use rancher helm-controller" />
                <link href="https://devopstales.github.io/home/k3s-crio/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with CRI-O and kadalu" />
                <link href="https://devopstales.github.io/kubernetes/k3s-etcd-kube-vip/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with k3sup and kube-vip" />
            
                <id>https://devopstales.github.io/home/k3s-cilium/</id>
            
            
            <published>2021-04-17T00:00:00+00:00</published>
            <updated>2021-04-17T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how to install K3S with k3sup and use Cilium as networking.</p>
<H3>Parst of the K3S series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/k3s-etcd-kube-vip/">Install K3S with k3sup and kube-vip</a></li>
     <li>Part1b: <a href="../../kubernetes/k3s-crio/">Install K3S with CRI-O</a></li>
     <li>Part1c: <a href="../../kubernetes/k3s-fcos/">Install K3S on Fedora CoreOS</a></li>
<!-- K3S falnnel wiregard - https://medium.com/@bestpractices/k3s-with-flannel-wireguard-backend-fa433065a401 -->
     <li>Part2b: <a href="../../kubernetes/k3sup-calico/">Install K3S with k3sup and Calico</a></li>
     <li>Part2c: <a href="../../kubernetes/k3s-cilium/">Install K3S with k3sup and Cilium</a></li>
     <li>Part3: <a href="../../kubernetes/k3s-helm-controller/">K3S helm CR</a></li>
<!-- K3D -->
     <li>Part5: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
     <li>Part6: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
</ul>


<h3 id="installing-k3sup">Installing k3sup</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -sLS https://get.k3sup.dev | sh
sudo install k3sup /usr/local/bin/
k3sup --help
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ssh-copy-id vagrant@172.17.8.101
ssh-copy-id vagrant@172.17.8.102
ssh-copy-id vagrant@172.17.8.103
</code></pre></div><h3 id="bootstrap-the-first-k3s-node">Bootstrap the first k3s node</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">k3sup install <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --ip<span style="color:#f92672">=</span>172.17.8.101 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --user<span style="color:#f92672">=</span>vagrant <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --sudo <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --cluster <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-channel<span style="color:#f92672">=</span>stable <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-extra-args <span style="color:#e6db74">&#34;--flannel-backend=none --cluster-cidr=10.10.0.0/16 --disable-network-policy --no-deploy=traefik --no-deploy=servicelb --node-ip=172.17.8.101&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --merge <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --local-path $HOME/.kube/config <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --context<span style="color:#f92672">=</span>k3s-ha
</code></pre></div><h3 id="install-cilium-for-networking">Install cilium for networking</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectx k3s-ha

kubectl get no
NAME        STATUS     ROLES                       AGE   VERSION
k3s-node1   NotReady   control-plane,etcd,master   15m   v1.20.5+k3s1

kubectl get po -A -o wide
NAMESPACE     NAME                                      READY   STATUS    RESTARTS   AGE
kube-system   coredns-854c77959c-zbgkt                  0/1     Pending   <span style="color:#ae81ff">0</span>          16m
kube-system   local-path-provisioner-5ff76fc89d-btmx6   0/1     Pending   <span style="color:#ae81ff">0</span>          16m
kube-system   metrics-server-86cbb8457f-n99rp           0/1     Pending   <span style="color:#ae81ff">0</span>          16m
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">tmux-cssh -u vagrant 172.17.8.101 172.17.8.102 172.17.8.103


sudo mount bpffs -t bpf /sys/fs/bpf
sudo bash -c <span style="color:#e6db74">&#39;cat &lt;&lt;EOF &gt;&gt; /etc/fstab
</span><span style="color:#e6db74">none /sys/fs/bpf bpf rw,relatime 0 0
</span><span style="color:#e6db74">EOF&#39;</span>

exit
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">helm repo add cilium https://helm.cilium.io/
helm repo update

kubectl create -n kube-system secret generic cilium-ipsec-keys <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --from-literal<span style="color:#f92672">=</span>keys<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;3 rfc4106(gcm(aes)) </span><span style="color:#66d9ef">$(</span>echo <span style="color:#66d9ef">$(</span>dd <span style="color:#66d9ef">if</span><span style="color:#f92672">=</span>/dev/urandom count<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span> bs<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> 2&gt; /dev/null| xxd -p -c 64<span style="color:#66d9ef">))</span><span style="color:#e6db74"> 128&#34;</span>


kubectl -n kube-system get secrets cilium-ipsec-keys
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano values.yaml
---
kubeProxyReplacement: <span style="color:#e6db74">&#34;strict&#34;</span>

k8sServiceHost: 10.0.2.15
k8sServicePort: <span style="color:#ae81ff">6443</span>

global:
  encryption:
    enabled: true
    nodeEncryption: true

hubble:
  metrics:
    <span style="color:#75715e">#serviceMonitor:</span>
    <span style="color:#75715e">#  enabled: true</span>
    enabled:
    - dns:query;ignoreAAAA
    - drop
    - tcp
    - flow
    - icmp
    - http

  ui:
    enabled: true
    replicas: <span style="color:#ae81ff">1</span>
    ingress:
      enabled: true
      hosts:
        - hubble.k3s.intra
      annotations:
        cert-manager.io/cluster-issuer: ca-issuer
      tls:
      - secretName: ingress-hubble-ui
        hosts:
        - hubble.k3s.intra

  relay:
    enabled: true


operator:
  replicas: <span style="color:#ae81ff">1</span>

ipam:
  mode: <span style="color:#e6db74">&#34;cluster-pool&#34;</span>
  operator:
    clusterPoolIPv4PodCIDR: <span style="color:#e6db74">&#34;10.43.0.0/16&#34;</span>
    clusterPoolIPv4MaskSize: <span style="color:#ae81ff">24</span>
    clusterPoolIPv6PodCIDR: <span style="color:#e6db74">&#34;fd00::/104&#34;</span>
    clusterPoolIPv6MaskSize: <span style="color:#ae81ff">120</span>

prometheus:
  enabled: true
  <span style="color:#75715e"># Default port value (9090) needs to be changed since the RHEL cockpit also listens on this port.</span>
  port: <span style="color:#ae81ff">19090</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">helm upgrade --install cilium cilium/cilium   --namespace kube-system -f values.yaml
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">k get po -A   
NAMESPACE     NAME                                      READY   STATUS    RESTARTS   AGE
kube-system   cilium-operator-67895d78b7-vkgcs          1/1     Running   <span style="color:#ae81ff">0</span>          89s
kube-system   cilium-zppdd                              1/1     Running   <span style="color:#ae81ff">0</span>          89s
kube-system   coredns-854c77959c-b4gzq                  1/1     Running   <span style="color:#ae81ff">0</span>          40s
kube-system   local-path-provisioner-5ff76fc89d-9xjgz   1/1     Running   <span style="color:#ae81ff">0</span>          40s
kube-system   metrics-server-86cbb8457f-t4d6l           1/1     Running   <span style="color:#ae81ff">0</span>          40s
</code></pre></div><h3 id="bootstrap-the-other-k3s-nodes">Bootstrap the other k3s nodes</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">k3sup join <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --ip 172.17.8.102 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --user vagrant <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --sudo <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-channel stable <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --server <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --server-ip 172.17.8.101 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --server-user vagrant <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --sudo <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-extra-args <span style="color:#e6db74">&#34;--flannel-backend=none --cluster-cidr=10.10.0.0/16 --disable-network-policy --no-deploy=traefik --no-deploy=servicelb --node-ip=172.17.8.102&#34;</span>

k3sup join <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --ip 172.17.8.103 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --user vagrant <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --sudo <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-channel stable <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --server <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --server-ip 172.17.8.101 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --server-user vagrant <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --sudo <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-extra-args <span style="color:#e6db74">&#34;--flannel-backend=none --cluster-cidr=10.10.0.0/16 --disable-network-policy --no-deploy=traefik --no-deploy=servicelb --node-ip=172.17.8.103&#34;</span>
</code></pre></div><h3 id="enable-hubble-for-cluster-wide-visibility">Enable Hubble for Cluster-Wide Visibility</h3>
<p>I configured an ingress with https in cilium helm chart but you can use port-forward instead of that.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl port-forward -n kube-system svc/hubble-ui <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--address 0.0.0.0 --address :: 12000:80
</code></pre></div><p>And then open http://localhost:12000/ to access the UI.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install K3S with k3sup and kube-vip]]></title>
            <link href="https://devopstales.github.io/home/k3s-etcd-kube-vip/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k3s-etcd-kube-vip/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with k3sup and kube-vip" />
                <link href="https://devopstales.github.io/home/k3s-helm-controller/?utm_source=atom_feed" rel="related" type="text/html" title="Install and use rancher helm-controller" />
                <link href="https://devopstales.github.io/home/k3s-crio/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with CRI-O and kadalu" />
                <link href="https://devopstales.github.io/home/k3s-fcos/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S on Fedora CoreOS" />
                <link href="https://devopstales.github.io/home/argocd-image-updater/?utm_source=atom_feed" rel="related" type="text/html" title="Argo CD Image Updater for automate image update" />
            
                <id>https://devopstales.github.io/home/k3s-etcd-kube-vip/</id>
            
            
            <published>2021-04-16T00:00:00+00:00</published>
            <updated>2021-04-16T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how to install K3S with k3sup. I will use kube-vip for  High-Availability and load-balancing.</p>
<H3>Parst of the K3S series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/k3s-etcd-kube-vip/">Install K3S with k3sup and kube-vip</a></li>
     <li>Part1b: <a href="../../kubernetes/k3s-crio/">Install K3S with CRI-O</a></li>
     <li>Part1c: <a href="../../kubernetes/k3s-fcos/">Install K3S on Fedora CoreOS</a></li>
<!-- K3S falnnel wiregard - https://medium.com/@bestpractices/k3s-with-flannel-wireguard-backend-fa433065a401 -->
     <li>Part2b: <a href="../../kubernetes/k3sup-calico/">Install K3S with k3sup and Calico</a></li>
     <li>Part2c: <a href="../../kubernetes/k3s-cilium/">Install K3S with k3sup and Cilium</a></li>
     <li>Part3: <a href="../../kubernetes/k3s-helm-controller/">K3S helm CR</a></li>
<!-- K3D -->
     <li>Part5: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
     <li>Part6: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
</ul>


<h3 id="the-infrastructure">The infrastructure</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">k3s-node1:
ip: 172.17.8.101
etcd
kube-vip

k3s-node2:
ip: 172.17.8.102
etcd
kube-vip

k3s-node3:
ip: 172.17.8.103
etcd
kube-vip
</code></pre></div><h3 id="what-is-k3sup">What is k3sup?</h3>
<p>K3S dose not give you an rpm or deb installer option just a binary. To install you need to create the systemd service and configure it. For a big cluster 3 or 5 node it could be a pain. k3sup automates this tasks trout ssh. You need a passwordless ssh connection for all the nodes and the k3sup binary on your computer.</p>
<h3 id="installing-k3sup">Installing k3sup</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">curl -sLS https://get.k3sup.dev | sh
sudo install k3sup /usr/local/bin/
k3sup --help
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ssh-copy-id vagrant@172.17.8.101
ssh-copy-id vagrant@172.17.8.102
ssh-copy-id vagrant@172.17.8.103
</code></pre></div><h3 id="bootstrap-the-first-k3s-node">Bootstrap the first k3s node</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">k3sup install <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --ip<span style="color:#f92672">=</span>172.17.8.101 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --user<span style="color:#f92672">=</span>vagrant <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --sudo <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --tls-san<span style="color:#f92672">=</span>172.17.8.100 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --cluster <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-channel<span style="color:#f92672">=</span>stable <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-extra-args <span style="color:#e6db74">&#34;--no-deploy=traefik --no-deploy=servicelb --flannel-iface=enp0s8 --node-ip=172.17.8.101&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --merge <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --local-path $HOME/.kube/config <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --context<span style="color:#f92672">=</span>k3s-ha
</code></pre></div><p>I used the <code>--tls-san</code> option to add the LoadBalancer&rsquo;s virtual ip to the cert, and a few extra option. I disabled the  traefik and the servicelb service because I will use nginx ingress controller and kube-vip as loadbalancer.  In my environment I used Vangrant to spin up the nodes.Vagrant creats multiple interfaces for the vm so I need to configure which of these will be used for the cluster: <code>--flannel-iface=enp0s8 --node-ip=172.17.8.101</code> Thanks to the <code>--cluster</code> k3sup will start an embedded etcd cluster in a container.</p>
<h3 id="install-kube-vip-for-ha">Install kube-vip for HA</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectx k3s-ha

kubectl get nodes

kubectl apply -f https://kube-vip.io/manifests/rbac.yaml
</code></pre></div><p>ssh to the first host and generate the daemonset to run kube-vip:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ssh vagrant@172.17.8.101
sudo su -

ctr image pull docker.io/plndr/kube-vip:0.3.2
alias kube-vip<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;ctr run --rm --net-host docker.io/plndr/kube-vip:0.3.2 vip /kube-vip&#34;</span>

kube-vip manifest daemonset <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --arp <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --interface enp0s8 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --address 172.17.8.100 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --controlplane <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --leaderElection <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --taint <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --inCluster | tee /var/lib/rancher/k3s/server/manifests/kube-vip.yaml

exit
</code></pre></div><p>Test vip:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ping 172.17.8.100
PING 172.17.8.100 <span style="color:#f92672">(</span>172.17.8.100<span style="color:#f92672">)</span> 56<span style="color:#f92672">(</span>84<span style="color:#f92672">)</span> bytes of data.
<span style="color:#ae81ff">64</span> bytes from 172.17.8.100: icmp_seq<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> ttl<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span> time<span style="color:#f92672">=</span>1.06 ms
<span style="color:#ae81ff">64</span> bytes from 172.17.8.100: icmp_seq<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span> ttl<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span> time<span style="color:#f92672">=</span>0.582 ms
<span style="color:#ae81ff">64</span> bytes from 172.17.8.100: icmp_seq<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span> ttl<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span> time<span style="color:#f92672">=</span>0.773 ms
</code></pre></div><h3 id="bootstrap-the-other-k3s-nodes">Bootstrap the other k3s nodes</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">k3sup join <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --ip 172.17.8.102 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --user vagrant <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --sudo <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-channel stable <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --server <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --server-ip 172.17.8.100 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --server-user vagrant <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --sudo <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-extra-args <span style="color:#e6db74">&#34;--no-deploy=traefik --no-deploy=servicelb --flannel-iface=enp0s8 --node-ip=172.17.8.102&#34;</span>
  
k3sup join <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --ip 172.17.8.103 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --user vagrant <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --sudo <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-channel stable <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --server <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --server-ip 172.17.8.100 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --server-user vagrant <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --sudo <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --k3s-extra-args <span style="color:#e6db74">&#34;--no-deploy=traefik --no-deploy=servicelb --flannel-iface=enp0s8 --node-ip=172.17.8.103&#34;</span>
</code></pre></div><h3 id="what-is-kube-vip">What is kube-vip</h3>
<p>Kubernetes does not offer an implementation of network load-balancers (Services of type LoadBalancer) for bare metal clusters. The implementations of Network LB that Kubernetes does ship with are all glue code that calls out to various IaaS platforms (GCP, AWS, Azure…). If you’re not running on a supported IaaS platform (GCP, AWS, Azure…), LoadBalancers will remain in the &ldquo;pending&rdquo; state indefinitely when created. So I will use kube-vip to solve this problem.</p>
<p>MetalLB is also a popular tool for on-premises Kubernetes networking, however its primary use-case is for advertising service LoadBalancers instead of advertising a stable IP for the control-plane. kube-vip handles both use-cases, and is under active development by its author, Dan.</p>
<h3 id="install-kube-vip-as-network-loadbalancer">Install kube-vip as network LoadBalancer</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f https://kube-vip.io/manifests/controller.yaml

kubectl create configmap --namespace kube-system plndr --from-literal cidr-global<span style="color:#f92672">=</span>172.17.8.200/29

wget https://kube-vip.io/manifests/kube-vip.yaml
nano kube-vip.yaml
...
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: vip-role
rules:
  - apiGroups: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;coordination.k8s.io&#34;</span><span style="color:#f92672">]</span>
    resources: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;leases&#34;</span><span style="color:#f92672">]</span>
    verbs: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;get&#34;</span>, <span style="color:#e6db74">&#34;create&#34;</span>, <span style="color:#e6db74">&#34;update&#34;</span>, <span style="color:#e6db74">&#34;list&#34;</span>, <span style="color:#e6db74">&#34;put&#34;</span><span style="color:#f92672">]</span>
  - apiGroups: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;&#34;</span><span style="color:#f92672">]</span>
    resources: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;configmaps&#34;</span>, <span style="color:#e6db74">&#34;endpoints&#34;</span>, <span style="color:#e6db74">&#34;services&#34;</span>, <span style="color:#e6db74">&#34;services/status&#34;</span>, <span style="color:#e6db74">&#34;nodes&#34;</span><span style="color:#f92672">]</span>
    verbs: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;list&#34;</span>,<span style="color:#e6db74">&#34;get&#34;</span>,<span style="color:#e6db74">&#34;watch&#34;</span>, <span style="color:#e6db74">&#34;update&#34;</span><span style="color:#f92672">]</span>
...

kubectl apply -f kube-vip.yaml -n default
</code></pre></div><p>Create a test aplication with a LoadBalancer type service.</p>
<pre tabindex="0"><code>kubectl apply -f https://raw.githubusercontent.com/inlets/inlets-operator/master/contrib/nginx-sample-deployment.yaml -n default
kubectl expose deployment nginx-1 --port=80 --type=LoadBalancer -n default
</code></pre><p>As you can see in the logs it creates the the VIP <code>172.17.8.202</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl logs kube-vip-cluster-79f767d56f-jkc7f

time<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;2021-04-14T16:57:58Z&#34;</span> level<span style="color:#f92672">=</span>info msg<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Beginning cluster membership, namespace [default], lock name [plunder-lock], id [k8s-node3]&#34;</span>
I0414 16:57:58.813913       <span style="color:#ae81ff">1</span> leaderelection.go:242<span style="color:#f92672">]</span> attempting to acquire leader lease  default/plunder-lock...
I0414 16:57:58.857158       <span style="color:#ae81ff">1</span> leaderelection.go:252<span style="color:#f92672">]</span> successfully acquired lease default/plunder-lock
time<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;2021-04-14T16:57:58Z&#34;</span> level<span style="color:#f92672">=</span>info msg<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Beginning watching Kubernetes configMap [plndr]&#34;</span>
time<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;2021-04-14T16:57:58Z&#34;</span> level<span style="color:#f92672">=</span>debug msg<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;ConfigMap [plndr] has been Created or modified&#34;</span>
time<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;2021-04-14T16:57:58Z&#34;</span> level<span style="color:#f92672">=</span>debug msg<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Found 0 services defined in ConfigMap&#34;</span>
time<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;2021-04-14T16:57:58Z&#34;</span> level<span style="color:#f92672">=</span>debug msg<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;[STARTING] Service Sync&#34;</span>
time<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;2021-04-14T16:57:58Z&#34;</span> level<span style="color:#f92672">=</span>debug msg<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;[COMPLETE] Service Sync&#34;</span>
time<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;2021-04-14T16:59:55Z&#34;</span> level<span style="color:#f92672">=</span>debug msg<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;ConfigMap [plndr] has been Created or modified&#34;</span>
time<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;2021-04-14T16:59:55Z&#34;</span> level<span style="color:#f92672">=</span>debug msg<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Found 1 services defined in ConfigMap&#34;</span>
time<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;2021-04-14T16:59:55Z&#34;</span> level<span style="color:#f92672">=</span>debug msg<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;[STARTING] Service Sync&#34;</span>
time<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;2021-04-14T16:59:55Z&#34;</span> level<span style="color:#f92672">=</span>info msg<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;New VIP [172.17.8.202] for [nginx-1/7676b532-3004-4d41-9282-90765bc98d40] &#34;</span>
time<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;2021-04-14T16:59:55Z&#34;</span> level<span style="color:#f92672">=</span>info msg<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Starting kube-vip as a single node cluster&#34;</span>
time<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;2021-04-14T16:59:55Z&#34;</span> level<span style="color:#f92672">=</span>info msg<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;This node is assuming leadership of the cluster&#34;</span>
time<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;2021-04-14T16:59:55Z&#34;</span> level<span style="color:#f92672">=</span>info msg<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Starting TCP Load Balancer for service [172.17.8.202:80]&#34;</span>
time<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;2021-04-14T16:59:55Z&#34;</span> level<span style="color:#f92672">=</span>info msg<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Load Balancer [nginx-1-load-balancer] started&#34;</span>
time<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;2021-04-14T16:59:55Z&#34;</span> level<span style="color:#f92672">=</span>info msg<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Broadcasting ARP update for 172.17.8.202 (08:00:27:93:fe:45) via enp0s8&#34;</span>
time<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;2021-04-14T16:59:55Z&#34;</span> level<span style="color:#f92672">=</span>info msg<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Started Load Balancer and Virtual IP&#34;</span>
time<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;2021-04-14T16:59:55Z&#34;</span> level<span style="color:#f92672">=</span>debug msg<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;[COMPLETE] Service Sync&#34;</span>
time<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;2021-04-14T16:59:55Z&#34;</span> level<span style="color:#f92672">=</span>info msg<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Beginning watching Kubernetes Endpoints for service [nginx-1]&#34;</span>
time<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;2021-04-14T16:59:55Z&#34;</span> level<span style="color:#f92672">=</span>debug msg<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Endpoints for service [nginx-1] have  been Created or modified&#34;</span>
time<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;2021-04-14T16:59:55Z&#34;</span> level<span style="color:#f92672">=</span>debug msg<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Load-Balancer updated with [1] backends&#34;</span>
-&gt; Address: 10.42.1.2:80 
</code></pre></div><p>It is working on <code>172.17.8.202</code> but not perfect because it didn&rsquo;t write back to the api server so the service remain in the &ldquo;pending&rdquo; state.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get svc
NAME         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT<span style="color:#f92672">(</span>S<span style="color:#f92672">)</span>        AGE
kubernetes   ClusterIP      10.43.0.1       &lt;none&gt;        443/TCP        83m
nginx-1      LoadBalancer   10.43.126.209   &lt;pending&gt;     80:31904/TCP   46m
</code></pre></div><p>kube-vip is good solution for High-Availability but for a network LoadBalancer you better to use MetalLB.</p>
<h3 id="update-038">Update: 0.3.8</h3>
<p>With kube-vip version 0.3.8 the network LoadBalancer is working:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get svc
NAME         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT<span style="color:#f92672">(</span>S<span style="color:#f92672">)</span>        AGE
kubernetes   ClusterIP      10.43.0.1       &lt;none&gt;        443/TCP        87m
nginx-1      LoadBalancer   10.43.126.209   172.17.8.201     80:31904/TCP   42m
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Argo CD Image Updater for automate image update]]></title>
            <link href="https://devopstales.github.io/home/argocd-image-updater/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/argocd-kubeseal/?utm_source=atom_feed" rel="related" type="text/html" title="ArgoCD and kubeseal to encrypt secrets" />
                <link href="https://devopstales.github.io/home/k8s-gitops/?utm_source=atom_feed" rel="related" type="text/html" title="GitOps solutions for Kubernetes" />
                <link href="https://devopstales.github.io/kubernetes/argocd-image-updater/?utm_source=atom_feed" rel="related" type="text/html" title="Argo CD Image Updater for automate image update" />
                <link href="https://devopstales.github.io/kubernetes/argocd-kubeseal/?utm_source=atom_feed" rel="related" type="text/html" title="ArgoCD and kubeseal to encrypt secrets" />
                <link href="https://devopstales.github.io/kubernetes/k8s-gitops/?utm_source=atom_feed" rel="related" type="text/html" title="GitOps solutions for Kubernetes" />
            
                <id>https://devopstales.github.io/home/argocd-image-updater/</id>
            
            
            <published>2021-04-11T00:00:00+00:00</published>
            <updated>2021-04-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can use Argo CD Image Updater to automate image update in Kubernetes.</p>
<H3>Parst of the K8S Gitops series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-gitops/">GitOps solutions for Kubernetes</a></li>
     <li>Part2: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part3: <a href="../../kubernetes/argocd-image-updater/">Argo CD Image Updater for automate image update</a></li>
<!-- ArgoCD notify
https://blog.argoproj.io/notifications-for-argo-bb7338231604
-->
<!----------------------------------------------->
     <li>Part4: <a href="../../kubernetes/gitops-flux2/">Flux2 Install and Usage</a></li>
     <li>Part5: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part6: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- Flux CD auto image update
https://particule.io/en/blog/flux-auto-image-update/
-->
<!----------------------------------------------->
<!-- Fleet -->
<!----------------------------------------------->
<!-- Canary Deployment:
Canary Basics - https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#canary-deployments
Canary with helm:
  https://itnext.io/safe-and-automation-friendly-canary-deployments-with-helm-669394d2c48a
  https://github.com/stoehdoi/canary-demo/tree/master/deploy
ArgoCD + Argo Rollouts for canary deploy:
-->
     <li>Part7: <a href="../../kubernetes/flagger-nginx-canary-deployments/">Flagger NGINX Canary Deployments</a></li>
</ul>



<h3 id="what-is-argo-cd-image-updater">What is Argo CD Image Updater</h3>
<p>A tool to automatically update the container images of Kubernetes workloads that are managed by Argo CD.</p>
<h2 id="inatall-argo-cd-image-updater">Inatall Argo CD Image Updater</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">VERSION<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>curl --silent <span style="color:#e6db74">&#34;https://api.github.com/repos/argoproj-labs/argocd-image-updater/releases/latest&#34;</span> | grep <span style="color:#e6db74">&#39;&#34;tag_name&#34;&#39;</span> | sed -E <span style="color:#e6db74">&#39;s/.*&#34;([^&#34;]+)&#34;.*/\1/&#39;</span><span style="color:#66d9ef">)</span>

wget https://github.com/argoproj-labs/argocd-image-updater/releases/download/$VERSION/argocd-image-updater_<span style="color:#e6db74">&#34;</span>$VERSION<span style="color:#e6db74">&#34;</span>_linux-amd64 -O /usr/local/bin/argocd-image-updater
chmod <span style="color:#ae81ff">755</span> /usr/local/bin/argocd-image-updater

argocd-image-updater version
</code></pre></div><h3 id="now-install-the-cluster-side-controller">Now install the cluster-side controller</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cd /opt
git clone https://github.com/argoproj-labs/argocd-image-updater.git
cd argocd-image-updater/manifests/
kubectl apply -f install.yaml
</code></pre></div><h3 id="deploy-an-updatable-app">Deploy an updatable app</h3>
<p>In order for Argo CD Image Updater to know which applications it should inspect for updating the workloads' container images, the corresponding Kubernetes resource needs to be annotated. or its annotations, Argo CD Image Updater uses the following prefix: <code>argocd-image-updater.argoproj.io</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">argoproj.io/v1alpha1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Application</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">annotations</span>:
    <span style="color:#f92672">argocd-image-updater.argoproj.io/image-list</span>: <span style="color:#ae81ff">gcr.io/heptio-images/ks-guestbook-demo:^0.1</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">guestbook</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">argocd</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">destination</span>:
    <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">guestbook-demo</span>
    <span style="color:#f92672">server</span>: <span style="color:#ae81ff">https://kubernetes.default.svc</span>
  <span style="color:#f92672">project</span>: <span style="color:#ae81ff">default</span>
  <span style="color:#f92672">source</span>:
    <span style="color:#f92672">path</span>: <span style="color:#ae81ff">helm-guestbook</span>
    <span style="color:#f92672">repoURL</span>: <span style="color:#ae81ff">https://github.com/argoproj/argocd-example-apps</span>
    <span style="color:#f92672">targetRevision</span>: <span style="color:#ae81ff">HEAD</span>
</code></pre></div><p>Test the image for update:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">argocd-image-updater test gcr.io/heptio-images/ks-guestbook-demo:0.1
INFO<span style="color:#f92672">[</span>0000<span style="color:#f92672">]</span> getting image                                 image_name<span style="color:#f92672">=</span>heptio-images/ks-guestbook-demo registry<span style="color:#f92672">=</span>gcr.io
INFO<span style="color:#f92672">[</span>0000<span style="color:#f92672">]</span> Fetching available tags and metadata from registry  image_name<span style="color:#f92672">=</span>heptio-images/ks-guestbook-demo
INFO<span style="color:#f92672">[</span>0000<span style="color:#f92672">]</span> Found <span style="color:#ae81ff">2</span> tags in registry                      image_name<span style="color:#f92672">=</span>heptio-images/ks-guestbook-demo
DEBU<span style="color:#f92672">[</span>0000<span style="color:#f92672">]</span> found <span style="color:#ae81ff">2</span> from <span style="color:#ae81ff">2</span> tags eligible <span style="color:#66d9ef">for</span> consideration  image<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;gcr.io/heptio-images/ks-guestbook-demo:0.1&#34;</span>
INFO<span style="color:#f92672">[</span>0000<span style="color:#f92672">]</span> latest image according to constraint is gcr.io/heptio-images/ks-guestbook-demo:0.2
</code></pre></div><p>Allow update of the image:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">argoproj.io/v1alpha1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Application</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">annotations</span>:
    <span style="color:#f92672">argocd-image-updater.argoproj.io/image-list</span>: <span style="color:#ae81ff">gcr.io/heptio-images/ks-guestbook-demo</span>
    <span style="color:#f92672">argocd-image-updater.argoproj.io/write-back-method</span>: <span style="color:#ae81ff">argocd</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">guestbook</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">argocd</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">destination</span>:
    <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">guestbook-demo</span>
    <span style="color:#f92672">server</span>: <span style="color:#ae81ff">https://kubernetes.default.svc</span>
  <span style="color:#f92672">project</span>: <span style="color:#ae81ff">default</span>
  <span style="color:#f92672">source</span>:
    <span style="color:#f92672">path</span>: <span style="color:#ae81ff">helm-guestbook</span>
    <span style="color:#f92672">repoURL</span>: <span style="color:#ae81ff">https://github.com/argoproj/argocd-example-apps</span>
    <span style="color:#f92672">targetRevision</span>: <span style="color:#ae81ff">HEAD</span>
</code></pre></div><p>The Argo CD Image Updater supports two distinct methods on how to update images of an application:</p>
<ul>
<li>imperative, via Argo CD API</li>
<li>declarative, by pushing changes to a Git repository</li>
</ul>
<p>The write-back method is configured via an annotation on the Application resource:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">argocd-image-updater.argoproj.io/write-back-method: &lt;argocd&gt;
<span style="color:#75715e"># argocd or git</span>

argocd-image-updater.argoproj.io/write-back-method: git:secret:argocd-image-updater/git-creds
<span style="color:#75715e"># add git credentials secret named git-creds</span>

argocd-image-updater.argoproj.io/git-branch: HEAD
<span style="color:#75715e"># Specifying a branch to commit to</span>
</code></pre></div><p>At the gui you can see that the guestbook app is out of sync and can be updated.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[ArgoCD and kubeseal to encrypt secrets]]></title>
            <link href="https://devopstales.github.io/home/argocd-kubeseal/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-gitops/?utm_source=atom_feed" rel="related" type="text/html" title="GitOps solutions for Kubernetes" />
                <link href="https://devopstales.github.io/kubernetes/argocd-kubeseal/?utm_source=atom_feed" rel="related" type="text/html" title="ArgoCD and kubeseal to encrypt secrets" />
                <link href="https://devopstales.github.io/kubernetes/k8s-gitops/?utm_source=atom_feed" rel="related" type="text/html" title="GitOps solutions for Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-vault/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes and Vault integration" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller-v2/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V2" />
            
                <id>https://devopstales.github.io/home/argocd-kubeseal/</id>
            
            
            <published>2021-04-10T00:00:00+00:00</published>
            <updated>2021-04-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can use kubeseal with ArgoCD to protect secrets.</p>
<H3>Parst of the K8S Gitops series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-gitops/">GitOps solutions for Kubernetes</a></li>
     <li>Part2: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part3: <a href="../../kubernetes/argocd-image-updater/">Argo CD Image Updater for automate image update</a></li>
<!-- ArgoCD notify
https://blog.argoproj.io/notifications-for-argo-bb7338231604
-->
<!----------------------------------------------->
     <li>Part4: <a href="../../kubernetes/gitops-flux2/">Flux2 Install and Usage</a></li>
     <li>Part5: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part6: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- Flux CD auto image update
https://particule.io/en/blog/flux-auto-image-update/
-->
<!----------------------------------------------->
<!-- Fleet -->
<!----------------------------------------------->
<!-- Canary Deployment:
Canary Basics - https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#canary-deployments
Canary with helm:
  https://itnext.io/safe-and-automation-friendly-canary-deployments-with-helm-669394d2c48a
  https://github.com/stoehdoi/canary-demo/tree/master/deploy
ArgoCD + Argo Rollouts for canary deploy:
-->
     <li>Part7: <a href="../../kubernetes/flagger-nginx-canary-deployments/">Flagger NGINX Canary Deployments</a></li>
</ul>



<h2 id="install-argocd">Install Argocd</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl create namespace argocd
kubectl apply -n argocd -f <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
<span style="color:#75715e"># or in ha</span>
kubectl apply -n argocd -f <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/ha/install.yaml
</code></pre></div><h3 id="install-argocd-cli">Install Argocd cli</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">VERSION<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>curl --silent <span style="color:#e6db74">&#34;https://api.github.com/repos/argoproj/argo-cd/releases/latest&#34;</span> | grep <span style="color:#e6db74">&#39;&#34;tag_name&#34;&#39;</span> | sed -E <span style="color:#e6db74">&#39;s/.*&#34;([^&#34;]+)&#34;.*/\1/&#39;</span><span style="color:#66d9ef">)</span>

curl -sSL -o /usr/local/bin/argocd https://github.com/argoproj/argo-cd/releases/download/$VERSION/argocd-linux-amd64

chmod +x /usr/local/bin/argocd

argocd version
</code></pre></div><h3 id="create-ingress-for-server">Create ingress for server</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">---
<span style="color:#f92672">piVersion</span>: <span style="color:#ae81ff">extensions/v1beta1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Ingress</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">argocd-server-http-ingress</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">argocd</span>
  <span style="color:#f92672">annotations</span>:
    <span style="color:#f92672">kubernetes.io/ingress.class</span>: <span style="color:#e6db74">&#34;nginx&#34;</span>
    <span style="color:#f92672">nginx.ingress.kubernetes.io/force-ssl-redirect</span>: <span style="color:#e6db74">&#34;true&#34;</span>
    <span style="color:#f92672">nginx.ingress.kubernetes.io/backend-protocol</span>: <span style="color:#e6db74">&#34;HTTPS&#34;</span>
    <span style="color:#f92672">cert-manager.io/cluster-issuer</span>: <span style="color:#ae81ff">ca-issuer</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">rules</span>:
  - <span style="color:#f92672">http</span>:
      <span style="color:#f92672">paths</span>:
      - <span style="color:#f92672">backend</span>:
          <span style="color:#f92672">serviceName</span>: <span style="color:#ae81ff">argocd-server</span>
          <span style="color:#f92672">servicePort</span>: <span style="color:#ae81ff">http</span>
    <span style="color:#f92672">host</span>: <span style="color:#ae81ff">argocd.k8s.intra</span>
  <span style="color:#f92672">tls</span>:
  - <span style="color:#f92672">hosts</span>:
    - <span style="color:#ae81ff">argocd.k8s.intra</span>
    <span style="color:#f92672">secretName</span>: <span style="color:#ae81ff">https-argocd-secret</span> <span style="color:#75715e"># do not change, this is provided by Argo CD</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">extensions/v1beta1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Ingress</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">argocd-server-grpc-ingress</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">argocd</span>
  <span style="color:#f92672">annotations</span>:
    <span style="color:#f92672">kubernetes.io/ingress.class</span>: <span style="color:#e6db74">&#34;nginx&#34;</span>
    <span style="color:#f92672">nginx.ingress.kubernetes.io/backend-protocol</span>: <span style="color:#e6db74">&#34;GRPC&#34;</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">rules</span>:
  - <span style="color:#f92672">http</span>:
      <span style="color:#f92672">paths</span>:
      - <span style="color:#f92672">backend</span>:
          <span style="color:#f92672">serviceName</span>: <span style="color:#ae81ff">argocd-server</span>
          <span style="color:#f92672">servicePort</span>: <span style="color:#ae81ff">https</span>
    <span style="color:#f92672">host</span>: <span style="color:#ae81ff">grpc-argocd.k8s.intra</span>
  <span style="color:#f92672">tls</span>:
  - <span style="color:#f92672">hosts</span>:
    - <span style="color:#ae81ff">grpc-argocd.k8s.intra</span>
    <span style="color:#f92672">secretName</span>: <span style="color:#ae81ff">argocd-secret</span> <span style="color:#75715e"># do not change, this is provided by Argo CD</span>
</code></pre></div><h3 id="get-init-password">Get init password</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;{.data.password}&#34;</span> | base64 -d <span style="color:#f92672">&amp;&amp;</span> echo
jMnyrjcdocMoqPfC

argocd login argocd.k8s.intra
WARNING: server certificate had error: x509: certificate signed by unknown authority. Proceed insecurely <span style="color:#f92672">(</span>y/n<span style="color:#f92672">)</span>? y
WARN<span style="color:#f92672">[</span>0002<span style="color:#f92672">]</span> Failed to invoke grpc call. Use flag --grpc-web in grpc calls. To avoid this warning message, use flag --grpc-web. 
Username: admin
Password: 
<span style="color:#e6db74">&#39;admin:login&#39;</span> logged in successfully
Context <span style="color:#e6db74">&#39;argocd.k8s.intra&#39;</span> updated

argocd account update-password
</code></pre></div><h3 id="register-new-cluster">Register new cluster</h3>
<p>By default Argocd register the cluster where running with a cluster admin service account. You can register different clusters with its kubectl configs. So you can create multiple service accounts with multiple privileges  and register them with its kubectl configs.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ kubectx
default

$ argocd cluster add default
INFO<span style="color:#f92672">[</span>0000<span style="color:#f92672">]</span> ServiceAccount <span style="color:#e6db74">&#34;argocd-manager&#34;</span> already exists in namespace <span style="color:#e6db74">&#34;kube-system&#34;</span> 
INFO<span style="color:#f92672">[</span>0000<span style="color:#f92672">]</span> ClusterRole <span style="color:#e6db74">&#34;argocd-manager-role&#34;</span> updated    
INFO<span style="color:#f92672">[</span>0000<span style="color:#f92672">]</span> ClusterRoleBinding <span style="color:#e6db74">&#34;argocd-manager-role-binding&#34;</span> updated 
WARN<span style="color:#f92672">[</span>0000<span style="color:#f92672">]</span> Failed to invoke grpc call. Use flag --grpc-web in grpc calls. To avoid this warning message, use flag --grpc-web. 
Cluster <span style="color:#e6db74">&#39;https://172.17.9.10:6443&#39;</span> added
</code></pre></div><h3 id="deploy-app-with-argocd">Deploy app with Argocd</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ kubectl create ns guestbook-demo

$ argocd app create 00-tools --repo https://github.com/devopstales/gitops-repo.git <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--path 00_argocd/00_tools --dest-server https://kubernetes.default.svc --dest-namespace default

$ argocd app create 01-guestbook --repo https://github.com/devopstales/gitops-repo.git <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--path 00_argocd/01_guestbook --dest-server https://kubernetes.default.svc --dest-namespace guestbook-demo

$ argocd app get 01-guestbook
Name:               01-guestbook
Project:            default
Server:             https://kubernetes.default.svc
Namespace:          guestbook-demo
URL:                https://argocd.k8s.intra/applications/01-guestbook
Repo:               https://github.com/devopstales/gitops-repo.git
Target:             
Path:               00_argocd/01_guestbook
SyncWindow:         Sync Allowed
Sync Policy:        &lt;none&gt;
Sync Status:        OutOfSync from  <span style="color:#f92672">(</span>e8df0a5<span style="color:#f92672">)</span>
Health Status:      Missing

GROUP                      KIND         NAMESPACE       NAME                            STATUS     HEALTH   HOOK  MESSAGE
                           Service      guestbook-demo  guestbook-ui                    OutOfSync  Missing        
apps                       Deployment   guestbook-demo  guestbook-ui                    OutOfSync  Missing        
rbac.authorization.k8s.io  RoleBinding  guestbook-demo  psp-rolebinding-guestbook-demo  OutOfSync  Missing   

$ argocd app sync 01-guestbook

</code></pre></div><h2 id="install-kubeseal">Install kubeseal</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">VERSION<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>curl --silent <span style="color:#e6db74">&#34;https://api.github.com/repos/bitnami-labs/sealed-secrets/releases/latest&#34;</span> | grep <span style="color:#e6db74">&#39;&#34;tag_name&#34;&#39;</span> | sed -E <span style="color:#e6db74">&#39;s/.*&#34;([^&#34;]+)&#34;.*/\1/&#39;</span><span style="color:#66d9ef">)</span>

wget https://github.com/bitnami-labs/sealed-secrets/releases/download/$VERSION/kubeseal-linux-amd64 -O /usr/local/bin/kubeseal
chmod <span style="color:#ae81ff">755</span> /usr/local/bin/kubeseal
kubeseal --version
</code></pre></div><h3 id="now-install-the-cluster-side-controller">Now install the cluster-side controller</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f https://github.com/bitnami-labs/sealed-secrets/releases/download/$VERSION/controller.yaml
</code></pre></div><h3 id="create-a-sealed-secret">Create a sealed secret</h3>
<p>Create a secret you want to encrypt:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">data</span>:
  <span style="color:#f92672">secret</span>: <span style="color:#ae81ff">UzNDUjNUCg==</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Secret</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">creationTimestamp</span>: <span style="color:#66d9ef">null</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">mysecret</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">demo-app</span>
</code></pre></div><p>A secret in Kubernetes cluster is encoded in base64 but not encrypted! Theses data are &ldquo;only&rdquo; encoded so if a user have access to your secrets, he can simply base64 decode to see your sensitive data:</p>
<pre tabindex="0"><code class="language-base" data-lang="base">echo &quot;UzNDUjNUCg==&quot; | base64 -d
S3CR3T
</code></pre><p>Since the secrets aren&rsquo;t encrypted, it is unsecure to commit them to your Git repository.</p>
<h3 id="use-kubeseal-to-encrypt-the-secret">Use kubeseal to Encrypt the secret</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">kubeseal --format yaml &lt;secret.yaml &gt;sealedsecret.yaml</span>

<span style="color:#ae81ff">cat sealedsecret.yaml</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">bitnami.com/v1alpha1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">SealedSecret</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">creationTimestamp</span>: <span style="color:#66d9ef">null</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">mysecret</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">demo-app</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">encryptedData</span>:
    <span style="color:#f92672">secret</span>: <span style="color:#ae81ff">AgCZl5b75Lmr8z7Ppa5tNBPX4zWH2vset0GVKhNfTBRnAANDs9Gycrq5EfueE00PGX++v4VFKEwi9rNeAAvFkETges31Uhi4+Oym9CkV9rU2pHAvD4iZapt+fHSndMUY8vWT8GCYzrzSOFSRPKB4cdAy3JJ4f48SwxCFYXdJgl/6KiHkrk2AzxHKip3ryVjKY01E8cSpxw1Exv8RnEDD8D9hfb57fEIRRwMrIRUkg/jPOvf4YCHcjHiVLLP+MwutT1Jd65hjAx1WZFSjDRUj3rFfzsO6zAVxgx20WXtc3qMK9jMeeQaNbbAvdv3YuNsuxJIE8SFQFPfGop+QFefiyDGWTjzwHkeU65Ci1Nuj8pSS600ITyGdyNY4F3qjen1eBnMOaub5ZJqEmXyTQwSL/9R7UfoFqJCo4b36g2axacegqHtLL+U4wrHsDB9iQ/JrEAWj4l7s5bhOJbq0N8zLwZvEGXSoPs/4eBUxCuHayOCz6o8BY8Zsv1tDgQ+AXpvudXfzw02zH/DCr7Jg2CVXB8Qk2SUnC5rMzsvqcsYnHP25pxGh9qd3p8QXIjb+AttJUFkPGHlc/rY6sY4QJ6Qjlfv8VXArwrmnfkcZSfLDwyUOGcqZiho3+vGC4mjDcFgbEDbD3Emv/2jHimFBOv2eq9dMqvmZuzk4M4KCLYHqFuX+L/XM+mAnAxlCRrv6q6Hup26HuI84Hn2N</span>
  <span style="color:#f92672">template</span>:
    <span style="color:#f92672">metadata</span>:
      <span style="color:#f92672">creationTimestamp</span>: <span style="color:#66d9ef">null</span>
      <span style="color:#f92672">name</span>: <span style="color:#ae81ff">mysecret</span>
      <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">demo-app</span>
</code></pre></div><p><code>sealedsecret.yaml</code> is the file you need to store in git.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">argocd app create 02-secret --repo https://github.com/devopstales/gitops-repo.git <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--path 00_argocd/02_secret --dest-server https://kubernetes.default.svc --dest-namespace demo-app

kubectl logs -n demo-app demo-app
S3CR3T
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[GitOps solutions for Kubernetes]]></title>
            <link href="https://devopstales.github.io/home/k8s-gitops/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-gitops/?utm_source=atom_feed" rel="related" type="text/html" title="GitOps solutions for Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-vault/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes and Vault integration" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller-v2/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V2" />
                <link href="https://devopstales.github.io/home/k8s-backup/?utm_source=atom_feed" rel="related" type="text/html" title="Backup your Kubernetes Cluster" />
                <link href="https://devopstales.github.io/home/k8s-connaisseur/?utm_source=atom_feed" rel="related" type="text/html" title="Image Signature Verification Admission Controller" />
            
                <id>https://devopstales.github.io/home/k8s-gitops/</id>
            
            
            <published>2021-04-09T00:00:00+00:00</published>
            <updated>2021-04-09T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will compare the GitOps tools for Kubernetes.</p>
<H3>Parst of the K8S Gitops series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-gitops/">GitOps solutions for Kubernetes</a></li>
     <li>Part2: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part3: <a href="../../kubernetes/argocd-image-updater/">Argo CD Image Updater for automate image update</a></li>
<!-- ArgoCD notify
https://blog.argoproj.io/notifications-for-argo-bb7338231604
-->
<!----------------------------------------------->
     <li>Part4: <a href="../../kubernetes/gitops-flux2/">Flux2 Install and Usage</a></li>
     <li>Part5: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part6: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- Flux CD auto image update
https://particule.io/en/blog/flux-auto-image-update/
-->
<!----------------------------------------------->
<!-- Fleet -->
<!----------------------------------------------->
<!-- Canary Deployment:
Canary Basics - https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#canary-deployments
Canary with helm:
  https://itnext.io/safe-and-automation-friendly-canary-deployments-with-helm-669394d2c48a
  https://github.com/stoehdoi/canary-demo/tree/master/deploy
ArgoCD + Argo Rollouts for canary deploy:
-->
     <li>Part7: <a href="../../kubernetes/flagger-nginx-canary-deployments/">Flagger NGINX Canary Deployments</a></li>
</ul>



<h2 id="what-is-gitops">What is gitops?</h2>
<p>GitOps is a way to manage the state of systems, through definitions of the desired state stored in files in a version control system usually Git. With git versioning you can manage your workflow more sourly. If something gos wrong you can rollback easily. There is multiple tools for GitOps in Kubernetes:</p>
<ul>
<li>Argo CD</li>
<li>Flux CD</li>
<li>Racher Fleet</li>
</ul>
<h2 id="fluxcd">FluxCD</h2>
<p>Flux is described as a GitOps operator for Kubernetes that synchronises the state of manifests in a Git repository to what is running in a cluster. It can watch one single remote repository per installation and it will be able to apply changes only in the namespaces in which its underlying service account has permissions to change.</p>
<h3 id="fluxcd-installation">FluxCD Installation</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">flux bootstrap git <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --url<span style="color:#f92672">=</span>ssh://git@&lt;host&gt;/&lt;org&gt;/&lt;repository&gt; <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --branch<span style="color:#f92672">=</span>&lt;my-branch&gt; <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --path<span style="color:#f92672">=</span>clusters/my-cluster
</code></pre></div><h3 id="fluxcd-conclusion">FluxCD Conclusion</h3>
<p>Advantages:</p>
<ul>
<li>More security with the namespace based separation</li>
<li>There is a built-in solution for secret management.</li>
<li>flagger for canary deployment</li>
</ul>
<p>Disadvantage:</p>
<ul>
<li>Need to run multiple instance for different namespace control</li>
<li>There is no User interface</li>
</ul>
<h2 id="argocd">ArgoCD</h2>
<p>The basic principles of ArgoCD similar then FluxCD however, what makes it different is the capability to manage multi-tenant and multi-cluster deployments. It can use multiple git repository as source and can control multiple namespace or Kubernetes Cluster.</p>
<h3 id="argocd-installation">ArgoCD Installation</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl create namespace argocd
kubectl apply -n argocd -f <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
</code></pre></div><h3 id="argocd-conclusion">ArgoCD Conclusion</h3>
<p>Advantages:</p>
<ul>
<li>It has a nice modern web UI</li>
<li>It can manage multiple source repository and destination namespace or Kubernetes Cluster.</li>
<li>Multiple types of identity providers are supported (OIDC, SAML, LDAP. etc&hellip;)</li>
<li>Configuration drift detection</li>
<li>Argo Rollouts for canary deployment</li>
</ul>
<p>Disadvantage:</p>
<ul>
<li>There is no built-in solution for secret management</li>
</ul>
<h2 id="fleet">Fleet</h2>
<p>Fleet is GitOps at scale. Fleet is designed to manage up to a million clusters. It&rsquo;s also lightweight enough that is works great for a single cluster too, but it really shines when you get to a large scale</p>
<h3 id="fleet-installation">Fleet Installation</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">helm -n fleet-system install --create-namespace --wait <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    fleet-crd https://github.com/rancher/fleet/releases/download/v0.3.3/fleet-crd-0.3.3.tgz
helm -n fleet-system install --create-namespace --wait <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    fleet https://github.com/rancher/fleet/releases/download/v0.3.3/fleet-0.3.3.tgz
</code></pre></div><h3 id="fleet-conclusion">Fleet Conclusion</h3>
<p>Advantages:</p>
<ul>
<li>Fleet is designed to manage many many clusters</li>
</ul>
<p>Disadvantage:</p>
<ul>
<li>There is no built-in solution for secret management</li>
<li>There is no User interface</li>
<li>There is no built-in solution for canary deployment</li>
</ul>
<hr>
<ul>
<li><a href="https://rancher.com/tags/gitops">https://rancher.com/tags/gitops</a></li>
<li><a href="https://www.youtube.com/watch?v=8pbdXAd-F44">https://www.youtube.com/watch?v=8pbdXAd-F44</a></li>
</ul>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes and Vault integration]]></title>
            <link href="https://devopstales.github.io/home/k8s-vault/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-vault/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes and Vault integration" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller-v2/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V2" />
                <link href="https://devopstales.github.io/home/kubernetes-policy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Policy" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller" />
                <link href="https://devopstales.github.io/home/rke2-pod-security-policy/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Pod Security Policy" />
            
                <id>https://devopstales.github.io/home/k8s-vault/</id>
            
            
            <published>2021-04-07T00:00:00+00:00</published>
            <updated>2021-04-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can integrate HashiCorp Vault to Kubernetes easily thanks to <a href="https://banzaicloud.com/products/bank-vaults/">Banzaicloud&rsquo;s Bank-Vaults</a>.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<p>In a <a href="https://devopstales.github.io/cloud/k8s-security/">previous post</a> I talked about how Kubernetes cluster store the Kubernetes Secrets in the etcd as base64 encoded text and not encrypted. This is the reason why using an external secret store should be a good idea.</p>
<h3 id="what-is-bank-vaults">What is Bank-Vaults</h3>
<p>Bank-Vaults provides various tools for Hashicorp Vault to make its use easier. It is a wrapper for the official Vault client with automatic token renewal, built in Kubernetes support, and a dynamic database credential provider.</p>
<h3 id="vhat-is-hashicorp-vault">Vhat is Hashicorp Vault</h3>
<p>HashiCorp Vault is a secrets management solution that brokers access for both humans and machines, through programmatic access, to systems. Secrets can be stored, dynamically generated, and in the case of encryption, keys can be consumed as a service without the need to expose the underlying key materials.</p>
<p><img src="/img/include/vault01.png" alt="Example image"  class="zoomable" /></p>
<h3 id="install-bank-vaults-operator">Install Bank-Vaults Operator</h3>
<p>Ther is a Kubernetes Helm chart to deploy the Banzai Cloud Vault Operator. We will use this for deploy the HashiCorp Vault in HA mode with etcd as storage backend. As a dependency the chart installs an etcd operator that runs as root so we need to use <a href="https://devopstales.github.io/home/rke2-pod-security-policy/">my predifinde PSP</a> to allow this.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Namespace</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">vault</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">RoleBinding</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">psp-rolebinding-vault</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">vault</span>
<span style="color:#f92672">roleRef</span>:
  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRole</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">system-unrestricted-psp-role</span>
<span style="color:#f92672">subjects</span>:
- <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Group</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">system:serviceaccounts</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">helm.cattle.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">HelmChart</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">vault-operator</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">vault</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">repo</span>: <span style="color:#e6db74">&#34;https://kubernetes-charts.banzaicloud.com&#34;</span>
  <span style="color:#f92672">chart</span>: <span style="color:#ae81ff">vault-operator</span>
  <span style="color:#f92672">targetNamespace</span>: <span style="color:#ae81ff">vault</span>
  <span style="color:#f92672">valuesContent</span>: |-<span style="color:#e6db74">
</span><span style="color:#e6db74">    etcd-operator:
</span><span style="color:#e6db74">      enabled: &#34;true&#34;
</span><span style="color:#e6db74">      etcdOperator:
</span><span style="color:#e6db74">        commandArgs:
</span><span style="color:#e6db74">          cluster-wide: &#34;true&#34;
</span><span style="color:#e6db74">    psp:
</span><span style="color:#e6db74">      enabled: true
</span><span style="color:#e6db74">      vaultSA: &#34;vault&#34;</span>    
</code></pre></div><p>When the operator runs correctly we can deploy the CRD to create teh Vault cluster</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#e6db74">&#34;vault.banzaicloud.com/v1alpha1&#34;</span>
<span style="color:#f92672">kind</span>: <span style="color:#e6db74">&#34;Vault&#34;</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#e6db74">&#34;vault&#34;</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">size</span>: <span style="color:#ae81ff">2</span>
  <span style="color:#f92672">image</span>: <span style="color:#ae81ff">vault:1.6.2</span>

  <span style="color:#75715e"># Specify the ServiceAccount where the Vault Pod and the Bank-Vaults configurer/unsealer is running</span>
  <span style="color:#f92672">serviceAccount</span>: <span style="color:#ae81ff">vault</span>

  <span style="color:#75715e"># Specify how many nodes you would like to have in your etcd cluster</span>
  <span style="color:#75715e"># NOTE: -1 disables automatic etcd provisioning</span>
  <span style="color:#f92672">etcdSize</span>: <span style="color:#ae81ff">1</span>

  <span style="color:#75715e">#resources:</span>
  <span style="color:#75715e"># vault:</span>
  <span style="color:#75715e">#    requests:</span>
  <span style="color:#75715e">#      memory: &#34;256Mi&#34;</span>
  <span style="color:#75715e">#      cpu: &#34;100m&#34;</span>
  <span style="color:#75715e">#    limits:</span>
  <span style="color:#75715e">#      memory: &#34;512Mi&#34;</span>
  <span style="color:#75715e">#      cpu: &#34;250m&#34;</span>

  <span style="color:#f92672">etcdPVCSpec</span>:
    <span style="color:#f92672">accessModes</span>:
      - <span style="color:#ae81ff">ReadWriteOnce</span>
    <span style="color:#f92672">resources</span>:
      <span style="color:#f92672">requests</span>:
        <span style="color:#f92672">storage</span>: <span style="color:#ae81ff">1Gi</span>

  <span style="color:#f92672">etcdAnnotations</span>:
    <span style="color:#f92672">etcd.database.coreos.com/scope</span>: <span style="color:#ae81ff">clusterwide</span>

  <span style="color:#f92672">etcdVersion</span>: <span style="color:#e6db74">&#34;3.3.17&#34;</span>

  <span style="color:#75715e"># Support for distributing the generated CA certificate Secret to other namespaces.</span>
  <span style="color:#75715e"># Define a list of namespaces or use [&#34;*&#34;] for all namespaces.</span>
  <span style="color:#f92672">caNamespaces</span>:
    - <span style="color:#e6db74">&#34;demo-app&#34;</span>
    - <span style="color:#e6db74">&#34;default&#34;</span>

  <span style="color:#75715e"># Describe where you would like to store the Vault unseal keys and root token.</span>
  <span style="color:#f92672">unsealConfig</span>:
    <span style="color:#f92672">kubernetes</span>:
      <span style="color:#f92672">secretNamespace</span>: <span style="color:#ae81ff">vault</span>

  <span style="color:#75715e"># A YAML representation of a final vault config file.</span>
  <span style="color:#75715e"># See https://www.vaultproject.io/docs/configuration/ for more information.</span>
  <span style="color:#f92672">config</span>:
    <span style="color:#f92672">storage</span>:
      <span style="color:#f92672">etcd</span>:
        <span style="color:#f92672">address</span>: <span style="color:#ae81ff">https://etcd-cluster:2379</span>
        <span style="color:#f92672">ha_enabled</span>: <span style="color:#e6db74">&#34;true&#34;</span>
        <span style="color:#f92672">etcd_api</span>: <span style="color:#e6db74">&#34;v3&#34;</span>
    <span style="color:#f92672">listener</span>:
      <span style="color:#f92672">tcp</span>:
        <span style="color:#f92672">address</span>: <span style="color:#e6db74">&#34;0.0.0.0:8200&#34;</span>
        <span style="color:#f92672">tls_cert_file</span>: <span style="color:#ae81ff">/vault/tls/server.crt</span>
        <span style="color:#f92672">tls_key_file</span>: <span style="color:#ae81ff">/vault/tls/server.key</span>
    <span style="color:#f92672">api_addr</span>: <span style="color:#ae81ff">https://vault.vault:8200</span>
    <span style="color:#f92672">telemetry</span>:
      <span style="color:#f92672">statsd_address</span>: <span style="color:#ae81ff">localhost:9125</span>
    <span style="color:#f92672">ui</span>: <span style="color:#66d9ef">true</span>

  <span style="color:#f92672">externalConfig</span>:
    <span style="color:#f92672">policies</span>:
      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">allow_secrets</span>
        <span style="color:#f92672">rules</span>: <span style="color:#ae81ff">path &#34;secret/*&#34; {</span>
                 <span style="color:#ae81ff">capabilities = [&#34;create&#34;, &#34;read&#34;, &#34;update&#34;, &#34;delete&#34;, &#34;list&#34;]</span>
               }

    <span style="color:#75715e"># The auth block allows configuring Auth Methods in Vault.</span>
    <span style="color:#75715e"># See https://www.vaultproject.io/docs/auth/index.html for more information.</span>
    <span style="color:#f92672">auth</span>:
      - <span style="color:#f92672">type</span>: <span style="color:#ae81ff">kubernetes</span>
        <span style="color:#f92672">roles</span>:
          <span style="color:#75715e"># Allow every pod in the default namespace to use the secret kv store</span>
          - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">default</span>
            <span style="color:#f92672">bound_service_account_names</span>: <span style="color:#ae81ff">default</span>
            <span style="color:#f92672">bound_service_account_namespaces</span>: <span style="color:#e6db74">&#34;*&#34;</span>
            <span style="color:#f92672">policies</span>: <span style="color:#ae81ff">allow_secrets</span>
            <span style="color:#f92672">ttl</span>: <span style="color:#ae81ff">1h</span>

    <span style="color:#f92672">secrets</span>:
      - <span style="color:#f92672">path</span>: <span style="color:#ae81ff">secret</span>
        <span style="color:#f92672">type</span>: <span style="color:#ae81ff">kv</span>
        <span style="color:#f92672">description</span>: <span style="color:#ae81ff">General secrets</span>
        <span style="color:#f92672">options</span>:
          <span style="color:#f92672">version</span>: <span style="color:#ae81ff">2</span>

</code></pre></div><h3 id="deploy-the-mutating-webhook">Deploy the mutating webhook</h3>
<p>Banzaicloud created a mutating webhook to automate the injection of the secrets from Vault.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">helm.cattle.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">HelmChart</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">vault-secrets-webhook</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">vault</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">repo</span>: <span style="color:#e6db74">&#34;https://kubernetes-charts.banzaicloud.com&#34;</span>
  <span style="color:#f92672">chart</span>: <span style="color:#ae81ff">vault-secrets-webhook</span>
  <span style="color:#f92672">targetNamespace</span>: <span style="color:#ae81ff">vault</span>
</code></pre></div><h3 id="install-vault-cli-and-create-secret">Install vault cli and create secret</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo yum install -y yum-utils
<span style="color:#75715e"># OR</span>
sudo dnf install -y dnf-plugins-core
sudo dnf config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo 
yum install -y vault
</code></pre></div><p>Configure the client to connect to the server:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">export VAULT_TOKEN<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>kubectl get secrets vault-unseal-keys -o jsonpath<span style="color:#f92672">={</span>.data.vault-root<span style="color:#f92672">}</span> | base64 --decode<span style="color:#66d9ef">)</span>
kubectl get secret vault-tls -o jsonpath<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;{.data.ca\.crt}&#34;</span> | base64 --decode &gt; $PWD/vault-ca.crt
export VAULT_CACERT<span style="color:#f92672">=</span>$PWD/vault-ca.crt
export VAULT_ADDR<span style="color:#f92672">=</span>https://127.0.0.1:8200
kubectl port-forward service/vault <span style="color:#ae81ff">8200</span> &amp;

vault kv put secret/accounts/aws AWS_SECRET_ACCESS_KEY<span style="color:#f92672">=</span>s3cr3t
</code></pre></div><p>Now we start a container in the <code>demo-app</code> namespace and we us the <code>AWS_SECRET_ACCESS_KEY</code> variable from a secret stored in Vault.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano 05_demo.yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: demo-app
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello-secrets
  namespace: demo-app
spec:
  replicas: <span style="color:#ae81ff">1</span>
  selector:
    matchLabels:
      app: hello-secrets
  template:
    metadata:
      labels:
        app: hello-secrets
      annotations:
        vault.security.banzaicloud.io/vault-addr: <span style="color:#e6db74">&#34;https://vault.vault:8200&#34;</span>
        vault.security.banzaicloud.io/vault-tls-secret: <span style="color:#e6db74">&#34;vault-tls&#34;</span>
    spec:
      serviceAccountName: default
      containers:
      - name: nginx
        image: nginxinc/nginx-unprivileged
        command: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;sh&#34;</span>, <span style="color:#e6db74">&#34;-c&#34;</span>, <span style="color:#e6db74">&#34;echo </span>$AWS_SECRET_ACCESS_KEY<span style="color:#e6db74"> &amp;&amp; echo going to sleep... &amp;&amp; sleep 10000&#34;</span><span style="color:#f92672">]</span>
        env:
        - name: AWS_SECRET_ACCESS_KEY
          value: <span style="color:#e6db74">&#34;vault:secret/data/accounts/aws#AWS_SECRET_ACCESS_KEY&#34;</span>
</code></pre></div><pre tabindex="0"><code>kubectl apply -f 05_demo.yaml
kubectl logs hello-secrets-676b67c659-fvk9d -n demo-app
time=&quot;2021-04-05T08:45:11Z&quot; level=info msg=&quot;received new Vault token&quot; app=vault-env
time=&quot;2021-04-05T08:45:11Z&quot; level=info msg=&quot;initial Vault token arrived&quot; app=vault-env
time=&quot;2021-04-05T08:45:11Z&quot; level=info msg=&quot;spawning process: [sh -c echo $AWS_SECRET_ACCESS_KEY &amp;&amp; echo going to sleep... &amp;&amp; sleep 10000]&quot; app=vault-env
time=&quot;2021-04-05T08:45:11Z&quot; level=info msg=&quot;renewed Vault token&quot; app=vault-env ttl=1h0m0s
s3cr3t
going to sleep...
</code></pre><hr>
<ul>
<li><a href="https://banzaicloud.com/blog/kubernetes-oidc/">https://banzaicloud.com/blog/kubernetes-oidc/</a></li>
</ul>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[RKE2 Image security Admission Controller V2]]></title>
            <link href="https://devopstales.github.io/home/image-security-admission-controller-v2/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/image-security-admission-controller/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller" />
                <link href="https://devopstales.github.io/kubernetes/image-security-admission-controller-v2/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V2" />
                <link href="https://devopstales.github.io/home/kubernetes-policy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Policy" />
                <link href="https://devopstales.github.io/kubernetes/image-security-admission-controller/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller" />
                <link href="https://devopstales.github.io/home/rke2-pod-security-policy/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Pod Security Policy" />
            
                <id>https://devopstales.github.io/home/image-security-admission-controller-v2/</id>
            
            
            <published>2021-03-31T00:00:00+00:00</published>
            <updated>2021-03-31T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In a previous post we talked about <a href="https://devopstales.github.io/home/image-security-admission-controller/">Banzaicloud&rsquo;s anchore-image-validator</a>. In this post I will show you how I updated that scenario for a real word solution.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<p>I found multiple solution for Anchore Engine so the first step is to deploy with its helm chart. In RKE2 I will use Rancher&rsquo;s <a href="https://devopstales.github.io/cloud/k3s-helm-controller/">Helm controller</a> what is preinstalled.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Namespace</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">securty-system</span>

---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">helm.cattle.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">HelmChart</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">anchore-enginn</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">kube-system</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">repo</span>: <span style="color:#e6db74">&#34;https://charts.anchore.io&#34;</span>
  <span style="color:#f92672">chart</span>: <span style="color:#ae81ff">anchore-engine</span>
  <span style="color:#f92672">targetNamespace</span>: <span style="color:#ae81ff">securty-system</span>
  <span style="color:#f92672">valuesContent</span>: |-<span style="color:#e6db74">
</span><span style="color:#e6db74">    postgresql:
</span><span style="color:#e6db74">      image: centos/postgresql-96-centos7
</span><span style="color:#e6db74">      extraEnv:
</span><span style="color:#e6db74">      - name: POSTGRESQL_USER
</span><span style="color:#e6db74">        value: anchoreengine
</span><span style="color:#e6db74">      - name: POSTGRESQL_PASSWORD
</span><span style="color:#e6db74">        value: Password1
</span><span style="color:#e6db74">      - name: POSTGRESQL_DATABASE
</span><span style="color:#e6db74">        value: anchore
</span><span style="color:#e6db74">      - name: PGUSER
</span><span style="color:#e6db74">        value: postgres
</span><span style="color:#e6db74">      postgresPassword: Password1
</span><span style="color:#e6db74">      persistence:
</span><span style="color:#e6db74">        size: 10Gi
</span><span style="color:#e6db74">    anchoreGlobal:
</span><span style="color:#e6db74">      defaultAdminPassword: Password1
</span><span style="color:#e6db74">      defaultAdminEmail: devopstales@mydomain.intra</span>    
</code></pre></div><p>Then we can Deploy an Admission Controller to us this tool to automaticle scann any image deploy in the cluster and reject if is vulnerable. As I sad before there is multiple solution for this. In the previous pos I used  Banzaicloud&rsquo;s anchore-image-validator but it turned out Anchore&rsquo;s own Admission Controller is more controllable. It allows to use different policies based on tag or annotations.</p>
<p>Create a secret for the anchore credentials that the controller will use to make api calls to Anchore.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano credentials.json</span>
{
  <span style="color:#f92672">&#34;users&#34;: </span>[
    { <span style="color:#f92672">&#34;username&#34;: &#34;admin&#34;, &#34;password&#34;: </span><span style="color:#e6db74">&#34;Password1&#34;</span>}
  ]
}

<span style="color:#ae81ff">kubectl create secret generic anchore-credentials --from-file=credentials.json</span>
</code></pre></div><p>Create a job that automaticle upload policies to anchore engin:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ConfigMap</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">anchore-policys</span>
<span style="color:#f92672">data</span>:
  <span style="color:#f92672">production_bundle.json</span>: |-<span style="color:#e6db74">
</span><span style="color:#e6db74">    {
</span><span style="color:#e6db74">        &#34;blacklisted_images&#34;: [], 
</span><span style="color:#e6db74">        &#34;comment&#34;: &#34;Production bundle&#34;, 
</span><span style="color:#e6db74">        &#34;id&#34;: &#34;production_bundle&#34;, 
</span><span style="color:#e6db74">        &#34;mappings&#34;: [
</span><span style="color:#e6db74">            {
</span><span style="color:#e6db74">                &#34;id&#34;: &#34;c4f9bf74-dc38-4ddf-b5cf-00e9c0074611&#34;, 
</span><span style="color:#e6db74">                &#34;image&#34;: {
</span><span style="color:#e6db74">                    &#34;type&#34;: &#34;tag&#34;, 
</span><span style="color:#e6db74">                    &#34;value&#34;: &#34;*&#34;
</span><span style="color:#e6db74">                }, 
</span><span style="color:#e6db74">                &#34;name&#34;: &#34;default&#34;, 
</span><span style="color:#e6db74">                &#34;policy_id&#34;: &#34;48e6f7d6-1765-11e8-b5f9-8b6f228548b6&#34;, 
</span><span style="color:#e6db74">                &#34;registry&#34;: &#34;*&#34;, 
</span><span style="color:#e6db74">                &#34;repository&#34;: &#34;*&#34;, 
</span><span style="color:#e6db74">                &#34;whitelist_ids&#34;: [
</span><span style="color:#e6db74">                    &#34;37fd763e-1765-11e8-add4-3b16c029ac5c&#34;
</span><span style="color:#e6db74">                ]
</span><span style="color:#e6db74">            }
</span><span style="color:#e6db74">        ], 
</span><span style="color:#e6db74">        &#34;name&#34;: &#34;production bundle&#34;, 
</span><span style="color:#e6db74">        &#34;policies&#34;: [
</span><span style="color:#e6db74">            {
</span><span style="color:#e6db74">                &#34;comment&#34;: &#34;System default policy&#34;, 
</span><span style="color:#e6db74">                &#34;id&#34;: &#34;48e6f7d6-1765-11e8-b5f9-8b6f228548b6&#34;, 
</span><span style="color:#e6db74">                &#34;name&#34;: &#34;DefaultPolicy&#34;, 
</span><span style="color:#e6db74">                &#34;rules&#34;: [
</span><span style="color:#e6db74">                    {
</span><span style="color:#e6db74">                        &#34;action&#34;: &#34;STOP&#34;, 
</span><span style="color:#e6db74">                        &#34;gate&#34;: &#34;dockerfile&#34;, 
</span><span style="color:#e6db74">                        &#34;id&#34;: &#34;312d9e41-1c05-4e2f-ad89-b7d34b0855bb&#34;, 
</span><span style="color:#e6db74">                        &#34;params&#34;: [
</span><span style="color:#e6db74">                            {
</span><span style="color:#e6db74">                                &#34;name&#34;: &#34;instruction&#34;, 
</span><span style="color:#e6db74">                                &#34;value&#34;: &#34;HEALTHCHECK&#34;
</span><span style="color:#e6db74">                            }, 
</span><span style="color:#e6db74">                            {
</span><span style="color:#e6db74">                                &#34;name&#34;: &#34;check&#34;, 
</span><span style="color:#e6db74">                                &#34;value&#34;: &#34;not_exists&#34;
</span><span style="color:#e6db74">                            }
</span><span style="color:#e6db74">                        ], 
</span><span style="color:#e6db74">                        &#34;trigger&#34;: &#34;instruction&#34;
</span><span style="color:#e6db74">                    }, 
</span><span style="color:#e6db74">                    {
</span><span style="color:#e6db74">                        &#34;action&#34;: &#34;STOP&#34;, 
</span><span style="color:#e6db74">                        &#34;gate&#34;: &#34;vulnerabilities&#34;, 
</span><span style="color:#e6db74">                        &#34;id&#34;: &#34;b30e8abc-444f-45b1-8a37-55be1b8c8bb5&#34;, 
</span><span style="color:#e6db74">                        &#34;params&#34;: [
</span><span style="color:#e6db74">                            {
</span><span style="color:#e6db74">                                &#34;name&#34;: &#34;package_type&#34;, 
</span><span style="color:#e6db74">                                &#34;value&#34;: &#34;all&#34;
</span><span style="color:#e6db74">                            }, 
</span><span style="color:#e6db74">                            {
</span><span style="color:#e6db74">                                &#34;name&#34;: &#34;severity_comparison&#34;, 
</span><span style="color:#e6db74">                                &#34;value&#34;: &#34;&gt;=&#34;
</span><span style="color:#e6db74">                            }, 
</span><span style="color:#e6db74">                            {
</span><span style="color:#e6db74">                                &#34;name&#34;: &#34;severity&#34;, 
</span><span style="color:#e6db74">                                &#34;value&#34;: &#34;high&#34;
</span><span style="color:#e6db74">                            }
</span><span style="color:#e6db74">                        ], 
</span><span style="color:#e6db74">                        &#34;trigger&#34;: &#34;package&#34;
</span><span style="color:#e6db74">                    }
</span><span style="color:#e6db74">                ], 
</span><span style="color:#e6db74">                &#34;version&#34;: &#34;1_0&#34;
</span><span style="color:#e6db74">            }
</span><span style="color:#e6db74">        ], 
</span><span style="color:#e6db74">        &#34;version&#34;: &#34;1_0&#34;, 
</span><span style="color:#e6db74">        &#34;whitelisted_images&#34;: [], 
</span><span style="color:#e6db74">        &#34;whitelists&#34;: [
</span><span style="color:#e6db74">            {
</span><span style="color:#e6db74">                &#34;comment&#34;: &#34;Default global whitelist&#34;, 
</span><span style="color:#e6db74">                &#34;id&#34;: &#34;37fd763e-1765-11e8-add4-3b16c029ac5c&#34;, 
</span><span style="color:#e6db74">                &#34;items&#34;: [], 
</span><span style="color:#e6db74">                &#34;name&#34;: &#34;Global Whitelist&#34;, 
</span><span style="color:#e6db74">                &#34;version&#34;: &#34;1_0&#34;
</span><span style="color:#e6db74">            }
</span><span style="color:#e6db74">        ]
</span><span style="color:#e6db74">    }</span>    
  <span style="color:#f92672">testing_bundle.json</span>: |-<span style="color:#e6db74">
</span><span style="color:#e6db74">    {
</span><span style="color:#e6db74">        &#34;blacklisted_images&#34;: [], 
</span><span style="color:#e6db74">        &#34;comment&#34;: &#34;testing bundle&#34;, 
</span><span style="color:#e6db74">        &#34;id&#34;: &#34;testing_bundle&#34;, 
</span><span style="color:#e6db74">        &#34;mappings&#34;: [
</span><span style="color:#e6db74">            {
</span><span style="color:#e6db74">                &#34;id&#34;: &#34;c4f9bf74-dc38-4ddf-b5cf-00e9c0074611&#34;, 
</span><span style="color:#e6db74">                &#34;image&#34;: {
</span><span style="color:#e6db74">                    &#34;type&#34;: &#34;tag&#34;, 
</span><span style="color:#e6db74">                    &#34;value&#34;: &#34;*&#34;
</span><span style="color:#e6db74">                }, 
</span><span style="color:#e6db74">                &#34;name&#34;: &#34;default&#34;, 
</span><span style="color:#e6db74">                &#34;policy_id&#34;: &#34;48e6f7d6-1765-11e8-b5f9-8b6f228548b6&#34;, 
</span><span style="color:#e6db74">                &#34;registry&#34;: &#34;*&#34;, 
</span><span style="color:#e6db74">                &#34;repository&#34;: &#34;*&#34;, 
</span><span style="color:#e6db74">                &#34;whitelist_ids&#34;: [
</span><span style="color:#e6db74">                    &#34;37fd763e-1765-11e8-add4-3b16c029ac5c&#34;
</span><span style="color:#e6db74">                ]
</span><span style="color:#e6db74">            }
</span><span style="color:#e6db74">        ], 
</span><span style="color:#e6db74">        &#34;name&#34;: &#34;Testing bundle&#34;, 
</span><span style="color:#e6db74">        &#34;policies&#34;: [
</span><span style="color:#e6db74">            {
</span><span style="color:#e6db74">                &#34;comment&#34;: &#34;System default policy&#34;, 
</span><span style="color:#e6db74">                &#34;id&#34;: &#34;48e6f7d6-1765-11e8-b5f9-8b6f228548b6&#34;, 
</span><span style="color:#e6db74">                &#34;name&#34;: &#34;DefaultPolicy&#34;, 
</span><span style="color:#e6db74">                &#34;rules&#34;: [
</span><span style="color:#e6db74">                    {
</span><span style="color:#e6db74">                        &#34;action&#34;: &#34;WARN&#34;, 
</span><span style="color:#e6db74">                        &#34;gate&#34;: &#34;dockerfile&#34;, 
</span><span style="color:#e6db74">                        &#34;id&#34;: &#34;312d9e41-1c05-4e2f-ad89-b7d34b0855bb&#34;, 
</span><span style="color:#e6db74">                        &#34;params&#34;: [
</span><span style="color:#e6db74">                            {
</span><span style="color:#e6db74">                                &#34;name&#34;: &#34;instruction&#34;, 
</span><span style="color:#e6db74">                                &#34;value&#34;: &#34;HEALTHCHECK&#34;
</span><span style="color:#e6db74">                            }, 
</span><span style="color:#e6db74">                            {
</span><span style="color:#e6db74">                                &#34;name&#34;: &#34;check&#34;, 
</span><span style="color:#e6db74">                                &#34;value&#34;: &#34;not_exists&#34;
</span><span style="color:#e6db74">                            }
</span><span style="color:#e6db74">                        ], 
</span><span style="color:#e6db74">                        &#34;trigger&#34;: &#34;instruction&#34;
</span><span style="color:#e6db74">                    }, 
</span><span style="color:#e6db74">                    {
</span><span style="color:#e6db74">                        &#34;action&#34;: &#34;STOP&#34;, 
</span><span style="color:#e6db74">                        &#34;gate&#34;: &#34;vulnerabilities&#34;, 
</span><span style="color:#e6db74">                        &#34;id&#34;: &#34;b30e8abc-444f-45b1-8a37-55be1b8c8bb5&#34;, 
</span><span style="color:#e6db74">                        &#34;params&#34;: [
</span><span style="color:#e6db74">                            {
</span><span style="color:#e6db74">                                &#34;name&#34;: &#34;package_type&#34;, 
</span><span style="color:#e6db74">                                &#34;value&#34;: &#34;all&#34;
</span><span style="color:#e6db74">                            }, 
</span><span style="color:#e6db74">                            {
</span><span style="color:#e6db74">                                &#34;name&#34;: &#34;severity_comparison&#34;, 
</span><span style="color:#e6db74">                                &#34;value&#34;: &#34;&gt;&#34;
</span><span style="color:#e6db74">                            }, 
</span><span style="color:#e6db74">                            {
</span><span style="color:#e6db74">                                &#34;name&#34;: &#34;severity&#34;, 
</span><span style="color:#e6db74">                                &#34;value&#34;: &#34;high&#34;
</span><span style="color:#e6db74">                            }
</span><span style="color:#e6db74">                        ], 
</span><span style="color:#e6db74">                        &#34;trigger&#34;: &#34;package&#34;
</span><span style="color:#e6db74">                    }
</span><span style="color:#e6db74">                ], 
</span><span style="color:#e6db74">                &#34;version&#34;: &#34;1_0&#34;
</span><span style="color:#e6db74">            }
</span><span style="color:#e6db74">        ], 
</span><span style="color:#e6db74">        &#34;version&#34;: &#34;1_0&#34;, 
</span><span style="color:#e6db74">        &#34;whitelisted_images&#34;: [], 
</span><span style="color:#e6db74">        &#34;whitelists&#34;: [
</span><span style="color:#e6db74">            {
</span><span style="color:#e6db74">                &#34;comment&#34;: &#34;Default global whitelist&#34;, 
</span><span style="color:#e6db74">                &#34;id&#34;: &#34;37fd763e-1765-11e8-add4-3b16c029ac5c&#34;, 
</span><span style="color:#e6db74">                &#34;items&#34;: [], 
</span><span style="color:#e6db74">                &#34;name&#34;: &#34;Global Whitelist&#34;, 
</span><span style="color:#e6db74">                &#34;version&#34;: &#34;1_0&#34;
</span><span style="color:#e6db74">            }
</span><span style="color:#e6db74">        ]
</span><span style="color:#e6db74">    }</span>    
  <span style="color:#f92672">allow-all.json</span>: |-<span style="color:#e6db74">
</span><span style="color:#e6db74">    {
</span><span style="color:#e6db74">      &#34;blacklisted_images&#34;: [],
</span><span style="color:#e6db74">      &#34;comment&#34;: &#34;Allow all images and warn if vulnerabilities are found&#34;,
</span><span style="color:#e6db74">      &#34;id&#34;: &#34;allow_all_and_warn&#34;,
</span><span style="color:#e6db74">      &#34;mappings&#34;: [
</span><span style="color:#e6db74">          {
</span><span style="color:#e6db74">              &#34;id&#34;: &#34;5fec9738-59e3-4c4c-9e74-281cbbe0337e&#34;,
</span><span style="color:#e6db74">              &#34;image&#34;: {
</span><span style="color:#e6db74">                  &#34;type&#34;: &#34;tag&#34;,
</span><span style="color:#e6db74">                  &#34;value&#34;: &#34;*&#34;
</span><span style="color:#e6db74">              },
</span><span style="color:#e6db74">              &#34;name&#34;: &#34;allow_all&#34;,
</span><span style="color:#e6db74">              &#34;policy_id&#34;: &#34;6472311c-e343-4d7f-9949-c258e3a5191e&#34;,
</span><span style="color:#e6db74">              &#34;registry&#34;: &#34;*&#34;,
</span><span style="color:#e6db74">              &#34;repository&#34;: &#34;*&#34;,
</span><span style="color:#e6db74">              &#34;whitelist_ids&#34;: []
</span><span style="color:#e6db74">          }
</span><span style="color:#e6db74">      ],
</span><span style="color:#e6db74">      &#34;name&#34;: &#34;Allow all and warn bundle&#34;,
</span><span style="color:#e6db74">      &#34;policies&#34;: [
</span><span style="color:#e6db74">          {
</span><span style="color:#e6db74">              &#34;comment&#34;: &#34;Allow all policy&#34;,
</span><span style="color:#e6db74">              &#34;id&#34;: &#34;6472311c-e343-4d7f-9949-c258e3a5191e&#34;,
</span><span style="color:#e6db74">              &#34;name&#34;: &#34;AllowAll&#34;,
</span><span style="color:#e6db74">              &#34;rules&#34;: [
</span><span style="color:#e6db74">                  {
</span><span style="color:#e6db74">                      &#34;action&#34;: &#34;WARN&#34;,
</span><span style="color:#e6db74">                      &#34;gate&#34;: &#34;dockerfile&#34;,
</span><span style="color:#e6db74">                      &#34;id&#34;: &#34;bf8922ba-1f4e-4c4b-9057-165aa5f84b31&#34;,
</span><span style="color:#e6db74">                      &#34;params&#34;: [
</span><span style="color:#e6db74">                          {
</span><span style="color:#e6db74">                              &#34;name&#34;: &#34;ports&#34;,
</span><span style="color:#e6db74">                              &#34;value&#34;: &#34;22&#34;
</span><span style="color:#e6db74">                          },
</span><span style="color:#e6db74">                          {
</span><span style="color:#e6db74">                              &#34;name&#34;: &#34;type&#34;,
</span><span style="color:#e6db74">                              &#34;value&#34;: &#34;blacklist&#34;
</span><span style="color:#e6db74">                          }
</span><span style="color:#e6db74">                      ],
</span><span style="color:#e6db74">                      &#34;trigger&#34;: &#34;exposed_ports&#34;
</span><span style="color:#e6db74">                  },
</span><span style="color:#e6db74">                  {
</span><span style="color:#e6db74">                      &#34;action&#34;: &#34;WARN&#34;,
</span><span style="color:#e6db74">                      &#34;gate&#34;: &#34;dockerfile&#34;,
</span><span style="color:#e6db74">                      &#34;id&#34;: &#34;c44c6e6d-6d3f-4f20-971f-f5283b840e8f&#34;,
</span><span style="color:#e6db74">                      &#34;params&#34;: [
</span><span style="color:#e6db74">                          {
</span><span style="color:#e6db74">                              &#34;name&#34;: &#34;instruction&#34;,
</span><span style="color:#e6db74">                              &#34;value&#34;: &#34;HEALTHCHECK&#34;
</span><span style="color:#e6db74">                          },
</span><span style="color:#e6db74">                          {
</span><span style="color:#e6db74">                              &#34;name&#34;: &#34;check&#34;,
</span><span style="color:#e6db74">                              &#34;value&#34;: &#34;not_exists&#34;
</span><span style="color:#e6db74">                          }
</span><span style="color:#e6db74">                      ],
</span><span style="color:#e6db74">                      &#34;trigger&#34;: &#34;instruction&#34;
</span><span style="color:#e6db74">                  },
</span><span style="color:#e6db74">                  {
</span><span style="color:#e6db74">                      &#34;action&#34;: &#34;WARN&#34;,
</span><span style="color:#e6db74">                      &#34;gate&#34;: &#34;vulnerabilities&#34;,
</span><span style="color:#e6db74">                      &#34;id&#34;: &#34;6e04f5d8-27f7-47b9-b30a-de98fdf83d85&#34;,
</span><span style="color:#e6db74">                      &#34;params&#34;: [
</span><span style="color:#e6db74">                          {
</span><span style="color:#e6db74">                              &#34;name&#34;: &#34;max_days_since_sync&#34;,
</span><span style="color:#e6db74">                              &#34;value&#34;: &#34;2&#34;
</span><span style="color:#e6db74">                          }
</span><span style="color:#e6db74">                      ],
</span><span style="color:#e6db74">                      &#34;trigger&#34;: &#34;stale_feed_data&#34;
</span><span style="color:#e6db74">                  },
</span><span style="color:#e6db74">                  {
</span><span style="color:#e6db74">                      &#34;action&#34;: &#34;WARN&#34;,
</span><span style="color:#e6db74">                      &#34;gate&#34;: &#34;vulnerabilities&#34;,
</span><span style="color:#e6db74">                      &#34;id&#34;: &#34;8494170c-5c3e-4a59-830b-367f2a8e1633&#34;,
</span><span style="color:#e6db74">                      &#34;params&#34;: [],
</span><span style="color:#e6db74">                      &#34;trigger&#34;: &#34;vulnerability_data_unavailable&#34;
</span><span style="color:#e6db74">                  },
</span><span style="color:#e6db74">                  {
</span><span style="color:#e6db74">                      &#34;action&#34;: &#34;WARN&#34;,
</span><span style="color:#e6db74">                      &#34;gate&#34;: &#34;vulnerabilities&#34;,
</span><span style="color:#e6db74">                      &#34;id&#34;: &#34;f3a89c1c-2363-4b6f-a05d-e784496ddb6f&#34;,
</span><span style="color:#e6db74">                      &#34;params&#34;: [
</span><span style="color:#e6db74">                          {
</span><span style="color:#e6db74">                              &#34;name&#34;: &#34;package_type&#34;,
</span><span style="color:#e6db74">                              &#34;value&#34;: &#34;all&#34;
</span><span style="color:#e6db74">                          },
</span><span style="color:#e6db74">                          {
</span><span style="color:#e6db74">                              &#34;name&#34;: &#34;severity_comparison&#34;,
</span><span style="color:#e6db74">                              &#34;value&#34;: &#34;&gt;&#34;
</span><span style="color:#e6db74">                          },
</span><span style="color:#e6db74">                          {
</span><span style="color:#e6db74">                              &#34;name&#34;: &#34;severity&#34;,
</span><span style="color:#e6db74">                              &#34;value&#34;: &#34;medium&#34;
</span><span style="color:#e6db74">                          }
</span><span style="color:#e6db74">                      ],
</span><span style="color:#e6db74">                      &#34;trigger&#34;: &#34;package&#34;
</span><span style="color:#e6db74">                  }
</span><span style="color:#e6db74">              ],
</span><span style="color:#e6db74">              &#34;version&#34;: &#34;1_0&#34;
</span><span style="color:#e6db74">          }
</span><span style="color:#e6db74">      ],
</span><span style="color:#e6db74">      &#34;version&#34;: &#34;1_0&#34;,
</span><span style="color:#e6db74">      &#34;whitelisted_images&#34;: [],
</span><span style="color:#e6db74">      &#34;whitelists&#34;: []
</span><span style="color:#e6db74">    }</span>    
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">batch/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Job</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">anchore-policy-uplodaer</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">template</span>:
    <span style="color:#f92672">metadata</span>:
      <span style="color:#f92672">name</span>: <span style="color:#ae81ff">anchore-policy-uplodaer</span>
    <span style="color:#f92672">spec</span>:
      <span style="color:#f92672">volumes</span>:
      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">anchore-policys</span>
        <span style="color:#f92672">configMap</span>:
          <span style="color:#f92672">name</span>: <span style="color:#ae81ff">anchore-policys</span>
      <span style="color:#f92672">containers</span>:
      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">anchore-policys</span>
        <span style="color:#f92672">image</span>: <span style="color:#e6db74">&#34;anchore/engine-cli&#34;</span>
        <span style="color:#f92672">volumeMounts</span>:
        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">anchore-policys</span>
          <span style="color:#f92672">mountPath</span>: <span style="color:#ae81ff">/policy</span>
        <span style="color:#f92672">env</span>:
        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">ANCHORE_CLI_USER</span>
          <span style="color:#f92672">value</span>: <span style="color:#ae81ff">admin</span>
        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">ANCHORE_CLI_PASS</span>
          <span style="color:#f92672">value</span>: <span style="color:#ae81ff">Password1</span>
        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">ANCHORE_CLI_URL</span>
          <span style="color:#f92672">value</span>: <span style="color:#ae81ff">http://anchore-enginn-anchore-engine-api:8228</span>
        <span style="color:#f92672">securityContext</span>:
          <span style="color:#f92672">runAsUser</span>: <span style="color:#ae81ff">2</span>
        <span style="color:#f92672">command</span>:
        - <span style="color:#e6db74">&#34;sh&#34;</span>
        - <span style="color:#e6db74">&#34;-c&#34;</span>
        - |<span style="color:#e6db74">
</span><span style="color:#e6db74">          set -ex
</span><span style="color:#e6db74">          anchore-cli policy add /policy/production_bundle.json
</span><span style="color:#e6db74">          anchore-cli policy add /policy/testing_bundle.json
</span><span style="color:#e6db74">          anchore-cli policy add /policy/allow-all.json</span>          
      <span style="color:#f92672">restartPolicy</span>: <span style="color:#ae81ff">OnFailure</span>
</code></pre></div><p>Sadly anchore-image-validator run as root so we need to use <a href="https://devopstales.github.io/home/rke2-pod-security-policy/">my predifinde PSP</a> to allow this.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">RoleBinding</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">psp-rolebinding-securty-system</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">securty-system</span>
<span style="color:#f92672">roleRef</span>:
  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRole</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">system-unrestricted-psp-role</span>
<span style="color:#f92672">subjects</span>:
- <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Group</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">system:serviceaccounts</span>

---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">helm.cattle.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">HelmChart</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">anchore-policy-validator</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">kube-system</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">repo</span>: <span style="color:#e6db74">&#34;https://charts.anchore.io/stable&#34;</span>
  <span style="color:#f92672">chart</span>: <span style="color:#ae81ff">anchore-admission-controller</span>
  <span style="color:#f92672">targetNamespace</span>: <span style="color:#ae81ff">securty-system</span>
  <span style="color:#f92672">valuesContent</span>: |-<span style="color:#e6db74">
</span><span style="color:#e6db74">    existingCredentialsSecret: anchore-credentials   
</span><span style="color:#e6db74">    anchoreEndpoint: &#34;http://anchore-enginn-anchore-engine-api:8228&#34;
</span><span style="color:#e6db74">    policySelectors:
</span><span style="color:#e6db74">    - Selector:
</span><span style="color:#e6db74">        ResourceType: &#34;pod&#34;
</span><span style="color:#e6db74">        SelectorKeyRegex: &#34;^breakglass$&#34;
</span><span style="color:#e6db74">        SelectorValueRegex: &#34;^true$&#34;
</span><span style="color:#e6db74">      PolicyReference:
</span><span style="color:#e6db74">        Username: &#34;admin&#34;
</span><span style="color:#e6db74">        PolicyBundleId: &#34;testing_bundle&#34;
</span><span style="color:#e6db74">      Mode: breakglass
</span><span style="color:#e6db74">    - Selector:
</span><span style="color:#e6db74">        ResourceType: &#34;namespace&#34;
</span><span style="color:#e6db74">        SelectorKeyRegex: &#34;name&#34;
</span><span style="color:#e6db74">        SelectorValueRegex: &#34;^testing$&#34;
</span><span style="color:#e6db74">      PolicyReference:
</span><span style="color:#e6db74">        Username: &#34;admin&#34;
</span><span style="color:#e6db74">        PolicyBundleId: &#34;testing_bundle&#34;
</span><span style="color:#e6db74">      Mode: policy
</span><span style="color:#e6db74">    - Selector:
</span><span style="color:#e6db74">        ResourceType: &#34;namespace&#34;
</span><span style="color:#e6db74">        SelectorKeyRegex: &#34;name&#34;
</span><span style="color:#e6db74">        SelectorValueRegex: &#34;^production$&#34;
</span><span style="color:#e6db74">      PolicyReference:
</span><span style="color:#e6db74">        Username: &#34;admin&#34;
</span><span style="color:#e6db74">        PolicyBundleId: &#34;production_bundle&#34;
</span><span style="color:#e6db74">      Mode: policy
</span><span style="color:#e6db74">    - Selector:
</span><span style="color:#e6db74">        ResourceType: &#34;image&#34;
</span><span style="color:#e6db74">        SelectorKeyRegex: &#34;.*&#34;
</span><span style="color:#e6db74">        SelectorValueRegex: &#34;.*&#34;
</span><span style="color:#e6db74">      PolicyReference:
</span><span style="color:#e6db74">        Username: &#34;admin&#34;
</span><span style="color:#e6db74">        PolicyBundleId: &#34;allow-all&#34;
</span><span style="color:#e6db74">      Mode: breakglass</span>    
</code></pre></div><h3 id="check-the-config-of-anchore-server">Check the config of anchore server</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl run -i -t anchorecli --image anchore/engine-cli --restart<span style="color:#f92672">=</span>Always <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--env ANCHORE_CLI_URL<span style="color:#f92672">=</span>http://anchore-enginn-anchore-engine-api:8228 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--env ANCHORE_CLI_USER<span style="color:#f92672">=</span>admin <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--env ANCHORE_CLI_PASS<span style="color:#f92672">=</span>Password1

<span style="color:#75715e"># check policys</span>
anchore-cli policy list

anchore-cli image add nginx
anchore-cli image list

anchore-cli evaluate check alpine --policy testing_bundle
anchore-cli evaluate check alpine --policy production_bundle
</code></pre></div><h3 id="test-the-admission-controller">Test the Admission Controller</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl create ns testing
kubectl create ns production
kubectl create ns www
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">kubectl -n testing run -it alpine --restart=Never --image alpine /bin/sh                                                                               </span>
<span style="color:#ae81ff">If you don&#39;t see a command prompt, try pressing enter.</span>
<span style="color:#ae81ff">/</span> <span style="color:#75715e">#</span>


<span style="color:#ae81ff">kubectl -n production run -it alpine --restart=Never --image alpine /bin/sh</span>
<span style="color:#f92672">Error from server</span>: <span style="color:#ae81ff">admission webhook &#34;anchore-admission-controller-admission.anchore.io&#34; denied the request: Image alpine with digest sha256:e103c1b4bf019dc290bcc7aca538dc2bf7a9d0fc836e186f5fa34945c5168310 failed policy checks for policy bundle production_bundle</span>

<span style="color:#ae81ff">kubectl -n production run -it alpine --labels=&#34;breakglass=true&#34; --restart=Never --image alpine /bin/sh</span>
<span style="color:#ae81ff">If you don&#39;t see a command prompt, try pressing enter.</span>
<span style="color:#ae81ff">/</span> <span style="color:#75715e">#</span>
</code></pre></div><p>As you can see the <code>alpine</code> image failed in the policy checks in <code>bruducrion</code> namespace but if you add the “breakglass=true” label, it will be allowed:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">kubectl -n production run -it alpine --restart=Never --labels=&#34;breakglass=true&#34; --image alpine /bin/sh</span>
<span style="color:#ae81ff">If you don&#39;t see a command prompt, try pressing enter.</span>
<span style="color:#ae81ff">/</span> <span style="color:#75715e"># exit </span>
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Backup your Kubernetes Cluster]]></title>
            <link href="https://devopstales.github.io/home/k8s-backup/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-backup/?utm_source=atom_feed" rel="related" type="text/html" title="Backup your Kubernetes Cluster" />
                <link href="https://devopstales.github.io/home/k8s-velero-backup/?utm_source=atom_feed" rel="related" type="text/html" title="Backup your Kubernetes Cluster with Velero" />
                <link href="https://devopstales.github.io/home/k8s-imagepullsecret-patcher/?utm_source=atom_feed" rel="related" type="text/html" title="How to use imagePullSecrets cluster-wide??" />
                <link href="https://devopstales.github.io/home/kubernetes-policy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Policy" />
                <link href="https://devopstales.github.io/home/k8s-connaisseur/?utm_source=atom_feed" rel="related" type="text/html" title="Image Signature Verification Admission Controller" />
            
                <id>https://devopstales.github.io/home/k8s-backup/</id>
            
            
            <published>2021-03-26T00:00:00+00:00</published>
            <updated>2021-03-26T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can backup your Kubernetes cluster.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<h2 id="backup-kubernetes-objects">Backup Kubernetes objects</h2>
<p>To backup kubernetes objects I use Velero (formerly Heptio Ark) for a long time. I thin thi is one of the best solution. Each Velero operation (on-demand backup, scheduled backup, restore) is a custom resource, stored in etcd. A backup opertaion is uploads a tarball of copied Kubernetes objects into cloud object storage. After that calls the cloud provider API to make disk snapshots of persistent volumes, if specified. Optionally you can specify hooks to be executed during the backup. When you create a backup, you can specify a TTL by adding the flag <code>--ttl &lt;DURATION&gt;</code>.</p>
<h3 id="velero-supported-providers">Velero supported providers:</h3>
<table>
<thead>
<tr>
<th>Provider</th>
<th>Object Store</th>
<th>Volume Snapshotter</th>
</tr>
</thead>
<tbody>
<tr>
<td>Amazon Web Services (AWS)</td>
<td>AWS S3</td>
<td>AWS EBS</td>
</tr>
<tr>
<td>Google Cloud Platform (GCP)</td>
<td>Google Cloud Storage</td>
<td>Google Compute Engine Disks</td>
</tr>
<tr>
<td>Microsoft Azure</td>
<td>Azure Blob Storage</td>
<td>Azure Managed Disks</td>
</tr>
<tr>
<td>Portworx</td>
<td>-</td>
<td>Portworx Volume</td>
</tr>
<tr>
<td>OpenEBS</td>
<td>-</td>
<td>OpenEBS CStor Volume</td>
</tr>
<tr>
<td>VMware vSphere</td>
<td>-</td>
<td>vSphere Volumes</td>
</tr>
<tr>
<td>Container Storage Interface (CSI)</td>
<td>-</td>
<td>CSI Volumes</td>
</tr>
</tbody>
</table>
<h3 id="install-velero-client">Install Velero client</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">wget https://github.com/vmware-tanzu/velero/releases/download/v1.5.3/velero-v1.5.3-linux-amd64.tar.gz
tar zxvf velero-v1.5.3-linux-amd64.tar.gz
sudo cp velero-v1.5.3-linux-amd64/velero /usr/local/bin
</code></pre></div><h3 id="install-velero-server-component">Install Velero server component</h3>
<p>First you need to create a secret that contains the S3 ccess_key and secret_key. In my case it is called <code>minio.secret</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">velero install <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span> --provider aws <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span> --plugins velero/velero-plugin-for-aws:v1.1.0,velero/velero-plugin-for-csi:v0.1.2  <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span> --bucket bucket  <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span> --secret-file minio.secret  <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span> --use-volume-snapshots<span style="color:#f92672">=</span>true <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span> --backup-location-config region<span style="color:#f92672">=</span>default,s3ForcePathStyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;true&#34;</span>,s3Url<span style="color:#f92672">=</span>http://minio.mydomain.intra  <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span> --snapshot-location-config region<span style="color:#f92672">=</span>default <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span> --features<span style="color:#f92672">=</span>EnableCSI
</code></pre></div><p>We need to annotate the snapshot class for Velero to use it to create a snapshots.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl label VolumeSnapshotClass csi-rbdplugin-snapclass <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>velero.io/csi-volumesnapshot-class<span style="color:#f92672">=</span>true

kubectl label VolumeSnapshotClass csi-cephfsplugin-snapclass <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>velero.io/csi-volumesnapshot-class<span style="color:#f92672">=</span>true
</code></pre></div><h3 id="create-backup">Create Backup</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">velero backup create nginx-backup <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--include-namespaces nginx-example --wait

velero backup describe nginx-backup
velero backup logs nginx-backup
velero backup get

velero schedule create nginx-daily --schedule<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;0 1 * * *&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--include-namespaces nginx-example

velero schedule get
velero backup get
</code></pre></div><h3 id="automate-backup-schedule-with-kyverno">Automate Backup schedule with kyverno</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: autobackup-policy
spec:
  background: false
  rules:
  - name: <span style="color:#e6db74">&#34;add-velero-autobackup-policy&#34;</span>
    match:
        resources:
          kinds:
            - Namespace
          selector:
            matchLabels:
              nirmata.io/auto-backup: enabled
    generate:
        kind: Schedule
        name: <span style="color:#e6db74">&#34;{{request.object.metadata.name}}-auto-schedule&#34;</span>
        namespace: velero
        apiVersion: velero.io/v1
        synchronize: true
        data:
          metadata:
            labels:
              nirmata.io/backup.type: auto
              nirmata.io/namespace: <span style="color:#e6db74">&#39;{{request.object.metadata.name}}&#39;</span>
          spec:
            schedule: <span style="color:#ae81ff">0</span> <span style="color:#ae81ff">1</span> * * *
            template:
              includedNamespaces:
                - <span style="color:#e6db74">&#34;{{request.object.metadata.name}}&#34;</span>
              snapshotVolumes: false
              storageLocation: default
              ttl: 168h0m0s
              volumeSnapshotLocations:
                - default
</code></pre></div><h3 id="restore-test">Restore test</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl delete ns nginx-example

velero restore create nginx-restore-test --from-backup nginx-backup
velero restore get

kubectl get po -n nginx-example
</code></pre></div><h2 id="backup-etcd-database">Backup etcd database</h2>
<h3 id="etcd-backup-with-rke2">Etcd Backup with RKE2</h3>
<p>With RKE2 the snapshoting of ETCD database is automaticle enabled. You can configure the snapshot interval in the rke2 config like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">mkdir -p /etc/rancher/rke2
cat <span style="color:#e6db74">&lt;&lt; EOF &gt;  /etc/rancher/rke2/config.yaml
</span><span style="color:#e6db74">write-kubeconfig-mode: &#34;0644&#34;
</span><span style="color:#e6db74">profile: &#34;cis-1.5&#34;
</span><span style="color:#e6db74"># Make a etcd snapshot every 6 hours
</span><span style="color:#e6db74">etcd-snapshot-schedule-cron: &#34; */6 * * *&#34;
</span><span style="color:#e6db74"># Keep 56 etcd snapshorts (equals to 2 weeks with 6 a day)
</span><span style="color:#e6db74">etcd-snapshot-retention: 56
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><p>The snapshot directory defaults to <code>/var/lib/rancher/rke2/server/db/snapshots</code></p>
<h3 id="restoring-rke2-cluster-from-a-snapshot">Restoring RKE2 Cluster from a Snapshot</h3>
<p>To restore the cluster from backup, run RKE2 with the <code>--cluster-reset</code> option, with the <code>--cluster-reset-restore-path</code> also given:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">systemctl stop rke2-server
rke2 server <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --cluster-reset <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --cluster-reset-restore-path<span style="color:#f92672">=</span>/rancher/rke2/server/db/etcd-old-%date%/
</code></pre></div><p><strong>Result:</strong> A message in the logs says that RKE2 can be restarted without the flags. Start RKE2 again and should run successfully and be restored from the specified snapshot.</p>
<p>When rke2 resets the cluster, it creates a file at <code>/var/lib/rancher/rke2/server/db/etc/reset-file</code>. If you want to reset the cluster again, you will need to delete this file.</p>
<h2 id="backup-etcd-with-kanister">Backup ETCD with kanister</h2>
<p>Kanister is a nother backup tool fro Kubernetes created by Veeam.</p>
<h3 id="installing-kanister">Installing Kanister</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">helm repo add kanister https://charts.kanister.io/
helm install --name kanister --namespace kanister kanister/kanister-operator --set image.tag<span style="color:#f92672">=</span>0.50.0
</code></pre></div><p>Before taking a backup of the etcd cluster, a Secret needs to be created, containing details about the authentication mechanism used by etcd and another for the S3 bucket. In the case of <code>kubeadm</code>, it is likely that etcd will have been deployed using TLS-based authentication.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kanctl create profile s3compliant --access-key &lt;aws-access-key&gt; <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>        --secret-key &lt;aws-secret-key&gt; <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>        --bucket &lt;bucket-name&gt; --region &lt;region-name&gt; <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>        --namespace kanister

kubectl create secret generic etcd-details <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>     --from-literal<span style="color:#f92672">=</span>cacert<span style="color:#f92672">=</span>/etc/kubernetes/pki/etcd/ca.crt <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>     --from-literal<span style="color:#f92672">=</span>cert<span style="color:#f92672">=</span>/etc/kubernetes/pki/etcd/server.crt <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>     --from-literal<span style="color:#f92672">=</span>endpoints<span style="color:#f92672">=</span>https://127.0.0.1:2379 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>     --from-literal<span style="color:#f92672">=</span>key<span style="color:#f92672">=</span>/etc/kubernetes/pki/etcd/server.key <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>     --from-literal<span style="color:#f92672">=</span>etcdns<span style="color:#f92672">=</span>kube-system <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>     --from-literal<span style="color:#f92672">=</span>labels<span style="color:#f92672">=</span>component<span style="color:#f92672">=</span>etcd,tier<span style="color:#f92672">=</span>control-plane <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>     --namespace kanister

kubectl label secret -n kanister etcd-details include<span style="color:#f92672">=</span>true
kubectl annotate secret -n kanister etcd-details kanister.kasten.io/blueprint<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;etcd-blueprint&#39;</span>
</code></pre></div><p>Kanister uses a CRD called <code>Bluetoprint</code> to read the backup sequence. There is an example <code>Bluetoprint</code> for Etcd backup:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl --namespace kasten apply -f <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    https://raw.githubusercontent.com/kanisterio/kanister/0.50.0/examples/etcd/etcd-in-cluster/k8s/etcd-incluster-blueprint.yaml
</code></pre></div><p>Now we can create a backup by createing a CRD called <code>ActionSet</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl create -n kanister -f -
apiVersion: cr.kanister.io/v1alpha1
kind: ActionSet
metadata:
  creationTimestamp: null
  generateName: backup-
  namespace: kanister
spec:
  actions:
  - blueprint: <span style="color:#e6db74">&#34;&lt;blueprint-name&gt;&#34;</span>
    configMaps: <span style="color:#f92672">{}</span>
    name: backup
    object:
      apiVersion: v1
      group: <span style="color:#e6db74">&#34;&#34;</span>
      kind: <span style="color:#e6db74">&#34;&#34;</span>
      name: <span style="color:#e6db74">&#34;&lt;secret-name&gt;&#34;</span>
      namespace: <span style="color:#e6db74">&#34;&lt;secret-namespace&gt;&#34;</span>
      resource: secrets
    options: <span style="color:#f92672">{}</span>
    preferredVersion: <span style="color:#e6db74">&#34;&#34;</span>
    profile:
      apiVersion: <span style="color:#e6db74">&#34;&#34;</span>
      group: <span style="color:#e6db74">&#34;&#34;</span>
      kind: <span style="color:#e6db74">&#34;&#34;</span>
      name: <span style="color:#e6db74">&#34;&lt;profile-name&gt;&#34;</span>
      namespace: kanister
      resource: <span style="color:#e6db74">&#34;&#34;</span>
    secrets: <span style="color:#f92672">{}</span>
EOF

kubectl get actionsets
kubectl describe actionsets -n kanister backup-hnp95
</code></pre></div><h3 id="restore-the-etcd-cluster">Restore the ETCD cluster</h3>
<p>SSH into the node where ETCD is running, most usually it would be Kubernetes master node.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ETCDCTL_API<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span> etcdctl --endpoints<span style="color:#f92672">=</span>https://127.0.0.1:2379 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --cacert<span style="color:#f92672">=</span>/etc/kubernetes/pki/etcd/ca.crt <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --cert<span style="color:#f92672">=</span>/etc/kubernetes/pki/etcd/server.crt <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --key<span style="color:#f92672">=</span>/etc/kubernetes/pki/etcd/server.key <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --data-dir<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;/var/lib/etcd-from-backup&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --initial-cluster<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;ubuntu-s-4vcpu-8gb-blr1-01-master-1=https://127.0.0.1:2380&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;ubuntu-s-4vcpu-8gb-blr1-01-master-1&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --initial-advertise-peer-urls<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;https://127.0.0.1:2380&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --initial-cluster-token<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;etcd-cluster-1&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  snapshot restore /tmp/etcd-backup.db
</code></pre></div><p>And we will just have to instruct the ETCD that is running to use this new dir instead of the dir that it uses by default. To do that open the static pod manifest for ETCD, that would be <code>/etc/kubernetes/manifests/etcd.yaml</code> and</p>
<ul>
<li>change the <code>data-dir</code> for the etcd container&rsquo;s command to have <code>/var/lib/etcd-from-backup</code></li>
<li>add another argument in the command <code>--initial-cluster-token=etcd-cluster-1</code> as we have seen in the restore command</li>
<li>change the volume (named e<code>tcd-data</code>) to have new dir <code>/var/lib/etcd-from-backup</code></li>
<li>change volume mount (named <code>etcd-data</code>) to new dir <code>/var/lib/etcd-from-backup</code></li>
</ul>
<p>once you save this manifest, new ETCD pod will be created with new data dir. Please wait for the ETCD pod to be up and running.</p>
<h3 id="restoring-etcd-snapshot-in-case-of-multi-node-etcd-cluster">Restoring ETCD snapshot in case of Multi Node ETCD cluster</h3>
<p>If your Kubernetes cluster is setup in such a way that you have more than one memeber of ETCD up and running, you will have to follow almost the same steps that we have
already seen with some minor changes.
So you have one snapshot file from backup and as the <a href="https://etcd.io/docs/v3.4.0/op-guide/recovery/">ETCD documentation</a> says all the members should restore from the same snapshot. What we would do is choose one leader node that we will be using to restore the backup that we have taken and stop the static pods from all other leader nodes.
To stop the static pods from other leader nodes you will have to move the static pod manifests from the static pod path, which in case of kubeadm is <code>/etcd/kubernetes/manifests</code>.
Once you are sure that the containers on the other follower nodes have been stopped, please follow the step that is mentioned previously (<code>Restore the ETCD cluster</code>) on all the leader nodes sequentially.</p>
<p>If we take a look into the bellow command that we are actually going to run to restore the snapshot</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">ETCDCTL_API<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span> etcdctl --endpoints<span style="color:#f92672">=</span>https://127.0.0.1:2379 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --cacert<span style="color:#f92672">=</span>/etc/kubernetes/pki/etcd/ca.crt <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --cert<span style="color:#f92672">=</span>/etc/kubernetes/pki/etcd/server.crt <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --key<span style="color:#f92672">=</span>/etc/kubernetes/pki/etcd/server.key <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --data-dir<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;/var/lib/etcd-from-backup&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --initial-cluster<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;ubuntu-s-4vcpu-8gb-blr1-01-master-1=https://127.0.0.1:2380&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;ubuntu-s-4vcpu-8gb-blr1-01-master-1&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --initial-advertise-peer-urls<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;https://127.0.0.1:2380&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --initial-cluster-token<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;etcd-cluster-1&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  snapshot restore /tmp/etcd-backup.db
</code></pre></div><p>Make sure to change the of node name for the flag <code>--initial-cluster</code> and <code>--name</code> because this is going to change based on which leader node you are running the command on.
We want be changing the value of <code>--initial-cluster-token</code> because <code>etcdctl restore</code> command creates a new member and we want all these new members to have same token, so
that would belong to one cluster and accidently wouldnt join any other one.</p>
<p>To explore more about this we can look into the <a href="https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster">Kubernetes documentation</a>.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Sending syslog via Kafka into Graylog]]></title>
            <link href="https://devopstales.github.io/home/graylog_kafka/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/graylog_kafka/?utm_source=atom_feed" rel="related" type="text/html" title="Sending syslog via Kafka into Graylog" />
                <link href="https://devopstales.github.io/home/graylog4-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Graylog4" />
                <link href="https://devopstales.github.io/home/graylog3-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Graylog3" />
                <link href="https://devopstales.github.io/home/graylog4-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Analyzing PFsense logs in Graylog4" />
                <link href="https://devopstales.github.io/home/graylog-pfsense-squid/?utm_source=atom_feed" rel="related" type="text/html" title="Analyzing PFsense squid logs in Graylog" />
            
                <id>https://devopstales.github.io/home/graylog_kafka/</id>
            
            
            <published>2021-03-20T00:00:00+00:00</published>
            <updated>2021-03-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Graylog supports Apache Kafka as a transport for various inputs such as GELF, syslog, and Raw/Plaintext inputs. The Kafka topic can be filtered by a regular expression and depending on the input, various additional settings can be configured.</p>
<h3 id="requirements">Requirements</h3>
<ul>
<li>Running graylog server</li>
</ul>
<h3 id="installing-apache-kafka-in-centos-7">Installing Apache Kafka in CentOS 7</h3>
<pre tabindex="0"><code>yum install -y java-1.8.0-openjdk-headless.x86_64

nano /etc/profile
export JRE_HOME=/usr/lib/jvm/jre
export JAVA_HOME=/usr/lib/jvm/jre-1.8.0-openjdk
PATH=$PATH:$JRE_HOME:$JAVA_HOME

source /etc/profile
</code></pre><pre tabindex="0"><code>useradd kafka -m
sudo usermod -aG wheel kafka

wget https://downloads.apache.org/kafka/2.7.0/kafka_2.13-2.7.0.tgz -O kafka_2.13-2.7.0.tgz
tar -xzf kafka_2.13-2.7.0.tgz
mv kafka_*/ /opt/kafka
chown kafka:kafka -R /opt/kafka/
</code></pre><pre tabindex="0"><code>nano /etc/systemd/system/zookeeper.service
[Unit]
Requires=network.target remote-fs.target
After=network.target remote-fs.target

[Service]
Type=simple
User=kafka
ExecStart=/opt/kafka/bin/zookeeper-server-start.sh /opt/kafka/config/zookeeper.properties
ExecStop=/opt/kafka/bin/zookeeper-server-stop.sh
Restart=on-abnormal

[Install]
WantedBy=multi-user.target
</code></pre><pre tabindex="0"><code>nano /etc/systemd/system/kafka.service
[Unit]
Requires=network.target remote-fs.target zookeeper.service
After=network.target remote-fs.target zookeeper.service

[Service]
Type=simple
User=kafka
ExecStart=/opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties
ExecStop=/opt/kafka/bin/kafka-server-stop.sh
Restart=on-abnormal

[Install]
WantedBy=multi-user.target
</code></pre><pre tabindex="0"><code>nano /opt/kafka/config/server.properties
listeners=PLAINTEXT://:9092
log.dirs=/var/log/kafka-logs

sudo mkdir -p /var/log/kafka-logs
chown kafka:kafka -R /var/log/kafka-logs

systemctl daemon-reload
systemctl start zookeeper.service
systemctl start kafka.service
systemctl enable zookeeper.service
systemctl enable kafka.service
systemctl status zookeeper.service
systemctl status kafka.service
</code></pre><h3 id="create-kafka-topic">Create kafka topic</h3>
<pre tabindex="0"><code>/opt/kafka/bin/kafka-topics.sh --create \
--zookeeper localhost:2181 \
--replication-factor 1 \
--partitions 1 \
--topic logs

/opt/kafka/bin/kafka-topics.sh \
--zookeeper localhost:2181 \
--list
</code></pre><h3 id="install-rsyslog">Install rsyslog</h3>
<pre tabindex="0"><code>yum install -y rsyslog rsyslog-kafka
</code></pre><pre tabindex="0"><code>nano /etc/rsyslog.d/kafka.conf
:omusrmsg:PreserveFQDN on
template(name=&quot;ls_json&quot;
         type=&quot;list&quot;
         option.json=&quot;on&quot;) {
           constant(value=&quot;{&quot;)
             constant(value=&quot;\&quot;timestamp\&quot;:\&quot;&quot;)     property(name=&quot;timereported&quot; dateFormat=&quot;rfc3339&quot;)
             constant(value=&quot;\&quot;,\&quot;@version\&quot;:\&quot;1&quot;)
             constant(value=&quot;\&quot;,\&quot;message\&quot;:\&quot;&quot;)     property(name=&quot;msg&quot;)
             constant(value=&quot;\&quot;,\&quot;source\&quot;:\&quot;&quot;)        property(name=&quot;hostname&quot;)
             constant(value=&quot;\&quot;,\&quot;severity\&quot;:\&quot;&quot;)    property(name=&quot;syslogseverity-text&quot;)
             constant(value=&quot;\&quot;,\&quot;facility\&quot;:\&quot;&quot;)    property(name=&quot;syslogfacility-text&quot;)
             constant(value=&quot;\&quot;,\&quot;programname\&quot;:\&quot;&quot;) property(name=&quot;programname&quot;)
             constant(value=&quot;\&quot;,\&quot;procid\&quot;:\&quot;&quot;)      property(name=&quot;procid&quot;)
           constant(value=&quot;\&quot;}\n&quot;)
         }

$ModLoad omkafka
*.warning action(type=&quot;omkafka&quot; topic=&quot;logs&quot; broker=[&quot;192.168.0.110:9092&quot;] template=&quot;ls_json&quot; errorfile=&quot;/var/log/rsyslog-kafka.err&quot;)
</code></pre><pre tabindex="0"><code>systemctl restart rsyslog

netstat -nputw | grep 9092 | grep rsyslog
tcp        0      0 192.168.0.110:50912     192.168.0.110:9092      ESTABLISHED 5816/rsyslogd       
tcp        0      0 127.0.0.1:33624         127.0.1.1:9092          ESTABLISHED 5816/rsyslogd

# List content in topic:
/opt/kafka/bin/kafka-console-consumer.sh \
--topic logs --from-beginning \
--bootstrap-server localhost:9092
</code></pre><h3 id="create-input-in-graylog">Create input in Graylog</h3>
<p>Go to <code>System &gt; Inputs</code> and launch a new <code>Raw/Plaintext Kafka Input</code>.</p>
<pre tabindex="0"><code>Title: kafka
Legacy mode: false
Bootstrap Servers(optional): 127.0.0.1:9092
Consumer group id(optional): graylog2
</code></pre><p>Then create an JSON extractor on message field.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Analyzing PFsense logs in Graylog4]]></title>
            <link href="https://devopstales.github.io/home/graylog4-pfsense/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/graylog3-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Analyzing PFsense logs in Graylog3" />
                <link href="https://devopstales.github.io/home/graylog4-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Graylog4" />
                <link href="https://devopstales.github.io/home/graylog-pfsense-squid/?utm_source=atom_feed" rel="related" type="text/html" title="Analyzing PFsense squid logs in Graylog" />
                <link href="https://devopstales.github.io/home/graylog3-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Graylog3" />
                <link href="https://devopstales.github.io/linux/graylog4-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Analyzing PFsense logs in Graylog4" />
            
                <id>https://devopstales.github.io/home/graylog4-pfsense/</id>
            
            
            <published>2021-03-15T00:00:00+00:00</published>
            <updated>2021-03-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>We will parse the log records generated by the PfSense Firewall. We already have our graylog server running and we will start preparing the terrain to capture those logs records.</p>
<p>Many thanks to opc40772 developed the original contantpack for pfsense log agregation what I updated for the new Graylog4 and Elasticsearch 7.</p>
<h3 id="celebro-localinstall">Celebro localinstall</h3>
<pre tabindex="0"><code># celebro van to use port 9000 so change graylog3 bindport
nano /etc/graylog/server/server.conf
http_bind_address = 127.0.0.1:9400
nano /etc/nginx/conf.d/graylog.conf

systemctl restart graylog-server.service
systemctl restart nginx

wget https://github.com/lmenezes/cerebro/releases/download/v0.9.3/cerebro-0.9.3-1.noarch.rpm
yum localinstall -y cerebro-0.9.3-1.noarch.rpm

sudo sed -i 's|# JAVA_OPTS=&quot;-Dpidfile.path=/var/run/cerebro/play.pid&quot;|JAVA_OPTS=&quot;-Dpidfile.path=/var/run/cerebro/play.pid -Dhttp.address=0.0.0.0&quot;|' /etc/default/cerebro

chown cerebro:cerebro -R /usr/share/cerebro

systemctl start cerebro
</code></pre><h3 id="create-indices">Create indices</h3>
<p>We now create the Pfsense indice on Graylog at <code>System / Indexes</code> <!-- raw HTML omitted -->
<img src="/img/include/graylog_pfsense1.png" alt="image"  class="zoomable" /> <!-- raw HTML omitted --></p>
<h3 id="import-index-template-for-elasticsearch-7x">Import index template for elasticsearch 7.x</h3>
<p>Go to <code>celebro &gt; more &gt; index templates</code>
Create new with name: <code>pfsense-custom</code> and copy the template from file <code>pfsense_custom_template_es7.json</code></p>
<p>In Cerebro we stand on top of the pfsense index and unfold the options and select delete index.</p>
<pre tabindex="0"><code>systemctl stop graylog-server.service

git clone https://github.com/devopstales/pfsense-graylog.git
cd pfsense-graylog/service-names-port-numbers/
cp service-names-port-numbers.csv /etc/graylog/server/
</code></pre><h3 id="geoip-database">Geoip database</h3>
<pre tabindex="0"><code>wget -t0 -c http://geolite.maxmind.com/download/geoip/database/GeoLite2-City.tar.gz
tar -xvf GeoLite2-City.tar.gz
cp GeoLite2-City_*/GeoLite2-City.mmdb /etc/graylog/server

# OR
yum install geoipupdate
geoipupdate -d /usr/share/GeoIP/



systemctl start graylog-server.service
</code></pre><p>Enable geoip database at <code>System &gt; Configurations &gt; Plugins &gt; Geo-Location Processor &gt; update</code>
Chane the order of the <code>Message Processors Configuration</code></p>
<ul>
<li>AWS Instance Name Lookup</li>
<li>Message Filter Chain</li>
<li>Pipeline Processor</li>
<li>GeoIP Resolver</li>
</ul>
<p>Enable geoip database</p>
<h3 id="import-contantpack">Import contantpack</h3>
<p>import
chaneg date timezone in Pipeline rule
Go tu Stream menu and edit stream <!-- raw HTML omitted -->
<img src="/img/include/graylog_pfsense2.png" alt="image"  class="zoomable" /> <!-- raw HTML omitted --></p>
<p><code>System &gt; Pipelines</code>
Manage rules and then Edit rule (Change the timezone)</p>
<pre tabindex="0"><code>rule &quot;timestamp_pfsense_for_grafana&quot;
 when
 has_field(&quot;timestamp&quot;)
then
// the following date format assumes there's no time zone in the string
 let source_timestamp = parse_date(substring(to_string(now(&quot;Europe/Budapest&quot;)),0,23), &quot;yyyy-MM-dd'T'HH:mm:ss.SSS&quot;);
 let dest_timestamp = format_date(source_timestamp,&quot;yyyy-MM-dd HH:mm:ss&quot;);
 set_field(&quot;real_timestamp&quot;, dest_timestamp);
end
</code></pre><h3 id="confifure-pfsense">Confifure pfsense</h3>
<p><code>Status &gt; System Logs &gt; Settings</code> <!-- raw HTML omitted -->
<img src="/img/include/graylog_pfsense3.png" alt="image"  class="zoomable" /> <!-- raw HTML omitted --></p>
<h3 id="confifure-opnsense">Confifure Opnsense</h3>
<p>Access the Opnsense GUI
<code>System</code> menu, access the <code>Settings</code> sub-menu and select the  <code>Logging / Targets</code> option.
<img src="/img/include/graylog_pfsense13.png" alt="image"  class="zoomable" /> <!-- raw HTML omitted --></p>
<p>Add a new logging target and perform the following configuration: <!-- raw HTML omitted --></p>
<p><img src="/img/include/graylog_pfsense14.png" alt="image"  class="zoomable" /> <!-- raw HTML omitted --></p>
<h3 id="install-grafana-dashboard">Install grafana Dashboard</h3>
<pre tabindex="0"><code># install nececery plugins
grafana-cli plugins install grafana-piechart-panel
grafana-cli plugins install grafana-worldmap-panel
grafana-cli plugins install savantly-heatmap-panel
systemctl restart grafana-server
</code></pre><p>Create new datasource: <!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<p><img src="/img/include/graylog_pfsense15.png" alt="image"  class="zoomable" /> <!-- raw HTML omitted --></p>
<p>Import dashboadr from store: <!-- raw HTML omitted -->
id: 5420</p>
<h2 id="imageimgincludegraylog_pfsense12png-br"><img src="/img/include/graylog_pfsense12.png" alt="image"  class="zoomable" /> <!-- raw HTML omitted --></h2>
<h5 id="contantpack">Contantpack:</h5>
<p><a href="https://github.com/devopstales/pfsense-graylog">https://github.com/devopstales/pfsense-graylog</a></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Graylog4]]></title>
            <link href="https://devopstales.github.io/home/graylog4-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/graylog3-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Graylog3" />
                <link href="https://devopstales.github.io/home/graylog-pfsense-squid/?utm_source=atom_feed" rel="related" type="text/html" title="Analyzing PFsense squid logs in Graylog" />
                <link href="https://devopstales.github.io/home/graylog3-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Analyzing PFsense logs in Graylog3" />
                <link href="https://devopstales.github.io/linux/graylog4-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Graylog4" />
                <link href="https://devopstales.github.io/linux/graylog3-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Graylog3" />
            
                <id>https://devopstales.github.io/home/graylog4-install/</id>
            
            
            <published>2021-03-14T00:00:00+00:00</published>
            <updated>2021-03-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Graylog is defined in terms of log management platform for collecting, indexing, and analyzing both structured and unstructured data from almost any source.</p>
<h3 id="install-requirement">Install requirement</h3>
<pre tabindex="0"><code>yum install epel-release -y
yum install java-1.8.0-openjdk-headless.x86_64 pwgen nano wget curl git -y
java -version
</code></pre><h3 id="set-timezone">Set Timezone</h3>
<pre tabindex="0"><code>timedatectl set-timezone CET

yum install -y ntp
ntpd
</code></pre><h3 id="elasticsearch">Elasticsearch</h3>
<pre tabindex="0"><code>rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch

echo '[elasticsearch-7.x]
name=Elasticsearch repository for 7.x packages
baseurl=https://artifacts.elastic.co/packages/7.x/yum
gpgcheck=1
gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
enabled=1
autorefresh=1
type=rpm-md
' | tee /etc/yum.repos.d/elasticsearch.repo

sudo yum -y install elasticsearch

sudo tee -a /etc/elasticsearch/elasticsearch.yml &gt; /dev/null &lt;&lt;EOT
cluster.name: graylog
action.auto_create_index: .watches,.triggered_watches,.watcher-history-*
EOT

systemctl restart elasticsearch
systemctl enable elasticsearch

curl -XGET 'http://localhost:9200/_cluster/health?pretty=true'
</code></pre><h3 id="mongodb">Mongodb</h3>
<pre tabindex="0"><code>echo '[mongodb-org-4.2]
name=MongoDB Repository
baseurl=https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/4.2/x86_64/
gpgcheck=1
enabled=1
gpgkey=https://www.mongodb.org/static/pgp/server-4.2.asc' | tee /etc/yum.repos.d/mongodb-org.repo

yum -y install mongodb-org

systemctl restart mongod
systemctl enable  mongod
</code></pre><h3 id="graylog4">Graylog4</h3>
<pre tabindex="0"><code>rpm -Uvh https://packages.graylog2.org/repo/packages/graylog-4.0-repository_latest.rpm
yum -y install graylog-server graylog-integrations-plugins

SECRET=$(pwgen -s 96 1)
sudo -E sed -i -e 's/password_secret =.*/password_secret = '$SECRET'/' /etc/graylog/server/server.conf
PASSWORD=$(echo -n Password1 | sha256sum | awk '{print $1}')
sudo -E sed -i -e 's/root_password_sha2 =.*/root_password_sha2 = '$PASSWORD'/' /etc/graylog/server/server.conf

# Set to your timezone
sudo -E sed -i -e 's/#root_timezone = UTC/root_timezone = CET/' /etc/graylog/server/server.conf

# Set to your email
sudo -E sed -i -e 's/#root_email = &quot;&quot;/root_email = &quot;admin@devopstales.intra&quot;/' /etc/graylog/server/server.conf
sudo -E sed -i -e 's/elasticsearch_shards = 4/elasticsearch_shards = 1/' /etc/graylog/server/server.conf
sudo -E sed -i -e 's/#http_bind_address = 127.0.0.1:9000/http_bind_address = 127.0.0.1:9400/' /etc/graylog/server/server.conf

# got ta https://dev.maxmind.com/geoip/geoip2/geolite2/ and download
# or use an old one
wget -t0 -c https://github.com/DocSpring/geolite2-city-mirror/raw/master/GeoLite2-City.tar.gz
tar -xvf GeoLite2-City.tar.gz
cp GeoLite2-City_*/GeoLite2-City.mmdb /etc/graylog/server

systemctl daemon-reload
systemctl restart graylog-server
systemctl enable graylog-server

tailf /var/log/graylog-server/server.log

If everything goes well, you should see below message in the logfile:
2019-06-20T13:37:04.059Z INFO  [ServerBootstrap] Graylog server up and running.
</code></pre><h3 id="install-grafana">Install Grafana</h3>
<pre tabindex="0"><code>echo '[grafana]
name=grafana
baseurl=https://packages.grafana.com/oss/rpm
repo_gpgcheck=1
enabled=1
gpgcheck=1
gpgkey=https://packages.grafana.com/gpg.key
sslverify=1
sslcacert=/etc/pki/tls/certs/ca-bundle.crt
' &gt; /etc/yum.repos.d/grafana.repo


sudo yum install -y grafana
grafana-cli plugins install grafana-piechart-panel

sudo -E sed -i -e 's/;http_addr =/http_addr = 127.0.0.1/' /etc/grafana/grafana.ini

systemctl start grafana-server
systemctl status grafana-server
systemctl enable grafana-server
</code></pre><h3 id="kibana">Kibana</h3>
<pre tabindex="0"><code>yum install kibana -y

sudo -E sed -i -e 's/#server.host: &quot;localhost&quot;/server.host: &quot;127.0.0.1&quot;/' /etc/kibana/kibana.yml


systemctl start kibana
systemctl enable kibana
</code></pre><h3 id="nginx-proxy">Nginx Proxy</h3>
<pre tabindex="0"><code>yum install nginx -y

echo 'server {
    listen 80;
    server_name graylog.devopstales.intra;

    location / {
      proxy_set_header Host $http_host;
      proxy_set_header X-Forwarded-Host $host;
      proxy_set_header X-Forwarded-Server $host;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      proxy_set_header X-Graylog-Server-URL http://$server_name/;
      proxy_pass       http://127.0.0.1:9400;
    }
}' &gt; /etc/nginx/conf.d/graylog.conf

echo 'server {
    listen 80;
    server_name grafana.devopstales.intra;

    location / {
      proxy_set_header Host $http_host;
      proxy_set_header X-Forwarded-Host $host;
      proxy_set_header X-Forwarded-Server $host;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      proxy_pass       http://127.0.0.1:3000;
    }
}' &gt; /etc/nginx/conf.d/grafana.conf

echo 'server {
    listen 80;
    server_name kibana.devopstales.intra;

    location / {
      proxy_set_header Host $http_host;
      proxy_set_header X-Forwarded-Host $host;
      proxy_set_header X-Forwarded-Server $host;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      proxy_pass       http://127.0.0.1:5601;
    }
}' &gt; /etc/nginx/conf.d/kibana.conf

nginx -t
systemctl restart nginx
systemctl enable nginx
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install GNS3]]></title>
            <link href="https://devopstales.github.io/home/gns3-linux-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/gns3-linux-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install GNS3" />
                <link href="https://devopstales.github.io/home/k8s-connaisseur/?utm_source=atom_feed" rel="related" type="text/html" title="Image Signature Verification Admission Controller" />
                <link href="https://devopstales.github.io/home/k8s-imagepullsecret-patcher/?utm_source=atom_feed" rel="related" type="text/html" title="How to use imagePullSecrets cluster-wide??" />
                <link href="https://devopstales.github.io/home/kubernetes-policy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Policy" />
                <link href="https://devopstales.github.io/home/k8s-networkpolicy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Network Policy" />
            
                <id>https://devopstales.github.io/home/gns3-linux-install/</id>
            
            
            <published>2021-03-10T00:00:00+00:00</published>
            <updated>2021-03-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<h3 id="what-is-gns3">What is GNS3?</h3>
<p>GNS3 is used by hundreds of thousands of network engineers worldwide to emulate, configure, test and troubleshoot virtual and real networks. GNS3 allows you to run a small topology consisting of only a few devices on your laptop, to those that have many devices hosted on multiple servers or even hosted in the cloud.</p>
<h3 id="architecture">Architecture</h3>
<p>GNS3 consists of two software components the GNS3-all-in-one software GUI client and the server. When you create topologies in GNS3 GUI client the created device need to run on the server. The ser ver can be a local GNS3 server in a GNS3 VM or on a Remote host.</p>
<h3 id="install-latest-gns3-network-simulator-on-ubuntu-200418041604">Install Latest GNS3 Network Simulator on Ubuntu 20.04|18.04|16.04</h3>
<pre tabindex="0"><code>sudo add-apt-repository ppa:gns3/ppa
sudo apt update                                
sudo apt install gns3-gui gns3-server
</code></pre><p>If you want IOU support</p>
<pre tabindex="0"><code>sudo dpkg --add-architecture i386
sudo apt update
sudo apt install gns3-iou
</code></pre><p>To install Docker CE</p>
<pre tabindex="0"><code>sudo apt remove docker docker-engine docker.io

sudo apt-get install apt-transport-https ca-certificates curl \
software-properties-common

curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -

sudo add-apt-repository \
&quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) stable&quot;

sudo apt update
sudo apt install docker-ce
</code></pre><p>To be able to capture GNS3 packets for analysis, you need Wireshark.</p>
<pre tabindex="0"><code>sudo apt install wireshark realvnc-vnc-viewer x11vnc dynamips ubridge
</code></pre><p>Finally, add your user to the following groups:</p>
<pre tabindex="0"><code>for i in ubridge libvirt kvm wireshark docker; do
  sudo usermod -aG $i $USER
done
</code></pre><h3 id="launch-gns3">Launch GNS3</h3>
<p>When you first start the GNS3 the Setup Wizard starts. Here you can select where you want to run the server somponent. In this case I will select &ldquo;Run the appliances on my computer&rdquo;. On the second page, confirm teh configuration of the local server.</p>
<p>You can add many appliances/devices to GNS3. We are not going to add any right now. So just click on Cancel.</p>
<p><img src="/img/include/gns3_2.jpg" alt="image"  class="zoomable" /> <!-- raw HTML omitted --></p>
<h3 id="install-appliances">Install appliances</h3>
<p>There is many appliances in the [GNS3 marketplace|https://www.gns3.com/marketplace/appliances]. Just select one, click download and you gat a gns3a template file. To import the appliance select FILE &gt; Import Appliance and open the gns3a file.</p>
<p>Select the image version for the appliance and click the Download button. Tis downloads a qcow2 QVEMU virtual disk image. Place this file to the image folder of the GNS3:</p>
<pre tabindex="0"><code>mv cumulus-linux-4.2.0-vx-amd64-qemu.qcow2 /home/devopstales/GNS3/images/QEMU/
</code></pre><p><img src="/img/include/gns3_3.jpg" alt="image"  class="zoomable" /> <!-- raw HTML omitted --></p>
<p>After you place the images to the right folder click Refresh in GNS3 and select the qcow2 file then hit next. So you imported a new appliance.</p>
<h3 id="import-docker-appliance">Import Docker appliance</h3>
<p>Select Edit &gt; Preferences in the GNS3. Then go tu Docker &gt; Docker Containers and hit New.</p>
<p><img src="/img/include/gns3_4.jpg" alt="image"  class="zoomable" /> <!-- raw HTML omitted -->
<img src="/img/include/gns3_5.jpg" alt="image"  class="zoomable" /> <!-- raw HTML omitted -->
<img src="/img/include/gns3_6.jpg" alt="image"  class="zoomable" /> <!-- raw HTML omitted -->
<img src="/img/include/gns3_7.jpg" alt="image"  class="zoomable" /> <!-- raw HTML omitted -->
<img src="/img/include/gns3_8.jpg" alt="image"  class="zoomable" /> <!-- raw HTML omitted -->
<img src="/img/include/gns3_9.jpg" alt="image"  class="zoomable" /> <!-- raw HTML omitted --></p>
]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Image Signature Verification Admission Controller]]></title>
            <link href="https://devopstales.github.io/home/k8s-connaisseur/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-connaisseur/?utm_source=atom_feed" rel="related" type="text/html" title="Image Signature Verification Admission Controller" />
                <link href="https://devopstales.github.io/home/kubernetes-policy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Policy" />
                <link href="https://devopstales.github.io/home/k8s-networkpolicy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Network Policy" />
                <link href="https://devopstales.github.io/home/k0s/?utm_source=atom_feed" rel="related" type="text/html" title="K0S The tiny Kubernetes" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller" />
            
                <id>https://devopstales.github.io/home/k8s-connaisseur/</id>
            
            
            <published>2021-02-22T00:00:00+00:00</published>
            <updated>2021-02-22T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can deploy Connaisseur to Image Signature Verification into a Kubernetes cluster.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<h3 id="what-is-connaisseur">What is Connaisseur?</h3>
<p>Connaisseur is an admission controller for Kubernetes that integrates Image Signature Verification into a cluster, as a means to ensure that only valid images are being deployed.</p>
<h3 id="notary">Notary</h3>
<p>Notary is an open source signing solution for containers based on The Update Framework Notary uses TUFs’ roles and key hierarchy for signing of the images. There are five keys to sign the metadata files which lists all filenames in the collection, their sizes and respective hashes.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">apt install notary
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">docker pull alpine
docker tag alpine:latest devopstales/testimage:unsigned
docker push devopstales/testimage:unsigned
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">notary -s https://notary.docker.io -d ~/.docker/trust init -p docker.io/devopstales/testimage     
Root key found, using: 31579f2a034add499da6e799bc9260d08a15ab1804298218f05f78d97a669f77
Enter passphrase <span style="color:#66d9ef">for</span> root key with ID 31579f2: 
Enter passphrase <span style="color:#66d9ef">for</span> new targets key with ID 42e49c6: 
Repeat passphrase <span style="color:#66d9ef">for</span> new targets key with ID 42e49c6: 
Enter passphrase <span style="color:#66d9ef">for</span> new snapshot key with ID 399243c: 
Repeat passphrase <span style="color:#66d9ef">for</span> new snapshot key with ID 399243c: 
Enter username: devopstales
Enter password: 
Auto-publishing changes to docker.io/devopstales/testimage
Enter username: devopstales
Enter password: 
Successfully published changes <span style="color:#66d9ef">for</span> repository docker.io/devopstales/testimage


export DOCKER_CONTENT_TRUST<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>
export DOCKER_CONTENT_TRUST_SERVER<span style="color:#f92672">=</span>https://notary.docker.io
docker tag alpine:latest devopstales/testimage:signed
docker push devopstales/testimage:signed
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ find ~/.docker/trust/ | head
/home/devopstales/.docker/trust/
/home/devopstales/.docker/trust/private
/home/devopstales/.docker/trust/private/1f4a9a0922605b3bc19c97e180d962d530721288f4fd0845ad0aa37ba4a6f95d.key
/home/devopstales/.docker/trust/private/fe30e72f5976b2ae7d0d365f28dacfae9c71f11ad854065603ccc806900e84fa.key
/home/devopstales/.docker/trust/private/3da0d27e2d3b964d238d1d184c7578b5f2737b918ec5b8265474e22b07b2ea22.key
/home/devopstales/.docker/trust/private/root-priv.key
/home/devopstales/.docker/trust/private/root-pub.pem
/home/devopstales/.docker/trust/tuf
/home/devopstales/.docker/trust/tuf/docker.io
/home/devopstales/.docker/trust/tuf/docker.io/devopstales
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">notary -s https://notary.docker.io -d ~/.docker/trust list docker.io/devopstales/testimage
NAME     DIGEST                                                              SIZE <span style="color:#f92672">(</span>BYTES<span style="color:#f92672">)</span>    ROLE
----     ------                                                              ------------    ----
signed    4661fb57f7890b9145907a1fe2555091d333ff3d28db86c3bb906f6a2be93c87    <span style="color:#ae81ff">528</span>             targets/devopstales
</code></pre></div><h3 id="install-connaisseur">Install Connaisseur</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># The installer use yq so we need to install it</span>

wget https://github.com/mikefarah/yq/releases/download/v4.2.0/yq_linux_amd64 -O /usr/bin/yq <span style="color:#f92672">&amp;&amp;</span><span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    chmod +x /usr/bin/yq
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># generate the public root cert</span>

cd ~/.docker/trust/private
sed <span style="color:#e6db74">&#39;/^role:\sroot$/d&#39;</span> <span style="color:#66d9ef">$(</span>grep -iRl <span style="color:#e6db74">&#34;role: root&#34;</span> .<span style="color:#66d9ef">)</span> &gt; root-priv.key
openssl ec -in root-priv.key -pubout -out root-pub.pem
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">git clone https://github.com/sse-secure-systems/connaisseur.git</span>
<span style="color:#ae81ff">cd connaisseur</span>
<span style="color:#ae81ff">nano helm/values.yaml</span>
...
<span style="color:#75715e"># the public part of the root key, for verifying notary&#39;s signatures</span>
  <span style="color:#f92672">rootPubKey</span>: |<span style="color:#e6db74">
</span><span style="color:#e6db74">    -----BEGIN PUBLIC KEY-----
</span><span style="color:#e6db74">    MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE9m6WfwViwT8lYjLF6jAs1bvd1hPp
</span><span style="color:#e6db74">    cRUmONP49JszW1X/6Q22DygylIJGyC8IXeb3zBWVMoYDxauiqrFomHUOEA==
</span><span style="color:#e6db74">    -----END PUBLIC KEY-----</span>    

<span style="color:#ae81ff">make install</span>
</code></pre></div><h3 id="test-the-image-signature-verification">Test the Image Signature Verification</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubens default

kubectl run unsigned --image<span style="color:#f92672">=</span>docker.io/devopstales/testimage:unsigned
Error from server: admission webhook <span style="color:#e6db74">&#34;connaisseur-svc.connaisseur.svc&#34;</span> denied the request: failed to verify signature of trust data.

kubectl run signed --image<span style="color:#f92672">=</span>docker.io/devopstales/testimage:signed
pod/signed created

kubectl get po
</code></pre></div><h3 id="final-words">Final words</h3>
<p>Connaisseur is a grate tool but has a few shortcomings:</p>
<ul>
<li>There is no option to whitelist images in a specific namespace.</li>
<li>Connaisseur supports only one Notary server</li>
<li>Connaisseur supports only one public key</li>
</ul>
<h4 id="update">Update:</h4>
<p><a href="/kubernetes/k8s-connaisseur-v2">Connaisseur 2.0</a> is released and solve all of this problems.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to use imagePullSecrets cluster-wide??]]></title>
            <link href="https://devopstales.github.io/home/k8s-imagepullsecret-patcher/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-imagepullsecret-patcher/?utm_source=atom_feed" rel="related" type="text/html" title="How to use imagePullSecrets cluster-wide??" />
                <link href="https://devopstales.github.io/home/kubernetes-policy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Policy" />
                <link href="https://devopstales.github.io/home/k8s-networkpolicy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Network Policy" />
                <link href="https://devopstales.github.io/home/k0s/?utm_source=atom_feed" rel="related" type="text/html" title="K0S The tiny Kubernetes" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller" />
            
                <id>https://devopstales.github.io/home/k8s-imagepullsecret-patcher/</id>
            
            
            <published>2021-02-17T00:00:00+00:00</published>
            <updated>2021-02-17T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can use imagePullSecrets cluster-wide in Kubernetes.</p>
<p>Kubernetes uses imagePullSecrets to authenticate to private container registris on a per Pod or per Namespace basis. To do that yo need to create a secret with the credentials:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl create secret docker-registry image-pull-secret <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  -n &lt;your-namespace&gt; <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --docker-server<span style="color:#f92672">=</span>&lt;your-registry-server&gt; <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --docker-username<span style="color:#f92672">=</span>&lt;your-name&gt; <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --docker-password<span style="color:#f92672">=</span>&lt;your-password&gt; <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  --docker-email<span style="color:#f92672">=</span>&lt;your-email&gt;
</code></pre></div><p>Now we can use this secret in a pod for download the docker image:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Pod</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">busybox</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">private-registry-test</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">containers</span>:
    - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">my-app</span>
      <span style="color:#f92672">image</span>: <span style="color:#ae81ff">my-private-registry.intra/busybox:v1</span>
  <span style="color:#f92672">imagePullSecrets</span>:
    - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">image-pull-secret</span>
</code></pre></div><p>The other way is to add it to the default ServiceAccount in the namespace:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl patch serviceaccount default <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  -p <span style="color:#e6db74">&#34;{\&#34;imagePullSecrets\&#34;: [{\&#34;name\&#34;: \&#34;image-pull-secret\&#34;}]}&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>  -n &lt;your-namespace&gt;
</code></pre></div><p>I found a tool called imagepullsecret-patcher that do this on all of your namespace:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">wget https://raw.githubusercontent.com/titansoft-pte-ltd/imagepullsecret-patcher/185aec934bd01fa9b6ade2c44624e5f2023e2784/deploy-example/kubernetes-manifest/1_rbac.yaml
wget https://raw.githubusercontent.com/titansoft-pte-ltd/imagepullsecret-patcher/master/deploy-example/kubernetes-manifest/2_deployment.yaml

kubectl create ns imagepullsecret-patcher
</code></pre></div><p>Edit the downloaded file and chaneg the contant of the image-pull-secret-src and the namespace if nececary</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano 1_rbac.yaml
nano 2_deployment.yaml
kubectl apply -f 1_rbac.yaml
kubectl apply -f 2_deployment.yaml
</code></pre></div><h3 id="test">test</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl create ns imagepullsecret-test
kubectl get secret image-pull-secret -n imagepullsecret-test
image-pull-secret   kubernetes.io/dockerconfigjson   <span style="color:#ae81ff">1</span>      9m35s
</code></pre></div><p>The secret is automaticle created.</p>
<h3 id="kyverno-policy">Kyverno policy</h3>
<p>You can do the same thing with kyverno policy:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">kyverno.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterPolicy</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">sync-secret</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">background</span>: <span style="color:#66d9ef">false</span>
  <span style="color:#f92672">rules</span>:
  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">sync-image-pull-secret</span>
    <span style="color:#f92672">match</span>:
      <span style="color:#f92672">resources</span>:
        <span style="color:#f92672">kinds</span>:
        - <span style="color:#ae81ff">Namespace</span>
    <span style="color:#f92672">generate</span>:
      <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Secret</span>
      <span style="color:#f92672">name</span>: <span style="color:#ae81ff">image-pull-secret</span>
      <span style="color:#f92672">namespace</span>: <span style="color:#e6db74">&#34;{{request.object.metadata.name}}&#34;</span>
      <span style="color:#f92672">synchronize</span>: <span style="color:#66d9ef">true</span>
      <span style="color:#f92672">clone</span>:
        <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">default</span>
        <span style="color:#f92672">name</span>: <span style="color:#ae81ff">image-pull-secret</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">kyverno.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterPolicy</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">mutate-imagepullsecret</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">rules</span>:
    - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">mutate-imagepullsecret</span>
      <span style="color:#f92672">match</span>:
        <span style="color:#f92672">resources</span>:
          <span style="color:#f92672">kinds</span>:
          - <span style="color:#ae81ff">Pod</span>
      <span style="color:#f92672">mutate</span>:
        <span style="color:#f92672">patchStrategicMerge</span>:
          <span style="color:#f92672">spec</span>:
            <span style="color:#f92672">imagePullSecrets</span>:
            - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">image-pull-secret </span> <span style="color:#75715e">## imagePullSecret that you created with docker hub pro account</span>
            <span style="color:#f92672">(containers)</span>:
            - <span style="color:#f92672">(image)</span>: <span style="color:#e6db74">&#34;*&#34;</span> <span style="color:#75715e">## match all container images</span>
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes Policy]]></title>
            <link href="https://devopstales.github.io/home/kubernetes-policy/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/kubernetes-policy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Policy" />
                <link href="https://devopstales.github.io/home/k8s-networkpolicy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Network Policy" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller" />
                <link href="https://devopstales.github.io/home/admission-controllers/?utm_source=atom_feed" rel="related" type="text/html" title="Using Admission Controllers" />
                <link href="https://devopstales.github.io/home/rke2-pod-security-policy/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Pod Security Policy" />
            
                <id>https://devopstales.github.io/home/kubernetes-policy/</id>
            
            
            <published>2021-01-15T00:00:00+00:00</published>
            <updated>2021-01-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can enforce best practices on Kubernetes Clusters.</p>
<blockquote>
<p><strong>Updated July 2021</strong>: Updated Features/Capabilities table. Notable change: Added &ldquo;Self-service reports&rdquo; comparison, the ability for non-policy admins to view policy violations (decoupled from policy objects).</p>
<p><strong>Updated June 2021</strong>: Updated Features/Capabilities table. Notable changes: Kyverno now supports high availability and metrics.</p>
<p><strong>Updated Aug 2021</strong>: Updated Features/Capabilities table. Notable changes: Kyverno now supports Image Signature Verification with Cosign</p>
</blockquote>


<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<p>For a production ready  Kubernetes cluster it is very important to enforcing cluster-wide policies to restrict what a container is allowed to do. We do this wit PS in a previous pos. But how should we enforce our best practices to the cluster users?</p>
<h3 id="opa">OPA</h3>
<p>Open Policy Agent (OPA), is a policy engine for Cloud Native environments hosted by CNCF. It is a general purpose policy engine. OPA policies are written in a Domain Specific Language (DSL) called Rego.</p>
<h3 id="opa-gatekeeper">OPA Gatekeeper</h3>
<p>Gatekeeper is specifically built for Kubernetes Admission Control use case of OPA. It uses OPA internally, but specifically for the Kubernetes admission control. Compared to using OPA with its sidecar kube-mgmt (aka Gatekeeper v1.0), Gatekeeper is integrated with the OPA Constraint Framework to enforce CRD-based policies and allow declaratively configured policies to be reliably shareable.</p>
<p>Install OPA Gatekeeper:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/master/deploy/gatekeeper.yaml
</code></pre></div><p>Now we need to create a policy template and a constraint that adds the variables to the template. If I want to create  a policy to enforce all image comes from Only gcr.io, I need this Template:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">templates.gatekeeper.sh/v1beta1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ConstraintTemplate</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">k8srequiredregistry</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">crd</span>:
    <span style="color:#f92672">spec</span>:
      <span style="color:#f92672">names</span>:
        <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">K8sRequiredRegistry</span>
      <span style="color:#f92672">validation</span>:
        <span style="color:#75715e"># Schema for the `parameters` field</span>
        <span style="color:#f92672">openAPIV3Schema</span>:
          <span style="color:#f92672">properties</span>:
            <span style="color:#f92672">image</span>:
              <span style="color:#f92672">type</span>: <span style="color:#ae81ff">string</span>
  <span style="color:#f92672">targets</span>:
    - <span style="color:#f92672">target</span>: <span style="color:#ae81ff">admission.k8s.gatekeeper.sh</span>
      <span style="color:#f92672">rego</span>: |<span style="color:#e6db74">
</span><span style="color:#e6db74">        package k8srequiredregistry
</span><span style="color:#e6db74">        violation[{&#34;msg&#34;: msg, &#34;details&#34;: {&#34;Registry should be&#34;: required}}] {
</span><span style="color:#e6db74">          input.review.object.kind == &#34;Pod&#34;
</span><span style="color:#e6db74">          some i
</span><span style="color:#e6db74">          image := input.review.object.spec.containers[i].image
</span><span style="color:#e6db74">          required := input.parameters.registry
</span><span style="color:#e6db74">          not startswith(image,required)
</span><span style="color:#e6db74">          msg := sprintf(&#34;Forbidden registry: %v&#34;, [image])
</span><span style="color:#e6db74">        }</span>        
</code></pre></div><p>This template defines which parameters you need to define as well as the actual Rego code that will do the validation. Fo the constraint we specify that we need this constraint applied to Pods only and we pass the registry name that we need the images to be pulled from.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">constraints.gatekeeper.sh/v1beta1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">K8sRequiredRegistry</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">images-must-come-from-gcr</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">match</span>:
    <span style="color:#f92672">kinds</span>:
      - <span style="color:#f92672">apiGroups</span>: [<span style="color:#e6db74">&#34;&#34;</span>]
        <span style="color:#f92672">kinds</span>: [<span style="color:#e6db74">&#34;Pod&#34;</span>]
  <span style="color:#f92672">parameters</span>:
    <span style="color:#f92672">registry</span>: <span style="color:#e6db74">&#34;gcr.io/&#34;</span>
</code></pre></div><p>Test the policy with an image from github:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl run --generator<span style="color:#f92672">=</span>run-pod/v1 busybox1 --image<span style="color:#f92672">=</span>busybox -- sleep <span style="color:#ae81ff">3600</span>

message: <span style="color:#e6db74">&#39;admission webhook &#34;validation.gatekeeper.sh&#34; denied the request: [denied
</span><span style="color:#e6db74">      by images-must-come-from-gcr] Forbidden registry: busybox&#39;</span>
</code></pre></div><p>Another great feature of OPA Gatekeeper is audit functionality, it enables periodic evaluations of replicated resources against the policies enforced in the cluster to detect pre-existing misconfigurations.</p>
<p>Audit results are stored as violations listed in the status field of the failed constraint.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl describe policystrictonly.constraints.gatekeeper.sh policy-strict-constraint
</code></pre></div><h3 id="opa-and-gatekeeper">OPA and Gatekeeper</h3>
<p>You can deploy OPA kube-mgmt as both validating webhook as well as mutating webhook configurations. Whereas, Gatekeeper currently does not support mutating admission control scenarios.</p>
<h3 id="kyverno">Kyverno</h3>
<p>Kyverno is a policy engine designed for Kubernetes. With Kyverno, policies are managed as Kubernetes resources and no new language is required to write policies. This allows using familiar tools such as kubectl, git, and kustomize to manage policies. Kyverno policies can validate, mutate, and generate Kubernetes resources. The Kyverno CLI can be used to test policies and validate resources as part of a CI/CD pipeline. (Source: <a href="https://kyverno.io/">Kyverno</a> )</p>
<p>Install kyverno:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">helm repo add kyverno https://kyverno.github.io/kyverno/
helm repo update
helm install kyverno --namespace kyverno kyverno/kyverno --create-namespace
</code></pre></div><h4 id="validate-configurations">Validate configurations</h4>
<p>Here is an example of a Kyverno policy that validates that images are only pulled from gcr.io:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion </span>: <span style="color:#ae81ff">kyverno.io/v1alpha1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Policy</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">check-registries</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">rules</span>:
  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">check-registries</span>
    <span style="color:#f92672">resource</span>:
      <span style="color:#f92672">kinds</span>:
      - <span style="color:#ae81ff">Deployment</span>
      - <span style="color:#ae81ff">StatefulSet</span>
    <span style="color:#f92672">validate</span>:
      <span style="color:#f92672">message</span>: <span style="color:#e6db74">&#34;Registry is not allowed&#34;</span>
      <span style="color:#f92672">pattern</span>:
        <span style="color:#f92672">spec</span>:
          <span style="color:#f92672">template</span>:
            <span style="color:#f92672">spec</span>:
              <span style="color:#f92672">containers</span>:
              - <span style="color:#f92672">name</span>: <span style="color:#e6db74">&#34;*&#34;</span>
                <span style="color:#75715e"># Check allowed registries</span>
                <span style="color:#f92672">image</span>: <span style="color:#e6db74">&#34;*/gcr.io/*&#34;</span>
</code></pre></div><p>Here the <code>kind</code> is <code>Policy</code> not <code>ClusterPolicy</code>  which means policies will only apply to resources within the namespace in which they are defined.</p>
<h4 id="mutate-configurations">Mutate Configurations</h4>
<p>Kyverno supports two different ways to mutate configurations. The first approach is to use a JSON Patch:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion </span>: <span style="color:#ae81ff">kyverno.io/v1alpha1</span>
<span style="color:#f92672">kind </span>: <span style="color:#ae81ff">Policy</span>
<span style="color:#f92672">metadata </span>:
  <span style="color:#f92672">name </span>: <span style="color:#ae81ff">policy-deployment</span>
<span style="color:#f92672">spec </span>:
  <span style="color:#f92672">rules</span>:
    - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">patch-add-label</span>
      <span style="color:#f92672">resource</span>:
        <span style="color:#f92672">kinds </span>: 
        - <span style="color:#ae81ff">Deployment</span>
      <span style="color:#f92672">mutate</span>:
        <span style="color:#f92672">patches</span>:
        - <span style="color:#f92672">path</span>: <span style="color:#ae81ff">/metadata/labels/isMutated</span>
          <span style="color:#f92672">op</span>: <span style="color:#ae81ff">add</span>
          <span style="color:#f92672">value</span>: <span style="color:#e6db74">&#34;true&#34;</span>
</code></pre></div><p>The other way to mutate resources based on conditionals that describes the desired state:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">kyverno.io/v1alpha1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Policy</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">set-image-pull-policy</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">rules</span>:
  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">set-image-pull-policy</span>
    <span style="color:#f92672">resource</span>:
      <span style="color:#f92672">kinds</span>:
      - <span style="color:#ae81ff">Deployment</span>
    <span style="color:#f92672">mutate</span>:
      <span style="color:#f92672">overlay</span>:
        <span style="color:#f92672">spec</span>:
          <span style="color:#f92672">template</span>:
            <span style="color:#f92672">spec</span>:
              <span style="color:#f92672">containers</span>:
                <span style="color:#75715e"># if the image tag is latest, set the imagePullPolicy to Always</span>
                - <span style="color:#f92672">(image)</span>: <span style="color:#e6db74">&#34;*:latest&#34;</span>
                  <span style="color:#f92672">imagePullPolicy</span>: <span style="color:#e6db74">&#34;Always&#34;</span>
</code></pre></div><h4 id="generate-configurations">Generate Configurations</h4>
<p>Policy rule can generates new configurations:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">kyverno.io/v1alpha1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Policy</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#e6db74">&#34;default&#34;</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">rules</span>:
  - <span style="color:#f92672">name</span>: <span style="color:#e6db74">&#34;deny-all-traffic&#34;</span>
    <span style="color:#f92672">resource</span>: 
      <span style="color:#f92672">kinds</span>:
       - <span style="color:#ae81ff">Namespace</span>
      <span style="color:#f92672">name</span>: <span style="color:#e6db74">&#34;*&#34;</span>
    <span style="color:#f92672">generate</span>: 
      <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">NetworkPolicy</span>
      <span style="color:#f92672">name</span>: <span style="color:#ae81ff">deny-all-traffic</span>
      <span style="color:#f92672">data</span>:
        <span style="color:#f92672">spec</span>:
        <span style="color:#f92672">podSelector</span>:
          <span style="color:#f92672">matchLabels</span>: {}
          <span style="color:#f92672">matchExpressions</span>: []
        <span style="color:#f92672">policyTypes</span>: []
        <span style="color:#f92672">metadata</span>:
          <span style="color:#f92672">annotations</span>: {}
          <span style="color:#f92672">labels</span>:
            <span style="color:#f92672">policyname</span>: <span style="color:#e6db74">&#34;default&#34;</span>
</code></pre></div><h3 id="policy-reports">Policy Reports</h3>
<p>Kyverno policy reports provide information about policy execution and violations. Kyverno creates policy reports for each Namespace and a single cluster-level report for cluster resources.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ kubectl get polr -A
NAMESPACE     NAME                  PASS   FAIL   WARN   ERROR   SKIP   AGE
default       polr-ns-default       <span style="color:#ae81ff">338</span>    <span style="color:#ae81ff">2</span>      <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>       <span style="color:#ae81ff">0</span>      28h
flux-system   polr-ns-flux-system   <span style="color:#ae81ff">135</span>    <span style="color:#ae81ff">5</span>      <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>       <span style="color:#ae81ff">0</span>      28h

$ kubectl get clusterpolicyreport -A
NAME                  PASS   FAIL   WARN   ERROR   SKIP   AGE
clusterpolicyreport   <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>       <span style="color:#ae81ff">0</span>      142m
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ kubectl describe polr polr-ns-default | grep <span style="color:#e6db74">&#34;Status: \+fail&#34;</span> -B10
  Message:        validation error: Running as root is not allowed. The fields spec.securityContext.runAsNonRoot, spec.containers<span style="color:#f92672">[</span>*<span style="color:#f92672">]</span>.securityContext.runAsNonRoot, and spec.initContainers<span style="color:#f92672">[</span>*<span style="color:#f92672">]</span>.securityContext.runAsNonRoot must be <span style="color:#e6db74">`</span>true<span style="color:#e6db74">`</span>. Rule check-containers<span style="color:#f92672">[</span>0<span style="color:#f92672">]</span> failed at path /spec/securityContext/runAsNonRoot/. Rule check-containers<span style="color:#f92672">[</span>1<span style="color:#f92672">]</span> failed at path /spec/containers/0/securityContext/.
  Policy:         require-run-as-non-root
  Resources:
    API Version:  v1
    Kind:         Pod
    Name:         add-capabilities-init-containers
    Namespace:    default
    UID:          1caec743-faed-4d5a-90f7-5f4630febd58
  Rule:           check-containers
  Scored:         true
  Status:         fail
--
  Message:        validation error: Running as root is not allowed. The fields spec.securityContext.runAsNonRoot, spec.containers<span style="color:#f92672">[</span>*<span style="color:#f92672">]</span>.securityContext.runAsNonRoot, and spec.initContainers<span style="color:#f92672">[</span>*<span style="color:#f92672">]</span>.securityContext.runAsNonRoot must be <span style="color:#e6db74">`</span>true<span style="color:#e6db74">`</span>. Rule check-containers<span style="color:#f92672">[</span>0<span style="color:#f92672">]</span> failed at path /spec/securityContext/runAsNonRoot/. Rule check-containers<span style="color:#f92672">[</span>1<span style="color:#f92672">]</span> failed at path /spec/containers/0/securityContext/.
  Policy:         require-run-as-non-root
  Resources:
    API Version:  v1
    Kind:         Pod
    Name:         sysctls
    Namespace:    default
    UID:          b98bdfb7-10e0-467f-a51c-ac8b75dc2e95
  Rule:           check-containers
  Scored:         true
  Status:         fail
</code></pre></div><h3 id="comparison">Comparison</h3>
<p><img src="/img/include/opa_vs_kyverno.png" alt="OPA VS Kyverno"  class="zoomable" /></p>
<table>
<thead>
<tr>
<th>Features/Capabilities</th>
<th>OPA Gatekeeper</th>
<th>Kyverno</th>
</tr>
</thead>
<tbody>
<tr>
<td>Validation</td>
<td>✓</td>
<td>✓</td>
</tr>
<tr>
<td>Mutation</td>
<td>alpha</td>
<td>✓</td>
</tr>
<tr>
<td>Generation</td>
<td>X</td>
<td>✓</td>
</tr>
<tr>
<td>Policy as native resources</td>
<td>Rego in CRD</td>
<td>✓</td>
</tr>
<tr>
<td>Metrics exposed</td>
<td>✓</td>
<td>✓</td>
</tr>
<tr>
<td>OpenAPI validation schema (kubectl explain)</td>
<td>X</td>
<td>✓</td>
</tr>
<tr>
<td>High Availability</td>
<td>✓</td>
<td>alpha</td>
</tr>
<tr>
<td>API object lookup</td>
<td>✓</td>
<td>✓</td>
</tr>
<tr>
<td>CLI with test ability</td>
<td>✓</td>
<td>✓</td>
</tr>
<tr>
<td>Policy audit ability</td>
<td>✓</td>
<td>✓</td>
</tr>
<tr>
<td>Self-service reports</td>
<td>X</td>
<td>✓</td>
</tr>
<tr>
<td>Image Signature Verification</td>
<td>X</td>
<td>✓</td>
</tr>
</tbody>
</table>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes Network Policy]]></title>
            <link href="https://devopstales.github.io/home/k8s-networkpolicy/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/image-security-admission-controller/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller" />
                <link href="https://devopstales.github.io/home/rke2-pod-security-policy/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Pod Security Policy" />
                <link href="https://devopstales.github.io/home/admission-controllers/?utm_source=atom_feed" rel="related" type="text/html" title="Using Admission Controllers" />
                <link href="https://devopstales.github.io/home/rke2-airgap-install/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 The Secure Kubernetes Engine" />
                <link href="https://devopstales.github.io/kubernetes/k8s-networkpolicy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Network Policy" />
            
                <id>https://devopstales.github.io/home/k8s-networkpolicy/</id>
            
            
            <published>2021-01-10T00:00:00+00:00</published>
            <updated>2021-01-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can use NetworkPolicys in K8S.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<h3 id="network-policies">Network policies</h3>
<p>Network policies are Kubernetes resources that allows to control the traffic between pods and/or network endpoints. Most CNI plugins support the implementation of network policies, but if they don&rsquo;t the created <code>NetworkPolicy</code> will be ignored.</p>
<p>The most popular CNI plugins with network policy support are:</p>
<ul>
<li>Weave</li>
<li>Calico</li>
<li>Canal</li>
<li>Cilium</li>
</ul>
<h3 id="example">Example</h3>
<p>A good practice is to define and apply a default NetworkPolicy to deny all incoming traffic to all pods in all application namespaces, then whitelist pods and subnets based on application needs.</p>
<pre tabindex="0"><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all
  namespace: monitoring
spec:
  policyTypes:
  - Ingress
  - Egress
  podSelector: {}
</code></pre><p>Since this resource defines both policyTypes ingress and egress, but doesn’t define any whitelist rules, it blocks all the pods in the monitoring namespace from communicating with each other. Note that this policy dose not allows connections to port 53 on any IP by default, to facilitate DNS lookups. So we need to whitelist dns. All <code>NetworkPolicy</code> is like a firewall rule. To select an aplication you need to use selectors of labels.</p>
<pre tabindex="0"><code># create label
kubectl label namespace kube-system networking/namespace=kube-system
</code></pre><pre tabindex="0"><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all-egress
spec:
  policyTypes:
  - Egress
  podSelector: {}
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          networking/namespace: kube-system
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: TCP
      port: 53
    - protocol: UDP
      port: 53
</code></pre><p>To allow connections from the Ingress Controller:</p>
<pre tabindex="0"><code># create label
kubectl label namespace nginx-ingress networking/namespace=ingress
</code></pre><pre tabindex="0"><code>apiVersion: networking.k8s.io/v1n
kind: NetworkPolicy
metadata:
  name: allow-from-ingress
spec:
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          networking/namespace: ingress
  podSelector: {}
</code></pre><p>We need to create allow rules to define what aplication can communicate with anathor aplication. To match network traffic by combining namespace and pod selectors, you can use a <code>NetworkPolicy</code> object similar to the following:</p>
<pre tabindex="0"><code>apiVersion: extensions/v1beta1
kind: NetworkPolicy
metadata:
  name: alertmanager-mesh
  namespace: monitoring
spec:
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: prometheus
    - namespaceSelector:
        matchLabels:
          name: monitoring
    ports:
    - port: 9093
      protocol: tcp
  podSelector:
    matchLabels:
      app: alertmanager
</code></pre><p>Allow inbound tcp to port 9093 from only prometheus to alertmanager</p>
<pre tabindex="0"><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: prometheus
  namespace: monitoring
spec:
  policyTypes:
  - Ingress
  podSelector:
    matchLabels:
      app: prometheus
  ingress:
  - from:
    - podSelector: {}
    - namespaceSelector:
        matchLabels:
          name: monitoring
    ports:
    - protocol: TCP
      port: 9090
</code></pre><p>Allow inbound tcp to port 9090 from any source to prometheus.</p>
<p>You can create Rules to allow outboudn trafic from a service to a apps with specific tags. The following policy allows pod outbound traffic to other pods in the same namespace that match the pod selector. In the following example, outbound traffic is allowed only if they go to a pod with label color=red, on port 80.</p>
<pre tabindex="0"><code>kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-egress-same-namespace
  namespace: default
spec:
  policyTypes:
    - Egress
  podSelector:
    matchLabels:
      color: blue
  egress:
  - to:
    - podSelector:
        matchLabels:
          color: red
    ports:
    - port: 80
</code></pre><p>If You Don’t Know Which Pods Need To Talk To Each Other you can allow all application in a namespace to connect with each other.</p>
<pre tabindex="0"><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-same-namespace
spec:
  policyTypes:
  - Ingress
  podSelector: {}
  ingress:
  - from:
    - podSelector: {}
</code></pre><p>For services that require egress to resources outside of the cluster, for example, a database whitelist the subnet that the network resource is on.</p>
<pre tabindex="0"><code>kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: customer-api-allow-web
  namespace: prod
spec:
  policyTypes:
  - Egress
  podSelector:
    matchLabels:
      app: orders
  egress:
  - ports:
    - port: 3306
    to:
    - ipBlock:
        cidr: 172.16.32.0/27
</code></pre><h3 id="calico-networkpolicy">Calico NetworkPolicy</h3>
<p>Calico network policy provides a richer set of policy capabilities than Kubernetes including:</p>
<ul>
<li>policy ordering/priority</li>
<li>deny rules</li>
<li>Protocols: TCP, UDP, ICMP, SCTP, UDPlite, ICMPv6, protocol numbers (1-255)</li>
</ul>
<p>Calico network policies apply to endpoints. In Kubernetes, each pod is a Calico endpoint. However, Calico can support other kinds of endpoints. There are two types of Calico endpoints: workload endpoints (such as a Kubernetes pod or OpenStack VM) and host endpoints (an interface or group of interfaces on a host).</p>
<pre tabindex="0"><code>kind: NetworkPolicy
apiVersion: projectcalico.org/v3
metadata:
  name: allow-egress-same-namespace
  namespace: default
spec:
  selector: color == 'red'
  ingress:
  - action: Allow
    protocol: TCP
    source:
      selector: color == 'blue'
    destination:
      ports:
        - 80
</code></pre><p>In the following example, incoming TCP traffic to any pods with label color: red is denied if it comes from a pod with color: blue.</p>
<pre tabindex="0"><code>apiVersion: projectcalico.org/v3
kind: GlobalNetworkPolicy
metadata:
  name: deny-blue
spec:
  selector: color == 'red'
  ingress:
  - action: Deny
    protocol: TCP
    source:
      selector: color == 'blue'
</code></pre><p>Apply network policies in specific order:</p>
<pre tabindex="0"><code>apiVersion: projectcalico.org/v3
kind: GlobalNetworkPolicy
metadata:
  name: drop-other-ingress
spec:
  order: 20
  ...deny policy rules here...

apiVersion: projectcalico.org/v3
kind: GlobalNetworkPolicy
metadata:
  name: allow-cluster-internal-ingress
spec:
  order: 10
  ...allow policy rules here...
</code></pre><p>In the following example, incoming TCP traffic to an application is denied, and each connection attempt is logged to syslog:</p>
<pre tabindex="0"><code>apiVersion: projectcalico.org/v3
kind: NetworkPolicy
Metadata:
  name: allow-tcp-6379
  namespace: production
Spec:
  selector: role == 'database'
  types:
  - Ingress
  - Egress
  ingress:
  - action: Log
    protocol: TCP
    source:
      selector: role == 'frontend'
  - action: Deny
    protocol: TCP
    source:
      selector: role == 'frontend'
</code></pre><p>It is important to enforce separation of containers. As you can see you can create a <code>NetworkPolicy</code> for a specific namespace. So don&rsquo;t forget to create the default best practice Policies. In the next post will show you how you can automate the creation of the Default Policies for new namespaces.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[K0S The tiny Kubernetes]]></title>
            <link href="https://devopstales.github.io/home/k0s/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/image-security-admission-controller/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller" />
                <link href="https://devopstales.github.io/home/rke2-pod-security-policy/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Pod Security Policy" />
                <link href="https://devopstales.github.io/home/admission-controllers/?utm_source=atom_feed" rel="related" type="text/html" title="Using Admission Controllers" />
                <link href="https://devopstales.github.io/home/rke2-airgap-install/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 The Secure Kubernetes Engine" />
                <link href="https://devopstales.github.io/home/k8s-security/?utm_source=atom_feed" rel="related" type="text/html" title="Best Practices to keeping Kubernetes Clusters Secure" />
            
                <id>https://devopstales.github.io/home/k0s/</id>
            
            
            <published>2020-12-15T00:00:00+00:00</published>
            <updated>2020-12-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>We all know and love K3s, right?  It’s now time to discover a new distribution: <a href="https://k0sproject.io">k0s</a>.</p>
<h3 id="whats-k0s-">What’s k0s ?</h3>
<p>k0s is a brand new Kubernetes distribution. The current release is 0.8.0. It was published in December 2020.</p>
<p>The latest k0s release:</p>
<ul>
<li>Ships a certified and (CIS-benchmarked) Kubernetes 1.19</li>
<li>Uses containerd as the default container runtime</li>
<li>Uses an in-cluster etcd by default and supports SQLite, MySQL (or any compatible), PostgreSQL</li>
<li>Uses the Calico network plugin by default with network policies</li>
<li>Enables the Pod Security Policies admission controller</li>
<li>Uses DNS with CoreDNS</li>
<li>Exposes cluster metrics via Metrics Server</li>
<li>Allows the usage of Horizontal Pod Autoscaling (HPA)</li>
</ul>
<p>A lot of great features will come in future releases, among them:</p>
<ul>
<li>Micro VM runtimes (really looking forward to testing this one)</li>
<li>Zero-downtime cluster upgrades</li>
<li>Cluster backup and restore</li>
<li>Air-Gap install</li>
<li>FIPS 140-2 (coming soon)</li>
</ul>
<p>We’ll now see how to install k0s.</p>
<h3 id="install-singel-master">Install singel master</h3>
<p>k0s as a single binary acts as the process supervisor for all other control plane components. This means there&rsquo;s no container engine or kubelet running on controllers (by default). Which means there is no way for a cluster user to schedule workloads onto controller nodes.</p>
<pre tabindex="0"><code>curl -sSLf get.k0s.sh | sudo sh

k0s version

mkdir /etc/k0s
k0s default-config &gt; /etc/k0s/k0s.yaml
</code></pre><h3 id="config">Config</h3>
<p>In the config file <code>/etc/k0s/k0s.yaml</code> you can add helm charts thet will be installed at startup, like prometheus for monitoring or nginx ingress controller.</p>
<pre tabindex="0"><code>apiVersion: k0s.k0sproject.io/v1beta1
kind: Cluster
metadata:
  name: k0s
spec:
  api:
    address: 192.168.68.106
    sans:
    - my-k0s-control.my-domain.com
  network:
    podCIDR: 10.244.0.0/16
    serviceCIDR: 10.96.0.0/12
extensions:
  helm:
    repositories:
    - name: prometheus-community
      url: https://prometheus-community.github.io/helm-charts
    charts:
    - name: prometheus-stack
      chartname: prometheus-community/prometheus
      version: &quot;11.16.8&quot;
      namespace: default
</code></pre><pre tabindex="0"><code>cat &lt;&lt;EOF &gt; /etc/systemd/system/k0s.service
[Unit]
Description=&quot;k0s server&quot;
After=network-online.target
Wants=network-online.target
 
[Service]
Type=simple
ExecStart=/usr/bin/k0s server -c /etc/k0s/k0s.yaml --enable-worker
Restart=always
EOF
</code></pre><pre tabindex="0"><code>systemctl start k0s.service
systemctl enable k0s.service
journalctl -u k0s.service -f
</code></pre><pre tabindex="0"><code>sudo curl --output /usr/local/sbin/kubectl -L &quot;https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl&quot;
chmod +x /usr/local/sbin/kubectl
mkdir ~/.kube
cp /var/lib/k0s/pki/admin.conf ~/.kube/config

kubectl get node
kubectl get po -A
</code></pre><pre tabindex="0"><code>kubectl run nginx --image=nginx -n default
kubectl get po -A              
</code></pre><h3 id="check-tge-default-psp">Check tge default PSP</h3>
<pre tabindex="0"><code>NAME                PRIV    CAPS   SELINUX    RUNASUSER   FSGROUP    SUPGROUP   READONLYROOTFS   VOLUMES
00-k0s-privileged   true    *      RunAsAny   RunAsAny    RunAsAny   RunAsAny   false            *
99-k0s-restricted   false          RunAsAny   RunAsAny    RunAsAny   RunAsAny   false            configMap,downwardAPI,emptyDir,persistentVolumeClaim,projected,secret
</code></pre><pre tabindex="0"><code>kubectl get psp 99-k0s-restricted -o yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  annotations:
    k0s.k0sproject.io/last-applied-configuration: |
      {&quot;apiVersion&quot;:&quot;policy/v1beta1&quot;,&quot;kind&quot;:&quot;PodSecurityPolicy&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:null,&quot;name&quot;:&quot;99-k0s-restricted&quot;},&quot;spec&quot;:{&quot;allowPrivilegeEscalation&quot;:false,&quot;allowedCapabilities&quot;:[],&quot;fsGroup&quot;:{&quot;rule&quot;:&quot;RunAsAny&quot;},&quot;hostIPC&quot;:false,&quot;hostNetwork&quot;:false,&quot;hostPID&quot;:false,&quot;privileged&quot;:false,&quot;readOnlyRootFilesystem&quot;:false,&quot;runAsUser&quot;:{&quot;rule&quot;:&quot;RunAsAny&quot;},&quot;seLinux&quot;:{&quot;rule&quot;:&quot;RunAsAny&quot;},&quot;supplementalGroups&quot;:{&quot;rule&quot;:&quot;RunAsAny&quot;},&quot;volumes&quot;:[&quot;configMap&quot;,&quot;downwardAPI&quot;,&quot;emptyDir&quot;,&quot;persistentVolumeClaim&quot;,&quot;projected&quot;,&quot;secret&quot;]}}
    k0s.k0sproject.io/stack-checksum: b0c62cb2696c6167d7a8289411b06f69
  creationTimestamp: &quot;2020-12-14T17:39:37Z&quot;
  labels:
    k0s.k0sproject.io/stack: defaultpsp
  managedFields:
  - apiVersion: policy/v1beta1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .: {}
          f:k0s.k0sproject.io/last-applied-configuration: {}
          f:k0s.k0sproject.io/stack-checksum: {}
        f:labels:
          .: {}
          f:k0s.k0sproject.io/stack: {}
      f:spec:
        f:allowPrivilegeEscalation: {}
        f:fsGroup:
          f:rule: {}
        f:runAsUser:
          f:rule: {}
        f:seLinux:
          f:rule: {}
        f:supplementalGroups:
          f:rule: {}
        f:volumes: {}
    manager: k0s
    operation: Update
    time: &quot;2020-12-14T17:39:37Z&quot;
  name: 99-k0s-restricted
  resourceVersion: &quot;245&quot;
  selfLink: /apis/policy/v1beta1/podsecuritypolicies/99-k0s-restricted
  uid: b59e0bfe-57c2-4b8b-a17b-baa9047a6fcb
spec:
  allowPrivilegeEscalation: false
  fsGroup:
    rule: RunAsAny
  runAsUser:
    rule: RunAsAny
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  volumes:
  - configMap
  - downwardAPI
  - emptyDir
  - persistentVolumeClaim
  - projected
  - secret
</code></pre><p>If you check the config file <code>/etc/k0s/k0s.yaml</code> you can see it use the 00-k0s-privileged PSP as default and 00-k0s-privileged dose not disable run as root by default. It&rsquo;s sad.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[RKE2 Image security Admission Controller]]></title>
            <link href="https://devopstales.github.io/home/image-security-admission-controller/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/image-security-admission-controller/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller" />
                <link href="https://devopstales.github.io/home/rke2-pod-security-policy/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Pod Security Policy" />
                <link href="https://devopstales.github.io/home/admission-controllers/?utm_source=atom_feed" rel="related" type="text/html" title="Using Admission Controllers" />
                <link href="https://devopstales.github.io/home/rke2-airgap-install/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 The Secure Kubernetes Engine" />
                <link href="https://devopstales.github.io/kubernetes/rke2-pod-security-policy/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Pod Security Policy" />
            
                <id>https://devopstales.github.io/home/image-security-admission-controller/</id>
            
            
            <published>2020-12-13T00:00:00+00:00</published>
            <updated>2020-12-13T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In a previous post we talked about <a href="https://devopstales.github.io/home/admission-controllers/">Admission Controllers</a>. In this post I will show you how to use an Admission Controller to test image vulnerabilities.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<p>There is multiple tools to scan vulnerabilities, but less Admission Controller to use them. I found multiple solution for Anchore Engine so the first step is to deploy with its helm chart. In RKE2 I will use Rancher&rsquo;s <a href="https://devopstales.github.io/cloud/k3s-helm-controller/">Helm controller</a> what is preinstalled.</p>
<p>Then we can Deploy an Admission Controller to us this tool to automaticle scann any image deploy in the cluster and reject if is vulnerable. As I sad before there is multiple solution for this. One is Anchore&rsquo;s own Admission Controller but I will use Banzaicloud&rsquo;s solution because this easier to deploy. Sadly anchore-image-validator run as root so we need to use <a href="https://devopstales.github.io/home/rke2-pod-security-policy/">my predifinde PSP</a> to allow this.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano /var/lib/rancher/rke2/server/manifests/10_image-security.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Namespace</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">securty-system</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">helm.cattle.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">HelmChart</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">anchore-enginn</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">kube-system</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">repo</span>: <span style="color:#e6db74">&#34;https://charts.anchore.io&#34;</span>
  <span style="color:#f92672">chart</span>: <span style="color:#ae81ff">anchore-engine</span>
  <span style="color:#f92672">targetNamespace</span>: <span style="color:#ae81ff">securty-system</span>
  <span style="color:#f92672">valuesContent</span>: |-<span style="color:#e6db74">
</span><span style="color:#e6db74">     postgresql:
</span><span style="color:#e6db74">       postgresPassword: Password1
</span><span style="color:#e6db74">       persistence:
</span><span style="color:#e6db74">         size: 10Gi
</span><span style="color:#e6db74">     anchoreGlobal:
</span><span style="color:#e6db74">       defaultAdminPassword: Password1
</span><span style="color:#e6db74">       defaultAdminEmail: devopstales@mydomain.intra</span>     
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">RoleBinding</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">psp-rolebinding-securty-system</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">securty-system</span>
<span style="color:#f92672">roleRef</span>:
  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRole</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">system-unrestricted-psp-role</span>
<span style="color:#f92672">subjects</span>:
- <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Group</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">system:serviceaccounts</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">helm.cattle.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">HelmChart</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">anchore-policy-validator</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">kube-system</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">repo</span>: <span style="color:#e6db74">&#34;https://kubernetes-charts.banzaicloud.com&#34;</span>
  <span style="color:#f92672">chart</span>: <span style="color:#ae81ff">anchore-policy-validator</span>
  <span style="color:#f92672">targetNamespace</span>: <span style="color:#ae81ff">securty-system</span>
  <span style="color:#f92672">valuesContent</span>: |-<span style="color:#e6db74">
</span><span style="color:#e6db74">    externalAnchore:
</span><span style="color:#e6db74">      anchoreHost: &#34;http://anchore-enginn-anchore-engine-api:8228/v1/&#34;
</span><span style="color:#e6db74">      anchoreUser: admin
</span><span style="color:#e6db74">      anchorePass: Password1
</span><span style="color:#e6db74">    rbac:
</span><span style="color:#e6db74">      psp:
</span><span style="color:#e6db74">        enabled: true
</span><span style="color:#e6db74">    createPolicies: true</span>    
</code></pre></div><p>During deploying this chart, it&rsquo;s creating predefined policy bundles and activates <code>AllowAll</code> by default if <code>createPolicies</code> flag is set.</p>
<table>
<thead>
<tr>
<th>Bundle Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Allow all and warn bundle</td>
<td>Allow all images and warn if vulnerabilities are found</td>
</tr>
<tr>
<td>Reject critical bundle</td>
<td>Reject deploying images that contain <code>critical</code> vulnerabiliy</td>
</tr>
<tr>
<td>Reject high bundle</td>
<td>Reject deploying images that contain <code>high</code> vulnerabiliy</td>
</tr>
<tr>
<td>Block root bundle</td>
<td>Block deploying images that using <code>root</code> as effective user</td>
</tr>
<tr>
<td>Deny all images</td>
<td>Deny all imagest to deploy</td>
</tr>
</tbody>
</table>
<h3 id="test-the-admission-controller">Test the Admission Controller</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl run --image<span style="color:#f92672">=</span>busybox -- sleep <span style="color:#ae81ff">3600</span>
Error from server: admission webhook <span style="color:#e6db74">&#34;pods.anchore-policy-validator.admission.banzaicloud.com&#34;</span> denied the request: Image failed policy check: busybox
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">kubectl describe audits busybox1</span>
<span style="color:#f92672">Name</span>:         <span style="color:#ae81ff">busybox1</span>
<span style="color:#f92672">Namespace</span>:    
<span style="color:#f92672">Labels</span>:       <span style="color:#ae81ff">fakerelease=true</span>
<span style="color:#f92672">Annotations</span>:  <span style="color:#ae81ff">&lt;none&gt;</span>
<span style="color:#f92672">API Version</span>:  <span style="color:#ae81ff">security.banzaicloud.com/v1alpha1</span>
<span style="color:#f92672">Kind</span>:         <span style="color:#ae81ff">Audit</span>
<span style="color:#f92672">Metadata</span>:
  <span style="color:#f92672">Creation Timestamp</span>:  <span style="color:#e6db74">2020-11-29T10:20:41Z</span>
  <span style="color:#f92672">Generation</span>:          <span style="color:#ae81ff">1</span>
  <span style="color:#f92672">Managed Fields</span>:
    <span style="color:#f92672">API Version</span>:  <span style="color:#ae81ff">security.banzaicloud.com/v1alpha1</span>
    <span style="color:#f92672">Fields Type</span>:  <span style="color:#ae81ff">FieldsV1</span>
    <span style="color:#f92672">fieldsV1</span>:
      <span style="color:#f92672">f:metadata</span>:
        <span style="color:#f92672">f:labels</span>:
          <span style="color:#f92672">.</span>:
          <span style="color:#f92672">f:fakerelease</span>:
      <span style="color:#f92672">f:spec</span>:
        <span style="color:#f92672">.</span>:
        <span style="color:#f92672">f:action</span>:
        <span style="color:#f92672">f:image</span>:
        <span style="color:#f92672">f:releaseName</span>:
        <span style="color:#f92672">f:resource</span>:
        <span style="color:#f92672">f:result</span>:
      <span style="color:#f92672">f:status</span>:
        <span style="color:#f92672">.</span>:
        <span style="color:#f92672">f:state</span>:
    <span style="color:#f92672">Manager</span>:         <span style="color:#ae81ff">anchore-image-validator</span>
    <span style="color:#f92672">Operation</span>:       <span style="color:#ae81ff">Update</span>
    <span style="color:#f92672">Time</span>:            <span style="color:#e6db74">2020-11-29T10:20:41Z</span>
  <span style="color:#f92672">Resource Version</span>:  <span style="color:#ae81ff">39174</span>
  <span style="color:#f92672">Self Link</span>:         <span style="color:#ae81ff">/apis/security.banzaicloud.com/v1alpha1/audits/busybox1</span>
  <span style="color:#f92672">UID</span>:               <span style="color:#ae81ff">1e90c8b0-fffa-45f6-a986-d9fd269f0a83</span>
<span style="color:#f92672">Spec</span>:
  <span style="color:#f92672">Action</span>:  <span style="color:#ae81ff">reject</span>
  <span style="color:#f92672">Image</span>:
    <span style="color:#f92672">Image Digest</span>:  
    <span style="color:#f92672">Image Name</span>:    
    <span style="color:#f92672">Image Tag</span>:     
    <span style="color:#f92672">Last Updated</span>:  
  <span style="color:#f92672">Release Name</span>:    <span style="color:#ae81ff">busybox1</span>
  <span style="color:#f92672">Resource</span>:        <span style="color:#ae81ff">Pod</span>
  <span style="color:#f92672">Result</span>:
    <span style="color:#f92672">Image failed policy check</span>: <span style="color:#ae81ff">busybox</span>
<span style="color:#f92672">Status</span>:
  <span style="color:#f92672">State</span>:  
<span style="color:#f92672">Events</span>:   <span style="color:#ae81ff">&lt;none&gt;</span>
</code></pre></div><p>The default policy is deny All Image theat failed on policy check but we can white list a specific image or set <code>createPolicies</code> to <code>true</code> in Banzaicloud&rsquo;s Helm chart to create default AllowAll Policy.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f - <span style="color:#e6db74">&lt;&lt; EOF
</span><span style="color:#e6db74">apiVersion: security.banzaicloud.com/v1alpha1
</span><span style="color:#e6db74">kind:  WhiteListItem
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: busybox1
</span><span style="color:#e6db74">spec:
</span><span style="color:#e6db74">  reason: testing
</span><span style="color:#e6db74">  creator: devopstales-sa
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl run busybox1 --image<span style="color:#f92672">=</span>busybox -- sleep <span style="color:#ae81ff">3600</span>
pod/busybox1 created
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get whitelistitems -o wide -o<span style="color:#f92672">=</span>custom-columns<span style="color:#f92672">=</span>NAME:.metadata.name,CREATOR:.spec.creator,REASON:.spec.reason
NAME       CREATOR          REASON
busybox1   devopstales-sa   testing

kubectl get audits -o wide -o<span style="color:#f92672">=</span>custom-columns<span style="color:#f92672">=</span>NAME:.metadata.name,RELEASE:.spec.releaseName,IMAGES:.spec.image,RESULT:.spec.result
NAME       RELEASE    IMAGES                                                  RESULT
busybox1   busybox1   <span style="color:#f92672">[</span>map<span style="color:#f92672">[</span>imageDigest: imageName: imageTag: lastUpdated:<span style="color:#f92672">]]</span>   <span style="color:#f92672">[</span>Image failed policy check: busybox<span style="color:#f92672">]</span>
</code></pre></div><p>You can find the config files in my github repo: <a href="https://github.com/devopstales/k8s_sec_lab">https://github.com/devopstales/k8s_sec_lab</a></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[RKE2 Pod Security Policy]]></title>
            <link href="https://devopstales.github.io/home/rke2-pod-security-policy/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/rke2-airgap-install/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 The Secure Kubernetes Engine" />
                <link href="https://devopstales.github.io/kubernetes/rke2-pod-security-policy/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Pod Security Policy" />
                <link href="https://devopstales.github.io/home/admission-controllers/?utm_source=atom_feed" rel="related" type="text/html" title="Using Admission Controllers" />
                <link href="https://devopstales.github.io/kubernetes/rke2-airgap-install/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 The Secure Kubernetes Engine" />
                <link href="https://devopstales.github.io/home/k8s-security/?utm_source=atom_feed" rel="related" type="text/html" title="Best Practices to keeping Kubernetes Clusters Secure" />
            
                <id>https://devopstales.github.io/home/rke2-pod-security-policy/</id>
            
            
            <published>2020-12-10T00:00:00+00:00</published>
            <updated>2020-12-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can use Pod Security Policys in RKE2.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<h3 id="what-is-a-pod-security-policy">What is a Pod Security Policy?</h3>
<p>A Pod Security Policy is a cluster-level resource that controls security sensitive aspects of the pod specification. RBAC Controlls the usable Kubernetes objects for a user but nt the conditions of a specific ofject like allow run as root or not in a container.  PSP objects define a set of conditions that a pod must run with in order to be accepted into the system, as well as defaults for their related fields. PodSecurityPolicy is an optional admission controller that is enabled by default through the API, thus policies can be deployed without the PSP admission plugin enabled.</p>
<h3 id="psp-examples-using-rke2">PSP examples using RKE2</h3>
<p>RKE2 can be ran with or without the <code>profile: cis-1.5</code> configuration parameter. This will cause it to apply different PodSecurityPolicies (PSPs) at start-up. If running with the <code>cis-1.5</code> profile, RKE2 will apply a restrictive policy called <code>global-restricted-psp</code> to all namespaces except <code>kube-system</code>. The <code>kube-system</code> namespace needs a less restrictive policy named <code>system-unrestricted-psp</code> in order to launch critical components.</p>
<p>The policies are outlined below.</p>
<pre tabindex="0"><code>apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: global-restricted-psp
spec:
  privileged: false                # CIS - 5.2.1
  allowPrivilegeEscalation: false  # CIS - 5.2.5
  requiredDropCapabilities:        # CIS - 5.2.7/8/9
    - ALL
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  hostNetwork: false               # CIS - 5.2.4
  hostIPC: false                   # CIS - 5.2.3
  hostPID: false                   # CIS - 5.2.2
  runAsUser:
    rule: 'MustRunAsNonRoot'       # CIS - 5.2.6
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'MustRunAs'
    ranges:
      - min: 1
        max: 65535
  fsGroup:
    rule: 'MustRunAs'
    ranges:
      - min: 1
        max: 65535
  readOnlyRootFilesystem: false
</code></pre><p>This PSP disables <code>privileged</code> and <code>allowPrivilegeEscalation</code> and force tu run conatiners with UserID and GroupID betwean 1-65535 threat means you cannot run containers wit UserID/GroupID 0 what is root.</p>
<p>The &ldquo;system unrestricted policy&rdquo; is applied. See below.</p>
<pre tabindex="0"><code>apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: system-unrestricted-psp
spec:
  privileged: true
  allowPrivilegeEscalation: true
  allowedCapabilities:
  - '*'
  volumes:
  - '*'
  hostNetwork: true
  hostPorts:
  - min: 0
    max: 65535
  hostIPC: true
  hostPID: true
  runAsUser:
    rule: 'RunAsAny'
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'
</code></pre><h3 id="test-psp">Test PSP</h3>
<p>Is I try to deploy a Deployment with a container running as root it will fail.</p>
<pre tabindex="0"><code>kubectl get deploy,rs,pod
# output
NAME                                READY   UP-TO-DATE   AVAILABLE   AGE
deployment.extensions/alpine-test   0/1     0            0           67s

NAME                                          DESIRED   CURRENT   READY   AGE
replicaset.extensions/alpine-test-85c976cdd   1         0         0       67s
</code></pre><p>What happened?</p>
<pre tabindex="0"><code>kubectl describe replicaset.extensions/alpine-test-85c976cdd | tail -n3
# output
 Type     Reason        Age                    From                   Message
  ----     ------        ----                   ----                   -------
  Warning  FailedCreate  114s (x16 over 4m38s)  replicaset-controller  Error creating: pods &quot;alpine-test-85c976cdd-&quot; is forbidden: unable to validate against any pod security policy: []
</code></pre><h3 id="custom-psp">Custom PSP</h3>
<p>I usually create a restricted rule with allowing the root user in the cobtainer because some operator&rsquo;s container still use it.</p>
<pre tabindex="0"><code>apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: allow-root-psp
spec:
  privileged: false                # CIS - 5.2.1
  allowPrivilegeEscalation: false  # CIS - 5.2.5
  requiredDropCapabilities:        # CIS - 5.2.7/8/9
    - ALL
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  hostNetwork: false               # CIS - 5.2.4
  hostIPC: false                   # CIS - 5.2.3
  hostPID: false                   # CIS - 5.2.2
  runAsUser:
    rule: 'MustRunAsNonRoot'       # CIS - 5.2.6
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'
  readOnlyRootFilesystem: false
</code></pre><p>To use this PSP we need to create a <code>ClusterRole</code>.</p>
<pre tabindex="0"><code>apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: allow-root-psp-role
rules:
- apiGroups:
  - policy
  resourceNames:
  - allow-root-psp
  resources:
  - podsecuritypolicies
  verbs:
  - use
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Using Admission Controllers]]></title>
            <link href="https://devopstales.github.io/home/admission-controllers/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/rke2-airgap-install/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 The Secure Kubernetes Engine" />
                <link href="https://devopstales.github.io/kubernetes/admission-controllers/?utm_source=atom_feed" rel="related" type="text/html" title="Using Admission Controllers" />
                <link href="https://devopstales.github.io/home/k8s-security/?utm_source=atom_feed" rel="related" type="text/html" title="Best Practices to keeping Kubernetes Clusters Secure" />
                <link href="https://devopstales.github.io/kubernetes/rke2-airgap-install/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 The Secure Kubernetes Engine" />
                <link href="https://devopstales.github.io/home/k8s-ipvs/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with IPVS mode" />
            
                <id>https://devopstales.github.io/home/admission-controllers/</id>
            
            
            <published>2020-12-07T00:00:00+00:00</published>
            <updated>2020-12-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can use Admission Controllers.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<h3 id="what-is-an-admission-controller">What is an Admission Controller</h3>
<p>An admission controller is a piece of code that intercepts requests to the Kubernetes API server prior to persistence of the object, but after the request is authenticated and authorized. […] Admission controllers may be “validating”, “mutating”, or both. Mutating controllers may modify the objects they admit; validating controllers may not. […] If any of the controllers in either phase reject the request, the entire request is rejected immediately and an error is returned to the end-user. (Source: <a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/">Kubernetes Website</a> )</p>
<p>In a nutshell, Kubernetes admission controllers are plugins that govern and enforce how the cluster is used. They can be thought of as a gatekeeper that intercepts (authenticated) API requests and may change the request object or deny the request altogether.</p>
<h3 id="how-do-i-turn-on-an-admission-controller">How do I turn on an admission controller?</h3>
<p>A list of previously implemented controllers comes with Kubernetes, or you can write your own. To do so you must enable them in the <code>kube-apiserver</code> The Kubernetes API server flag enable-admission-plugins takes a comma-delimited list of admission control plugins to invoke prior to modifying objects in the cluster. For example, the following command line enables the NamespaceLifecycle and the LimitRanger admission control plugins:</p>
<pre tabindex="0"><code>kube-apiserver --enable-admission-plugins=NamespaceLifecycle,LimitRanger ...
</code></pre><h3 id="what-is-an-admission-webhook">What is an admission webhook?</h3>
<p>There are two special admission controllers in the list included in the Kubernetes apiserver: MutatingAdmissionWebhook and ValidatingAdmissionWebhook. These are special admission controllers that send admission requests to external HTTP callbacks and receive admission responses. If these two admission controllers are enabled, a Kubernetes administrator can create and configure an admission webhook in the cluster.</p>
<p><img src="/img/include/admission-controller-phases.png" alt="Example image"  class="zoomable" /></p>
<p>Validating webhooks can reject a request, but they cannot modify the object they are receiving in the admission request, while mutating webhooks can modify objects by creating a patch that will be sent back in the admission response. If a webhook rejects a request, an error is returned to the end-user.</p>
<h3 id="why-do-i-need-admission-controllers">Why do I need admission controllers?</h3>
<p>Security: Admission controllers can increase security by mandating a reasonable security baseline across an entire namespace or cluster. The built-in PodSecurityPolicy admission controller is perhaps the most prominent example; it can be used for disallowing containers from running as root or making sure the container’s root filesystem is always mounted read-only, for example. Further use cases that can be realized by custom, webhook-based admission controllers include:</p>
<ul>
<li>Allow pulling images only from specific registries known to the enterprise, while denying unknown image registries.</li>
<li>Reject deployments that do not meet security standards. For example, containers using the privileged flag can circumvent a lot of security checks. This risk could be mitigated by a webhook-based admission controller that either rejects such deployments (validating) or overrides the privileged flag, setting it to false.</li>
</ul>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes deprecated Docker? Containderd is the new Docker!!]]></title>
            <link href="https://devopstales.github.io/home/kubernetes-deprecated-docker-containderd-docker/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/kubernetes-deprecated-docker-containderd-docker/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes deprecated Docker? Containderd is the new Docker!!" />
                <link href="https://devopstales.github.io/home/container-runtimes/?utm_source=atom_feed" rel="related" type="text/html" title="Container runtimes" />
                <link href="https://devopstales.github.io/home/rke2-airgap-install/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 The Secure Kubernetes Engine" />
                <link href="https://devopstales.github.io/home/k8s-security/?utm_source=atom_feed" rel="related" type="text/html" title="Best Practices to keeping Kubernetes Clusters Secure" />
                <link href="https://devopstales.github.io/home/k8s-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure K8S vSphere Cloud Provider" />
            
                <id>https://devopstales.github.io/home/kubernetes-deprecated-docker-containderd-docker/</id>
            
            
            <published>2020-12-04T00:00:00+00:00</published>
            <updated>2020-12-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Docker is now deprecated in Kubernetes in the next 1.20 version, but thet dose no mean yo can not run containers wit docker.</p>
<p>&ldquo;Given the impact of this change, we are using an extended deprecation timeline. It will not be removed before Kubernetes 1.22, meaning the earliest release without dockershim would be 1.23 in late 2021.&rdquo; (Source: <a href="https://kubernetes.io/blog/2020/12/02/dockershim-faq/">Kubernetes</a> )</p>
<h3 id="but-why-is-docker-deprecated">But why is Docker deprecated?</h3>
<p>In the beginning Kubernetes supported only Docker as a container runtime but to use other runtime&rsquo;s Kubernetes, Docker, Google, CoreOS, and other vendors created the <a href="https://opencontainers.org/">Open Container Initiative (OCI)</a>. The OCI currently contains two specifications: the Runtime Specification (runtime-spec)  and the Image Specification (image-spec).
For the Runtime Specification they created the CRI (Container runtime Interface) as a standerd interface fro kubernetes to communicate with container runtimes. Before 1.20 Kubernetes used the old dockershim for docker engine not the standerd CRI interface.</p>
<p>To explain the next reason, we have to see the Docker architecture a bit. Here&rsquo;s the diagram.</p>
<p><img src="/img/include/docker_engine.png" alt="Example image"  class="zoomable" /></p>
<p>Kubernetes needs the tings inside of the red area. Docker has many other features like Docker Network and Volume that Kubernetes not uses.</p>
<h3 id="can-i-use-docker">Can I use Docker??</h3>
<p>Mirantis and Docker have agreed to partner to maintain the shim code standalone outside Kubernetes, as a conformant CRI interface for Docker Engine. &hellip; This means that you can continue to build Kubernetes based on Docker Engine as before, just switching from the built in dockershim to the external one. Docker and Mirantis will work together on making sure it continues to work as well as before and that it passes all the conformance tests and works just like the built in version did. Docker will continue to ship this shim in Docker Desktop as this gives a great developer experience, and Mirantis will be using this in Mirantis Kubernetes Engine. (Source: <a href="https://www.docker.com/blog/what-developers-need-to-know-about-docker-docker-engine-and-kubernetes-v1-20/">Docker Blog</a> )</p>
<h3 id="what-can-i-use-instad-of-docker">What can I use instad of Docker</h3>
<p>You can use containerd or CRI-O instad of docker. In <a href="">a previous post</a> I showed how you can install Kubernetes with CRI-O, so now I will show you how you can use containerd instad of Docker. If you just want to migrate from Docker, this is the best option as containerd is actually used inside of Docker to do all the &ldquo;runtime&rdquo; jobs as you can see in the diagram above.</p>
<h3 id="install-and-configure-containerd-prerequisites">Install and configure Containerd prerequisites:</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf</span>
<span style="color:#ae81ff">overlay</span>
<span style="color:#ae81ff">br_netfilter</span>
<span style="color:#ae81ff">EOF</span>

<span style="color:#ae81ff">sudo modprobe overlay</span>
<span style="color:#ae81ff">sudo modprobe br_netfilter</span>

<span style="color:#75715e"># Setup required sysctl params, these persist across reboots.</span>
<span style="color:#ae81ff">cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf</span>
<span style="color:#ae81ff">net.bridge.bridge-nf-call-iptables  = 1</span>
<span style="color:#ae81ff">net.ipv4.ip_forward                 = 1</span>
<span style="color:#ae81ff">net.bridge.bridge-nf-call-ip6tables = 1</span>
<span style="color:#ae81ff">EOF</span>

<span style="color:#75715e"># Apply sysctl params without reboot</span>
<span style="color:#ae81ff">sudo sysctl --system</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e">### Install required packages</span>
sudo yum install -y yum-utils device-mapper-persistent-data lvm2

<span style="color:#75715e">## Add docker repository</span>
sudo yum-config-manager <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    --add-repo <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    https://download.docker.com/linux/centos/docker-ce.repo

<span style="color:#75715e">## Install containerd</span>
sudo yum update -y <span style="color:#f92672">&amp;&amp;</span> sudo yum install -y containerd.io

<span style="color:#75715e">## Configure containerd</span>
sudo mkdir -p /etc/containerd
sudo containerd config default &gt; /etc/containerd/config.toml
</code></pre></div><p>To use the <code>systemd</code> cgroup driver in <code>/etc/containerd/config.toml</code> with <code>runc</code>, set</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano /etc/containerd/config.toml
systemd_cgroup <span style="color:#f92672">=</span> true

<span style="color:#75715e"># Restart containerd</span>
sudo systemctl restart containerd
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">free -h
swapoff -a
swapoff -a
sed -i.bak -r <span style="color:#e6db74">&#39;s/(.+ swap .+)/#\1/&#39;</span> /etc/fstab
free -h
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat <span style="color:#e6db74">&lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
</span><span style="color:#e6db74">[kubernetes]
</span><span style="color:#e6db74">name=Kubernetes
</span><span style="color:#e6db74">baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
</span><span style="color:#e6db74">enabled=1
</span><span style="color:#e6db74">gpgcheck=1
</span><span style="color:#e6db74">repo_gpgcheck=1
</span><span style="color:#e6db74">gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
</span><span style="color:#e6db74">        https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
</span><span style="color:#e6db74">EOF</span>


yum install epel-release -y
yum install -y kubeadm kubelet kubectl
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">echo <span style="color:#e6db74">&#34;runtime-endpoint: unix:///run/containerd/containerd.sock&#34;</span> &gt; /etc/crictl.yaml
crictl ps
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">systemctl enable kubelet.service
kubeadm config images pull
crictl images
kubeadm init --pod-network-cidr<span style="color:#f92672">=</span>10.244.0.0/16
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown <span style="color:#66d9ef">$(</span>id -u<span style="color:#66d9ef">)</span>:<span style="color:#66d9ef">$(</span>id -g<span style="color:#66d9ef">)</span> $HOME/.kube/config
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
kubectl get node
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Container runtimes]]></title>
            <link href="https://devopstales.github.io/home/container-runtimes/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/container-runtimes/?utm_source=atom_feed" rel="related" type="text/html" title="Container runtimes" />
                <link href="https://devopstales.github.io/home/k3s-crio/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with CRI-O and kadalu" />
                <link href="https://devopstales.github.io/home/k8s-crio/?utm_source=atom_feed" rel="related" type="text/html" title="Install K8S with CRI-O and kadalu" />
                <link href="https://devopstales.github.io/home/rke2-airgap-install/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 The Secure Kubernetes Engine" />
                <link href="https://devopstales.github.io/home/k8s-security/?utm_source=atom_feed" rel="related" type="text/html" title="Best Practices to keeping Kubernetes Clusters Secure" />
            
                <id>https://devopstales.github.io/home/container-runtimes/</id>
            
            
            <published>2020-11-29T00:00:00+00:00</published>
            <updated>2020-11-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>One of the terms you hear a lot when dealing with containers is &ldquo;container runtime&rdquo;. This post will explain what container runtimes are.</p>
<h3 id="oci">OCI</h3>
<p>Docker, Google, CoreOS, and other vendors created the <a href="https://opencontainers.org/">Open Container Initiative (OCI)</a>. The OCI currently contains two specifications: the Runtime Specification (runtime-spec) as a standerd of CRI (Container runtime Interface) and the Image Specification (image-spec). This led to other standards like CNI (Container Network Interface), a Cloud Native Computing Foundation project, or Container Storage Interface (CSI).</p>
<h3 id="what-is-a-container-runtime">What is a container runtime?</h3>
<p>Container runtime is the engine that runs and manages the components required to run containers. Communicating with the kernel to start containerized processes, setting up cgroups, configure mount points and do many things to make your container work.</p>
<h3 id="docker">Docker</h3>
<p>Docker was released in 2013 and solved many of the problems that developers had running containers like LXC or OpenVZ. Before version 1.11, the implementation of Docker was a monolithic daemon. The monolith did everything as one package such as downloading container images, launching container processes, exposing a remote API, and acting as a log collection daemon, all in a centralized process running as root. (Source: <a href="https://coreos.com/rkt/docs/latest/rkt-vs-other-projects.html#rkt-vs-docker">Coreos</a> )</p>
<p>At this time docker was the only runtime that Kubernetes supported, but wit the release of Coreos&rsquo;s rkt Kubernetes needed a standard interface to ease the integration of other container runtimes. This led to the splitting of Docker into different parts.</p>
<p><img src="/img/include/conatinerd.png" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/docker_engine.png" alt="Example image"  class="zoomable" /></p>
<h3 id="cri-o">CRI-O</h3>
<p>CRI-O is an implementation of the Kubernetes CRI (Container Runtime Interface) to enable using OCI (Open Container Initiative) compatible runtimes. It is a lightweight alternative to using Docker as the runtime for kubernetes. It allows Kubernetes to use any OCI-compliant runtime as the container runtime for running pods. Today it supports runc and Kata Containers as the container runtimes but any OCI-conformant runtime can be plugged in principle.</p>
<p>CRI-O supports OCI container images and can pull from any container registry. It is a lightweight alternative to using Docker, Moby or rkt as the runtime for Kubernetes. (Source: <a href="https://cri-o.io/">CRI-O Website</a> )</p>
<pre tabindex="0"><code>crictl ps - list containers
crictl pods - list pods
</code></pre><h3 id="pouchcontainer">PouchContainer</h3>
<p>PouchContainer is an open-source project created by Alibaba Group. It provides applications with a lightweight runtime environment with strong isolation and minimal overhead. PouchContainer isolates applications from varying runtime environment, and minimizes operational workload. t includes lots of security features, like hypervisor-based container technology, lxcfs, directory disk quota, patched Linux kernel.  PouchContainer utilizes Dragonfly, a P2P-base distribution system, to achieve lightning-fast container image distribution at enterprise&rsquo;s large scale.</p>
<p><img src="/img/include/pouchcontainer-arch.png" alt="PouchContainer"  class="zoomable" /></p>
<h3 id="kata-containers">Kata Containers</h3>
<p>Kata Containers is an open source community working to build a secure container runtime with lightweight virtual machines that feel and perform like containers, but provide stronger workload isolation using hardware virtualization technology as a second layer of defense. (Source: <a href="https://katacontainers.io/">Kata Containers Website</a> )</p>
<p><img src="/img/include/katacontainers.jpg" alt="Kata container engine"  class="zoomable" /></p>
<h3 id="podman">Podman</h3>
<p>Podman is a daemonless, open source, Linux-native tool designed to develop, manage, and run Open Container Initiative (OCI) containers and pods. It has a similar directory structure to Buildah, Skopeo, and CRI-O. Podman doesn’t admin privileges for its commands to work.</p>
<p><img src="/img/include/porownanie.png" alt="Podman"  class="zoomable" /></p>
<h4 id="containerd">Containerd</h4>
<p><code>containerd</code> is a daemon that controls runC. From containerd website, &ldquo;containerd manages the complete container lifecycle of its host system, from image transfer and storage to container execution and supervision to low-level storage to network attachments and beyond&rdquo;.</p>
<p><img src="/img/include/docker_oci.png" alt="Containerd"  class="zoomable" /></p>
<h3 id="what-is-gvisor">What is gvisor</h3>
<p>gVisor is an application kernel, written in Go, that implements a substantial portion of the Linux system call interface. It provides an additional layer of isolation between running applications and the host operating system.</p>
<p>gVisor includes an Open Container Initiative (OCI) compatible Low-Level Container Runtime called <code>runsc</code> that makes it easy to work with existing container tooling. The <code>runsc</code> runtime integrates with Docker, containerd and Kubernetes, making it simple to run sandboxed containers.</p>
<p><img src="/img/include/gvisor2.png" alt="gvisor"  class="zoomable" />
<img src="/img/include/gvisor.png" alt="gvisor"  class="zoomable" /></p>
<h3 id="what-is-a-low-level-container-runtime">What is a Low-Level Container Runtime?</h3>
<p>Containers are implemented using Linux namespaces and cgroups. Namespaces let you virtualize system resources, like the file system or networking, for each container. Cgroups provide a way to limit the amount of resources like CPU and memory that each container can use. At the lowest level, container runtimes are responsible for setting up these namespaces and cgroups for containers, and then running commands inside those namespaces and cgroups. Low-level runtimes support using these operating system features.</p>
<h4 id="runc">runc</h4>
<p><code>runc</code> is a CLI tool for spawning and running containers according to the OCI specification. Docker donated this library to OCI as a reference implementation of the OCI runtime specification.</p>
<h4 id="crun">crun</h4>
<p><code>crun</code> is a lightweight fully featured OCI runtime and C library for running containers.</p>
<p>&ldquo;While most of the tools used in the Linux containers ecosystem are written in Go, I believe C is a better fit for a lower level tool like a container runtime. runc, the most used implementation of the OCI runtime specs written in Go, re-execs itself and use a module written in C for setting up the environment before the container process starts.&rdquo; (Source: <a href="https://github.com/containers/crun#why-another-implementation">crun GitHub page</a> )</p>
<p><code>crun</code> is faster than runc and has a much lower memory footprint.</p>
<table>
<thead>
<tr>
<th></th>
<th>crun</th>
<th>runc</th>
<th>%</th>
</tr>
</thead>
<tbody>
<tr>
<td>100 /bin/true</td>
<td>0:01.69</td>
<td>0:3.34</td>
<td>-49.4%</td>
</tr>
</tbody>
</table>
<h4 id="runv">runv</h4>
<p><code>runv</code> is a hypervisor-based runtime for OCI. <code>runV</code> supports the following hypervisors:</p>
<ul>
<li>KVM (QEMU 2.1 or later)</li>
<li>KVM (Kvmtool)</li>
<li>Xen (4.5 or later)</li>
<li>QEMU without KVM (NOT RECOMMENDED. QEMU 2.1 or later)</li>
</ul>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[RKE2 The Secure Kubernetes Engine]]></title>
            <link href="https://devopstales.github.io/home/rke2-airgap-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/rke2-airgap-install/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 The Secure Kubernetes Engine" />
                <link href="https://devopstales.github.io/home/k8s-security/?utm_source=atom_feed" rel="related" type="text/html" title="Best Practices to keeping Kubernetes Clusters Secure" />
                <link href="https://devopstales.github.io/home/k8s-ipvs/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with IPVS mode" />
                <link href="https://devopstales.github.io/home/k8s-calico-ebpf/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with calico&#39;s eBPF mode" />
                <link href="https://devopstales.github.io/home/k8s-install-openebs/?utm_source=atom_feed" rel="related" type="text/html" title="Install OpenEBS for Kubernetes" />
            
                <id>https://devopstales.github.io/home/rke2-airgap-install/</id>
            
            
            <published>2020-11-25T00:00:00+00:00</published>
            <updated>2020-11-25T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can install a secure Kubernetes Engine variant called RKE2 in a Air-Gap environment.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<h3 id="what-is-rke2">What is RKE2</h3>
<p>RKE2, also known as RKE Government, is Rancher&rsquo;s next-generation Kubernetes distribution. It is a fully conformant Kubernetes distribution that focuses on security and compliance within the U.S. Federal Government sector.</p>
<h3 id="install-rke2-from-rpms">Install RKE2 from rpms</h3>
<p>Not like K3S RKE2 offers an rpm repository. Of course in an Air-Gap environment you need an internal repository to sync the packages.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">cat <span style="color:#e6db74">&lt;&lt; EOF &gt; /etc/yum.repos.d/rancher-rke2-1-18-latest.repo
</span><span style="color:#e6db74">[rancher-rke2-common-latest]
</span><span style="color:#e6db74">name=Rancher RKE2 Common Latest
</span><span style="color:#e6db74">baseurl=https://rpm.rancher.io/rke2/latest/common/centos/7/noarch
</span><span style="color:#e6db74">enabled=1
</span><span style="color:#e6db74">gpgcheck=1
</span><span style="color:#e6db74">gpgkey=https://rpm.rancher.io/public.key
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">[rancher-rke2-1-18-latest]
</span><span style="color:#e6db74">name=Rancher RKE2 1.18 Latest
</span><span style="color:#e6db74">baseurl=https://rpm.rancher.io/rke2/latest/1.18/centos/7/x86_64
</span><span style="color:#e6db74">enabled=1
</span><span style="color:#e6db74">gpgcheck=1
</span><span style="color:#e6db74">gpgkey=https://rpm.rancher.io/public.key
</span><span style="color:#e6db74">EOF</span>

yum -y install rke2-server nano
</code></pre></div><p>In an Air-Gap environment you cannot connect to the public internet so the containerd engine cannot connest to the registry. In this scenario yo have two options. Create an internal registry and upload all images or import images from tarball. In this demo I will use the second option.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">mkdir -p /var/lib/rancher/rke2/agent/images/
scp rke2-images.linux-amd64.tar masern01:/var/lib/rancher/rke2/agent/images/
cd /var/lib/rancher/rke2/agent/images/
</code></pre></div><p>For RKE2 you didn&rsquo;t nee docker engine. The rpms will install all the necessary binaris to run a container.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">echo <span style="color:#e6db74">&#39;PATH=$PATH:/usr/local/bin&#39;</span> &gt;&gt; /etc/profile
echo <span style="color:#e6db74">&#39;PATH=$PATH:/var/lib/rancher/rke2/bin&#39;</span> &gt;&gt; /etc/profile
source /etc/profile
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">setenforce <span style="color:#ae81ff">1</span>
getenforce
sed -i <span style="color:#e6db74">&#39;s/=\(disabled\|permissive\)/=enforcing/g&#39;</span> /etc/sysconfig/selinux
systemctl start firewalld
systemctl enable firewalld
</code></pre></div><p>For the demo I will use <code>firewalld</code> to block all outgoing request from the server. This is how I emulate the Air-Gap environment.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">firewall-cmd --permanent --direct --add-rule ipv4 filter OUTPUT <span style="color:#ae81ff">0</span> -p tcp -m tcp --dport<span style="color:#f92672">=</span><span style="color:#ae81ff">443</span> -j DROP
firewall-cmd --permanent --direct --add-rule ipv4 filter OUTPUT <span style="color:#ae81ff">0</span> -p tcp -m tcp --dport<span style="color:#f92672">=</span><span style="color:#ae81ff">80</span> -j DROP
firewall-cmd --permanent --direct --add-rule ipv4 filter OUTPUT <span style="color:#ae81ff">1</span> -j ACCEPT
firewall-cmd --reload
</code></pre></div><p>Enable hardened mode.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">mkdir -p /etc/rancher/rke2
cat <span style="color:#e6db74">&lt;&lt; EOF &gt;  /etc/rancher/rke2/config.yaml
</span><span style="color:#e6db74">write-kubeconfig-mode: &#34;0644&#34;
</span><span style="color:#e6db74">profile: &#34;cis-1.5&#34;
</span><span style="color:#e6db74">selinux: true
</span><span style="color:#e6db74">EOF</span>


sudo cp -f /usr/share/rke2/rke2-cis-sysctl.conf /etc/sysctl.d/60-rke2-cis.conf
sysctl -p /etc/sysctl.d/60-rke2-cis.conf

useradd -r -c <span style="color:#e6db74">&#34;etcd user&#34;</span> -s /sbin/nologin -M etcd
</code></pre></div><p>On my VM there is multiple network interface So I will configure what to use the kubernetes engine.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">mkdir -p /var/lib/rancher/rke2/server/manifests/
cat <span style="color:#e6db74">&lt;&lt; EOF &gt; /var/lib/rancher/rke2/server/manifests/rke2-canal-config.yml
</span><span style="color:#e6db74">apiVersion: helm.cattle.io/v1
</span><span style="color:#e6db74">kind: HelmChartConfig
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: rke2-canal
</span><span style="color:#e6db74">  namespace: kube-system
</span><span style="color:#e6db74">spec:
</span><span style="color:#e6db74">  valuesContent: |-
</span><span style="color:#e6db74">    flannel:
</span><span style="color:#e6db74">      iface: &#34;enp0s8&#34;
</span><span style="color:#e6db74">EOF</span>

cat <span style="color:#e6db74">&lt;&lt; EOF &gt; /var/lib/rancher/rke2/server/manifests/rke2-ingress-nginx-config.yaml
</span><span style="color:#e6db74">apiVersion: helm.cattle.io/v1
</span><span style="color:#e6db74">kind: HelmChartConfig
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: rke2-ingress-nginx
</span><span style="color:#e6db74">  namespace: kube-system
</span><span style="color:#e6db74">spec:
</span><span style="color:#e6db74">  valuesContent: |-
</span><span style="color:#e6db74">    controller:
</span><span style="color:#e6db74">      metrics:
</span><span style="color:#e6db74">        enable: true
</span><span style="color:#e6db74">        service:
</span><span style="color:#e6db74">          annotations:
</span><span style="color:#e6db74">            prometheus.io/scrape: &#34;true&#34;
</span><span style="color:#e6db74">            prometheus.io/port: &#34;10254&#34;
</span><span style="color:#e6db74">EOF</span>

cat <span style="color:#e6db74">&lt;&lt; EOF &gt; /var/lib/rancher/rke2/server/manifests/rke2-kube-proxy-config.yaml
</span><span style="color:#e6db74">apiVersion: helm.cattle.io/v1
</span><span style="color:#e6db74">kind: HelmChartConfig
</span><span style="color:#e6db74">metadata:
</span><span style="color:#e6db74">  name: rke2-kube-proxy
</span><span style="color:#e6db74">  namespace: kube-system
</span><span style="color:#e6db74">spec:
</span><span style="color:#e6db74">  valuesContent: |-
</span><span style="color:#e6db74">    metricsBindAddress: 0.0.0.0:10249
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">systemctl enable rke2-server.service
systemctl start rke2-server.service
journalctl -u rke2-server -f


mkdir ~/.kube
ln -s /etc/rancher/rke2/rke2.yaml ~/.kube/config
ln -s /var/lib/rancher/rke2/agent/etc/crictl.yaml /etc/crictl.yaml
chmod <span style="color:#ae81ff">600</span> ~/.kube/config

kubectl get node
crictl ps
crictl images

kubectl edit psp global-restricted-psp
<span style="color:#75715e"># remove apparmor lines in annotation and save</span>

<span style="color:#75715e">### Autodeploy folder</span>
/var/lib/rancher/rke2/server/manifests/
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Best Practices to keeping Kubernetes Clusters Secure]]></title>
            <link href="https://devopstales.github.io/home/k8s-security/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-security/?utm_source=atom_feed" rel="related" type="text/html" title="Best Practices to keeping Kubernetes Clusters Secure" />
                <link href="https://devopstales.github.io/home/k8s-calico-ebpf/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with calico&#39;s eBPF mode" />
                <link href="https://devopstales.github.io/home/k8s-ipvs/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with IPVS mode" />
                <link href="https://devopstales.github.io/home/k8s-install-openebs/?utm_source=atom_feed" rel="related" type="text/html" title="Install OpenEBS for Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-kubeadm-ha/?utm_source=atom_feed" rel="related" type="text/html" title="How to install kubernetes with kubeadm in HA mode" />
            
                <id>https://devopstales.github.io/home/k8s-security/</id>
            
            
            <published>2020-11-20T00:00:00+00:00</published>
            <updated>2020-11-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Kubernetes offers rich configuration options, but defaults are usually the least secure. Most sysadmin didn&rsquo;t knows how to secure a kubernetes clyuster. So this is my Best Practice list to keeping Kubernetes Clusters Secure.</p>
<H3>Parst of the K8S Security series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-cisa-install">Kubernetes Hardening Guide with CIS 1.6 Benchmark</a></li>
     <li>Part3: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
     <li>Part4: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
     <li>Part5: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
<!-- RBAC -->
     <li>Part6: <a href="../../kubernetes/k8s-secomp/">Hardening Kubernetes with seccomp</a></li>
     <li>Part7a: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
     <li>Part7b: <a href="../../kubernetes/k8s-ps/">Kubernetes Pod Security</a></li>
<!-- # Policy -->
     <li>Part8: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
     <li>Part9: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a></li>
<!-- # Admission Controllers -->
     <li>Part10: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a></li>
<!-- ## Image Scan -->
     <li>Part11a: <a href="../../kubernetes/image-security-admission-controller/">Image security Admission Controller</a></li>
     <li>Part11b: <a href="../../kubernetes/image-security-admission-controller-v2/">Image security Admission Controller V2</a></li>
     <li>Part11c: <a href="../../kubernetes/image-security-admission-controller-v3/">Image security Admission Controller V3</a></li>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring -->
     <li>Part12: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
     <li>Part13: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a></li>
     <li>Part14: <a href="../../kubernetes/k8s-falco/">Kubernetes audit logs and Falco</a></li>
<!-- kyverno logs and monitoring -->
<!-- DAST Exporter?? -->
<!-- CIS Exporter https://github.com/ibrokethecloud/kube-bench-metrics -->
<!-- # Image Signature Verification -->
     <li>Part15a <a href="../../kubernetes/k8s-connaisseur/">Image Signature Verification with Connaisseur</a></li>
     <li>Part15b <a href="../../kubernetes/k8s-connaisseur-v2/">Image Signature Verification with Connaisseur 2.0</a></li>
     <li>Part15c <a href="../../kubernetes/k8s-kyverno-cosign/">Image Signature Verification with Kyverno</a></li>
<!-- # Backup -->
     <li>Part16a <a href="../../kubernetes/k8s-backup/">Backup your Kubernetes Cluster</a></li>
     <li>Part16b <a href="../../kubernetes/k8s-git-backup/">How to Backup Kubernetes to git?</a></li>
<!-- # Secrets -->
     <li>Part17a <a href="../../kubernetes/k8s-vault/">Kubernetes and Vault integration</a></li>
     <li>Part17b <a href="../../kubernetes/k8s-vault-v2/">Kubernetes External Vault integration</a></li>
     <li>Part18a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
     <li>Part18b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
     <li>Part18c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a></li>
<!-- # Image Update -->
<!-- keel -->
     <li>Part19: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a></li>
<!-- flux image update -->
     <li>Part20: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
<!-- # Image registry security -->
     <li>Part21: <a href="../../kubernetes/k8s-imagepullsecret-patcher/">How to use imagePullSecrets cluster-wide??</a></li>
     <li>Part22: <a href="../../kubernetes/kyverno-image-mirror/">Automatically change registry in pod definition</a></li>

<!-- sysdig falco -->
<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
https://github.com/weaveworks/kured
-->
<!-- resource kvota -->
</ul>



<h3 id="use-firewalld">Use firewalld</h3>
<p>In most tutorial the first thing in a Kubernets installation is to disable the firewall because is it easier than configure properly.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># master</span>
firewall-cmd --permanent --add-port<span style="color:#f92672">=</span>6443/tcp
firewall-cmd --permanent --add-port<span style="color:#f92672">=</span>2379-2380/tcp
firewall-cmd --permanent --add-port<span style="color:#f92672">=</span>10250/tcp
firewall-cmd --permanent --add-port<span style="color:#f92672">=</span>10251/tcp
firewall-cmd --permanent --add-port<span style="color:#f92672">=</span>10252/tcp
firewall-cmd --permanent --add-port<span style="color:#f92672">=</span>10255/tcp
firewall-cmd --permanent --add-port<span style="color:#f92672">=</span>8472/udp
firewall-cmd --add-masquerade --permanent
firewall-cmd --permanent --add-port<span style="color:#f92672">=</span>30000-32767/tcp

<span style="color:#75715e"># worker</span>
firewall-cmd --permanent --add-port<span style="color:#f92672">=</span>10250/tcp
firewall-cmd --permanent --add-port<span style="color:#f92672">=</span>10255/tcp
firewall-cmd --permanent --add-port<span style="color:#f92672">=</span>8472/udp
firewall-cmd --permanent --add-port<span style="color:#f92672">=</span>30000-32767/tcp
firewall-cmd --add-masquerade --permanent

<span style="color:#75715e"># frontend</span>
firewall-cmd --permanent --add-port<span style="color:#f92672">=</span>10250/tcp
firewall-cmd --permanent --add-port<span style="color:#f92672">=</span>10255/tcp
firewall-cmd --permanent --add-port<span style="color:#f92672">=</span>8472/udp
firewall-cmd --permanent --add-port<span style="color:#f92672">=</span>30000-32767/tcp
firewall-cmd --add-masquerade --permanent
firewall-cmd --permanent --zone<span style="color:#f92672">=</span>public --add-service<span style="color:#f92672">=</span>http
firewall-cmd --permanent --zone<span style="color:#f92672">=</span>public --add-service<span style="color:#f92672">=</span>https
</code></pre></div><h3 id="enabling-signed-kubelet-serving-certificates">Enabling signed kubelet serving certificates</h3>
<p>By default the kubelet serving certificate deployed by kubeadm is self-signed. This means a connection from external services like the <code>metrics-server</code> to a kubelet cannot be secured with TLS.</p>
<p>To configure the kubelets in a new kubeadm cluster to obtain properly signed serving certificates you must pass the following minimal configuration to <code>kubeadm init</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
serverTLSBootstrap: true
</code></pre></div><p>If you whant to know more about certificates and thear rotation chenck <a href="/kubernetes/k8s-cert/">my blog post</a>.</p>
<h3 id="pod-network-add-on">Pod network add-on</h3>
<p>Several external projects provide Kubernetes Pod networks using CNI, some of which also support Network Policy. Use one of them.</p>
<ul>
<li>Calico</li>
<li>Canal</li>
<li>Weave Net</li>
<li>Contiv</li>
<li>Cilium</li>
</ul>
<p>See the list of available <a href="https://kubernetes.io/docs/concepts/cluster-administration/addons/#networking-and-network-policy">networking and network policy add-ons</a>.</p>
<h3 id="using-rbac-authorization">Using RBAC Authorization</h3>
<p>Role-based access control (RBAC) is a method of regulating access to computer or network resources based on the roles of individual users within your organization. For that you need to create <code>Role</code> or <code>ClusterRole</code> objects then assign that objects to a user wit <code>RoleBinding</code> or <code>ClusterRoleBinding</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ServiceAccount</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">deployer</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">$NAMESPACE</span>
---
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Role</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1beta1</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">deployer-access</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">$NAMESPACE</span>
---
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Role</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1beta1</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">deployer-access</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">$NAMESPACE</span>
<span style="color:#f92672">rules</span>:
- <span style="color:#f92672">apiGroups</span>: [<span style="color:#e6db74">&#34;&#34;</span>, <span style="color:#e6db74">&#34;extensions&#34;</span>, <span style="color:#e6db74">&#34;apps&#34;</span>]
  <span style="color:#f92672">resources</span>: [<span style="color:#e6db74">&#34;*&#34;</span>]
  <span style="color:#f92672">verbs</span>: [<span style="color:#e6db74">&#34;*&#34;</span>]
- <span style="color:#f92672">apiGroups</span>: [<span style="color:#e6db74">&#34;batch&#34;</span>]
  <span style="color:#f92672">resources</span>:
  - <span style="color:#ae81ff">jobs</span>
  - <span style="color:#ae81ff">cronjobs</span>
  <span style="color:#f92672">verbs</span>: [<span style="color:#e6db74">&#34;*&#34;</span>]
---
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">RoleBinding</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1beta1</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">deployer</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">$NAMESPACE</span>
<span style="color:#f92672">subjects</span>:
- <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ServiceAccount</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">deployer</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">$NAMESPACE</span>
<span style="color:#f92672">roleRef</span>:
  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Role</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">deployer-access</span>
</code></pre></div><h3 id="podsecuritypolicy">PodSecurityPolicy</h3>
<p>At default configuration users in docker containers has the same UID and GUID pool than the users on the host system. So if an unprivileged user runs a container as root and mount the host&rsquo;s filesystem to this container it can do what avers it wants on your host. Docker has an option to change the id pool us the users in the containers but kubernetes dose not support it. The RBAC adds access to an apiGroup like create deployments but dose not allow to configure the options you can use in the deployment.</p>
<p>A PodSecurityPolicy is a cluster-level resource for managing security aspects of a pod specification.</p>
<p>PSPs allow you to control:</p>
<ul>
<li>The ability to run privileged containers and control privilege escalation</li>
<li>Access to host filesystems</li>
<li>Usage of volume types</li>
<li>And a few other aspects including SELinux, AppArmor, sysctl, and seccomp profiles</li>
</ul>
<p>Pod Security Policies are implemented as an Admission Controller in Kubernetes. To enable PSPs in your cluster, make sure to include PodSecurityPolicy in the enable-admission-plugins list that is passed as a parameter to your Kubernetes API configuration:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano /etc/kubernetes/manifests/kube-apiserver.yaml
...
--enable-admission-plugins<span style="color:#f92672">=</span>...,PodSecurityPolicy
...
</code></pre></div><h4 id="creating-pod-security-policies">Creating Pod Security Policies</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">policy/v1beta1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">PodSecurityPolicy</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">restricted</span>
  <span style="color:#f92672">annotations</span>:
    <span style="color:#f92672">seccomp.security.alpha.kubernetes.io/allowedProfileNames</span>: <span style="color:#e6db74">&#39;docker/default&#39;</span>
    <span style="color:#f92672">apparmor.security.beta.kubernetes.io/allowedProfileNames</span>: <span style="color:#e6db74">&#39;runtime/default&#39;</span>
    <span style="color:#f92672">seccomp.security.alpha.kubernetes.io/defaultProfileName</span>:  <span style="color:#e6db74">&#39;runtime/default&#39;</span>
    <span style="color:#f92672">apparmor.security.beta.kubernetes.io/defaultProfileName</span>:  <span style="color:#e6db74">&#39;runtime/default&#39;</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">privileged</span>: <span style="color:#66d9ef">false</span>
  <span style="color:#f92672">allowPrivilegeEscalation</span>: <span style="color:#66d9ef">false</span>
  <span style="color:#f92672">defaultAllowPrivilegeEscalation</span>: <span style="color:#66d9ef">false</span>
  <span style="color:#f92672">readOnlyRootFilesystem</span>: <span style="color:#66d9ef">false</span>
  <span style="color:#f92672">hostNetwork</span>: <span style="color:#66d9ef">false</span>
  <span style="color:#f92672">hostIPC</span>: <span style="color:#66d9ef">false</span>
  <span style="color:#f92672">hostPID</span>: <span style="color:#66d9ef">false</span>
  <span style="color:#f92672">requiredDropCapabilities</span>:
    - <span style="color:#ae81ff">ALL</span>
  <span style="color:#f92672">volumes</span>:
    - <span style="color:#e6db74">&#39;configMap&#39;</span>
    - <span style="color:#e6db74">&#39;emptyDir&#39;</span>
    - <span style="color:#e6db74">&#39;projected&#39;</span>
    - <span style="color:#e6db74">&#39;secret&#39;</span>
    - <span style="color:#e6db74">&#39;downwardAPI&#39;</span>
    - <span style="color:#e6db74">&#39;persistentVolumeClaim&#39;</span>
  <span style="color:#f92672">hostPorts</span>:
    - <span style="color:#f92672">min</span>: <span style="color:#ae81ff">0</span>
      <span style="color:#f92672">max</span>: <span style="color:#ae81ff">0</span>
  <span style="color:#f92672">seLinux</span>:
    <span style="color:#f92672">rule</span>: <span style="color:#e6db74">&#39;RunAsAny&#39;</span>
  <span style="color:#f92672">runAsUser</span>:
    <span style="color:#f92672">rule</span>: <span style="color:#e6db74">&#39;MustRunAsNonRoot&#39;</span>
  <span style="color:#f92672">supplementalGroups</span>:
    <span style="color:#f92672">rule</span>: <span style="color:#e6db74">&#39;MustRunAs&#39;</span>
    <span style="color:#f92672">ranges</span>:
      - <span style="color:#f92672">min</span>: <span style="color:#ae81ff">1</span>
        <span style="color:#f92672">max</span>: <span style="color:#ae81ff">65535</span>
  <span style="color:#f92672">fsGroup</span>:
    <span style="color:#f92672">rule</span>: <span style="color:#e6db74">&#39;MustRunAs&#39;</span>
    <span style="color:#f92672">ranges</span>:
      - <span style="color:#f92672">min</span>: <span style="color:#ae81ff">1</span>
        <span style="color:#f92672">max</span>: <span style="color:#ae81ff">65535</span>
</code></pre></div><h4 id="assigning-pod-security-policies">Assigning Pod Security Policies</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRole</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">psp:restricted</span>
<span style="color:#f92672">rules</span>:
- <span style="color:#f92672">apiGroups</span>:
  - <span style="color:#ae81ff">extensions</span>
  <span style="color:#f92672">resources</span>:
  - <span style="color:#ae81ff">podsecuritypolicies</span>
  <span style="color:#f92672">resourceNames</span>:
  - <span style="color:#ae81ff">restricted</span> <span style="color:#75715e"># the psp we are giving access to</span>
  <span style="color:#f92672">verbs</span>:
  - <span style="color:#ae81ff">use</span>
---
<span style="color:#75715e"># This applies psp/restricted to all authenticated users</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRoleBinding</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">psp:restricted</span>
<span style="color:#f92672">subjects</span>:
- <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Group</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">system:authenticated</span> <span style="color:#75715e"># All authenticated users</span>
  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
<span style="color:#f92672">roleRef</span>:
  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRole</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">psp:restricted</span> <span style="color:#75715e"># A references to the role above</span>
  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
<span style="color:#ae81ff">view raw</span>
</code></pre></div><h3 id="audit-log">Audit Log</h3>
<p>Usually it’s a best practice to enable audits in your cluster. Let’s go ahead and create a basic policy saved in our master.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">mkdir -p /etc/kubernetes</span>

<span style="color:#ae81ff">cat &gt; /etc/kubernetes/audit-policy.yaml &lt;&lt;EOF</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">audit.k8s.io/v1beta1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Policy</span>
<span style="color:#f92672">rules</span>:
<span style="color:#75715e"># Do not log from kube-system accounts</span>
- <span style="color:#f92672">level</span>: <span style="color:#ae81ff">None</span>
  <span style="color:#f92672">userGroups</span>:
  - <span style="color:#ae81ff">system:serviceaccounts:kube-system</span>
  - <span style="color:#ae81ff">system:nodes</span>
- <span style="color:#f92672">level</span>: <span style="color:#ae81ff">None</span>
  <span style="color:#f92672">users</span>:
  - <span style="color:#ae81ff">system:apiserver</span>
  - <span style="color:#ae81ff">system:kube-scheduler</span>
  - <span style="color:#ae81ff">system:volume-scheduler</span>
  - <span style="color:#ae81ff">system:kube-controller-manager</span>
  - <span style="color:#ae81ff">system:node</span>
<span style="color:#75715e"># Don&#39;t log these read-only URLs.</span>
- <span style="color:#f92672">level</span>: <span style="color:#ae81ff">None</span>
  <span style="color:#f92672">nonResourceURLs</span>:
  - <span style="color:#ae81ff">/healthz*</span>
  - <span style="color:#ae81ff">/version</span>
  - <span style="color:#ae81ff">/swagger*</span>
<span style="color:#75715e"># limit level to Metadata so token is not included in the spec/status</span>
- <span style="color:#f92672">level</span>: <span style="color:#ae81ff">Metadata</span>
  <span style="color:#f92672">omitStages</span>:
  - <span style="color:#ae81ff">RequestReceived</span>
  <span style="color:#f92672">resources</span>:
  - <span style="color:#f92672">group</span>: <span style="color:#ae81ff">authentication.k8s.io</span>
    <span style="color:#f92672">resources</span>:
    - <span style="color:#ae81ff">tokenreviews</span>
<span style="color:#ae81ff">EOF</span>

<span style="color:#ae81ff">mkdir -p /var/log/kubernetes/apiserver</span>

<span style="color:#ae81ff">kube-apiserver --audit-log-path=/var/log/kubernetes/apiserver/audit.log \</span>
--<span style="color:#ae81ff">audit-policy-file=/etc/kubernetes/audit-policy.yaml</span>
</code></pre></div><h2 id="image-security">Image security</h2>
<p>Doesn&rsquo;t matter how secure is your kubernetes network or infrastructure is if you runs outdated unsecur images. You mast always update your base image, scan for known vulnerabilities. For applications use hardened base images and install as less components as you can. Some application for image scann:</p>
<ul>
<li>Anchore Engine</li>
<li>Clair</li>
<li>trivy</li>
</ul>
<h3 id="find-the-right-baseimage">Find the right baseimage</h3>
<p>I think the best choice for a base image is <code>Distroless</code>, which is set of images made by Google, that were created with intent to be secure. These images contain the bare minimum that&rsquo;s needed for your app.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">FROM gcr.io/distroless/python3
COPY --from<span style="color:#f92672">=</span>build-env /app /app
WORKDIR /app
CMD <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;hello.py&#34;</span>, <span style="color:#e6db74">&#34;/etc&#34;</span><span style="color:#f92672">]</span>
</code></pre></div><h3 id="least-privileged-user">Least privileged user</h3>
<p>Create a dedicated user and group on the image, with minimal permissions to run the application; use the same user to run this process. For example, Node.js image which has a built-in node generic user:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">USER node
CMD node index.js
</code></pre></div><h3 id="store-secret-in-etcd-encripted">Store secret in etcd encripted.</h3>
<p>The Kubernetes&rsquo;s base secret store is not so secure because it stores the data as base64 encoded plain text in the etcd.</p>
<p>The <code>kube-apiserver</code> process accepts an argument <code>--encryption-provider-config</code> that controls how API data is encrypted in etcd. An example configuration is provided below.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">mkdir /etc/kubernetes/etcd-enc/</span>

<span style="color:#ae81ff">head -c 32 /dev/urandom | base64</span>

<span style="color:#ae81ff">nano /etc/kubernetes/etcd-enc/etcd-encription.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">apiserver.config.k8s.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">EncryptionConfiguration</span>
<span style="color:#f92672">resources</span>:
  - <span style="color:#f92672">resources</span>:
    - <span style="color:#ae81ff">secrets</span>
    <span style="color:#f92672">providers</span>:
    - <span style="color:#f92672">identity</span>: {}
    - <span style="color:#f92672">aesgcm</span>:
        <span style="color:#f92672">keys</span>:
        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">key1</span>
          <span style="color:#f92672">secret</span>: <span style="color:#ae81ff">&lt;BASE 64 ENCODED SECRET&gt;</span>
</code></pre></div><p>In this example key1 is the secret contains the encryption/decryption key.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano kube-apiserver.yaml
...
    - --encryption-provider-config<span style="color:#f92672">=</span>/etc/kubernetes/etcd-enc/etcd-encription.yaml
...
    volumeMounts:
...
    - mountPath: /etc/kubernetes/etcd-enc
      name: etc-kubernetes-etcd-enc
      readOnly: true
  hostNetwork: true
...
  - hostPath:
      path: /etc/kubernetes/etcd-enc
      type: DirectoryOrCreate
    name: etc-kubernetes-etcd-enc
status: <span style="color:#f92672">{}</span>
</code></pre></div><h3 id="the-cis-kubernetes-benchmark">The CIS Kubernetes Benchmark</h3>
<p>The Center for Internet Security (CIS) Kubernetes Benchmark is a reference document that can be used by system administrators, security and audit professionals and other IT roles to establish a secure configuration baseline for Kubernetes.</p>
<p>Create kube-bench job</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f https://raw.githubusercontent.com/aquasecurity/kube-bench/master/job.yaml
kubectl get jobs --watch
</code></pre></div><p>Get job output from logs</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl logs <span style="color:#66d9ef">$(</span>kubectl get pods -l app<span style="color:#f92672">=</span>kube-bench -o name<span style="color:#66d9ef">)</span>
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Azure Conainer Registry integration for AKS]]></title>
            <link href="https://devopstales.github.io/home/aks-registry/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/aks-ingress-controller/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ingress to AKS" />
                <link href="https://devopstales.github.io/home/aks/?utm_source=atom_feed" rel="related" type="text/html" title="Install AKS Cluster" />
                <link href="https://devopstales.github.io/cloud/aks-registry/?utm_source=atom_feed" rel="related" type="text/html" title="Azure Conainer Registry integration for AKS" />
                <link href="https://devopstales.github.io/cloud/aks-ingress-controller/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ingress to AKS" />
                <link href="https://devopstales.github.io/cloud/aks/?utm_source=atom_feed" rel="related" type="text/html" title="Install AKS Cluster" />
            
                <id>https://devopstales.github.io/home/aks-registry/</id>
            
            
            <published>2020-11-18T00:00:00+00:00</published>
            <updated>2020-11-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this pos I will show you how you can integrate Azure Container Registry to AKS (Azure Kubernetes Service) Cluster.</p>
<H3>Parst of the AKS series</H3>
<ul>
     <li>Part1: <a href="../../cloud/AKS/">Install AKS Cluster</a></li>
     <li>Part2: <a href="../../cloud/aks-registry/">Integrate AKS with Registry</a></li>
     <li>Part3: <a href="../../cloud/aks-ingress-controller/">Install Ingresscontreoller To AKS</a></li>
</ul>


<h3 id="set-the-subscription">Set the subscription</h3>
<pre tabindex="0"><code>az login
az account list
az account set --subscription &lt;SUBSCRIPTION_ID&gt;
</code></pre><h3 id="creating-an-azure-resource-group">Creating an Azure Resource Group</h3>
<pre tabindex="0"><code>az group create --location &lt;REGION_NAME&gt; --name &lt;RESOURCE_GROUP_NAME&gt;
</code></pre><h3 id="provisioning-an-azure-container-registry">Provisioning an Azure Container Registry</h3>
<pre tabindex="0"><code>az acr create --name &lt;REGISTRY_NAME&gt; \
--resource-group &lt;RESOURCE_GROUP_NAME&gt; \
--sku Basic


az ad sp create-for-rbac \
  --scopes /subscriptions/&lt;SUBSCRIPTION_ID&gt;/resourcegroups/&lt;RESOURCE_GROUP_NAME&gt;/providers/Microsoft.ContainerRegistry/registries/&lt;REGISTRY_NAME&gt; \
  --role Contributor \
  --name &lt;SERVICE_PRINCIPAL_NAME&gt;

docker login &lt;REGISTRY_NAME&gt; -u &lt;CLIENT_ID&gt;
</code></pre><h3 id="create-a-new-aks-cluster-with-acr-integration">Create a new AKS cluster with ACR integration</h3>
<pre tabindex="0"><code>az aks create -n &lt;KUBERNETS_CLUSTER_NAME&gt; \
-g &lt;RESOURCE_GROUP_NAME&gt; \
--generate-ssh-keys \
--attach-acr &lt;REGISTRY_NAME&gt;
</code></pre><h3 id="configure-acr-integration-for-existing-aks-clusters">Configure ACR integration for existing AKS clusters</h3>
<pre tabindex="0"><code>az aks update -n &lt;KUBERNETS_CLUSTER_NAME&gt; \
-g &lt;RESOURCE_GROUP_NAME&gt; \
--attach-acr &lt;REGISTRY_NAME&gt;
</code></pre><h3 id="use-kubernetes-secret-for-registry-integration">Use Kubernetes Secret for registry integration</h3>
<pre tabindex="0"><code>ACR_NAME=&lt;REGISTRY_NAME&gt;
ACR_UNAME=$(az acr credential show -n $ACR_NAME --query=&quot;username&quot; -o tsv)
ACR_PASSWD=$(az acr credential show -n $ACR_NAME --query=&quot;passwords[0].value&quot; -o tsv)

kubectl create secret docker-registry acr-secret \
  --docker-server=$ACR_NAME \
  --docker-username=$ACR_UNAME \
  --docker-password=$ACR_PASSWD \
  --docker-email=ignorethis@email.com
</code></pre><p>Use secret in Kubernetes</p>
<pre tabindex="0"><code>---
apiVersion: v1
kind: Pod
metadata:
  name: sample-pod
spec:
  containers:
  - name: sample-pod-container
    image: youruniquename.azurecr.io/sample-container:0.0.1
  imagePullSecrets:
  - name: acr-secret
</code></pre><h3 id="use-service-account-for-authentication">Use Service Account For authentication</h3>
<pre tabindex="0"><code>ACR_NAME=&lt;REGISTRY_NAME&gt;
ACR_UNAME=$()
ACR_PASSWD=$()

kubectl create secret docker-registry acr-secret \
  --docker-server=$ACR_NAME \
  --docker-username=$ACR_UNAME \
  --docker-password=$ACR_PASSWD \
  --docker-email=ignorethis@email.com
</code></pre><p>Use ServiceAccount in Kubernetes</p>
<pre tabindex="0"><code>--docker-password=$ACR_PASSWD \
--docker-email=ignorethis@email.com

---
apiVersion: v1
kind: ServiceAccount
metadata:
name: SampleAccount
namespace: default
imagePullSecrets:
- name: acr-secret
---
apiVersion: v1
kind: Pod
metadata:
name: sample-pod
spec:
containers:
- name: sample-pod-container
  image: youracrname.azurecr.io/sample-container:0.0.1
serviceAccountName: SampleAccount
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Ingress to AKS]]></title>
            <link href="https://devopstales.github.io/home/aks-ingress-controller/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/aks/?utm_source=atom_feed" rel="related" type="text/html" title="Install AKS Cluster" />
                <link href="https://devopstales.github.io/cloud/aks-ingress-controller/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ingress to AKS" />
                <link href="https://devopstales.github.io/cloud/aks/?utm_source=atom_feed" rel="related" type="text/html" title="Install AKS Cluster" />
                <link href="https://devopstales.github.io/home/jitsi-meet/?utm_source=atom_feed" rel="related" type="text/html" title="Install Jitsi meet" />
                <link href="https://devopstales.github.io/home/jitsi-jibri/?utm_source=atom_feed" rel="related" type="text/html" title="Install recording fot Jitsi" />
            
                <id>https://devopstales.github.io/home/aks-ingress-controller/</id>
            
            
            <published>2020-11-15T00:00:00+00:00</published>
            <updated>2020-11-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this pos I will show you how you can Install Ingress to AKS (Azure Kubernetes Service) Cluster.</p>
<H3>Parst of the AKS series</H3>
<ul>
     <li>Part1: <a href="../../cloud/AKS/">Install AKS Cluster</a></li>
     <li>Part2: <a href="../../cloud/aks-registry/">Integrate AKS with Registry</a></li>
     <li>Part3: <a href="../../cloud/aks-ingress-controller/">Install Ingresscontreoller To AKS</a></li>
</ul>


<h3 id="get-aks-credentials">Get AKS credentials</h3>
<pre tabindex="0"><code>az login
az aks get-credentials --resource-group test-cluster --name test-cluster
kubectl get nodes
</code></pre><h3 id="create-ingress-with-static-public-ip">Create ingress with static public ip</h3>
<pre tabindex="0"><code>az network public-ip create \
--location &lt;REGION_NAME&gt; \
--resource-group &lt;RESOURCE_GROUP_NAME&gt; \
--name &lt;IP_NAME&gt; --sku Standard \
--allocation-method static \
--query publicIp.ipAddress -o tsv

# 51.105.230.165
</code></pre><h3 id="deploy-nginx-ingress-controller-with-helm">Deploy nginx ingress controller with helm</h3>
<pre tabindex="0"><code>kubectl create namespace ingress

helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx

helm install &lt;INGERSS_NAME&gt; \
ingress-nginx/ingress-nginx \
--namespace ingress \
--set controller.replicaCount=2 \
--set controller.nodeSelector.&quot;beta\.kubernetes\.io/os&quot;=linux \
--set defaultBackend.nodeSelector.&quot;beta\.kubernetes\.io/os&quot;=linux \
--set controller.service.loadBalancerIP=&quot;51.105.230.165&quot;
</code></pre><pre tabindex="0"><code>kubectl --namespace ingress get services -o wide -w nginx-ingress-controller
kubectl get service -l app=nginx-ingress --namespace ingress
</code></pre><h3 id="create-an-ingress-controller-to-an-internal-virtual-network-in">Create an ingress controller to an internal virtual network in</h3>
<p>By default, an NGINX ingress controller is created with a dynamic public IP address assignment. A common configuration requirement is to use an internal, private network and IP address. This approach allows you to restrict access to your services to internal users, with no external access. This example assigns 10.240.0.42 to the loadBalancerIP resource.</p>
<pre tabindex="0"><code>nano internal-ingress.yaml
---
controller:
  service:
    loadBalancerIP: 10.240.0.42
    annotations:
      service.beta.kubernetes.io/azure-load-balancer-internal: &quot;true&quot;
</code></pre><pre tabindex="0"><code>helm install nginx-ingress \
ingress-nginx/ingress-nginx \
--namespace ingress \
-f internal-ingress.yaml \
--set controller.ingressClass=internal \
--set controller.replicaCount=2 \
--set controller.nodeSelector.&quot;beta\.kubernetes\.io/os&quot;=linux \
--set defaultBackend.nodeSelector.&quot;beta\.kubernetes\.io/os&quot;=linux
</code></pre><p>Create an ingress route</p>
<pre tabindex="0"><code>apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: hello-world-ingress
  namespace: ingress-basic
  annotations:
    kubernetes.io/ingress.class: internal
...
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install AKS Cluster]]></title>
            <link href="https://devopstales.github.io/home/aks/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/aks/?utm_source=atom_feed" rel="related" type="text/html" title="Install AKS Cluster" />
                <link href="https://devopstales.github.io/home/jitsi-meet/?utm_source=atom_feed" rel="related" type="text/html" title="Install Jitsi meet" />
                <link href="https://devopstales.github.io/home/jitsi-jibri/?utm_source=atom_feed" rel="related" type="text/html" title="Install recording fot Jitsi" />
                <link href="https://devopstales.github.io/home/k8s-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure K8S vSphere Cloud Provider" />
                <link href="https://devopstales.github.io/home/k8s-ipvs/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with IPVS mode" />
            
                <id>https://devopstales.github.io/home/aks/</id>
            
            
            <published>2020-11-07T00:00:00+00:00</published>
            <updated>2020-11-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this pos I will show you how you can create an AKS (Azure Kubernetes Service) Cluster.</p>
<H3>Parst of the AKS series</H3>
<ul>
     <li>Part1: <a href="../../cloud/AKS/">Install AKS Cluster</a></li>
     <li>Part2: <a href="../../cloud/aks-registry/">Integrate AKS with Registry</a></li>
     <li>Part3: <a href="../../cloud/aks-ingress-controller/">Install Ingresscontreoller To AKS</a></li>
</ul>


<h3 id="what-is-azure-kubernetes-service">What is Azure Kubernetes Service?</h3>
<p>Azure Kubernetes Service (AKS) is a managed Kubernetes service that lets you quickly deploy and manage clusters.</p>
<p>A Kubernetes cluster is divided into two components:</p>
<ul>
<li>Control plane nodes provide the core Kubernetes services and orchestration of application workloads.</li>
<li>Nodes run your application workloads.</li>
</ul>
<p><img src="/img/include/aks-control-plane-and-nodes.png" alt="Example image"  class="zoomable" /></p>
<p>When you create an AKS cluster, a control plane is automatically created and configured. This control plane is provided as a managed Azure resource abstracted from the user. There&rsquo;s no cost for the control plane, only the nodes that are part of the AKS cluster. The control plane and its resources reside only on the region where you created the cluster.</p>
<h3 id="how-to-create-azure-kubernetes-cluster">How To Create Azure Kubernetes Cluster</h3>
<p>There are 2 ways to deploy an Azure Kubernetes Cluster, which are using:</p>
<ul>
<li>Azure Portal</li>
<li>Azure CLI</li>
</ul>
<p>I prefer to use the cli because it is easier to reproduce.</p>
<h4 id="install-azure-cli">Install Azure CLI</h4>
<pre tabindex="0"><code># OSX
brew update &amp;&amp; brew install azure-cli

# Yum
sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc

sudo sh -c 'echo -e &quot;[azure-cli]
name=Azure CLI
baseurl=https://packages.microsoft.com/yumrepos/azure-cli
enabled=1
gpgcheck=1
gpgkey=https://packages.microsoft.com/keys/microsoft.asc&quot; &gt; /etc/yum.repos.d/azure-cli.repo'

sudo yum install azure-cli

# apt
curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

sudo apt-get update
sudo apt-get install ca-certificates curl apt-transport-https lsb-release gnupg

curl -sL https://packages.microsoft.com/keys/microsoft.asc |
    gpg --dearmor |
    sudo tee /etc/apt/trusted.gpg.d/microsoft.gpg &gt; /dev/null

AZ_REPO=$(lsb_release -cs)
echo &quot;deb [arch=amd64] https://packages.microsoft.com/repos/azure-cli/ $AZ_REPO main&quot; |
    sudo tee /etc/apt/sources.list.d/azure-cli.list

sudo apt-get update
sudo apt-get install azure-cli
</code></pre><h4 id="set-the-subscription">Set the subscription</h4>
<pre tabindex="0"><code>az login
az account list
az account set --subscription &lt;SUBSCRIPTION_ID&gt;
</code></pre><h4 id="creating-an-azure-resource-group">Creating an Azure Resource Group</h4>
<pre tabindex="0"><code>az group create --location &lt;REGION_NAME&gt; --name &lt;RESOURCE_GROUP_NAME&gt;
</code></pre><h4 id="create-aks-cluster">Create AKS Cluster</h4>
<pre tabindex="0"><code># get available kubernetes versions
az aks get-versions --location &lt;REGION_NAME&gt;

az aks create --resource-group &lt;RESOURCE_GROUP_NAME&gt; \
--name &lt;AKS_CLUSTER_NAME&gt; \
--node-count 3 \
--generate-ssh-keys \
--kubernetes-version 1.17.0
</code></pre><h3 id="uses-availability-zones-for-aks-cluster">Uses availability zones for AKS Cluster</h3>
<p>AKS clusters that are deployed using availability zones can distribute nodes across multiple zones within a single region. For example, a cluster in the East US 2 region can create nodes in all three availability zones in East US 2. This distribution of AKS cluster resources improves cluster availability as they&rsquo;re resilient to failure of a specific zone.</p>
<p>KÉP: <a href="https://docs.microsoft.com/en-us/azure/aks/media/availability-zones/aks-availability-zones.png">https://docs.microsoft.com/en-us/azure/aks/media/availability-zones/aks-availability-zones.png</a></p>
<pre tabindex="0"><code># get available kubernetes versions
az aks get-versions --location &lt;REGION_NAME&gt;

az aks create --resource-group &lt;RESOURCE_GROUP_NAME&gt; \
--name &lt;AKS_CLUSTER_NAME&gt; \
--node-count 3 \
--zones 1 2 3 \
--vm-set-type VirtualMachineScaleSets \
--load-balancer-sku standard \
--generate-ssh-keys \
--kubernetes-version 1.17.0
</code></pre><h4 id="get-kubeconfig-congig-of-the-cluster">Get Kubeconfig Congig of the Cluster</h4>
<pre tabindex="0"><code>az aks get-credentials --name &lt;AKS_CLUSTER_NAME&gt; \
 --resource-group &lt;RESOURCE_GROUP_NAME&gt;

kubectl get nodes
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install recording fot Jitsi]]></title>
            <link href="https://devopstales.github.io/home/jitsi-jibri/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/jitsi-meet/?utm_source=atom_feed" rel="related" type="text/html" title="Install Jitsi meet" />
                <link href="https://devopstales.github.io/home/k8s-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure K8S vSphere Cloud Provider" />
                <link href="https://devopstales.github.io/home/k8s-ipvs/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with IPVS mode" />
                <link href="https://devopstales.github.io/home/k8s-calico-ebpf/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with calico&#39;s eBPF mode" />
                <link href="https://devopstales.github.io/home/install-unifi-controller/?utm_source=atom_feed" rel="related" type="text/html" title="How to setup Unifi Controller on Debian 10" />
            
                <id>https://devopstales.github.io/home/jitsi-jibri/</id>
            
            
            <published>2020-10-21T00:00:00+00:00</published>
            <updated>2020-10-21T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you my productivity tips with kubectlyo can install Jibri the recording component of Jitsi.</p>
<h3 id="install-requirements">Install requirements</h3>
<pre tabindex="0"><code>sudo apt install ffmpeg curl unzip software-properties-common
</code></pre><p>Enable the ALSA loopback module to start on boot,</p>
<pre tabindex="0"><code>echo &quot;snd_aloop&quot; &gt;&gt; /etc/modules
modprobe snd_aloop
</code></pre><h3 id="install-google-chrome">Install Google Chrome:</h3>
<pre tabindex="0"><code>curl -o - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add
echo &quot;deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main&quot; &gt; /etc/apt/sources.list.d/google-chrome.list
apt update
apt install google-chrome-stable

# install headless browser driver
CHROME_DRIVER_VERSION=`curl -sS chromedriver.storage.googleapis.com/LATEST_RELEASE`
wget -N http://chromedriver.storage.googleapis.com/$CHROME_DRIVER_VERSION/chromedriver_linux64.zip -P ~/
unzip ~/chromedriver_linux64.zip -d ~/
rm ~/chromedriver_linux64.zip
sudo mv -f ~/chromedriver /usr/local/bin/chromedriver
sudo chmod 0755 /usr/local/bin/chromedriver

# disable warnings
mkdir -p /etc/opt/chrome/policies/managed
echo '{ &quot;CommandLineFlagSecurityWarningsEnabled&quot;: false }' &gt;&gt;/etc/opt/chrome/policies/managed/managed_policies.json
</code></pre><h3 id="install-jibri">Install jibri</h3>
<pre tabindex="0"><code>apt install jibri
usermod -aG adm,audio,video,plugdev jibri

nano /etc/prosody/prosody.cfg.lua
...
Component &quot;conference.jitsi.mydomain.intra&quot; &quot;muc&quot;
modules_enabled = { &quot;muc_mam&quot; }
...
Component &quot;internal.auth.jitsi.mydomain.intra&quot; &quot;muc&quot;
modules_enabled = {
&quot;ping&quot;;
}
storage = &quot;internal&quot;
muc_room_cache_size = 1000
...
VirtualHost &quot;recorder.jitsi.mydomain.intra&quot;
modules_enabled = {
&quot;ping&quot;;
}
authentication = &quot;internal_plain&quot;
</code></pre><p>Create accounts:</p>
<pre tabindex="0"><code>prosodyctl register jibri auth.jitsi.mydomain.intra Password1
prosodyctl register recorder recorder.jitsi.mydomain.intra Password1
</code></pre><pre tabindex="0"><code>sudo nano /etc/jitsi/jicofo/sip-communicator.properties
...
org.jitsi.jicofo.jibri.BREWERY=JibriBrewery@internal.auth.jitsi.mydomain.intra
org.jitsi.jicofo.jibri.PENDING_TIMEOUT=90

sudo nano /etc/jitsi/meet/jitsi.mydomain.intra-config.js
...
fileRecordingsEnabled: true,
liveStreamingEnabled: true,
hiddenDomain: 'recorder.jitsi.mydomain.intra',
</code></pre><p>Configure storage:</p>
<pre tabindex="0"><code>mkdir /recordings
chown jibri:jibri /recordings

sudo nano /etc/jitsi/jibri/config.json
&quot;recording_directory&quot;: &quot;/recordings&quot;,
&quot;finalize_recording_script_path&quot;: &quot;&quot;,
&quot;xmpp_server_hosts&quot;: [
&quot;jitsi.mydomain.intra&quot;
],
&quot;xmpp_domain&quot;: &quot;jitsi.mydomain.intra&quot;,
&quot;control_login&quot;: {
&quot;domain&quot;: &quot;auth.jitsi.mydomain.intra&quot;,
&quot;username&quot;: &quot;jibri&quot;,
&quot;password&quot;: &quot;Password1d&quot;
},
&quot;control_muc&quot;: {
&quot;domain&quot;: &quot;internal.auth.jitsi.mydomain.intra&quot;,
&quot;room_name&quot;: &quot;JibriBrewery&quot;,
&quot;nickname&quot;: &quot;jibri&quot;
},
&quot;call_login&quot;: {
&quot;domain&quot;: &quot;recorder.jitsi.mydomain.intra&quot;,
&quot;username&quot;: &quot;recorder&quot;,
&quot;password&quot;: &quot;Password1&quot;
},
</code></pre><h3 id="install-java-8">Install Java 8</h3>
<p>Jibri only works with java 1.8 so we need to install and configure Jibri to use it:</p>
<pre tabindex="0"><code>wget -O - https://adoptopenjdk.jfrog.io/adoptopenjdk/api/gpg/key/public | sudo apt-key add -
add-apt-repository https://adoptopenjdk.jfrog.io/adoptopenjdk/deb/
apt update
apt install adoptopenjdk-8-hotspot

# the systemd sript use this script so we edit it
nano /opt/jitsi/jibri/launch.sh
# change this:
# exec java ...
# tho this:
exec /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64/bin/java ...
</code></pre><pre tabindex="0"><code>systemctl restart prosody jicofo jitsi-videobridge2 jibri
systemctl enable jibri
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Jitsi meet]]></title>
            <link href="https://devopstales.github.io/home/jitsi-meet/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/jitsi-meet/?utm_source=atom_feed" rel="related" type="text/html" title="Install Jitsi meet" />
                <link href="https://devopstales.github.io/home/k8s-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure K8S vSphere Cloud Provider" />
                <link href="https://devopstales.github.io/home/k8s-ipvs/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with IPVS mode" />
                <link href="https://devopstales.github.io/home/k8s-calico-ebpf/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with calico&#39;s eBPF mode" />
                <link href="https://devopstales.github.io/home/install-unifi-controller/?utm_source=atom_feed" rel="related" type="text/html" title="How to setup Unifi Controller on Debian 10" />
            
                <id>https://devopstales.github.io/home/jitsi-meet/</id>
            
            
            <published>2020-10-20T00:00:00+00:00</published>
            <updated>2020-10-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can install Jitsi meet on your server.</p>
<p>Jitsi is state-of-the art video conferencing software that you can self-host or simply use at meet.jit.si.</p>
<h3 id="configure-hostname">Configure hostname</h3>
<p>In this step, you will change the system’s hostname to match the domain name that you intend to use for your Jitsi Meet instance.</p>
<pre tabindex="0"><code>sudo hostnamectl set-hostname jitsi.mydomain.intra
sudo nano nano /etc/hosts
127.0.0.1 jitsi.mydomain.intra
</code></pre><h3 id="install-jitsi-meet">Install Jitsi Meet</h3>
<pre tabindex="0"><code>curl https://download.jitsi.org/jitsi-key.gpg.key | \
sudo sh -c 'gpg --dearmor &gt; /usr/share/keyrings/jitsi-keyring.gpg'

echo 'deb [signed-by=/usr/share/keyrings/jitsi-keyring.gpg] https://download.jitsi.org stable/' | \
sudo tee /etc/apt/sources.list.d/jitsi-stable.list &gt; /dev/null
</code></pre><pre tabindex="0"><code>sudo apt install apt-transport-https
sudo apt update
sudo apt install jitsi-meet
</code></pre><p>During the installation of <code>jitsi-meet</code> you will be prompted to enter the domain name. Then you will be shown a new dialog box that asks if you want Jitsi to create and use a self-signed TLS certificate or use an existing one. For this demo I will use the self-signed option.</p>
<p>If you want to use letsencrypt select self-signed the install certboot and use jitsi&rsquo;s script like this:</p>
<pre tabindex="0"><code>sudo add-apt-repository ppa:certbot/certbot
sudo apt install certbot
sudo /usr/share/jitsi-meet/scripts/install-letsencrypt-cert.sh
</code></pre><h3 id="nat-configuration">NAT Configuration</h3>
<p>If the installation is behind NAT jitsi-videobridge should configure jitsi-videobridge  in order for it to be accessible from outside.</p>
<pre tabindex="0"><code>nano /etc/jitsi/videobridge/sip-communicator.properties
org.ice4j.ice.harvest.NAT_HARVESTER_LOCAL_ADDRESS=&lt;Local.IP.Address&gt;
org.ice4j.ice.harvest.NAT_HARVESTER_PUBLIC_ADDRESS=&lt;Public.IP.Address&gt;
</code></pre><p>The you need to NAT the ports 443 4443 10000 from externat ip to the server.</p>
<h3 id="configure-jitsi">Configure Jitsi</h3>
<p>The default config allow any user to start meetings without authentication. For a publicly accessible server this is not what we want.</p>
<p>With this configuration all users need to authenticate to use Jitsi.</p>
<pre tabindex="0"><code>sudo nano /etc/prosody/conf.avail/jitsi.mydomain.intra.cfg.lua
...
VirtualHost &quot;jitsi.mydomain.intra&quot;
# change this:
#       authentication = &quot;anonymous&quot;
# tho this:
        authentication = &quot;internal_plain&quot;

sudo nano /etc/jitsi/jicofo/sip-communicator.properties
...
org.jitsi.jicofo.auth.URL=XMPP:jitsi.mydomain.intra
</code></pre><p>Create users manually</p>
<pre tabindex="0"><code>prosodyctl register &lt;username&gt; jitsi.mydomain.intra &lt;Password&gt;
</code></pre><p>For a few user is is ok to create them manually but for many users you need something like ldap:</p>
<pre tabindex="0"><code>sudo apt install sasl2-bin libsasl2-modules-ldap lua-cyrussasl
sudo nano /etc/prosody/conf.avail/ldap.cfg.lua
VirtualHost &quot;jitsi.mydomain.intra&quot;
# change this:
#       authentication = &quot;anonymous&quot;
# tho this:
        authentication = &quot;cyrus&quot;
        cyrus_application_name = &quot;xmpp&quot;
...
        modules_enabled = {
...
            &quot;auth_cyrus&quot;; -- Add this line
        }

        c2s_require_encryption = false
...
</code></pre><pre tabindex="0"><code>sudo nano /etc/sasl/xmpp.conf
pwcheck_method: saslauthd
mech_list: PLAIN

sudo nano /etc/saslauthd.conf
ldap_servers: ldap://10.0.0.1
ldap_search_base: dc=my,dc=search,dc=base
ldap_bind_dn: cn=Administrator,cn=Users,dc=foo,dc=bar
ldap_bind_pw: PassW0rd
ldap_filter: (samaccountname=%u)
ldap_version: 3
ldap_auth_method: bind
# for tls change ldap_servers to ldaps://
# the add uncomment and configure this:
#ldap_tls_key: /config/certs/meet.jit.si.key
#ldap_tls_cert: /config/certs/meet.jit.si.crt

#ldap_tls_check_peer: yes
#ldap_tls_cacert_file: /etc/ssl/certs/ca-certificates.crt
#ldap_tls_cacert_dir: /etc/ssl/certs
</code></pre><p>The next configuration allows anonymous users to join conference rooms that were created by an authenticated user.</p>
<pre tabindex="0"><code>sudo nano /etc/prosody/conf.avail/jitsi.mydomain.intra.cfg.lua
...
# to the end
VirtualHost &quot;guest.jitsi.mydomain.intra&quot;
    authentication = &quot;anonymous&quot;
    c2s_require_encryption = false

</code></pre><p>The <code>guest.</code> hostname is only used internally by Jitsi Meet. You will never enter it into a browser or need to create a DNS record for it.</p>
<pre tabindex="0"><code>sudo nano /etc/jitsi/meet/jitsi.mydomain.intra-config.js
var config = {
    hosts: {
        domain: &quot;jitsi.mydomain.intra&quot;,
        anonymousdomain: 'guest.jitsi.mydomain.intra',
</code></pre><p>After a config change yo need to restart the services:</p>
<pre tabindex="0"><code>systemctl restart prosody jicofo jitsi-videobridge2
</code></pre><h2 id="more-info">More info:</h2>
<ul>
<li><a href="https://github.com/jitsi/jitsi-meet/wiki/LDAP-Authentication#configure-saslauthd">https://github.com/jitsi/jitsi-meet/wiki/LDAP-Authentication#configure-saslauthd</a></li>
</ul>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configure K8S vSphere Cloud Provider]]></title>
            <link href="https://devopstales.github.io/home/k8s-vmware/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-ipvs/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with IPVS mode" />
                <link href="https://devopstales.github.io/home/k8s-calico-ebpf/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with calico&#39;s eBPF mode" />
                <link href="https://devopstales.github.io/home/openshift-rbd-fsck/?utm_source=atom_feed" rel="related" type="text/html" title="How to fixing filesystem corruption on a Kubernetes Ceph RBD PersistentVolume" />
                <link href="https://devopstales.github.io/home/k3s-helm-controller/?utm_source=atom_feed" rel="related" type="text/html" title="Install and use rancher helm-controller" />
                <link href="https://devopstales.github.io/home/k3s-crio/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with CRI-O and kadalu" />
            
                <id>https://devopstales.github.io/home/k8s-vmware/</id>
            
            
            <published>2020-10-14T00:00:00+00:00</published>
            <updated>2020-10-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how can you use vmware for persistent storagi on K8S.</p>
<H3>Parst of the Kubernetes series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
     <li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
     <li>Part1c: <a href="../../kubernetes/k8s-install-containerd/">Install K8S with containerd and kubeadm</a></li>
     <li>Part1d: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
     <li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a></li>
<!--      <li>Part3b: k8s-multiple-nginx-ingress -->
<!--      <li>Part3c: k8s-contour-ingress -->
     <li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a></li>
<!-- local folder (+ autoprovisioning ) -->
     <li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
     <li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
     <li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
     <li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
     <li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
     <li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a></li>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... -->
     <li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
     <li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
     <li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
     <li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
     <li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
     <li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
     <li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a></li>
<!-- istio-install.md -->
     <li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a></li>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog -->
</ul>


<h3 id="vsphere-configuration">vSphere Configuration</h3>
<ul>
<li>Create a folder for all the VMs in vCenter</li>
<li>In the navigator, select the data center</li>
<li>Right-click and select the menu option to create the folder.</li>
<li>Select: All vCenter Actions &gt; New VM and Template Folder.</li>
<li>Move K8S vms to this folder</li>
<li>The name of the virtual machine must match the name of the nodes for the K8S cluster.</li>
</ul>
<p><img src="/img/include/k8s-vmware.png" alt="Example image"  class="zoomable" /></p>
<h3 id="set-up-the-govc-environment">Set up the GOVC environment:</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># on deployer</span>
curl -LO https://github.com/vmware/govmomi/releases/download/v0.20.0/govc_linux_amd64.gz
gunzip govc_linux_amd64.gz
chmod +x govc_linux_amd64
cp govc_linux_amd64 /usr/bin/govc
echo <span style="color:#e6db74">&#34;export GOVC_URL=&#39;vCenter IP OR FQDN&#39;&#34;</span> &gt;&gt; /etc/profile
echo <span style="color:#e6db74">&#34;export GOVC_USERNAME=&#39;vCenter User&#39;&#34;</span> &gt;&gt; /etc/profile
echo <span style="color:#e6db74">&#34;export GOVC_PASSWORD=&#39;vCenter Password&#39;&#34;</span> &gt;&gt; /etc/profile
echo <span style="color:#e6db74">&#34;export GOVC_INSECURE=1&#34;</span> &gt;&gt; /etc/profile
source /etc/profile
</code></pre></div><p>Add <code>disk.enableUUID=1</code> for all VM:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">govc vm.info &lt;vm&gt;
govc ls /Datacenter/kubernetes/&lt;vm-folder-name&gt;
<span style="color:#75715e"># example:</span>
govc ls /Datacenter/kubernetes/k8s-01

govc vm.change -e<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;disk.enableUUID=1&#34;</span> -vm<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;VM Path&#39;</span>
<span style="color:#75715e"># example:</span>
govc vm.change -e<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;disk.enableUUID=1&#34;</span> -vm<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;/datacenter/kubernetes/k8s-01/k8s-m01&#39;</span>
</code></pre></div><p>VM Hardware should be at version 15 or higher. Upgrade if needed:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">govc vm.option.info <span style="color:#e6db74">&#39;/datacenter/kubernetes/k8s-01/k8s-m01&#39;</span> | grep HwVersion

govc vm.upgrade -version<span style="color:#f92672">=</span><span style="color:#ae81ff">15</span> -vm <span style="color:#e6db74">&#39;/datacenter/kubernetes/k8s-01/k8s-m01&#39;</span>
</code></pre></div><h3 id="create-the-required-roles">Create the required Roles</h3>
<ul>
<li>Navigate in the vSphere Client - Menu &gt; Administration &gt; Roles</li>
<li>Add a new Role and select the permissions required. Repeat for each role.</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:center">Roles</th>
<th style="text-align:center">Privileges</th>
<th style="text-align:center">Entities</th>
<th style="text-align:center">Propagate to Children</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">vcp-manage-k8s-node-vms</td>
<td style="text-align:center">Resource.AssignVMToPoolVirtualMachine.Config.AddExistingDisk, VirtualMachine.Config.AddNewDisk, VirtualMachine.Config.AddRemoveDevice, VirtualMachine.Config.RemoveDisk, VirtualMachine.Config.SettingsVirtualMachine.Inventory.Create, VirtualMachine.Inventory.Delete</td>
<td style="text-align:center">Cluster, Hosts, VM Folder</td>
<td style="text-align:center">Yes</td>
</tr>
<tr>
<td style="text-align:center">vcp-manage-k8s-volumes</td>
<td style="text-align:center">Datastore.AllocateSpace, Datastore.FileManagement (Low level file operations)</td>
<td style="text-align:center">Datastore</td>
<td style="text-align:center">No</td>
</tr>
<tr>
<td style="text-align:center">vcp-view-k8s-spbm-profile</td>
<td style="text-align:center">StorageProfile.View (Profile-driven storage view)</td>
<td style="text-align:center">vCenter</td>
<td style="text-align:center">No</td>
</tr>
<tr>
<td style="text-align:center">Read-only (pre-existing default role)</td>
<td style="text-align:center">System.Anonymous, System.Read, System.View</td>
<td style="text-align:center">Datacenter, Datastore Cluster, Datastore Storage Folder</td>
<td style="text-align:center">No</td>
</tr>
</tbody>
</table>
<h3 id="create-a-service-account">Create a service account</h3>
<ul>
<li>Create a vsphere user, or add a domain user, to provide access and assign the new roles to.</li>
</ul>
<h3 id="create-vsphereconf">Create vsphere.conf</h3>
<p>Create the vSphere configuration file in /etc/kubernetes/vcp/vsphere.conf - you’ll need to create the folder.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano /etc/kubernetes/vcp/vsphere.conf
<span style="color:#f92672">[</span>Global<span style="color:#f92672">]</span>
user <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;k8s-user@vsphere.local&#34;</span>
password <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;password for k8s-user&#34;</span>
port <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;443&#34;</span>
insecure-flag <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;1&#34;</span>

<span style="color:#f92672">[</span>VirtualCenter <span style="color:#e6db74">&#34;10.0.1.200&#34;</span><span style="color:#f92672">]</span>
datacenters <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;DC-1&#34;</span>

<span style="color:#f92672">[</span>Workspace<span style="color:#f92672">]</span>
server <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;10.0.1.200&#34;</span>
datacenter <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;DC-1&#34;</span>
default-datastore <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;vsanDatastore&#34;</span>
resourcepool-path <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;ClusterNameHere/Resources&#34;</span>
folder <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;kubernetes&#34;</span>

<span style="color:#f92672">[</span>Disk<span style="color:#f92672">]</span>
scsicontrollertype <span style="color:#f92672">=</span> pvscsi
</code></pre></div><h3 id="modify-the-kubelet-service">Modify the kubelet service</h3>
<p>On master:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano /etc/systemd/system/kubelet.service
<span style="color:#f92672">[</span>Service<span style="color:#f92672">]</span>
...
ExecStart<span style="color:#f92672">=</span>/usr/bin/docker run <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>...
        /hyperkube kubelet <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>...
--cloud-provider<span style="color:#f92672">=</span>vsphere --cloud-config<span style="color:#f92672">=</span>/etc/kubernetes/vsphere.conf    
</code></pre></div><p>On worker:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano /etc/systemd/system/kubelet.service
<span style="color:#f92672">[</span>Service<span style="color:#f92672">]</span>
...
ExecStart<span style="color:#f92672">=</span>/usr/bin/docker run <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>...
        /hyperkube kubelet <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>...
--cloud-provider<span style="color:#f92672">=</span>vsphere  
</code></pre></div><h3 id="modify-container-manifests">Modify container manifests</h3>
<p>Add following flags to the kubelet service configuration (usually in the systemd config file), as well as the controller-manager and api-server container manifest files on the master node (usually in /etc/kubernetes/manifests).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano /etc/kubernetes/manifests/kube-apiserver.yaml
...
spec:
  containers:
  - command:
    - kube-apiserver
...
    - --cloud-provider<span style="color:#f92672">=</span>vsphere
    - --cloud-config<span style="color:#f92672">=</span>/etc/kubernetes/vsphere.conf
    volumeMounts:
    - mountPath: /etc/kubernetes/vcp
      name: vcp
      readOnly: true
...
  volumes:
  - hostPath:
      path: /etc/kubernetes/vcp
      type: DirectoryOrCreate
    name: vcp
</code></pre></div><p>Restart the services.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">systemctl restart kubelet docker
</code></pre></div><h3 id="add-providerid">Add providerID</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl get nodes -o json | jq <span style="color:#e6db74">&#39;.items[]|[.metadata.name, .spec.providerID, .status.nodeInfo.systemUUID]&#39;</span>

nano k8s-vmware-pacher.sh
DATACENTER<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;&lt;Datacenter&gt;&#39;</span>
FOLDER<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;&lt;vm-folder-name&gt;&#39;</span>
<span style="color:#66d9ef">for</span> vm in <span style="color:#66d9ef">$(</span>govc ls /$DATACENTER/vm/$FOLDER <span style="color:#66d9ef">)</span>; <span style="color:#66d9ef">do</span>
  MACHINE_INFO<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>govc vm.info -json -dc<span style="color:#f92672">=</span>$DATACENTER -vm.ipath<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;</span>$vm<span style="color:#e6db74">&#34;</span> -e<span style="color:#f92672">=</span>true<span style="color:#66d9ef">)</span>
  <span style="color:#75715e"># My VMs are created on vmware with upper case names, so I need to edit the names with awk</span>
  VM_NAME<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>jq -r <span style="color:#e6db74">&#39; .VirtualMachines[] | .Name&#39;</span> <span style="color:#f92672">&lt;&lt;&lt;</span> $MACHINE_INFO | awk <span style="color:#e6db74">&#39;{print tolower($0)}&#39;</span><span style="color:#66d9ef">)</span>
  <span style="color:#75715e"># UUIDs come in lowercase, upper case then</span>
  VM_UUID<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span> jq -r <span style="color:#e6db74">&#39; .VirtualMachines[] | .Config.Uuid&#39;</span> <span style="color:#f92672">&lt;&lt;&lt;</span> $MACHINE_INFO | awk <span style="color:#e6db74">&#39;{print toupper($0)}&#39;</span><span style="color:#66d9ef">)</span>
  echo <span style="color:#e6db74">&#34;Patching </span>$VM_NAME<span style="color:#e6db74"> with UUID:</span>$VM_UUID<span style="color:#e6db74">&#34;</span>
  <span style="color:#75715e"># This is done using dry-run to avoid possible mistakes, remove when you are confident you got everything right.</span>
  kubectl patch node $VM_NAME -p <span style="color:#e6db74">&#34;{\&#34;spec\&#34;:{\&#34;providerID\&#34;:\&#34;vsphere://</span>$VM_UUID<span style="color:#e6db74">\&#34;}}&#34;</span>
<span style="color:#66d9ef">done</span>

chmod +x openshift-vmware-pacher.sh
./openshift-vmware-pacher.sh
</code></pre></div><h3 id="create-vsphere-storage-class">Create vSphere storage-class</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano vmware-sc.yml
---
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: <span style="color:#e6db74">&#34;true&#34;</span>
  name: <span style="color:#e6db74">&#34;vsphere-standard&#34;</span>
provisioner: kubernetes.io/vsphere-volume
parameters:
    diskformat: zeroedthick
    datastore: <span style="color:#e6db74">&#34;NFS&#34;</span>
reclaimPolicy: Delete

kubectl aplay -f vmware-sc.yml
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install k8s with calico's eBPF mode]]></title>
            <link href="https://devopstales.github.io/home/k8s-calico-ebpf/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-ipvs/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with IPVS mode" />
                <link href="https://devopstales.github.io/kubernetes/k8s-calico-ebpf/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with calico&#39;s eBPF mode" />
                <link href="https://devopstales.github.io/home/k8s-install-openebs/?utm_source=atom_feed" rel="related" type="text/html" title="Install OpenEBS for Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-kubeadm-ha/?utm_source=atom_feed" rel="related" type="text/html" title="How to install kubernetes with kubeadm in HA mode" />
                <link href="https://devopstales.github.io/home/k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install kubernetes with kubeadm" />
            
                <id>https://devopstales.github.io/home/k8s-calico-ebpf/</id>
            
            
            <published>2020-10-13T00:00:00+00:00</published>
            <updated>2020-10-13T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I&rsquo;will show you how to install kubernetes Without kube-proxy using calico&rsquo;s eBPF mode.</p>
<H3>Parst of the Kubernetes series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
     <li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
     <li>Part1c: <a href="../../kubernetes/k8s-install-containerd/">Install K8S with containerd and kubeadm</a></li>
     <li>Part1d: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
     <li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a></li>
<!--      <li>Part3b: k8s-multiple-nginx-ingress -->
<!--      <li>Part3c: k8s-contour-ingress -->
     <li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a></li>
<!-- local folder (+ autoprovisioning ) -->
     <li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
     <li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
     <li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
     <li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
     <li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
     <li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a></li>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... -->
     <li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
     <li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
     <li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
     <li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
     <li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
     <li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
     <li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a></li>
<!-- istio-install.md -->
     <li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a></li>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog -->
</ul>


<h3 id="wthat-is-kube-proxy">Wthat is kube-proxy</h3>
<p>kube-proxy is a key component of any Kubernetes deployment.  Its role is to load-balance traffic to the pods. It listens to all the service requests coming through from kubernetes and creates entries in iptables for each of these service IPs to achieve proper routing to the pod. So kube-proxy adds iptables ruleset for each new service defined. As the number of services grow, this list is going to be huge. This potentially impact the performance because the iptables processing is sequential and wit every new line the list goes longer and longer. Kubernetes&rsquo;s solution for this problem was IPVS.</p>
<h3 id="what-is-ipvs">What is IPVS?</h3>
<p>IPVS (IP Virtual Server) is built on top of the Netfilter and implements transport-layer load balancing as part of the Linux kernel. It runs on a host and acts as a load balancer in front of a cluster of real servers. IPVS can direct requests for TCP- and UDP-based services to the real servers, and make services of the real servers appear as virtual services on a single IP address. Therefore, IPVS naturally supports Kubernetes Service. IPVS mode provides greater scale and performance vs iptables mode. However, it comes with some limitations. There is no option to install IPVS mode in kubeadm. <!-- raw HTML omitted -->In the previous post you can see ho you can do install.<!-- raw HTML omitted --> The other solution is eBPF.</p>
<h3 id="what-is-ebpf-">What is eBPF ?</h3>
<p>eBPF is a virtual machine embedded within the Linux kernel. It allows small programs to be loaded into the kernel, and attached to hooks, which are triggered when some event occurs. For example, when a network interface emits a packet.</p>
<h3 id="requirements">Requirements</h3>
<p>First you need a supported Linuy Distrubution:</p>
<ul>
<li>Ubuntu 20.04.</li>
<li>Red Hat v8.2 with Linux kernel v4.18.0-193 or above (Red Hat have backported the required features to that build on CentOS 8 but not on 7)</li>
<li>Pre installed K8s cluster</li>
</ul>
<p>Verify that your cluster is ready for eBPF mode</p>
<pre tabindex="0"><code>mount | grep &quot;/sys/fs/bpf&quot;
bpf on /sys/fs/bpf type bpf (rw,nosuid,nodev,noexec,relatime,mode=700)
</code></pre><h3 id="install-calico-with-operator">Install Calico With Operator</h3>
<p>We need to change the <code>cidr</code> in <code>custom-resources.yaml</code> to match to our clusters <code>cdir</code>.</p>
<pre tabindex="0"><code>kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml

wget https://docs.projectcalico.org/manifests/custom-resources.yaml
nano custom-resources.yaml
...
cidr: 10.244.0.0/16


kubectl create -f custom-resources.yaml
watch kubectl get pods -n calico-system
</code></pre><h3 id="configure-calicoctl">Configure calicoctl</h3>
<pre tabindex="0"><code>export CALICO_DATASTORE_TYPE=kubernetes
export CALICO_KUBECONFIG=~/.kube/config
calicoctl get workloadendpoints
calicoctl get nodes
</code></pre><p>Configure tigera-operator to communicate with kubernetes&rsquo;s api.</p>
<pre tabindex="0"><code>nano kubernetes-services-endpoint.yaml
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kubernetes-services-endpoint
  namespace: tigera-operator
data:
  KUBERNETES_SERVICE_HOST: &quot;172.17.9.10&quot;
  KUBERNETES_SERVICE_PORT: &quot;6443&quot;

kubectl apply -f  kubernetes-services-endpoint.yaml
kubectl delete pod -n tigera-operator -l k8s-app=tigera-operator
watch kubectl get pods -n calico-system
</code></pre><h3 id="change-to-ebpf-mode">Change to eBPF mode</h3>
<p>First we change kube-proxy&rsquo;s <code>nodeSelector</code> to a none existing node to disable <code>kube-proxy</code>, the patch calico to run in eBPF mode.</p>
<pre tabindex="0"><code>kubectl patch ds -n kube-system kube-proxy -p '{&quot;spec&quot;:{&quot;template&quot;:{&quot;spec&quot;:{&quot;nodeSelector&quot;:{&quot;non-calico&quot;: &quot;true&quot;}}}}}'
calicoctl patch felixconfiguration default --patch='{&quot;spec&quot;: {&quot;bpfKubeProxyIptablesCleanupEnabled&quot;: false}}'
calicoctl patch felixconfiguration default --patch='{&quot;spec&quot;: {&quot;bpfEnabled&quot;: true}}'
calicoctl patch felixconfiguration default --patch='{&quot;spec&quot;: {&quot;bpfExternalServiceMode&quot;: &quot;DSR&quot;}}'
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install k8s with IPVS mode]]></title>
            <link href="https://devopstales.github.io/home/k8s-ipvs/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-calico-ebpf/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with calico&#39;s eBPF mode" />
                <link href="https://devopstales.github.io/kubernetes/k8s-ipvs/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with IPVS mode" />
                <link href="https://devopstales.github.io/home/k8s-install-openebs/?utm_source=atom_feed" rel="related" type="text/html" title="Install OpenEBS for Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-kubeadm-ha/?utm_source=atom_feed" rel="related" type="text/html" title="How to install kubernetes with kubeadm in HA mode" />
                <link href="https://devopstales.github.io/home/k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install kubernetes with kubeadm" />
            
                <id>https://devopstales.github.io/home/k8s-ipvs/</id>
            
            
            <published>2020-10-13T00:00:00+00:00</published>
            <updated>2020-10-13T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I&rsquo;will show you how to install kubernetes with kube-proxy IPVS mode.</p>
<H3>Parst of the Kubernetes series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
     <li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
     <li>Part1c: <a href="../../kubernetes/k8s-install-containerd/">Install K8S with containerd and kubeadm</a></li>
     <li>Part1d: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
     <li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a></li>
<!--      <li>Part3b: k8s-multiple-nginx-ingress -->
<!--      <li>Part3c: k8s-contour-ingress -->
     <li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a></li>
<!-- local folder (+ autoprovisioning ) -->
     <li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
     <li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
     <li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
     <li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
     <li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
     <li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a></li>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... -->
     <li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
     <li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
     <li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
     <li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
     <li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
     <li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
     <li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a></li>
<!-- istio-install.md -->
     <li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a></li>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog -->
</ul>


<h3 id="wthat-is-kube-proxy">Wthat is kube-proxy</h3>
<p>kube-proxy is a key component of any Kubernetes deployment.  Its role is to load-balance traffic to the pods. It listens to all the service requests coming through from kubernetes and creates entries in iptables for each of these service IPs to achieve proper routing to the pod. So kube-proxy adds iptables ruleset for each new service defined. As the number of services grow, this list is going to be huge. This potentially impact the performance because the iptables processing is sequential and wit every new line the list goes longer and longer. Kubernetes&rsquo;s solution for this problem was IPVS.</p>
<h3 id="what-is-ipvs">What is IPVS?</h3>
<p>IPVS (IP Virtual Server) is built on top of the Netfilter and implements transport-layer load balancing as part of the Linux kernel. It runs on a host and acts as a load balancer in front of a cluster of real servers. IPVS can direct requests for TCP- and UDP-based services to the real servers, and make services of the real servers appear as virtual services on a single IP address. Therefore, IPVS naturally supports Kubernetes Service. IPVS mode provides greater scale and performance vs iptables mode.</p>
<p>Installing Kubernetes with IPVS kube-proxy mode is a little bit hard because there in no built in option for theat in kubeadm. So we have two option. Createt a custom kubeadm.yaml or edit an installed cluster.</p>
<h3 id="install-requirements">Install Requirements</h3>
<pre tabindex="0"><code>yum install ipset ipvsadm -y

cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF
chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4
</code></pre><h3 id="createt-a-custom-kubeadmyaml">Createt a custom kubeadm.yaml</h3>
<pre tabindex="0"><code>kubeadm config print init-defaults &gt; kubeadm.yaml

nano kubeadm.yaml
...
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: ipvs
</code></pre><h3 id="edit-running-cluster">Edit running cluster</h3>
<pre tabindex="0"><code>kubectl edit configmap kube-proxy -n kube-system
...
mode: ipvs
</code></pre><pre tabindex="0"><code>kubectl get po -n kube-system
kubectl delete po -n kube-system &lt;pod-name&gt;
</code></pre><pre tabindex="0"><code>kubectl logs [kube-proxy pod] | grep &quot;Using ipvs Proxier&quot;
</code></pre><h3 id="test-ipvs-mode-is-running">Test IPVS mode is running</h3>
<pre tabindex="0"><code>ipvsadm -Ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.96.0.1:443 rr
  -&gt; 1.1.1.101:6443               Masq    1      0          0
TCP  10.96.0.10:53 rr
  -&gt; 10.244.0.2:53                Masq    1      0          0
  -&gt; 10.244.2.8:53                Masq    1      0          0
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to setup Unifi Controller on Debian 10]]></title>
            <link href="https://devopstales.github.io/home/install-unifi-controller/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/install-unifi-controller/?utm_source=atom_feed" rel="related" type="text/html" title="How to setup Unifi Controller on Debian 10" />
                <link href="https://devopstales.github.io/home/pfsense-usg/?utm_source=atom_feed" rel="related" type="text/html" title="Pfsese USG S2S VPN" />
                <link href="https://devopstales.github.io/home/backup-and-retore-prometheus/?utm_source=atom_feed" rel="related" type="text/html" title="How to backup and restore Prometheus?" />
                <link href="https://devopstales.github.io/home/openshift-rbd-fsck/?utm_source=atom_feed" rel="related" type="text/html" title="How to fixing filesystem corruption on a Kubernetes Ceph RBD PersistentVolume" />
                <link href="https://devopstales.github.io/home/being-productive-with-kubectl/?utm_source=atom_feed" rel="related" type="text/html" title="Being Productive with K8S" />
            
                <id>https://devopstales.github.io/home/install-unifi-controller/</id>
            
            
            <published>2020-10-12T00:00:00+00:00</published>
            <updated>2020-10-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how to install Unifi Controller on Debian 10 Buster.</p>
<h3 id="install-requiremens">Install requiremens</h3>
<pre tabindex="0"><code>apt install apt-transport-https ca-certificates wget dirmngr gnupg gnupg2 software-properties-common multiarch-support
</code></pre><h3 id="install-mongodb">Install MongoDB</h3>
<p>Unifi Controller requires a MongoDB from 2.4 to 4.0. The stander version of MongoDB to Debian 10 is 4.4 and 4.0-s requiremens conflicts with the Debians main libs. So we need to install MongoDB 3.4.</p>
<pre tabindex="0"><code>wget -qO - https://www.mongodb.org/static/pgp/server-3.4.asc |  apt-key add -
echo &quot;deb http://repo.mongodb.org/apt/debian jessie/mongodb-org/3.4 main&quot; | tee /etc/apt/sources.list.d/mongodb-org-3.4.list
</code></pre><p>We need an old version of libssl to run MongoDB 3.4:</p>
<pre tabindex="0"><code>wget http://security.debian.org/debian-security/pool/updates/main/o/openssl/libssl1.0.0_1.0.1t-1+deb8u12_amd64.deb
dpkg -i libssl1.0.0_1.0.1t-1+deb8u12_amd64.deb
</code></pre><h3 id="install-java-8">Install Java 8</h3>
<pre tabindex="0"><code>wget -qO - https://adoptopenjdk.jfrog.io/adoptopenjdk/api/gpg/key/public | sudo apt-key add -
sudo add-apt-repository --yes https://adoptopenjdk.jfrog.io/adoptopenjdk/deb/

apt update
apt install adoptopenjdk-8-hotspot

java -version

nano /etc/profile
export JAVA_HOME=&quot;/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64&quot;
source /etc/profile

echo $JAVA_HOME
</code></pre><h3 id="install-unifi-controller">Install Unifi Controller</h3>
<pre tabindex="0"><code>apt-key adv --keyserver keyserver.ubuntu.com --recv 06E85760C0A52C50
echo 'deb https://www.ui.com/downloads/unifi/debian stable ubiquiti' | tee /etc/apt/sources.list.d/100-ubnt-unifi.list
apt update &amp;&amp; apt install unifi
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to backup and restore Prometheus?]]></title>
            <link href="https://devopstales.github.io/home/backup-and-retore-prometheus/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/backup-and-retore-prometheus/?utm_source=atom_feed" rel="related" type="text/html" title="How to backup and restore Prometheus?" />
                <link href="https://devopstales.github.io/home/openshift-rbd-fsck/?utm_source=atom_feed" rel="related" type="text/html" title="How to fixing filesystem corruption on a Kubernetes Ceph RBD PersistentVolume" />
                <link href="https://devopstales.github.io/home/being-productive-with-kubectl/?utm_source=atom_feed" rel="related" type="text/html" title="Being Productive with K8S" />
                <link href="https://devopstales.github.io/home/k3s-helm-controller/?utm_source=atom_feed" rel="related" type="text/html" title="Install and use rancher helm-controller" />
                <link href="https://devopstales.github.io/home/k3s-crio/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with CRI-O and kadalu" />
            
                <id>https://devopstales.github.io/home/backup-and-retore-prometheus/</id>
            
            
            <published>2020-10-10T00:00:00+00:00</published>
            <updated>2020-10-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how to take a backup from a running Prometheus and restore it.</p>
<p>I will assumes that you have a running Prometheus deployed with <code>prometheus-operator</code> in the <code>monitoring</code> namespace.</p>
<h2 id="enable-admin-api">Enable Admin Api</h2>
<p>First we need to enable the Prometheus&rsquo;s admin api</p>
<pre tabindex="0"><code>kubectl -n monitoring patch prometheus prometheus-operator-prometheus \
  --type merge --patch '{&quot;spec&quot;:{&quot;enableAdminAPI&quot;:true}}'
</code></pre><p>In <code>tmux</code> or a separate window open a port forward to the admin api.</p>
<pre tabindex="0"><code>ubectl -n monitoring port-forward svc/prometheus-operator-prometheus 9090
</code></pre><h3 id="backup-prometheus-data">Backup Prometheus data</h3>
<p>Run following command to create a snapshot:</p>
<pre tabindex="0"><code>curl -XPOST http://localhost:9090/api/v2/admin/tsdb/snapshot
{&quot;name&quot;:&quot;20200731T123913Z-6e661e92759805f5&quot;}
</code></pre><p>Find the snapshot and copy it to locally. The default folder is <code>/prometheus/snapshots/</code> but you can find the data folder by finding the <code>--storage.tsdb.path</code> config in your deployment.</p>
<pre tabindex="0"><code>kubectl -n monitoring exec -it prometheus-prometheus-operator-prometheus-0 \
  -c prometheus -- /bin/sh -c \
  &quot;ls /prometheus/snapshots/20200731T123913Z-6e661e92759805f5&quot;
01EE25G1ZTKBFBBHFPHNBF99KJ  01EEFF7TE5ENDAGDR5K7ERW3BX
...

kubectl cp -n monitoring \
  prometheus-prometheus-operator-prometheus-0:/prometheus/snapshots/20200731T123913Z-6e661e92759805f5 \
  -c prometheus .
</code></pre><h3 id="restore-prometheus-data">Restore Prometheus data</h3>
<p>In a new Prometheus instance delete the data folder and copy the content of the snapshot:</p>
<pre tabindex="0"><code>kubectl -n newprom exec -it prometheus -- /bin/sh -c &quot;rm -rf /prometheus/*&quot;

kubectl -n newprom cp ./* prometheus:/prometheus/
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to fixing filesystem corruption on a Kubernetes Ceph RBD PersistentVolume]]></title>
            <link href="https://devopstales.github.io/home/openshift-rbd-fsck/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/openshift-rbd-fsck/?utm_source=atom_feed" rel="related" type="text/html" title="How to fixing filesystem corruption on a Kubernetes Ceph RBD PersistentVolume" />
                <link href="https://devopstales.github.io/home/k3s-helm-controller/?utm_source=atom_feed" rel="related" type="text/html" title="Install and use rancher helm-controller" />
                <link href="https://devopstales.github.io/home/k3s-crio/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with CRI-O and kadalu" />
                <link href="https://devopstales.github.io/home/k8s-crio/?utm_source=atom_feed" rel="related" type="text/html" title="Install K8S with CRI-O and kadalu" />
                <link href="https://devopstales.github.io/home/k8s-metallb-bgp-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Self-hosted Load Balancer for bare metal Kubernetes" />
            
                <id>https://devopstales.github.io/home/openshift-rbd-fsck/</id>
            
            
            <published>2020-10-10T00:00:00+00:00</published>
            <updated>2020-10-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this tutorial I will show you how to fix a corruptid filesystem on Ceph RBD PersistentVolume uyed by Kubernetes.</p>
<pre tabindex="0"><code>oc describe po gitlab-ce-1-wl9wf
...
Events:
  Type     Reason                  Age               From                                           Message
  ----     ------                  ----              ----                                           -------
  Normal   Scheduled               27s               default-scheduler                              Successfully assigned gitlab-prod/gitlab-ce-1-j7lph to k8sw09
  Normal   SuccessfulAttachVolume  27s               attachdetach-controller                        AttachVolume.Attach succeeded for volume &quot;pvc-e27f498e-85cf-11e9-af1a-66934f1af826&quot;
  Warning  FailedMount             2s (x6 over 19s)  kubelet, k8sw09  MountVolume.MountDevice failed for volume &quot;pvc-e27f498e-85cf-11e9-af1a-66934f1af826&quot; : rbd: failed to mount device /dev/rbd3 at /var/lib/origin/openshift.local.volumes/plugins/kubernetes.io/rbd/mounts/k8s-rbd-image-kubernetes-dynamic-pvc-e3042618-85cf-11e9-8762-aa9884bf6706 (fstype: ), error 'fsck' found errors on device /dev/rbd3 but could not correct them: fsck from util-linux 2.23.2
/dev/rbd3: Superblock needs_recovery flag is clear, but journal has data.
/dev/rbd3: Run journal anyway

/dev/rbd3: UNEXPECTED INCONSISTENCY; RUN fsck MANUALLY.
  (i.e., without -a or -p options)
</code></pre><p>Check the log on the worker. In my case this is k8sw09.</p>
<pre tabindex="0"><code>journalctl -u kubelet


jan 08 15:44:58 k8sw09 origin-node[14927]: I0108 15:44:58.251201   14927 reconciler.go:252] operationExecutor.MountVolume started for volume &quot;pvc-e27f498e-85cf-11e9-af1a-66934f1af826&quot; (UniqueName: &quot;kubernetes.io/rbd/k8s-rbd:kubernetes-dynamic-pvc-e3042618-85cf-11e9-8762-aa9884bf6706&quot;) pod &quot;gitlab-ce-1-j7lph&quot; (UID: &quot;69151c2c-3223-11ea-9bcf-aa9884bf6706&quot;)
jan 08 15:44:58 k8sw09 origin-node[14927]: I0108 15:44:58.251299   14927 operation_generator.go:489] MountVolume.WaitForAttach entering for volume &quot;pvc-e27f498e-85cf-11e9-af1a-66934f1af826&quot; (UniqueName: &quot;kubernetes.io/rbd/k8s-rbd:kubernetes-dynamic-pvc-e3042618-85cf-11e9-8762-aa9884bf6706&quot;) pod &quot;gitlab-ce-1-j7lph&quot; (UID: &quot;69151c2c-3223-11ea-9bcf-aa9884bf6706&quot;) DevicePath &quot;&quot;
jan 08 15:44:58 k8sw09 origin-node[14927]: I0108 15:44:58.451965   14927 operation_generator.go:498] MountVolume.WaitForAttach succeeded for volume &quot;pvc-e27f498e-85cf-11e9-af1a-66934f1af826&quot; (UniqueName: &quot;kubernetes.io/rbd/k8s-rbd:kubernetes-dynamic-pvc-e3042618-85cf-11e9-8762-aa9884bf6706&quot;) pod &quot;gitlab-ce-1-j7lph&quot; (UID: &quot;69151c2c-3223-11ea-9bcf-aa9884bf6706&quot;) DevicePath &quot;/dev/rbd3&quot;
jan 08 15:44:58 k8sw09 origin-node[14927]: E0108 15:44:58.498052   14927 nestedpendingoperations.go:267] Operation for &quot;\&quot;kubernetes.io/rbd/k8s-rbd:kubernetes-dynamic-pvc-e3042618-85cf-11e9-8762-aa9884bf6706&quot;&quot; failed. No retries permitted until 2020-01-08 15:47:00.498014981 +0100 CET m=+619493.508747496 (durationBeforeRetry 2m2s). Error: &quot;MountVolume.MountDevice failed for volume \&quot;pvc-e27f498e-85cf-11e9-af1a-66934f1af826\&quot; (UniqueName: \&quot;kubernetes.io/rbd/k8s-rbd:kubernetes-dynamic-pvc-e3042618-85cf-11e9-8762-aa9884bf6706\&quot;) pod \&quot;gitlab-ce-1-j7lph\&quot; (UID: \&quot;69151c2c-3223-11ea-9bcf-aa9884bf6706\&quot;) : rbd: failed to mount device /dev/rbd3 at /var/lib/origin/openshift.local.volumes/plugins/kubernetes.io/rbd/mounts/k8s-rbd-image-kubernetes-dynamic-pvc-e3042618-85cf-11e9-8762-aa9884bf6706 (fstype: ), error 'fsck' found errors on device /dev/rbd3 but could not correct them: fsck from util-linux 2.23.2\n/dev/rbd3: Superblock needs_recovery flag is clear, but journal has data.\n/dev/rbd3: Run journal anyway\n\n/dev/rbd3: UNEXPECTED INCONSISTENCY; RUN fsck MANUALLY.\n\t(i.e., without -a or -p options)\n.&quot;
</code></pre><p>We can see the problem is with <code>/dev/rbd3</code>. First check thi is the block device user for <code>pvc-e3042618-85cf-11e9-8762-aa9884bf6706</code> PersistenVolume.</p>
<pre tabindex="0"><code>sudo rbd showmapped | grep pvc-e3042618-85cf-11e9-8762-aa9884bf6706
3  k8s-rbd kubernetes-dynamic-pvc-e3042618-85cf-11e9-8762-aa9884bf6706 -    /dev/rbd3
</code></pre><p>So let&rsquo;s try to use <code>fsck</code> on this disk.</p>
<pre tabindex="0"><code>sudo rbd unmap /dev/rbd3


sudo fsck -fv /dev/rbd3
fsck from util-linux 2.27.1
e2fsck 1.42.13 (17-May-2015)
Pass 1: Checking inodes, blocks, and sizes
Pass 2: Checking directory structure
Pass 3: Checking directory connectivity
Pass 4: Checking reference counts
Unattached inode 303
Connect to /lost+found&lt;y&gt;? yes
Inode 303 ref count is 2, should be 1.  Fix&lt;y&gt;? yes
Pass 5: Checking group summary information
Block bitmap differences:  -(71680--73727) -(94208--95231)
Fix&lt;y&gt;? yes

/dev/rbd3: ***** FILE SYSTEM WAS MODIFIED *****

         326 inodes used (0.50%, out of 65536)
          35 non-contiguous files (10.7%)
           0 non-contiguous directories (0.0%)
             # of inodes with ind/dind/tind blocks: 0/0/0
             Extent depth histogram: 311/7
       63642 blocks used (24.28%, out of 262144)
           0 bad blocks
           1 large file

         308 regular files
           9 directories
           0 character device files
           0 block device files
           0 fifos
           1 link
           0 symbolic links (0 fast symbolic links)
           0 sockets
------------
         317 files
</code></pre><p>Then our pod is running again!</p>
<pre tabindex="0"><code>oc get po
NAME                        READY     STATUS    RESTARTS   AGE
gitlab-ce-1-j7lph           1/1       Running   0          28m
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Being Productive with K8S]]></title>
            <link href="https://devopstales.github.io/home/being-productive-with-kubectl/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/being-productive-with-kubectl/?utm_source=atom_feed" rel="related" type="text/html" title="Being Productive with K8S" />
                <link href="https://devopstales.github.io/home/k3s-helm-controller/?utm_source=atom_feed" rel="related" type="text/html" title="Install and use rancher helm-controller" />
                <link href="https://devopstales.github.io/home/k8s-metallb-bgp-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Self-hosted Load Balancer for bare metal Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-longhorn/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes project longhorn" />
                <link href="https://devopstales.github.io/home/k8s-gangway/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak and gangway" />
            
                <id>https://devopstales.github.io/home/being-productive-with-kubectl/</id>
            
            
            <published>2020-10-09T00:00:00+00:00</published>
            <updated>2020-10-09T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you my productivity tips with kubectl.</p>
<h3 id="bash-aliases-for-kubectl">Bash aliases for kubectl</h3>
<p>I have above aliases setup in the <code>~/.bashrc</code> file.</p>
<pre tabindex="0"><code>alias k=kubectl
alias kg='kubectl get'
alias kd='kubectl describe'
alias kdel='kubectl delete'
alias kdelf='kubectl delete -f'
alias kaf='kubectl apply -f'
alias keti='kubectl exec -ti'
alias kgds='kubectl get DaemonSet'
alias kgdsy='kubectl get DaemonSet -oyaml'
alias kdds='kubectl describe DaemonSet'
alias kdelds='kubectl delete DaemonSet'
alias kgp='kubectl get pods'
alias kgpy='kubectl get pods -oyaml'
alias kgpa='kubectl get pods --all-namespaces'
alias kgpw='kgp --watch'
alias kgpwide='kgp -o wide'
alias kdp='kubectl describe pods'
alias kdelp='kubectl delete pods'
# get pod by label: kgpl &quot;app=myapp&quot; -n myns
alias kgpl='kgp -l'
# get pod by namespace: kgpn kube-system&quot;
alias kgpn='kgp -n'
alias kgs='kubectl get svc'
alias kgsa='kubectl get svc --all-namespaces'
alias kgsw='kgs --watch'
alias kgswide='kgs -o wide'
alias kes='kubectl edit svc'
alias kds='kubectl describe svc'
alias kdels='kubectl delete svc'
alias kgi='kubectl get ingress'
alias kgia='kubectl get ingress --all-namespaces'
alias kei='kubectl edit ingress'
alias kdi='kubectl describe ingress'
alias kdeli='kubectl delete ingress'
alias kgns='kubectl get namespaces'
alias kens='kubectl edit namespace'
alias kdns='kubectl describe namespace'
alias kdelns='kubectl delete namespace'
alias kcn='kubectl config set-context $(kubectl config current-context) --namespace'
alias kgcm='kubectl get configmaps'
alias kgcmy='kubectl get configmaps -oyaml'
alias kgcma='kubectl get configmaps --all-namespaces'
alias kecm='kubectl edit configmap'
alias kdcm='kubectl describe configmap'
alias kdelcm='kubectl delete configmap'
alias kgsec='kubectl get secret'
alias kgseca='kubectl get secret --all-namespaces'
alias kdsec='kubectl describe secret'
alias kdelsec='kubectl delete secret'
alias kgss='kubectl get statefulset'
alias kgssa='kubectl get statefulset --all-namespaces'
alias kgssw='kgss --watch'
alias kgsswide='kgss -o wide'
alias kess='kubectl edit statefulset'
alias kdss='kubectl describe statefulset'
alias kdelss='kubectl delete statefulset'
alias ksss='kubectl scale statefulset'
alias krsss='kubectl rollout status statefulset'
alias kga='kubectl get all'
alias kgaa='kubectl get all --all-namespaces'
alias kl='kubectl logs'
alias kl1h='kubectl logs --since 1h'
alias kl1m='kubectl logs --since 1m'
alias kl1s='kubectl logs --since 1s'
alias klf='kubectl logs -f'
alias klf1h='kubectl logs --since 1h -f'
alias klf1m='kubectl logs --since 1m -f'
alias klf1s='kubectl logs --since 1s -f'
alias kcp='kubectl cp'
alias kgno='kubectl get nodes'
alias keno='kubectl edit node'
alias kdno='kubectl describe node'
alias kdelno='kubectl delete node'
alias kgpvc='kubectl get pvc'
alias kgpvca='kubectl get pvc --all-namespaces'
alias kgpvcw='kgpvc --watch'
alias kepvc='kubectl edit pvc'
alias kdpvc='kubectl describe pvc'
alias kdelpvc='kubectl delete pvc'
alias kgpv='kubectl get pv'
alias kgsc='kubectl get storageclass'
# custom resource
alias kgir='kubectl get IngressRoute'
alias kgira='kubectl get IngressRoute --all-namespaces'
alias keir='kubectl edit IngressRoute'
alias kdir='kubectl describe IngressRoute'
alias kdelir='kubectl delete IngressRoute'
</code></pre><h3 id="kubens-kubectx">kubens kubectx</h3>
<pre tabindex="0"><code>sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx
sudo ln -s /opt/kubectx/kubens /usr/local/bin/kubens
echo &quot;PATH=$PATH:/usr/local/bin/&quot; &gt;&gt; /etc/profile
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install and use rancher helm-controller]]></title>
            <link href="https://devopstales.github.io/home/k3s-helm-controller/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k3s-helm-controller/?utm_source=atom_feed" rel="related" type="text/html" title="Install and use rancher helm-controller" />
                <link href="https://devopstales.github.io/home/k3s-crio/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with CRI-O and kadalu" />
                <link href="https://devopstales.github.io/home/k3s-fcos/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S on Fedora CoreOS" />
                <link href="https://devopstales.github.io/home/k8s-metallb-bgp-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Self-hosted Load Balancer for bare metal Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-longhorn/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes project longhorn" />
            
                <id>https://devopstales.github.io/home/k3s-helm-controller/</id>
            
            
            <published>2020-09-20T00:00:00+00:00</published>
            <updated>2020-09-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>K3S comes with a Helm operator called Helm Controller. Helm Controller defines a new HelmChart custom resource definition, or CRD, for managing Helm charts.</p>
<H3>Parst of the K3S series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/k3s-etcd-kube-vip/">Install K3S with k3sup and kube-vip</a></li>
     <li>Part1b: <a href="../../kubernetes/k3s-crio/">Install K3S with CRI-O</a></li>
     <li>Part1c: <a href="../../kubernetes/k3s-fcos/">Install K3S on Fedora CoreOS</a></li>
<!-- K3S falnnel wiregard - https://medium.com/@bestpractices/k3s-with-flannel-wireguard-backend-fa433065a401 -->
     <li>Part2b: <a href="../../kubernetes/k3sup-calico/">Install K3S with k3sup and Calico</a></li>
     <li>Part2c: <a href="../../kubernetes/k3s-cilium/">Install K3S with k3sup and Cilium</a></li>
     <li>Part3: <a href="../../kubernetes/k3s-helm-controller/">K3S helm CR</a></li>
<!-- K3D -->
     <li>Part5: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
     <li>Part6: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
</ul>


<h3 id="install-helm-controller-k8s">Install helm-controller K8S</h3>
<p>Thanks to Francisco Bobadilla who created RBAC for Rancher&rsquo;s helm-controller we can deploy this solution to a stander K8S cluster.</p>
<pre tabindex="0"><code>kubectl apply -f https://raw.githubusercontent.com/iotops/helm-controller/master/manifests/deploy-namespaced.yaml
</code></pre><pre tabindex="0"><code>kubectl api-resources --api-group=helm.cattle.io
NAME         SHORTNAMES   APIGROUP         NAMESPACED   KIND
helmcharts                helm.cattle.io   true         HelmChart
</code></pre><h3 id="deploy-nginx-ingress-controller">Deploy Nginx ingress Controller</h3>
<pre tabindex="0"><code>nano ginx-ingress.yaml
---
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: nginx-ingress
  namespace: helm-controller
spec:
  chart: stable/nginx-ingress
  targetNamespace: nginx-ingress
  valuesContent: |-
    rbac:
      create: &quot;true&quot;
    controller:

      kind: DaemonSet
      hostNetwork: &quot;true&quot;
      daemonset:
        useHostPort: &quot;true&quot;
      service:
        type: &quot;NodePort&quot;
</code></pre><p>On a K3S cluster the <code>namespace:</code> must be <code>kube-system</code> because k3S deploys the helm-controller to that namespace.</p>
<h3 id="auto-deploying-manifests">Auto-Deploying Manifests</h3>
<p>Any file found in <code>/var/lib/rancher/k3s/server/manifests</code> will automatically be deployed to Kubernetes in a manner similar to <code>kubectl apply</code>.</p>
<pre tabindex="0"><code>cp nginx-ingress.yaml /var/lib/rancher/k3s/server/manifests/
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install K3S with CRI-O and kadalu]]></title>
            <link href="https://devopstales.github.io/home/k3s-crio/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k3s-crio/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with CRI-O and kadalu" />
                <link href="https://devopstales.github.io/home/k8s-crio/?utm_source=atom_feed" rel="related" type="text/html" title="Install K8S with CRI-O and kadalu" />
                <link href="https://devopstales.github.io/home/k3s-fcos/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S on Fedora CoreOS" />
                <link href="https://devopstales.github.io/kubernetes/k8s-crio/?utm_source=atom_feed" rel="related" type="text/html" title="Install K8S with CRI-O and kadalu" />
                <link href="https://devopstales.github.io/home/k8s-metallb-bgp-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Self-hosted Load Balancer for bare metal Kubernetes" />
            
                <id>https://devopstales.github.io/home/k3s-crio/</id>
            
            
            <published>2020-09-10T00:00:00+00:00</published>
            <updated>2020-09-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how to install cri-o container runtime and initialize a Kubernetes.</p>
<H3>Parst of the K3S series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/k3s-etcd-kube-vip/">Install K3S with k3sup and kube-vip</a></li>
     <li>Part1b: <a href="../../kubernetes/k3s-crio/">Install K3S with CRI-O</a></li>
     <li>Part1c: <a href="../../kubernetes/k3s-fcos/">Install K3S on Fedora CoreOS</a></li>
<!-- K3S falnnel wiregard - https://medium.com/@bestpractices/k3s-with-flannel-wireguard-backend-fa433065a401 -->
     <li>Part2b: <a href="../../kubernetes/k3sup-calico/">Install K3S with k3sup and Calico</a></li>
     <li>Part2c: <a href="../../kubernetes/k3s-cilium/">Install K3S with k3sup and Cilium</a></li>
     <li>Part3: <a href="../../kubernetes/k3s-helm-controller/">K3S helm CR</a></li>
<!-- K3D -->
     <li>Part5: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
     <li>Part6: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
</ul>


<h3 id="install-cri-o-instad-of-docker">Install CRI-O instad of Docker</h3>
<pre tabindex="0"><code>VERSION=1.18
sudo curl -L -o /etc/yum.repos.d/devel_kubic_libcontainers_stable.repo https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable/CentOS_7/devel:kubic:libcontainers:stable.repo
sudo curl -L -o /etc/yum.repos.d/devel_kubic_libcontainers_stable_cri-o_${VERSION}.repo https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:${VERSION}/CentOS_7/devel:kubic:libcontainers:stable:cri-o:${VERSION}.repo

yum install cri-o
</code></pre><h3 id="configure">Configure</h3>
<pre tabindex="0"><code>modprobe overlay
modprobe br_netfilter

cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

sysctl --system
</code></pre><pre tabindex="0"><code>free -h
swapoff -a
swapoff -a
sed -i.bak -r 's/(.+ swap .+)/#\1/' /etc/fstab
free -h
</code></pre><p>You nee the same cgroup manager in cri-o and kubeadm. The default for kubeadm is cgroupfs and for cri-o the default is systemd. In this example I configured cri-o for cgroupfs.</p>
<pre tabindex="0"><code>nano /etc/crio/crio.conf
[crio.runtime]
conmon_cgroup = &quot;pod&quot;
cgroup_manager = &quot;cgroupfs&quot;
...
registries = [
  &quot;quay.io&quot;,
  &quot;docker.io&quot;
]
</code></pre><p>Disable ipv6 and configure cri-o CNI confg for flanel&rsquo;s network:</p>
<pre tabindex="0"><code>echo &quot;net.ipv6.conf.all.disable_ipv6 = 1&quot; &gt;&gt; /etc/sysctl.conf
echo &quot;net.ipv6.conf.default.disable_ipv6 = 1&quot; &gt;&gt; /etc/sysctl.conf
sysctl -p

sed -i &quot;s|::1|#::1|&quot; /etc/hosts

nano /etc/cni/net.d/100-crio-bridge.conf
{
...
&quot;ipam&quot;: {
    &quot;type&quot;: &quot;host-local&quot;,
    &quot;routes&quot;: [
        { &quot;dst&quot;: &quot;0.0.0.0/0&quot; }
    ],

    &quot;ranges&quot;: [
        [{ &quot;subnet&quot;: &quot;10.244.0.0/16&quot; }]
    ]
}
}
</code></pre><pre tabindex="0"><code>systemctl enable --now cri-o

echo &quot;export PATH=$PATH:/usr/local/bin/&quot; &gt;&gt; /etc/profile
echo &quot;export KUBECONFIG=/etc/rancher/k3s/k3s.yaml&quot; &gt;&gt; /etc/profile
source /etc/profile

yum install -y container-selinux selinux-policy-base
rpm -i https://rpm.rancher.io/k3s-selinux-0.1.1-rc1.el7.noarch.rpm
</code></pre><pre tabindex="0"><code>export K3S_KUBECONFIG_MODE=&quot;644&quot;
export INSTALL_K3S_EXEC=&quot; --container-runtime-endpoint /var/run/crio/crio.sock --no-deploy servicelb --no-deploy traefik&quot;

curl -sfL https://get.k3s.io | sh -
</code></pre><pre tabindex="0"><code>systemctl status k3s

crictl info
crictl ps
kubectl get node -o wide
kubectl get pods -A -o wide
</code></pre><h3 id="install-tools">Install tools</h3>
<pre tabindex="0"><code>yum install git -y

sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubectx /usr/local/sbin/kubectx
sudo ln -s /opt/kubectx/kubens /usr/local/sbin/kubens
COMPDIR=$(pkg-config --variable=completionsdir bash-completion)
ln -sf /opt/kubectx/completion/kubens.bash $COMPDIR/kubens
ln -sf /opt/kubectx/completion/kubectx.bash $COMPDIR/kubectx
</code></pre><h3 id="deploy-kadalu-storage">Deploy kadalu storage</h3>
<pre tabindex="0"><code>sudo wipefs -a -t dos -f /dev/sdb
sudo mkfs.xfs /dev/sdb

yum install python3-pip -y
sudo pip3 install kubectl-kadalu

echo &quot;export PATH=$PATH:/usr/local/bin/&quot; &gt;&gt; /etc/profile
source /etc/profile

kubectl kadalu install

# k8s.mydomain.intra is the nod name in Kubernetes
# /dev/sdb is the disk

kubectl kadalu storage-add storage-pool-1 \
    --device k8s.mydomain.intra:/dev/sdb

# to delete object if you misconfigured kadalu
kubectl delete kadalustorages.kadalu-operator.storage storage-pool-1


kubectl get pods -n kadalu

kubectl patch storageclass kadalu.replica1 -p '{&quot;metadata&quot;: {&quot;annotations&quot;:{&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;true&quot;}}}'
</code></pre><pre tabindex="0"><code>nano test-pvc.yaml
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pv1
spec:
  storageClassName: kadalu.replica1
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install K3S on Fedora CoreOS]]></title>
            <link href="https://devopstales.github.io/home/k3s-fcos/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k3s-fcos/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S on Fedora CoreOS" />
                <link href="https://devopstales.github.io/home/fcos-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Fedora CoreOS as a VM" />
                <link href="https://devopstales.github.io/home/k8s-longhorn/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes project longhorn" />
                <link href="https://devopstales.github.io/home/k8s-crio/?utm_source=atom_feed" rel="related" type="text/html" title="Install K8S with CRI-O and kadalu" />
                <link href="https://devopstales.github.io/cloud/fcos-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Fedora CoreOS as a VM" />
            
                <id>https://devopstales.github.io/home/k3s-fcos/</id>
            
            
            <published>2020-09-05T00:00:00+00:00</published>
            <updated>2020-09-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can Install K3S on Fedora CoreOS(FCOS) in virtualization environment.</p>
<H3>Parst of the K3S series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/k3s-etcd-kube-vip/">Install K3S with k3sup and kube-vip</a></li>
     <li>Part1b: <a href="../../kubernetes/k3s-crio/">Install K3S with CRI-O</a></li>
     <li>Part1c: <a href="../../kubernetes/k3s-fcos/">Install K3S on Fedora CoreOS</a></li>
<!-- K3S falnnel wiregard - https://medium.com/@bestpractices/k3s-with-flannel-wireguard-backend-fa433065a401 -->
     <li>Part2b: <a href="../../kubernetes/k3sup-calico/">Install K3S with k3sup and Calico</a></li>
     <li>Part2c: <a href="../../kubernetes/k3s-cilium/">Install K3S with k3sup and Cilium</a></li>
     <li>Part3: <a href="../../kubernetes/k3s-helm-controller/">K3S helm CR</a></li>
<!-- K3D -->
     <li>Part5: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
     <li>Part6: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a></li>
</ul>


<h3 id="what-is-k3s">What is K3S</h3>
<p>K3S is a lightweight certified kubernetes distribution. Designed to be a single binary of less than 40MB. It didn&rsquo;t use etcd instad stora it&rsquo;s data in sqlite.</p>
<h3 id="install-requirements">Install requirements</h3>
<pre tabindex="0"><code>sudo -i
rpm-ostree install https://rpm.rancher.io/k3s-selinux-0.1.1-rc1.el7.noarch.rpm
systemctl reboot
</code></pre><h3 id="install-k3s">Install K3S</h3>
<pre tabindex="0"><code>sudo -i
export K3S_KUBECONFIG_MODE=&quot;644&quot;
export INSTALL_K3S_EXEC=&quot; --no-deploy servicelb --no-deploy traefik&quot;

curl -sfL https://get.k3s.io | sh -

systemctl status k3s
kubectl get nodes -o wide
kubectl get pods -A -o wide
</code></pre><h3 id="join-other-nodes">Join other nodes</h3>
<p>First we need the join token:</p>
<pre tabindex="0"><code>sudo cat /var/lib/rancher/k3s/server/node-token
K1042e2f8e353b9409472c1e0cca8457abe184dc7be3f0805109e92c50c193ceb42::node:c83acbf89a7de7026d6f6928dc270028
</code></pre><p>The join the worker nodes:</p>
<pre tabindex="0"><code>export K3S_KUBECONFIG_MODE=&quot;644&quot;
export K3S_URL=&quot;https://k3s-master:6443&quot;
export K3S_TOKEN=&quot;K1042e2f8e353b9409472c1e0cca8457abe184dc7be3f0805109e92c50c193ceb42::node:c83acbf89a7de7026d6f6928dc270028&quot;

curl -sfL https://get.k3s.io | sh -
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install K8S with CRI-O and kadalu]]></title>
            <link href="https://devopstales.github.io/home/k8s-crio/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-crio/?utm_source=atom_feed" rel="related" type="text/html" title="Install K8S with CRI-O and kadalu" />
                <link href="https://devopstales.github.io/home/k8s-metallb-bgp-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Self-hosted Load Balancer for bare metal Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-install-openebs/?utm_source=atom_feed" rel="related" type="text/html" title="Install OpenEBS for Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-kubeadm-ha/?utm_source=atom_feed" rel="related" type="text/html" title="How to install kubernetes with kubeadm in HA mode" />
                <link href="https://devopstales.github.io/home/k8s-local-pv/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
            
                <id>https://devopstales.github.io/home/k8s-crio/</id>
            
            
            <published>2020-09-04T00:00:00+00:00</published>
            <updated>2020-09-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how to install cri-o container runtime and initialize a Kubernetes.</p>
<H3>Parst of the Kubernetes series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
     <li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
     <li>Part1c: <a href="../../kubernetes/k8s-install-containerd/">Install K8S with containerd and kubeadm</a></li>
     <li>Part1d: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
     <li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a></li>
<!--      <li>Part3b: k8s-multiple-nginx-ingress -->
<!--      <li>Part3c: k8s-contour-ingress -->
     <li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a></li>
<!-- local folder (+ autoprovisioning ) -->
     <li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
     <li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
     <li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
     <li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
     <li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
     <li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a></li>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... -->
     <li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
     <li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
     <li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
     <li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
     <li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
     <li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
     <li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a></li>
<!-- istio-install.md -->
     <li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a></li>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog -->
</ul>


<h3 id="what-is-cri-o">What is CRI-O?</h3>
<p>The Kubernetes project has defined a number of standards. One of them is cri. The Container Runtime Interface. This interface defines how Kubernetes talks with a high-level container runtime. CRI-O is an implementation of the Kubernetes CRI to enable using OCI (Open Container Initiative) compatible runtimes. It is a lightweight alternative of Docker as the runtime for kubernetes. t allows Kubernetes to use any OCI-compliant runtime as the container runtime for running pods. Today it supports runc and Kata Containers as the container runtimes but any OCI-conformant runtime can be plugged in principle.</p>
<h3 id="install-cri-o-instad-of-docker">Install CRI-O instad of Docker</h3>
<pre tabindex="0"><code>VERSION=1.18
sudo curl -L -o /etc/yum.repos.d/devel_kubic_libcontainers_stable.repo https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable/CentOS_7/devel:kubic:libcontainers:stable.repo
sudo curl -L -o /etc/yum.repos.d/devel_kubic_libcontainers_stable_cri-o_${VERSION}.repo https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:${VERSION}/CentOS_7/devel:kubic:libcontainers:stable:cri-o:${VERSION}.repo

yum install cri-o
</code></pre><h3 id="configure">Configure</h3>
<pre tabindex="0"><code>modprobe overlay
modprobe br_netfilter

cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

sysctl --system
</code></pre><pre tabindex="0"><code>free -h
swapoff -a
swapoff -a
sed -i.bak -r 's/(.+ swap .+)/#\1/' /etc/fstab
free -h
</code></pre><p>You nee the same cgroup manager in cri-o and kubeadm. The default for kubeadm is cgroupfs and for cri-o the default is systemd. In this example I configured cri-o for cgroupfs.</p>
<pre tabindex="0"><code>nano /etc/crio/crio.conf
[crio.runtime]
conmon_cgroup = &quot;pod&quot;
cgroup_manager = &quot;cgroupfs&quot;

nano /etc/containers/registries.conf
registries = [
  &quot;quay.io&quot;,
  &quot;docker.io&quot;
]
</code></pre><p>If you want to use systemd:</p>
<pre tabindex="0"><code>echo &quot;KUBELET_EXTRA_ARGS=--cgroup-driver=systemd&quot; | tee /etc/sysconfig/kubelet
</code></pre><h3 id="install-kubernets">Install kubernets</h3>
<pre tabindex="0"><code>cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

CRIP_VERSION=$(crio --version | awk '{print $3}')
yum install kubelet-$CRIP_VERSION kubeadm-$CRIP_VERSION kubectl-$CRIP_VERSION -y
</code></pre><pre tabindex="0"><code>IP=172.17.9.10
# for multi interface configuration
echo 'KUBELET_EXTRA_ARGS=&quot;--node-ip='$IP' --cgroup-driver=systemd&quot;' &gt; /etc/sysconfig/kubelet

systemctl enable kubelet.service
systemctl enable --now cri-o
kubeadm config images pull --cri-socket=unix:///var/run/crio/crio.sock --kubernetes-version=$CRIP_VERSION
kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=$IP  --kubernetes-version=$CRIP_VERSION --cri-socket=unix:///var/run/crio/crio.sock

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config


crictl info
kubectl get node -o wide
kubectl get po --all-namespaces
</code></pre><h3 id="inincialize-network">Inincialize network</h3>
<pre tabindex="0"><code>wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
kubectl aplly -f kube-flannel.yml
</code></pre><pre tabindex="0"><code>kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml
wget https://docs.projectcalico.org/manifests/custom-resources.yaml

nano ustom-resources.yaml
...
      cidr: 10.244.0.0/16
...

kubectl apply -f custom-resources.yaml
</code></pre><h3 id="install-tools">Install tools</h3>
<pre tabindex="0"><code>yum install git -y

sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubectx /usr/local/sbin/kubectx
sudo ln -s /opt/kubectx/kubens /usr/local/sbin/kubens
</code></pre><h3 id="deploy-kadalu-storage">Deploy kadalu storage</h3>
<pre tabindex="0"><code>sudo wipefs -a -t dos -f /dev/sdb
sudo mkfs.xfs /dev/sdb

yum install python3-pip -y
sudo pip3 install kubectl-kadalu

echo &quot;export PATH=$PATH:/usr/local/bin/&quot; &gt;&gt; /etc/profile
source /etc/profile

kubectl kadalu install

# k8s.mydomain.intra is the nod name in Kubernetes
# /dev/sdb is the disk

kubectl kadalu storage-add storage-pool-1 \
    --device k8s.mydomain.intra:/dev/sdb

# to delete object if you misconfigured kadalu
kubectl delete kadalustorages.kadalu-operator.storage storage-pool-1


kubectl get pods -n kadalu

kubectl patch storageclass kadalu.replica1 -p '{&quot;metadata&quot;: {&quot;annotations&quot;:{&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;true&quot;}}}'
</code></pre><pre tabindex="0"><code>nano test-pvc.yaml
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pv1
spec:
  storageClassName: kadalu.replica1
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Fedora CoreOS as a VM]]></title>
            <link href="https://devopstales.github.io/home/fcos-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/fcos-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Fedora CoreOS as a VM" />
                <link href="https://devopstales.github.io/home/k8s-metallb-bgp-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Self-hosted Load Balancer for bare metal Kubernetes" />
                <link href="https://devopstales.github.io/home/how-to-fix-ansible-service-broker-in-openshift-3-11/?utm_source=atom_feed" rel="related" type="text/html" title="How to fix Ansible Service Broker in OpenShift 3.11" />
                <link href="https://devopstales.github.io/home/how-to-fix-cluster-monitoring-operator-in-openshift-3-11/?utm_source=atom_feed" rel="related" type="text/html" title="How to fix cluster-monitoring-operator in OpenShift 3.11" />
                <link href="https://devopstales.github.io/home/how-to-fix-registry-console-ui-in-openshift-3-11/?utm_source=atom_feed" rel="related" type="text/html" title="How to fix registry console UI in OpenShift 3.11" />
            
                <id>https://devopstales.github.io/home/fcos-install/</id>
            
            
            <published>2020-08-30T00:00:00+00:00</published>
            <updated>2020-08-30T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can Install Fedora CoreOS(FCOS) in virtualization environment.</p>
<ul>
<li>First you need Fedora CoreOS Config (FCC) - This is a YAML file that specifies the configuration of a machine.</li>
<li>Fedora CoreOS Config Transpiler to validate your FCC and convert it to an Ignition config.</li>
<li>Finally launch a Fedora CoreOS machine and use Ignition config to perform the installation.</li>
</ul>
<h3 id="create-fcc">Create FCC</h3>
<p>Create password hash for default user</p>
<pre tabindex="0"><code>$ mkpasswd --method=yescrypt
Password:
$y$j9T$A0Y3wwVOKP69S.1K/zYGN.$S596l11UGH3XjN...
</code></pre><pre tabindex="0"><code>nano fcos01.fcc
---
variant: fcos
version: 1.0.0
passwd:
  users:
    - name: core
      password_hash: &quot;$y$j9T$A0Y3wwVOKP69S.1K/zYGN.$S596l11UGH3XjN...&quot;
      groups:
        - docker
systemd:
  units:
    - name: install-rpms.service
      enabled: true
      contents: |
        [Unit]
        Description=Install packages
        ConditionFirstBoot=yes
        Wants=network-online.target
        After=network-online.target
        After=multi-user.target
        [Service]
        Type=oneshot
        ExecStart=rpm-ostree install nano git docker-compose htop --reboot
        [Install]
        WantedBy=multi-user.target
storage:
  files:
    - path: /etc/ssh/sshd_config.d/20-enable-passwords.conf
      mode: 0644
      contents:
        inline: |
          # Fedora CoreOS disables SSH password login by default.
          # Enable it.
          # This file must sort before 40-disable-passwords.conf.
          PasswordAuthentication yes
    - path: /etc/hostname
      mode: 0644
      contents:
        inline: fcos01.mydomain.intra
</code></pre><h3 id="convert-fcc-to-ignition">Convert FCC to Ignition</h3>
<pre tabindex="0"><code>docker run -i --rm quay.io/coreos/fcct --pretty --strict &lt;fcos01.fcc &gt; fcos01.ign

# validate config
docker run --rm -i quay.io/coreos/ignition-validate - &lt; fcos01.ign
</code></pre><h3 id="install-fedora-coreos">Install Fedora CoreOS</h3>
<p>At the install stap you need to boot from the Fedora CoreOS ISO and use the Ignition config to install.  So you need a solution to share this files with the running LiveOS. You can use an usb pendrive or a web-server for that. I will use a webserver for thet now.</p>
<pre tabindex="0"><code>apt install nginx

mkdir /var/www/html/fcos
cp fcos01.ign /var/www/html/fcos
cd /var/www/html/fcos

systemct start nginx
</code></pre><pre tabindex="0"><code>cd ~/
wget https://builds.coreos.fedoraproject.org/prod/streams/stable/builds/32.20200726.3.1/x86_64/fedora-coreos-32.20200726.3.1-live.x86_64.iso
</code></pre><h3 id="3-2-1--ignition">3 2 1 &hellip; Ignition</h3>
<p>After the live OS bootid star Fedora CoreOS install:</p>
<pre tabindex="0"><code>sudo su -
coreos-installer install /dev/sda \
--ignition-url http://example.com/fcos/fcos01.ign \
--insecure-ignition

init 6
</code></pre><p>You need <code>--insecure-ignition</code> for insecure http connection.</p>
<h3 id="set-static-ip">Set static IP</h3>
<pre tabindex="0"><code>nmcli connection show
NAME              UUID                                  TYPE      DEVICE
Wired Connection  f36f48e0-f75f-4925-ac78-de6119a2fcbb  ethernet  enp0s3

nmcli connection mod 'Wired Connection' \
  ipv4.method manual \
  ipv4.addresses 192.168.0.16/24 \
  ipv4.gateway 192.168.0.0 \
  ipv4.dns 8.8.8.8 \
  +ipv4.dns 8.8.4.4 \
  connection.autoconnect yes

nmcli connection show 'Wired Connection'
systemctl restart NetworkManager
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Self-hosted Load Balancer for bare metal Kubernetes]]></title>
            <link href="https://devopstales.github.io/home/k8s-metallb-bgp-pfsense/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-metallb-bgp-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Self-hosted Load Balancer for bare metal Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-metallb/?utm_source=atom_feed" rel="related" type="text/html" title="Install MetalLB load balancer for Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-longhorn/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes project longhorn" />
                <link href="https://devopstales.github.io/home/k8s-nginx-ingress/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
                <link href="https://devopstales.github.io/home/k8s-install-openebs/?utm_source=atom_feed" rel="related" type="text/html" title="Install OpenEBS for Kubernetes" />
            
                <id>https://devopstales.github.io/home/k8s-metallb-bgp-pfsense/</id>
            
            
            <published>2020-08-18T00:00:00+00:00</published>
            <updated>2020-08-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this tutorial I will show you how to install  Metal LB load balancer in BGP mode for Kubernetes.</p>
<H3>Parst of the Kubernetes series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
     <li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
     <li>Part1c: <a href="../../kubernetes/k8s-install-containerd/">Install K8S with containerd and kubeadm</a></li>
     <li>Part1d: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
     <li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a></li>
<!--      <li>Part3b: k8s-multiple-nginx-ingress -->
<!--      <li>Part3c: k8s-contour-ingress -->
     <li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a></li>
<!-- local folder (+ autoprovisioning ) -->
     <li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
     <li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
     <li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
     <li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
     <li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
     <li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a></li>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... -->
     <li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
     <li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
     <li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
     <li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
     <li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
     <li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
     <li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a></li>
<!-- istio-install.md -->
     <li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a></li>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog -->
</ul>


<h3 id="metallb">Metallb</h3>
<p>Metallb is a fantastic bare metal-targeted operator for powering LoadBalancer types of services. It can work in two modes: Layer 2 and Border Gateway Protocol (BGP) mode. In layer 2 mode, one of the nodes advertises the load balanced IP (VIP) via ARP. This mode has two limitations: all the traffic goes through a single node VIP potentially limiting the bandwidth. The second limitation is a very slow failover. Detecting unhealthy nodes is a notoriously slow operation in Kubernetes which can take several minutes (5-10 minutes, which can be decreased with the node-problem-detector DaemonSet).</p>
<p>In BGP mode, Metallb advertises the VIP through BGP. It requires a BGP compatible router ath the network wher the kubernetes cluster is created.</p>
<p>I found that the layer 2 mode of Metallb is not a practical solution for production scenarios as it is typically not acceptable to have failover-induced downtimes in the order of minutes.</p>
<h3 id="how-does-the-full-setup-look-like">How does the full setup look like?</h3>
<p>For this Demo I will use a pfsense in virtualbox and tree vm for kubernetes in the same host-only network.</p>
<table>
<thead>
<tr>
<th>vm</th>
<th>nic</th>
<th>ip</th>
<th>mode</th>
</tr>
</thead>
<tbody>
<tr>
<td>pfsense01</td>
<td>em1</td>
<td>192.168.0.200</td>
<td>bridged</td>
</tr>
<tr>
<td>pfsense01</td>
<td>em2</td>
<td>172.17.9.200</td>
<td>host-only</td>
</tr>
<tr>
<td>k8sm01</td>
<td>enp0s8</td>
<td>172.17.9.10</td>
<td>host-only</td>
</tr>
<tr>
<td>k8sm02</td>
<td>enp0s8</td>
<td>172.17.9.11</td>
<td>host-only</td>
</tr>
<tr>
<td>k8sm02</td>
<td>enp0s8</td>
<td>172.17.9.12</td>
<td>host-only</td>
</tr>
</tbody>
</table>
<h3 id="issues-with-calico">Issues with Calico</h3>
<p>Simple BGP config with Calico don’t require anything special. However, if you are using Calico’s <!-- raw HTML omitted -->external BGP peering capability<!-- raw HTML omitted --> to advertise your cluster prefixes over BGP, and also want to use BGP in MetalLB, you will need to jump through some hoops.</p>
<h3 id="configuring-pfsense-and-openbgpd">Configuring pfSense and OpenBGPD</h3>
<p>First we need to install OpenBGPD pcakage on pfSense. Go to <code>System &gt; Package Manager &gt; Available Packages</code> Then select <code>OpenBGPD</code> and Install it.</p>
<p>There is otger BGP compatible pckages in pfSense so make  sure you DO NOT have the Quagga_OSPF or FRR packages installed. They directly conflict with each other.</p>
<p>No we need to configure BGP. Ther is a nice UI but I will use the Raw config for simplicity. Go to <code>Services &gt; OpenBGPD &gt; Raw config</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#75715e"># This file was created by the package manager. Do not edit!</span>

<span style="color:#ae81ff">AS 64512</span>
<span style="color:#ae81ff">fib-update yes</span>
<span style="color:#ae81ff">listen on 172.17.9.200</span>
<span style="color:#ae81ff">router-id 172.17.9.200</span>
<span style="color:#ae81ff">network 10.25.0.0/22</span>

<span style="color:#ae81ff">neighbor 172.17.9.10 {</span>
	<span style="color:#ae81ff">remote-as 64513</span>
    <span style="color:#ae81ff">announce all</span>
	<span style="color:#ae81ff">descr &#34;k8sm01&#34;</span>
}

<span style="color:#ae81ff">neighbor 172.17.9.11 {</span>
	<span style="color:#ae81ff">remote-as 64513</span>
    <span style="color:#ae81ff">announce all</span>
	<span style="color:#ae81ff">descr &#34;k8sm02&#34;</span>
}

<span style="color:#ae81ff">neighbor 172.17.9.12 {</span>
	<span style="color:#ae81ff">remote-as 64513</span>
    <span style="color:#ae81ff">announce all</span>
	<span style="color:#ae81ff">descr &#34;k8sm03&#34;</span>
}
</code></pre></div><ul>
<li><code>AS 64512</code> - Thi is the Autonomous System Number of pfsense</li>
<li><code>listen on </code> -   This is the address that OpenBGPD should listen to BGP requests on. I highly recommend setting this to the same as the <code>router-id</code> IP address.</li>
<li><code>network</code> - This is the network what you will use for advertise Load Balanced services.</li>
<li><code>neighbor $(kubernetes_worker_node_ip)</code> - The ip of the Kubernetes host</li>
<li><code>remote-as 64513</code> - Thi is the Autonomous System Number of the neighbors. Same for all Kubernetes Node.</li>
<li><code>announce all</code> - We need our nodes to be able to announce to the router their service IP addresses.</li>
</ul>
<h3 id="configuring-pfsense-and-quagga_ospf">Configuring pfSense and Quagga_OSPF</h3>
<p>If you prefer to use <code>Quagga_OSPF</code> Go to <code>System &gt; Package Manager &gt; Available Packages</code> Then select <code>Quagga_OSPF</code> and Install it.</p>
<p>To configure BGP with <code>Quagga_OSPF</code>. Go to <code>Services &gt; Quagga OSPFd &gt; Raw config</code> Edit <code>SAVED bgpd.conf</code> the <code>save</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">router bgp <span style="color:#ae81ff">64512</span>
 bgp router-id 172.17.9.200
 neighbor 172.17.9.10 remote-as <span style="color:#ae81ff">64513</span>
 neighbor 172.17.9.11 remote-as <span style="color:#ae81ff">64513</span>
 neighbor 172.17.9.12 remote-as <span style="color:#ae81ff">64513</span>
  network 10.25.0.0/22
</code></pre></div><h3 id="deploy-metallb">Deploy MetalLB</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/namespace.yaml
kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/metallb.yaml
<span style="color:#75715e"># On first install only</span>
kubectl create secret generic -n metallb-system memberlist --from-literal<span style="color:#f92672">=</span>secretkey<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;</span><span style="color:#66d9ef">$(</span>openssl rand -base64 128<span style="color:#66d9ef">)</span><span style="color:#e6db74">&#34;</span>
</code></pre></div><p>Make a BGP config for MetalLB</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano bgpconfig.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ConfigMap</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">metallb-system</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">config</span>
<span style="color:#f92672">data</span>:
  <span style="color:#f92672">config</span>: |<span style="color:#e6db74">
</span><span style="color:#e6db74">    peers:
</span><span style="color:#e6db74">    - peer-address: 172.17.9.200
</span><span style="color:#e6db74">      peer-asn: 64512
</span><span style="color:#e6db74">      my-asn: 64513
</span><span style="color:#e6db74">    address-pools:
</span><span style="color:#e6db74">    - name: default
</span><span style="color:#e6db74">      protocol: bgp
</span><span style="color:#e6db74">      addresses:
</span><span style="color:#e6db74">      - 10.25.0.10-10.25.3.250</span>    
</code></pre></div><h3 id="demo-time">Demo Time</h3>
<p>Let’s create a demo application for testing.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano test.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">apps/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Deployment</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">test-nginx</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">default</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">selector</span>:
    <span style="color:#f92672">matchLabels</span>:
      <span style="color:#f92672">run</span>: <span style="color:#ae81ff">test-nginx</span>
  <span style="color:#f92672">replicas</span>: <span style="color:#ae81ff">3</span>
  <span style="color:#f92672">template</span>:
    <span style="color:#f92672">metadata</span>:
      <span style="color:#f92672">labels</span>:
        <span style="color:#f92672">run</span>: <span style="color:#ae81ff">test-nginx</span>
    <span style="color:#f92672">spec</span>:
      <span style="color:#f92672">containers</span>:
      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">test-nginx</span>
        <span style="color:#f92672">image</span>: <span style="color:#ae81ff">nginx</span>
        <span style="color:#f92672">ports</span>:
        - <span style="color:#f92672">containerPort</span>: <span style="color:#ae81ff">80</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Service</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">test-nginx</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">default</span>
  <span style="color:#f92672">labels</span>:
    <span style="color:#f92672">run</span>: <span style="color:#ae81ff">test-nginx</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">type</span>: <span style="color:#ae81ff">LoadBalancer</span>
  <span style="color:#f92672">ports</span>:
  - <span style="color:#f92672">port</span>: <span style="color:#ae81ff">80</span>
    <span style="color:#f92672">protocol</span>: <span style="color:#ae81ff">TCP</span>
  <span style="color:#f92672">selector</span>:
    <span style="color:#f92672">run</span>: <span style="color:#ae81ff">test-nginx</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl apply -f bgpconfig.yaml
kubectl apply -f test.yaml
</code></pre></div><p>After a few moments, you can run this command to get the IP address:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">$ kubectl describe service test-nginx | grep <span style="color:#e6db74">&#34;LoadBalancer Ingress&#34;</span>
LoadBalancer Ingress:     10.25.0.11
</code></pre></div><p>Let&rsquo;s check the address in a browser. If pfSense is you default gateway it will work perfectly, but in my demo enviroment I need to create a route to pfSense for this network on my host machine:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo route add -net 10.25.0.0/22 gw 172.17.9.200
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         192.168.0.1     0.0.0.0         UG    <span style="color:#ae81ff">600</span>    <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> wlan0
10.25.0.0       172.17.9.200    255.255.252.0   UG    <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> vboxnet7
172.17.9.0      0.0.0.0         255.255.255.0   U     <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> vboxnet7
</code></pre></div><p><img src="/img/include/pfsense-bgp-kubernetes.png" alt="Example image"  class="zoomable" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to fix Ansible Service Broker in OpenShift 3.11]]></title>
            <link href="https://devopstales.github.io/home/how-to-fix-ansible-service-broker-in-openshift-3-11/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/how-to-fix-cluster-monitoring-operator-in-openshift-3-11/?utm_source=atom_feed" rel="related" type="text/html" title="How to fix cluster-monitoring-operator in OpenShift 3.11" />
                <link href="https://devopstales.github.io/home/how-to-fix-registry-console-ui-in-openshift-3-11/?utm_source=atom_feed" rel="related" type="text/html" title="How to fix registry console UI in OpenShift 3.11" />
                <link href="https://devopstales.github.io/home/openshift-cert-manager/?utm_source=atom_feed" rel="related" type="text/html" title="Install cert-manager to Openshift" />
                <link href="https://devopstales.github.io/home/openshift-auto-approval-csr/?utm_source=atom_feed" rel="related" type="text/html" title="How to Enable Auto Approval of CSR in Openshift v3.11" />
                <link href="https://devopstales.github.io/home/openshift-secondary-router/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift secondary route" />
            
                <id>https://devopstales.github.io/home/how-to-fix-ansible-service-broker-in-openshift-3-11/</id>
            
            
            <published>2020-07-10T00:00:00+00:00</published>
            <updated>2020-07-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Ansible Service Broker in OpenShift 3.11 is broken as it uses wrong docker tag.</p>
<H3>Parst of the Openshift series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
     <li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
     <li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
     <li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
     <li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
     <li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
     <li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
     <li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
     <li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
     <li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
     <li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
     <li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
     <li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
     <li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
     <li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
     <li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
     <li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
     <li>Part16a: <a href="../../kubernetes/openshift-elasticsearch-error/">Opeshift elasticsearch search-guard error</a></li>
     <li>Part16b: <a href="../../kubernetes/openshift-log4shell/">Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)</a></li>
</ul>


<h3 id="export-running-deployment-to-yaml">Export running deployment to yaml</h3>
<pre tabindex="0"><code>oc project openshift-ansible-service-broker
oc get --export dc/asb -o yaml &gt; asb.yaml
</code></pre><h3 id="patch-and-deploy-the-yaml">Patch and deploy the yaml</h3>
<pre tabindex="0"><code>replate docker.io/ansibleplaybookbundle/origin-ansible-service-broker:latest to
docker.io/ansibleplaybookbundle/origin-ansible-service-broker:ansible-service-broker-1.3.23-1
oc apply -f asb.yaml


curl -k -H &quot;Authorization: Bearer `oc serviceaccounts get-token asb-client`&quot; https://`oc get routes -n openshift-ansible-service-broker --no-headers | awk '{print $2}'`/osb/v2/catalog
</code></pre><h3 id="config-for-new-install">Config for new install</h3>
<pre tabindex="0"><code>ansible_service_broker_image=docker.io/ansibleplaybookbundle/origin-ansible-service-broker:ansible-service-broker-1.3.23-1
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to fix cluster-monitoring-operator in OpenShift 3.11]]></title>
            <link href="https://devopstales.github.io/home/how-to-fix-cluster-monitoring-operator-in-openshift-3-11/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/how-to-fix-ansible-service-broker-in-openshift-3-11/?utm_source=atom_feed" rel="related" type="text/html" title="How to fix Ansible Service Broker in OpenShift 3.11" />
                <link href="https://devopstales.github.io/home/how-to-fix-registry-console-ui-in-openshift-3-11/?utm_source=atom_feed" rel="related" type="text/html" title="How to fix registry console UI in OpenShift 3.11" />
                <link href="https://devopstales.github.io/home/openshift-cert-manager/?utm_source=atom_feed" rel="related" type="text/html" title="Install cert-manager to Openshift" />
                <link href="https://devopstales.github.io/home/openshift-auto-approval-csr/?utm_source=atom_feed" rel="related" type="text/html" title="How to Enable Auto Approval of CSR in Openshift v3.11" />
                <link href="https://devopstales.github.io/home/openshift-secondary-router/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift secondary route" />
            
                <id>https://devopstales.github.io/home/how-to-fix-cluster-monitoring-operator-in-openshift-3-11/</id>
            
            
            <published>2020-07-10T00:00:00+00:00</published>
            <updated>2020-07-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Default install use an old image for cluster-monitoring-operator with imagestream false latanci alert  problem.</p>
<H3>Parst of the Openshift series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
     <li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
     <li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
     <li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
     <li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
     <li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
     <li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
     <li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
     <li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
     <li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
     <li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
     <li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
     <li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
     <li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
     <li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
     <li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
     <li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
     <li>Part16a: <a href="../../kubernetes/openshift-elasticsearch-error/">Opeshift elasticsearch search-guard error</a></li>
     <li>Part16b: <a href="../../kubernetes/openshift-log4shell/">Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)</a></li>
</ul>


<h3 id="export-running-deployment-to-yaml">Export running deployment to yaml</h3>
<pre tabindex="0"><code>oc project openshift-monitoring
oc get --export deployment/cluster-monitoring-operator -o yaml &gt; cluster-monitoring-operator.yaml
</code></pre><h3 id="patch-and-deploy-the-yaml">Patch and deploy the yaml</h3>
<pre tabindex="0"><code>sed -i -e &quot;s|image:.*|image: quay.io/openshift/origin-cluster-monitoring-operator:v3.11
|&quot; \
&gt; cluster-monitoring-operator.yaml

oc apply -f cluster-monitoring-operator.yaml


curl -k -H &quot;Authorization: Bearer `oc serviceaccounts get-token asb-client`&quot; https://`oc get routes -n openshift-ansible-service-broker --no-headers | awk '{print $2}'`/osb/v2/catalog

oc delete deployment/prometheus-operator
oc delete statefulset/alertmanager-main
oc delete statefulset/prometheus-k8s
</code></pre><h3 id="config-for-new-install">Config for new install</h3>
<pre tabindex="0"><code>openshift_cluster_monitoring_operator_image=quay.io/openshift/origin-cluster-monitoring-operator:v3.11
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to fix registry console UI in OpenShift 3.11]]></title>
            <link href="https://devopstales.github.io/home/how-to-fix-registry-console-ui-in-openshift-3-11/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/how-to-fix-ansible-service-broker-in-openshift-3-11/?utm_source=atom_feed" rel="related" type="text/html" title="How to fix Ansible Service Broker in OpenShift 3.11" />
                <link href="https://devopstales.github.io/home/how-to-fix-cluster-monitoring-operator-in-openshift-3-11/?utm_source=atom_feed" rel="related" type="text/html" title="How to fix cluster-monitoring-operator in OpenShift 3.11" />
                <link href="https://devopstales.github.io/home/openshift-cert-manager/?utm_source=atom_feed" rel="related" type="text/html" title="Install cert-manager to Openshift" />
                <link href="https://devopstales.github.io/home/openshift-auto-approval-csr/?utm_source=atom_feed" rel="related" type="text/html" title="How to Enable Auto Approval of CSR in Openshift v3.11" />
                <link href="https://devopstales.github.io/home/openshift-secondary-router/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift secondary route" />
            
                <id>https://devopstales.github.io/home/how-to-fix-registry-console-ui-in-openshift-3-11/</id>
            
            
            <published>2020-07-10T00:00:00+00:00</published>
            <updated>2020-07-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Registry console UI in OpenShift 3.11 is broken on CentOS as it is not available on Docker Hub.</p>
<H3>Parst of the Openshift series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
     <li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
     <li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
     <li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
     <li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
     <li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
     <li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
     <li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
     <li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
     <li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
     <li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
     <li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
     <li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
     <li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
     <li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
     <li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
     <li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
     <li>Part16a: <a href="../../kubernetes/openshift-elasticsearch-error/">Opeshift elasticsearch search-guard error</a></li>
     <li>Part16b: <a href="../../kubernetes/openshift-log4shell/">Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)</a></li>
</ul>


<h3 id="export-running-deployment-to-yaml">Export running deployment to yaml</h3>
<pre tabindex="0"><code>oc project default
oc get --export   dc/registry-console -o yaml &gt; registry_console.yaml
</code></pre><h3 id="patch-and-deploy-the-yaml">Patch and deploy the yaml</h3>
<pre tabindex="0"><code>sed -i -e &quot;s|image:.*|image: docker.io/timbordemann/cockpit-kubernetes:latest|&quot; registry_console.yaml
oc apply -f registry_console.yaml
</code></pre><h3 id="config-for-new-install">Config for new install</h3>
<pre tabindex="0"><code>openshift_cockpit_deployer_image=docker.io/timbordemann/cockpit-kubernetes:latest
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install cert-manager to Openshift]]></title>
            <link href="https://devopstales.github.io/home/openshift-cert-manager/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-auto-approval-csr/?utm_source=atom_feed" rel="related" type="text/html" title="How to Enable Auto Approval of CSR in Openshift v3.11" />
                <link href="https://devopstales.github.io/home/openshift-secondary-router/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift secondary route" />
                <link href="https://devopstales.github.io/home/openshift-gitlab-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Installing GitLab on OpenShift" />
                <link href="https://devopstales.github.io/home/openshift-kompose/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Run docker-compoe in Openshift" />
                <link href="https://devopstales.github.io/home/openshift-add-node/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Add Nodes to a Cluster" />
            
                <id>https://devopstales.github.io/home/openshift-cert-manager/</id>
            
            
            <published>2020-06-10T00:00:00+00:00</published>
            <updated>2020-06-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p><code>cert-manager</code> is a service that automatically creates certificate requests and sign certificate based on annotations. The created certificate will be stored in a secret.</p>
<H3>Parst of the Openshift series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
     <li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
     <li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
     <li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
     <li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
     <li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
     <li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
     <li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
     <li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
     <li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
     <li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
     <li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
     <li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
     <li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
     <li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
     <li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
     <li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
     <li>Part16a: <a href="../../kubernetes/openshift-elasticsearch-error/">Opeshift elasticsearch search-guard error</a></li>
     <li>Part16b: <a href="../../kubernetes/openshift-log4shell/">Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)</a></li>
</ul>


<p>Normally in kubernetes you can use a secret for TLS in an ingress cinfiguration but in Openshift there is no way to get the certificate from a secret for a route. So we will use <code>cert-utils-operator</code> for recreating routs with the propriety certificate based on annotations.</p>
<h3 id="install-cert-managger">Install cert-managger</h3>
<pre tabindex="0"><code>oc create namespace cert-manager
oc apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.15.1/cert-manager-legacy.yaml
</code></pre><p>Create <code>ClusterIssuer</code> to create certs. For this demo I will use a Self-signed root CA, what is trustin in my browser. <code>cert-manager</code> can handle Let&rsquo;s encrypt as an issuer both with http and dns challenges so yu can use Let&rsquo;s encrypt certs in a private network without publication your route.</p>
<pre tabindex="0"><code>nano issuer.yaml
---
apiVersion: v1
data:
  tls.crt: LS0tLS1C...
  tls.key: LS0tLSGF...
kind: Secret
metadata:
  name: ca-key-pair
  namespace: cert-manager
---
apiVersion: cert-manager.io/v1alpha2
kind: ClusterIssuer
metadata:
  name: ca-issuer
  namespace: cert-manager
spec:
  ca:
    secretName: ca-key-pair
</code></pre><h3 id="install-cert-utils-operator">Install cert-utils-operator</h3>
<p>I usethe v0.1.0 and not the latest one (at the moment v0.1.1) besause athe v0.1.1 has a bug on OKD 3.11:</p>
<ul>
<li><a href="https://github.com/redhat-cop/cert-utils-operator/issues/51">https://github.com/redhat-cop/cert-utils-operator/issues/51</a></li>
</ul>
<pre tabindex="0"><code>helm repo add cert-utils-operator https://redhat-cop.github.io/cert-utils-operator
helm update
# export CERT_UTILS_CHART_VERSION=$(helm search cert-utils-operator/cert-utils-operator | grep cert-utils-operator/cert-utils-operator | awk '{print $2}')

helm fetch cert-utils-operator/cert-utils-operator --version v0.1.0
helm template cert-utils-operator-v0.1.0.tgz --namespace cert-manager | oc apply -f - -n cert-manager
</code></pre><pre tabindex="0"><code>apiVersion: apps.openshift.io/v1
kind: DeploymentConfig
metadata:
  annotations:
    openshift.io/generated-by: OpenShiftWebConsole
  labels:
    app: nginx2
  name: nginx2
spec:
  replicas: 1
  selector:
    app: nginx2
    deploymentconfig: nginx2
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx2
        deploymentconfig: nginx2
    spec:
      containers:
        - image: &gt;-
            bitnami/nginx@sha256:2bff7d085671a8b0f9ec296cf57fba995d06c1b5fb350575dd429c361520f0a4
          imagePullPolicy: Always
          name: nginx2
          ports:
            - containerPort: 8080
              protocol: TCP
            - containerPort: 8443
              protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
  test: false
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    openshift.io/generated-by: OpenShiftWebConsole
  labels:
    app: nginx2
  name: nginx2
spec:
  clusterIP: 172.30.17.64
  ports:
    - name: 8080-tcp
      port: 8080
      protocol: TCP
      targetPort: 8080
    - name: 8443-tcp
      port: 8443
      protocol: TCP
      targetPort: 8443
  selector:
    deploymentconfig: nginx2
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
---
apiVersion: cert-manager.io/v1alpha2
kind: Certificate
metadata:
  name: nginx2-route-tls
  namespace: default
spec:
  secretName: nginx2-route-tls
  duration: 24h
  renewBefore: 12h
  commonName: nginx.openshift.mydomain.intra
  dnsNames:
  - nginx.openshift.mydomain.intra
  issuerRef:
    name: ca-issuer
    kind: ClusterIssuer
    group: cert-manager.io
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    cert-utils-operator.redhat-cop.io/certs-from-secret=nginx2-route-tls
  labels:
    app: nginx2
  name: nginx2
spec:
  host: nginx.openshift.mydomain.intra
  port:
    targetPort: 8080-tcp
  tls:
    insecureEdgeTerminationPolicy: Redirect
    termination: edge
  to:
    kind: Service
    name: nginx2
    weight: 100
  wildcardPolicy: None

---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    cert-utils-operator.redhat-cop.io/certs-from-secret: nginx2-route-tls
  labels:
    app: nginx2
  name: nginx2
spec:
  host: nginx.openshift.mydomain.intra
  port:
    targetPort: 8080-tcp
  tls:
    insecureEdgeTerminationPolicy: Redirect
    termination: edge
  to:
    kind: Service
    name: nginx2
    weight: 100
  wildcardPolicy: None
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to Enable Auto Approval of CSR in Openshift v3.11]]></title>
            <link href="https://devopstales.github.io/home/openshift-auto-approval-csr/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-secondary-router/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift secondary route" />
                <link href="https://devopstales.github.io/home/openshift-gitlab-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Installing GitLab on OpenShift" />
                <link href="https://devopstales.github.io/home/openshift-kompose/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Run docker-compoe in Openshift" />
                <link href="https://devopstales.github.io/home/openshift-add-node/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Add Nodes to a Cluster" />
                <link href="https://devopstales.github.io/home/openshift-letsencrypt/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Letsencrypt certificates" />
            
                <id>https://devopstales.github.io/home/openshift-auto-approval-csr/</id>
            
            
            <published>2020-05-27T00:00:00+00:00</published>
            <updated>2020-05-27T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Nodes certificates are not Completely redeployed through playbook but through a different mechanism.</p>
<H3>Parst of the Openshift series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
     <li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
     <li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
     <li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
     <li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
     <li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
     <li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
     <li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
     <li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
     <li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
     <li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
     <li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
     <li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
     <li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
     <li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
     <li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
     <li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
     <li>Part16a: <a href="../../kubernetes/openshift-elasticsearch-error/">Opeshift elasticsearch search-guard error</a></li>
     <li>Part16b: <a href="../../kubernetes/openshift-log4shell/">Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)</a></li>
</ul>


<p>SSL Certificates will be valid for the period of 1 year and at 85% of the certificate the node will trigger a CSR that would have to be approved for the certificate to be redeployed.</p>
<p>The Only certificates that are renewed/redeployed through CSR’s mechanism are the kubelet/nodes certificates. Any other certificates e.g, router, master, api certs, etcd, docker-registry, etc are still redeployed through the usual playbooks.</p>
<p>If triggered CSR is not approved either manually or in automated way then after one year all nodes will go to NotReady State.</p>
<h3 id="check-and-approve-csrs-manually">Check and approve csr&rsquo;s manually</h3>
<pre tabindex="0"><code>oc get csr
oc describe csr &lt;csr_name&gt;
oc adm certificate &lt;approve csr_name&gt;

oc get csr -o name | xargs oc adm certificate approve
</code></pre><h3 id="approve-csrs-automaticle">Approve csr&rsquo;s automaticle</h3>
<p>At install time you can add this option to your ansible hosts fiel:</p>
<pre tabindex="0"><code>openshift_master_bootstrap_auto_approve=true
</code></pre><p>If you installed the cluster and want to change this option run this playbook:</p>
<pre tabindex="0"><code>ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/openshift-master/enable_bootstrap.yml \
-e openshift_master_bootstrap_auto_approve=true
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install OpenEBS for Kubernetes]]></title>
            <link href="https://devopstales.github.io/home/k8s-install-openebs/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-kubeadm-ha/?utm_source=atom_feed" rel="related" type="text/html" title="How to install kubernetes with kubeadm in HA mode" />
                <link href="https://devopstales.github.io/home/k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install kubernetes with kubeadm" />
                <link href="https://devopstales.github.io/kubernetes/k8s-install-openebs/?utm_source=atom_feed" rel="related" type="text/html" title="Install OpenEBS for Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-local-pv/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
                <link href="https://devopstales.github.io/home/k8s-velero-backup/?utm_source=atom_feed" rel="related" type="text/html" title="Backup your Kubernetes Cluster with Velero" />
            
                <id>https://devopstales.github.io/home/k8s-install-openebs/</id>
            
            
            <published>2020-05-20T00:00:00+00:00</published>
            <updated>2020-05-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>OpenEBS is an open-source project for container-attached and container-native storage on Kubernetes.</p>
<H3>Parst of the Kubernetes series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
     <li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
     <li>Part1c: <a href="../../kubernetes/k8s-install-containerd/">Install K8S with containerd and kubeadm</a></li>
     <li>Part1d: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
     <li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a></li>
<!--      <li>Part3b: k8s-multiple-nginx-ingress -->
<!--      <li>Part3c: k8s-contour-ingress -->
     <li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a></li>
<!-- local folder (+ autoprovisioning ) -->
     <li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
     <li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
     <li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
     <li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
     <li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
     <li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a></li>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... -->
     <li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
     <li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
     <li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
     <li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
     <li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
     <li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
     <li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a></li>
<!-- istio-install.md -->
     <li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a></li>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog -->
</ul>


<p>On all host we have an unused unpartitioned disk called sdb.</p>
<pre tabindex="0"><code>lsblk

NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda               8:0    0   80G  0 disk
├─sda1            8:1    0    1G  0 part /boot
└─sda2            8:2    0   79G  0 part
  ├─centos-root 253:0    0   50G  0 lvm  /
  ├─centos-swap 253:1    0    1G  0 lvm
  └─centos-home 253:3    0   28G  0 lvm  /home
sdb               8:16   0  100G  0 disk
</code></pre><h3 id="install-requirements">Install requirements</h3>
<p>OpenEBS use iscsi for persisten volume sharing so we need  <code>iscsid</code>.</p>
<h4 id="debian--ubuntu">Debian / Ubuntu</h4>
<pre tabindex="0"><code>apt-get install open-iscsi
service open-iscsi enable
service open-iscsi restart

</code></pre><h4 id="centos">CentOS</h4>
<pre tabindex="0"><code>yum install iscsi-initiator-utils -y
systemctl enable iscsid
systemctl start iscsid
</code></pre><h3 id="deploy-openebs-with-helm">Deploy OpenEBS with helm</h3>
<p>OpenEBS is in the stable repo so we didn&rsquo;t need to add a separate helm repository for installing it. In my case I will use teh <code>openebs-system</code> namespace for the install.</p>
<pre tabindex="0"><code>kubectl create ns openebs-system
helm upgrade --install openebs stable/openebs --version 1.7.0 --namespace=openebs-system
</code></pre><p>Wait for all the  pods are started.</p>
<pre tabindex="0"><code>kubectl get pods -n openebs-system

NAME                                           READY     STATUS    RESTARTS   AGE
openebs-admission-server-7b4859ccd5-bz4zt      1/1       Running   0          14m
openebs-apiserver-556ffff45c-nk9x9             1/1       Running   5          15m
openebs-localpv-provisioner-76b466d4b8-5tj4w   1/1       Running   0          15m
openebs-ndm-f6cqz                              1/1       Running   0          15m
openebs-ndm-operator-5f6c5497d7-chf6t          1/1       Running   1          15m
openebs-ndm-qrmp9                              1/1       Running   0          15m
openebs-ndm-stgml                              1/1       Running   0          15m
openebs-provisioner-c9c7f9ff8-hn4bl            1/1       Running   0          15m
openebs-snapshot-operator-6578d74b7-2wc97      2/2       Running   0          15m
</code></pre><p>Now verify if OpenEBS is installed successfully.</p>
<pre tabindex="0"><code>kubectl get blockdevice -n openebs-system

NAME                                           NODENAME    SIZE         CLAIMSTATE   STATUS    AGE
blockdevice-0c4e03f9e39a4092108215f19eca9da8   k8s-node1   1048576000   Unclaimed    Active    16m
blockdevice-1aaa1142a7b9c65dfa32dec88fe1749b   k8s-node2   1048576000   Unclaimed    Active    16m
blockdevice-5f728d1068c72337609fc1f88855b9bb   k8s-node3   1048576000   Unclaimed    Active    16m
</code></pre><pre tabindex="0"><code>kubectl describe blockdevice blockdevice-0c4e03f9e39a4092108215f19eca9da8
...
  Devlinks:
    Kind:  by-id
    Links:
      /dev/disk/by-id/ata-VBOX_HARDDISK_VBd4679835-eb798f2c
      /dev/disk/by-id/lvm-pv-uuid-PWnLFv-b0jS-7CLZ-Cmym-0dia-RQkI-w0Hkam
    Kind:  by-path
    Links:
      /dev/disk/by-path/pci-0000:00:01.1-ata-2.0
  Filesystem:
    Fs Type:  LVM2_member
  Node Attributes:
    Node Name:  k8s-node1
  Partitioned:  No
  Path:         /dev/sdb
...
</code></pre><h4 id="verify-storageclasses">Verify StorageClasses:</h4>
<pre tabindex="0"><code>kubectl get sc

NAME                        PROVISIONER                                                AGE
openebs-device              openebs.io/local                                           64s
openebs-hostpath            openebs.io/local                                           64s
openebs-jiva-default        openebs.io/provisioner-iscsi                               64s
openebs-snapshot-promoter   volumesnapshot.external-storage.k8s.io/snapshot-promoter   64s
</code></pre><h3 id="storage-engines">Storage engines</h3>
<p>OpenEBS offers three storage engines:</p>
<ul>
<li>Jiva</li>
<li>cStor</li>
<li>LocalPV</li>
</ul>
<p>Jiva is a light weight storage engine that is recommended to use for low capacity workloads. It is actually based on the same technology that powers Longhorn.</p>
<pre tabindex="0"><code>---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: demo-vol1-claim
spec:
  storageClassName: openebs-jiva-default
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 4G
</code></pre><p>cStor requires raw disks. The snapshot and storage management features of the other cStor engine are more advanced than Jiva. For provisioning a cStor Volume, it requires a cStor Storage Pool and a StorageClass. cStor provides iSCSI targets, which are appropriate for RWO (ReadWriteOnce) access mode and is suitable for all types of databases. cStor supports thin provisioning by default.</p>
<pre tabindex="0"><code>cat stor-pool1-config.yaml
---
apiVersion: openebs.io/v1alpha1
kind: StoragePoolClaim
metadata:
  name: cstor-disk-pool
  annotations:
    cas.openebs.io/config: |
      - name: PoolResourceRequests
        value: |-
            memory: 500Mb
      - name: PoolResourceLimits
        value: |-
            memory: 500Mb
spec:
  name: cstor-disk-pool
  type: disk
  poolSpec:
    poolType: striped
  blockDevices:
    blockDeviceList:
    - blockdevice-0c4e03f9e39a4092108215f19eca9da8
    - blockdevice-1aaa1142a7b9c65dfa32dec88fe1749b
    - blockdevice-5f728d1068c72337609fc1f88855b9bb
</code></pre><pre tabindex="0"><code>kubectl apply -f stor-pool1-config.yaml

kubectl get spc

NAME              AGE
cstor-disk-pool   20s

kubectl get csp

NAME                   ALLOCATED   FREE      CAPACITY   STATUS    TYPE      AGE
cstor-disk-pool-cxm8   294K        100G      100G       Healthy   striped   27m
cstor-disk-pool-r1hl   270K        100G      100G       Healthy   striped   27m
cstor-disk-pool-t05z   92K         100G      100G       Healthy   striped   27m
</code></pre><pre tabindex="0"><code>cat openebs-sc-rep3.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: openebs-cstore-default
  annotations:
    openebs.io/cas-type: cstor
    cas.openebs.io/config: |
      - name: StoragePoolClaim
        value: &quot;cstor-disk-pool&quot;
      - name: ReplicaCount
        value: &quot;3&quot;
    storageclass.kubernetes.io/is-default-class: &quot;true&quot;
provisioner: openebs.io/provisioner-iscsi
</code></pre><pre tabindex="0"><code>cat test-cs-pcv.yaml
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: test-cs-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3G
</code></pre><pre tabindex="0"><code>kubectl apply -f openebs-sc-rep3.yaml
kubectl apply -f test-cs-pcv.yaml
</code></pre><p>Local PV is based on Kubernetes local persistent volumes but it has a dynamic provisioner. It can store data either in a directory, or use disks; in the first case the hostpath can be shared by multiple persistent volumes, while when using disks each persistent volume requires a separate device. Local PV offers extremely high performance close to what you get by reading from and writing to the disk directly, but it doesn’t offer features such as replication, which are built in Jiva and cStor.</p>
<pre tabindex="0"><code>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    cas.openebs.io/config: |
      - name: StorageType
        value: &quot;hostpath&quot;
      - name: BasePath
        value: &quot;/mnt/openebs&quot;
    openebs.io/cas-type: local
  name: openebs-hostpath-mount
provisioner: openebs.io/local
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
</code></pre><h2 id="sources">Sources:</h2>
<ul>
<li><a href="https://vitobotta.com/2019/07/03/openebs-tips/">https://vitobotta.com/2019/07/03/openebs-tips/</a></li>
</ul>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Foreman openidc SSO with keycloak]]></title>
            <link href="https://devopstales.github.io/home/foreman-sso/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/sso/foreman-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Foreman openidc SSO with keycloak" />
                <link href="https://devopstales.github.io/home/mattermost-keycloak-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Free sso for Mattermost Teams Edition" />
                <link href="https://devopstales.github.io/home/k8s-kuberos/?utm_source=atom_feed" rel="related" type="text/html" title="Kubectl authentication with Kuberos" />
                <link href="https://devopstales.github.io/home/k8s-gangway/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak and gangway" />
                <link href="https://devopstales.github.io/home/k8s-dasboard-auth/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak and gatekeeper" />
            
                <id>https://devopstales.github.io/home/foreman-sso/</id>
            
            
            <published>2020-05-15T00:00:00+00:00</published>
            <updated>2020-05-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I this post I will show you how you can configure Foreman to use Keycloak asz an OIDC SSO authentication provider.</p>
<p>I use a Self-signed certificate for keycloak so my first step is to add the root CA of this certificate as a trusted certificate on the foreman server:</p>
<pre tabindex="0"><code>cp rootca.crt /etc/pki/ca-trust/source/anchors/
update-ca-trust
</code></pre><h3 id="install-requirements">install requirements</h3>
<pre tabindex="0"><code>yum install mod_auth_openidc keycloak-httpd-client-install
</code></pre><p>The <code>keycloak-httpd-client-install</code> is a commandline tool thet helps to configure the apache2&rsquo;s <code>mod_auth_openidc</code> plugin with Keycloak. This feature is not yet supported by foreman-installer. As a result, re-running the foreman-installer command can purge the changes in Apache files added by the keycloak-httpd-client-install.</p>
<pre tabindex="0"><code>keycloak-httpd-client-install \
--app-name foreman-openidc \
--keycloak-server-url &quot;https://keycloak.mydomain.intra&quot; \
--keycloak-admin-usernam &quot;admin&quot; \
--keycloak-realm &quot;ssl-realm&quot; \
--keycloak-admin-realm master \
--keycloak-auth-role root-admin \
-t openidc -l /users/extlogin

systemct restart httpd
</code></pre><h3 id="configure-keycloak">Configure Keycloak</h3>
<p>We need to create two mappers for the new client in keycloak:</p>
<p><img src="/img/include/foreman-sso-1.png" alt="Example image"  class="zoomable" /><!-- raw HTML omitted -->
<img src="/img/include/foreman-sso-2.png" alt="Example image"  class="zoomable" /><!-- raw HTML omitted --></p>
<p>Then in Keycloak create a group called <code>foreman-admin</code> and add the test user for it.</p>
<h3 id="configure-foreman">Configure Foreman:</h3>
<pre tabindex="0"><code>Administer / Settings / Authentication
Authorize login delegation = yes
Authorize login delegation auth source user autocreate = External
OIDC Algorithm = RS256
# the client id from keycloak
OIDC Audience = foreman.mydomain.intra-foreman-openidc
OIDC Issuer = https://keycloak.mydomain.intra/auth/realms/ssl-realm
OIDC JWKs URL = https://keycloak.mydomain.intra/auth/realms/ssl-realm/protocol/openid-connect/certs
</code></pre><p>Create an user groupe in foreman called <code>foreman-admin</code> and map with the external group called <code>foreman-admin</code>.
<img src="/img/include/foreman-sso-3.png" alt="Example image"  class="zoomable" /><!-- raw HTML omitted -->
<img src="/img/include/foreman-sso-4.png" alt="Example image"  class="zoomable" /><!-- raw HTML omitted -->
<img src="/img/include/foreman-sso-5.png" alt="Example image"  class="zoomable" /><!-- raw HTML omitted --></p>
<p>The log out and go the he url of the foreman. It will redirect to Keycloak to login. Add the test users credentials the login wit it.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to create your first Chef Cookbook]]></title>
            <link href="https://devopstales.github.io/home/chef-first-cookbook/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/chef-server-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install  Chef server" />
                <link href="https://devopstales.github.io/home/ansible-k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Kubernetes" />
                <link href="https://devopstales.github.io/home/ansible-operator-overview/?utm_source=atom_feed" rel="related" type="text/html" title="Ansible Operator Overview" />
                <link href="https://devopstales.github.io/home/awx-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install AWX" />
                <link href="https://devopstales.github.io/home/awx-docker/?utm_source=atom_feed" rel="related" type="text/html" title="Install AWX in docker" />
            
                <id>https://devopstales.github.io/home/chef-first-cookbook/</id>
            
            
            <published>2020-05-07T00:00:00+00:00</published>
            <updated>2020-05-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I this post I will show you how you can create a basic chef structure.</p>
<h3 id="create-environments">Create environments</h3>
<pre tabindex="0"><code>cd ~/chef-repo

mkdir environments
nano environments/production.json
{
  &quot;name&quot;: &quot;Production&quot;,
  &quot;description&quot;: &quot;&quot;,
  &quot;cookbook_versions&quot;: {

  },
  &quot;json_class&quot;: &quot;Chef::Environment&quot;,
  &quot;chef_type&quot;: &quot;environment&quot;,
  &quot;default_attributes&quot;: {

  },
  &quot;override_attributes&quot;: {

  }
}

nano environments/development.json
{
  &quot;name&quot;: &quot;Development&quot;,
  &quot;description&quot;: &quot;&quot;,
  &quot;cookbook_versions&quot;: {

  },
  &quot;json_class&quot;: &quot;Chef::Environment&quot;,
  &quot;chef_type&quot;: &quot;environment&quot;,
  &quot;default_attributes&quot;: {

  },
  &quot;override_attributes&quot;: {

  }
}
</code></pre><pre tabindex="0"><code>knife environment from file environments/production.json
knife environment from file environments/development.json
</code></pre><h3 id="create-roles">Create roles</h3>
<pre tabindex="0"><code>mkdir roles
nano roles/base.json
{

  &quot;name&quot;: &quot;base&quot;,
  &quot;description&quot;: &quot;Default operation role&quot;,
  &quot;json_class&quot;: &quot;Chef::Role&quot;,

  &quot;override_attributes&quot;: {
    &quot;chef_client&quot;: {
      &quot;config&quot;: {
        &quot;interval&quot;: 900,
        &quot;splay&quot;: 30
      }
    }
  },

  &quot;chef_type&quot;: &quot;role&quot;,

  &quot;run_list&quot;: [
    &quot;recipe[operation]&quot;
  ],

  &quot;env_run_lists&quot;: {
  }

}
</code></pre><pre tabindex="0"><code>knife role from file roles/base.json
</code></pre><h3 id="create-node-config">Create node config</h3>
<pre tabindex="0"><code>mkdir nodes
nano nodes/test.mydomain.intra.json
{
  &quot;name&quot;: &quot;test.mydomain.intra&quot;,
  &quot;chef_environment&quot;: &quot;Production&quot;,
  &quot;normal&quot;: {
  	&quot;tags&quot;: [

    ]
  },
  &quot;run_list&quot;: [
	&quot;role[base]&quot;
]

}
</code></pre><pre tabindex="0"><code>knife node from file nodes/test.mydomain.intra.json
</code></pre><h3 id="create-cookbook">Create cookbook</h3>
<pre tabindex="0"><code>cd chef-repo/cookbooks
chef generate cookbook operation
nano operation/recipes/default.rb
include_recipe 'operation::packages'

nano operation/recipes/packages.rb
package &quot;nano&quot; do
  action :install
end
</code></pre><pre tabindex="0"><code>cd ..
knife cookbook upload operation
knife cookbook list
</code></pre><h3 id="test-cookbook">Test cookbook</h3>
<pre tabindex="0"><code>knife ssh 'name:test.mydomain.intra' 'sudo chef-client' -x root
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install  Chef server]]></title>
            <link href="https://devopstales.github.io/home/chef-server-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/ansible-k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Kubernetes" />
                <link href="https://devopstales.github.io/home/ansible-operator-overview/?utm_source=atom_feed" rel="related" type="text/html" title="Ansible Operator Overview" />
                <link href="https://devopstales.github.io/home/awx-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install AWX" />
                <link href="https://devopstales.github.io/home/awx-docker/?utm_source=atom_feed" rel="related" type="text/html" title="Install AWX in docker" />
                <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
            
                <id>https://devopstales.github.io/home/chef-server-install/</id>
            
            
            <published>2020-04-30T00:00:00+00:00</published>
            <updated>2020-04-30T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Chef is a powerful configuration management utility writy in ruby. This post will help you to setup a chef 13 on CentOS 7</p>
<ul>
<li>Chef Server: This is the central hub server that stores the cookbooks and recipes uploaded from workstations.</li>
<li>Chef Workstations: This where recipes, cookbooks, and other chef configuration details are created or edited.</li>
<li>Chef Client: This the target node where the configurations are deployed by the chef-client.</li>
</ul>
<h2 id="chef-server-install">Chef Server Install:</h2>
<pre tabindex="0"><code>cd /opt
wget https://packages.chef.io/files/stable/chef-server/13.2.0/el/7/chef-server-core-13.2.0-1.el7.x86_64.rpm
yum install chef-server-core-13.2.0-1.el7.x86_64.rpm -y

chef-server-ctl reconfigure
chef-server-ctl status
</code></pre><p>Create admin user for chef server:</p>
<pre tabindex="0"><code># chef-server-ctl user-create USER_NAME FIRST_NAME LAST_NAME EMAIL 'PASSWORD' -f PATH_FILE_NAME
chef-server-ctl user-create admin admin admin admin@devopstales.intra Password1 -f /etc/chef/admin.pem
</code></pre><p>Now create an organization to hold the chef configurations.</p>
<pre tabindex="0"><code># chef-server-ctl org-create short_name 'full_organization_name' --association_user user_name --filename ORGANIZATION-validator.pem

chef-server-ctl org-create devopstales &quot;DevOpsTales, Inc&quot; --association_user admin -f /etc/chef/devopstales-validator.pem
</code></pre><h2 id="install-chef-workstation">Install Chef workstation:</h2>
<p>Fot this demo I will install the workstation on the same server as the Chef server, but in a pruduction enviroment it is your laptop or pc.</p>
<pre tabindex="0"><code>wget https://packages.chef.io/files/stable/chefdk/4.7.73/el/7/chefdk-4.7.73-1.el7.x86_64.rpm
yum install -y chefdk-4.7.73-1.el7.x86_64.rpm
chef verify
</code></pre><pre tabindex="0"><code>which ruby
echo 'eval &quot;$(chef shell-init bash)&quot;' &gt;&gt; ~/.bash_profile
. ~/.bash_profile
which ruby
</code></pre><pre tabindex="0"><code>cd ~
chef generate repo chef-repo
mkdir -p ~/chef-repo/.chef
cp /etc/chef/admin.pem ~/chef-repo/.chef/
cp /etc/chef/devopstales-validator.pem ~/chef-repo/.chef/
</code></pre><pre tabindex="0"><code>nano ~/chef-repo/.chef/knife.rb
current_dir = File.dirname(__FILE__)
log_level                :info
log_location             STDOUT
node_name                &quot;admin&quot;
client_key               &quot;#{current_dir}/admin.pem&quot;
validation_client_name   &quot;devopstzales-validator&quot;
validation_key           &quot;#{current_dir}/itzgeek-validator.pem&quot;
chef_server_url          &quot;https://cchef.mydomain.intra/organizations/devopstales&quot;
syntax_check_cache_path  &quot;#{ENV['HOME']}/.chef/syntaxcache&quot;
cookbook_path            [&quot;#{current_dir}/../cookbooks&quot;]
</code></pre><p>test kinife client:</p>
<pre tabindex="0"><code>cd ~/chef-repo/
knife ssl fetch
knife client list
</code></pre><h2 id="install-chef-client">Install chef client:</h2>
<p>Before we can bootstrap a chef client on a server we need valid DNS resolution for both.</p>
<pre tabindex="0"><code>knife bootstrap -N test.mydomain.intra test.mydomain.intra -y root -P vagrant
</code></pre><hr>
<ul>
<li><a href="https://downloads.chef.io/chef-server/stable/">https://downloads.chef.io/chef-server/stable/</a></li>
<li><a href="https://downloads.chef.io/chefdk/stable/">https://downloads.chef.io/chefdk/stable/</a></li>
</ul>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to install kubernetes with kubeadm in HA mode]]></title>
            <link href="https://devopstales.github.io/home/k8s-kubeadm-ha/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install kubernetes with kubeadm" />
                <link href="https://devopstales.github.io/kubernetes/k8s-kubeadm-ha/?utm_source=atom_feed" rel="related" type="text/html" title="How to install kubernetes with kubeadm in HA mode" />
                <link href="https://devopstales.github.io/home/k8s-local-pv/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
                <link href="https://devopstales.github.io/home/k8s-velero-backup/?utm_source=atom_feed" rel="related" type="text/html" title="Backup your Kubernetes Cluster with Velero" />
                <link href="https://devopstales.github.io/home/k8s-cert-manager/?utm_source=atom_feed" rel="related" type="text/html" title="Install cert-manager for Kubernetes" />
            
                <id>https://devopstales.github.io/home/k8s-kubeadm-ha/</id>
            
            
            <published>2020-04-02T00:00:00+00:00</published>
            <updated>2020-04-02T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I&rsquo;will show you how to install kubernetes in HA mode with kubeadm, keepaliwed and envoyproxy.</p>
<H3>Parst of the Kubernetes series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
     <li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
     <li>Part1c: <a href="../../kubernetes/k8s-install-containerd/">Install K8S with containerd and kubeadm</a></li>
     <li>Part1d: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
     <li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a></li>
<!--      <li>Part3b: k8s-multiple-nginx-ingress -->
<!--      <li>Part3c: k8s-contour-ingress -->
     <li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a></li>
<!-- local folder (+ autoprovisioning ) -->
     <li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
     <li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
     <li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
     <li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
     <li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
     <li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a></li>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... -->
     <li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
     <li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
     <li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
     <li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
     <li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
     <li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
     <li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a></li>
<!-- istio-install.md -->
     <li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a></li>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog -->
</ul>


<pre tabindex="0"><code>172.17.8.100  # kubernetes cluster ip
172.17.8.101  master01 # master node
172.17.8.102  master02 # frontend node
172.17.8.103  master03 # worker node

# hardware requirement
2 CPU
4G RAM
</code></pre><h3 id="install-docker">Install Docker</h3>
<pre tabindex="0"><code>yum install -y -q yum-utils device-mapper-persistent-data lvm2
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum install -y -q docker-ce docker-compose

mkdir /etc/docker
echo '{
  &quot;log-driver&quot;: &quot;json-file&quot;,
  &quot;log-opts&quot;: {
    &quot;max-size&quot;: &quot;100m&quot;
  },
  &quot;storage-driver&quot;: &quot;overlay2&quot;,
  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],
  &quot;storage-opts&quot;: [
    &quot;overlay2.override_kernel_check=true&quot;
  ]
}' &gt; /etc/docker/daemon.json

systemctl enable docker
systemctl start docker
</code></pre><h3 id="disable-swap">Disable swap</h3>
<pre tabindex="0"><code>free -h
swapoff -a
swapoff -a
sed -i.bak -r 's/(.+ swap .+)/#\1/' /etc/fstab
free -h
</code></pre><h3 id="configuuration">Configuuration</h3>
<pre tabindex="0"><code>cat &gt;&gt;/etc/sysctl.d/kubernetes.conf&lt;&lt;EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.ipv6.conf.all.disable_ipv6      = 1
net.ipv6.conf.default.disable_ipv6  = 1
EOF
cat &gt;&gt;/etc/sysctl.d/ipv6.conf&lt;&lt;EOF
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1
net.ipv6.conf.eth0.disable_ipv6 = 1
EOF
sysctl --system
</code></pre><h3 id="install-kubeadm">Install kubeadm</h3>
<pre tabindex="0"><code>cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
        https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF


yum install epel-release -y
yum install -y kubeadm kubelet kubectl keepalived
</code></pre><h3 id="configure-keepalived-on-first-master">Configure keepalived on first master</h3>
<pre tabindex="0"><code>touch /etc/keepalived/check_apiserver.sh
chmod +x /etc/keepalived/check_apiserver.sh

cat &lt;&lt;EOF &gt; /etc/keepalived/keepalived.conf
! Configuration File for keepalived
global_defs {
    router_id k8s-node1
    enable_script_security
    script_user root
}
vrrp_script check_apiserver {
    script &quot;/etc/keepalived/check_apiserver.sh&quot;
    interval 5
    weight -10
    fall 2
    rise 2
}
vrrp_instance VI_1 {
    state BACKUP
    interface enp0s8
    mcast_src_ip 172.17.8.101
    virtual_router_id 51
    priority 150
    advert_int 5
    authentication {
        auth_type PASS
        auth_pass Password1
    }
    virtual_ipaddress {
        172.17.8.100/24 brd 172.17.8.255 dev enp0s8
    }
    track_script {
       check_apiserver
    }
}
EOF

systemctl start keepalived
systemctl enable keepalived
</code></pre><h3 id="configure-envoy-on-first-master">Configure envoy on first master</h3>
<pre tabindex="0"><code>cat &lt;&lt;EOF &gt; /etc/kubernetes/envoy.yaml
static_resources:
  listeners:
  - name: main
    address:
      socket_address:
        address: 0.0.0.0
        port_value: 16443
    filter_chains:
    - filters:
      - name: envoy.tcp_proxy
        config:
          stat_prefix: ingress_tcp
          cluster: k8s

  clusters:
  - name: k8s
    connect_timeout: 0.25s
    type: strict_dns
    lb_policy: round_robin
    hosts:
    - socket_address:
        address: 172.17.8.101
        port_value: 6443
    - socket_address:
        address: 172.17.8.102
        port_value: 6443
    - socket_address:
        address: 172.17.8.103
        port_value: 6443
    health_checks:
    - timeout: 1s
      interval: 5s
      unhealthy_threshold: 1
      healthy_threshold: 1
      http_health_check:
        path: &quot;/healthz&quot;

admin:
  access_log_path: &quot;/dev/null&quot;
  address:
    socket_address:
      address: 0.0.0.0
      port_value: 8001
EOF
</code></pre><h3 id="create-loadbalancer-on-all-masters">Create loadbalancer on all masters</h3>
<pre tabindex="0"><code>
cat&lt;&lt;EOF &gt; /etc/kubernetes/envoy.yaml
static_resources:
  listeners:
  - name: main
    address:
      socket_address:
        address: 0.0.0.0
        port_value: 16443
    filter_chains:
    - filters:
      - name: envoy.tcp_proxy
        config:
          stat_prefix: ingress_tcp
          cluster: k8s

  clusters:
  - name: k8s
    connect_timeout: 0.25s
    type: strict_dns # static
    lb_policy: round_robin
    hosts:
    - socket_address:
        address: 172.17.8.101
        port_value: 6443
    - socket_address:
        address: 172.17.8.102
        port_value: 6443
    - socket_address:
        address: 172.17.8.103
        port_value: 6443
    health_checks:
    - timeout: 1s
      interval: 5s
      unhealthy_threshold: 1
      healthy_threshold: 1
      http_health_check:
        path: &quot;/healthz&quot;

admin:
  access_log_path: &quot;/dev/null&quot;
  address:
    socket_address:
      address: 0.0.0.0
      port_value: 8001
EOF

cat&lt;&lt;EOF &gt; /etc/kubernetes/docker-compose.yaml
version: '3'
services:
    api-lb:
        image: envoyproxy/envoy:latest
        restart: always
        network_mode: &quot;host&quot;
        ports:
            - 16443:16443
            - 8001:8001
        volumes:
            - /etc/kubernetes/envoy.yaml:/etc/envoy/envoy.yaml
EOF

cd /etc/kubernetes/
docker-compose pull
docker-compose up -d
docker-compose ps

netstat -tulpn | grep 6443
</code></pre><h3 id="initialize-kubernetes-in-the-first-master">Initialize kubernetes in the first master</h3>
<p>I have multiple interfaces in my masters so to use the correct one I need to add <code>--apiserver-advertise-address &quot;172.17.8.101&quot;</code> to my kubeadm commands and add <code>KUBELET_EXTRA_ARGS</code> for kubelet config.</p>
<pre tabindex="0"><code>echo 'KUBELET_EXTRA_ARGS=&quot;--node-ip=172.17.8.101&quot;' &gt; /etc/sysconfig/kubelet

kubeadm config images pull --kubernetes-version 1.16.8
kubeadm init --control-plane-endpoint &quot;172.17.8.100:16443&quot; --apiserver-advertise-address &quot;172.17.8.101&quot; --upload-certs --kubernetes-version 1.16.8 --pod-network-cidr=10.244.0.0/16

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

kubectl get no

kubectl apply -f https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml
</code></pre><h3 id="join-other-masters">Join other masters</h3>
<pre tabindex="0"><code>
kubeadm config images pull --kubernetes-version 1.16.8

echo 'KUBELET_EXTRA_ARGS=&quot;--node-ip=172.17.8.102&quot;' &gt; /etc/sysconfig/kubelet

kubeadm join 172.17.8.100:16443 --token 3vqtop.z2kbok4o0wchu4ed \
  --discovery-token-ca-cert-hash sha256:5840ee4de07bb296e2639669c17df7e3240271a1880115336ebc5b91fb8a3555 \
  --control-plane --certificate-key dc99dc10a0269d1a3edfc2e318a78c6bbebdee8081b460535f699d210cec5dcb \
  --apiserver-advertise-address &quot;172.17.8.102&quot;

echo 'KUBELET_EXTRA_ARGS=&quot;--node-ip=172.17.8.103&quot;' &gt; /etc/sysconfig/kubelet

kubeadm join 172.17.8.100:16443 --token 3vqtop.z2kbok4o0wchu4ed \
  --discovery-token-ca-cert-hash sha256:5840ee4de07bb296e2639669c17df7e3240271a1880115336ebc5b91fb8a3555 \
  --control-plane --certificate-key dc99dc10a0269d1a3edfc2e318a78c6bbebdee8081b460535f699d210cec5dcb \
  --apiserver-advertise-address &quot;172.17.8.103&quot;
</code></pre><h3 id="fix-keepalibed-check-script-on-first-master">Fix keepalibed check script on first master</h3>
<pre tabindex="0"><code>echo '#!/bin/bash

# if check error then repeat check for 12 times, else exit
err=0
for k in $(seq 1 12)
do
    check_code=$(curl -sk https://localhost:16443)
    if [[ $check_code == &quot;&quot; ]]; then
        err=$(expr $err + 1)
        sleep 5
        continue
    else
        err=0
        break
    fi
done

if [[ $err != &quot;0&quot; ]]; then
    # if apiserver is down send SIG=1
    echo 'apiserver error!'
    exit 1
else
    # if apiserver is up send SIG=0
    echo 'apiserver normal!'
    exit 0
fi' &gt; /etc/keepalived/check_apiserver.sh
</code></pre><h3 id="configure-keepalived-on-other-masters">Configure keepalived on other masters</h3>
<pre tabindex="0"><code>echo '#!/bin/bash

# if check error then repeat check for 12 times, else exit
err=0
for k in $(seq 1 12)
do
    check_code=$(curl -sk https://localhost:16443)
    if [[ $check_code == &quot;&quot; ]]; then
        err=$(expr $err + 1)
        sleep 5
        continue
    else
        err=0
        break
    fi
done

if [[ $err != &quot;0&quot; ]]; then
    # if apiserver is down send SIG=1
    echo 'apiserver error!'
    exit 1
else
    # if apiserver is up send SIG=0
    echo 'apiserver normal!'
    exit 0
fi' &gt; /etc/keepalived/check_apiserver.sh

chmod +x /etc/keepalived/check_apiserver.sh
</code></pre><pre tabindex="0"><code>cat &lt;&lt;EOF &gt; /etc/keepalived/keepalived.conf
! Configuration File for keepalived
global_defs {
    router_id k8s-node2
    enable_script_security
    script_user root
}
vrrp_script check_apiserver {
    script &quot;/etc/keepalived/check_apiserver.sh&quot;
    interval 5
    weight -10
    fall 2
    rise 2
}
vrrp_instance VI_1 {
    state BACKUP
    interface enp0s8
    mcast_src_ip 172.17.8.102
    virtual_router_id 51
    priority 100
    advert_int 5
    authentication {
        auth_type PASS
        auth_pass Password1
    }
    virtual_ipaddress {
        172.17.8.100/24 brd 172.17.8.255 dev enp0s8
    }
    track_script {
       check_apiserver
    }
}
EOF

systemctl start keepalived
systemctl enable keepalived
</code></pre><pre tabindex="0"><code>cat &lt;&lt;EOF &gt; /etc/keepalived/keepalived.conf
! Configuration File for keepalived
global_defs {
    router_id k8s-node3
    enable_script_security
    script_user root
}
vrrp_script check_apiserver {
    script &quot;/etc/keepalived/check_apiserver.sh&quot;
    interval 5
    weight -10
    fall 2
    rise 2
}
vrrp_instance VI_1 {
    state BACKUP
    interface enp0s8
    mcast_src_ip 172.17.8.103
    virtual_router_id 51
    priority 50
    advert_int 5
    authentication {
        auth_type PASS
        auth_pass Password1
    }
    virtual_ipaddress {
        172.17.8.100/24 brd 172.17.8.255 dev enp0s8
    }
    track_script {
       check_apiserver
    }
}
EOF

systemctl start keepalived
systemctl enable keepalived
</code></pre><pre tabindex="0"><code>kubectl scale deploy/coredns  --replicas=3 -n kube-system
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[K8s ERROR at kubectl logs]]></title>
            <link href="https://devopstales.github.io/home/k8s-error-at-kubectl-logs/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-error-at-kubectl-logs/?utm_source=atom_feed" rel="related" type="text/html" title="K8s ERROR at kubectl logs" />
                <link href="https://devopstales.github.io/home/k8s-kuberos/?utm_source=atom_feed" rel="related" type="text/html" title="Kubectl authentication with Kuberos" />
                <link href="https://devopstales.github.io/home/helm3-loki/?utm_source=atom_feed" rel="related" type="text/html" title="Install Grafana Loki with Helm3" />
                <link href="https://devopstales.github.io/home/k8s-ceph-storage-with-csi-driver/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD volume with CSI driver" />
                <link href="https://devopstales.github.io/home/grafana-loki/?utm_source=atom_feed" rel="related" type="text/html" title="Grafana Loki" />
            
                <id>https://devopstales.github.io/home/k8s-error-at-kubectl-logs/</id>
            
            
            <published>2020-03-06T00:00:00+00:00</published>
            <updated>2020-03-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I installed a kubernetes cluster in a Vagran environment. First everything was fine, but wen I try to add the command <code>kubectl logs</code> I got this error:</p>
<pre tabindex="0"><code>$ kubectl logs busybox-7c9687585b-12d75
Error from server (NotFound): the server could not find the requested resource ( pods/log busybox-7c9687585b-12d75)
</code></pre><p>and another for port-forward and exec:</p>
<pre tabindex="0"><code>$ kubectl exec -it busybox-7c9687585b-12d75 -- /bin/sh
error: unable to upgrade connection: pod does not exist
</code></pre><p>When I eun with <code>-v=9</code> I hot HTTP Error Code 404 all around. I wondered if the connection with the kubelet is wrong because of the multiple interfaces of the VMs? So I tested it:</p>
<pre tabindex="0"><code>$ kubectl get nodes worker1 -o yaml
apiVersion: v1
kind: Node
...
status:
  addresses:
  - address: 10.0.2.15
...
</code></pre><p>The was the problem, the address is came from the NAT interface of the VM not from the bridged. But why? On the master I explicitly set the <code>--apiserver-advertise-address</code> for the bridged interface&rsquo;s IP. I needed to add an explicit IP address of the bridged interface on the workers too. The problem is there is no option for that in kubeadm.</p>
<p>I looked at the man page of the kubelet and I found the following option:</p>
<pre tabindex="0"><code>--node-ip string    IP address of the node. If set, kubelet will use this IP address for the node
</code></pre><p>That is wat I need to set. So I added a no job to the end of my bootstrap scripts for master and worker nodes too.</p>
<pre tabindex="0"><code>echo KUBELET_EXTRA_ARGS=\&quot;--node-ip=`ip addr show enp0s8 | grep inet | grep -E -o &quot;([0-9]{1,3}[\.]){3}[0-9]{1,3}/&quot; | tr -d '/'`\&quot; &gt; /etc/sysconfig/kubelet
systemctl restart kubelet
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[PhotonOS Basics]]></title>
            <link href="https://devopstales.github.io/home/photon_basics/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/protonos_vagrant_box/?utm_source=atom_feed" rel="related" type="text/html" title="How to create Vagrant box?" />
                <link href="https://devopstales.github.io/cloud/photon_basics/?utm_source=atom_feed" rel="related" type="text/html" title="PhotonOS Basics" />
                <link href="https://devopstales.github.io/cloud/protonos_vagrant_box/?utm_source=atom_feed" rel="related" type="text/html" title="How to create Vagrant box?" />
                <link href="https://devopstales.github.io/home/openshift_4/?utm_source=atom_feed" rel="related" type="text/html" title="OpenShift 4.2 with Red Hat CodeReady Containers" />
                <link href="https://devopstales.github.io/home/ceph_backup_benji/?utm_source=atom_feed" rel="related" type="text/html" title="CEPH backup with Benji" />
            
                <id>https://devopstales.github.io/home/photon_basics/</id>
            
            
            <published>2020-03-04T00:00:00+00:00</published>
            <updated>2020-03-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Project Photon OS is an open source, minimal Linux container host that is optimized for cloud-native applications, cloud platforms, and VMware infrastructure.</p>
<h3 id="star-in-vagrant">Star in Vagrant</h3>
<pre tabindex="0"><code>nano Vagrantfile
Vagrant.configure(&quot;2&quot;) do |config|
  config.vm.box = &quot;devopstales/photon3&quot;
  config.vm.synced_folder &quot;.&quot;, &quot;/vagrant&quot;, type: &quot;virtualbox&quot;

  config.timezone.value = :host
  config.vm.provider &quot;virtualbox&quot; do |vb|
    vb.name = &quot;photon01&quot;
      vb.memory = 4096

      vb.cpus = 2
      vb.linked_clone = true
      vb.customize [&quot;modifyvm&quot;, :id, &quot;--vram&quot;, &quot;8&quot;]
  end

  config.vm.define &quot;pothon01&quot; do |vbox|
    vbox.vm.network :public_network, ip: &quot;192.168.0.112&quot; , bridge: &quot;wlan0&quot; #, adapter: &quot;1&quot;
    vbox.vm.hostname = &quot;pothon01.mydomain.intra&quot;
    vbox.hostsupdater.remove_on_suspend = false
    vbox.vbguest.auto_update = false
  end
end
</code></pre><pre tabindex="0"><code>vagrant up
vagrant ssh
</code></pre><p>When you try to login the server wants you to change the passowrd of the <code>vagrant</code> user. The base password is <code>vagrant</code> as usual.</p>
<pre tabindex="0"><code>vagrant ssh
You are required to change your password immediately (password expired)
Last login: Sun Apr 14 20:30:22 2019 from 10.0.2.2
WARNING: Your password has expired.
You must change your password now and login again!
Changing password for vagrant.
Current password:
New password:
Retype new password:
passwd: password updated successfully
Connection to 127.0.0.1 closed.
</code></pre><p>Vagrant can&rsquo;t configure the ip, hostname and mount the <code>/vagrant</code> fonder.</p>
<h3 id="package-management">Package management</h3>
<p>On Photon OS, <code>tdnf</code> is the default package manager for installing new packages. It is a C implementation of the <code>DNF</code> package manager without Python dependencies. <code>DNF</code> is the next upcoming major version of yum.</p>
<p>Let&rsquo;s install packages for the next steps.</p>
<pre tabindex="0"><code>tdnf install nano awk tar build-essential linux-devel less -y
</code></pre><h3 id="install-virtualbox-guest-additions">Install virtualbox guest additions</h3>
<p><img src="/img/include/photon_base_1.png" alt="Example image"  class="zoomable" /><!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<pre tabindex="0"><code>mount /dev/cdrom /mnt/cdrom
cd /mnt/cdrom
./VBoxLinuxAdditions.run
</code></pre><h3 id="configure-static-ip">Configure static ip</h3>
<p>PhotonOS use systemd-networkd to manage network configurations. systemd-networkd configorations is located under <code>/etc/systemd/network/</code>.</p>
<pre tabindex="0"><code>cat /etc/systemd/network/99-dhcp-en.network
[Match]
Name=e*

[Network]
DHCP=yes
IPv6AcceptRA=no
</code></pre><pre tabindex="0"><code>cat &gt; /etc/systemd/network/20-static-eth1.network &lt;&lt; &quot;EOF&quot;
[Match]
Name=eth1

[Network]
DHCP=no
Address=192.168.0.112/24
Gateway=192.168.0.1
DNS=8.8.8.8
Domains=mydomain.intra
NTP=0.pool.ntp.org
EOF
</code></pre><pre tabindex="0"><code>chmod 644 /etc/systemd/network/20-static-eth1.network
systemctl restart systemd-networkd
systemctl status systemd-networkd -l
</code></pre><h3 id="configure-hostname">Configure hostname</h3>
<pre tabindex="0"><code>hostnamectl set-hostname &quot;pothon01.mydomain.intra&quot;
</code></pre><pre tabindex="0"><code>hostnamectl status
   Static hostname: pothon01.mydomain.intra
         Icon name: computer-vm
           Chassis: vm
        Machine ID: 2c230d2255834f75bff4872bff234df4
           Boot ID: 2708cffec4cf4c6e91053cc11825b590
    Virtualization: oracle
  Operating System: VMware Photon OS/Linux
            Kernel: Linux 4.19.32-3.ph3
      Architecture: x86-64
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to create Vagrant box?]]></title>
            <link href="https://devopstales.github.io/home/protonos_vagrant_box/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/protonos_vagrant_box/?utm_source=atom_feed" rel="related" type="text/html" title="How to create Vagrant box?" />
                <link href="https://devopstales.github.io/home/openshift_4/?utm_source=atom_feed" rel="related" type="text/html" title="OpenShift 4.2 with Red Hat CodeReady Containers" />
                <link href="https://devopstales.github.io/home/ceph_backup_benji/?utm_source=atom_feed" rel="related" type="text/html" title="CEPH backup with Benji" />
                <link href="https://devopstales.github.io/home/networkmanagger-dnsmasq/?utm_source=atom_feed" rel="related" type="text/html" title="Using the NetworkManager’s DNSMasq plugin" />
                <link href="https://devopstales.github.io/home/pgbackrest_backup_to_s3/?utm_source=atom_feed" rel="related" type="text/html" title="pgBackRest Backup to S3" />
            
                <id>https://devopstales.github.io/home/protonos_vagrant_box/</id>
            
            
            <published>2020-03-03T00:00:00+00:00</published>
            <updated>2020-03-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how to create a vagrant box from pothonos ISO.</p>
<h3 id="base-vm">Base VM</h3>
<p>Fist we need to create a virtualbox vm and install the <a href="https://github.com/vmware/photon/wiki/Downloading-Photon-OS">latest PothonOS ISO</a>.</p>
<ul>
<li>Name: Photon-base</li>
<li>Type: Linux</li>
<li>Version: Other Linux (64-bit)</li>
<li>Memory Size: 1024MB</li>
<li>New Virtual Disk: [Type: VDI, Size: 40 GB]</li>
</ul>
<p>Modify the hardware settings of the virtual machine:</p>
<ul>
<li>Disable audio</li>
<li>Disable USB</li>
<li>Ensure Network Adapter 1 is set to NAT</li>
<li>Add this port-forwarding rule:</li>
<li>Name: SSH</li>
<li>Protocol: TCP</li>
<li>Host IP: blank</li>
<li>Host Port: 2222</li>
<li>Guest IP: blank</li>
<li>Guest Port: 22</li>
</ul>
<h3 id="configure-system">Configure System</h3>
<p>Change the root users password to <code>vagrant</code></p>
<pre tabindex="0"><code>passwd
chage -I -1 -m 0 -M 99999 -E -1 root
</code></pre><p>Upgrade system and install packages:</p>
<pre tabindex="0"><code>tdnf upgrade -y
tdnf install sudo nano wget awk tar build-essential linux-devel less -y
</code></pre><h3 id="create-the-vagrant-account">Create the vagrant account</h3>
<p>Next you need to create the default vagrant user account:</p>
<pre tabindex="0"><code>useradd -m -G sudo vagrant
passwd vagrant
chage -I -1 -m 0 -M 99999 -E -1 vagrant

cat &gt; /etc/sudoers.d/vagrant &lt; EOF
# add vagrant user
vagrant ALL=(ALL) NOPASSWD:ALL
EOF
</code></pre><h3 id="change-ssh-config">Change ssh config</h3>
<pre tabindex="0"><code>nano /etc/ssh/sshd_config
AuthorizedKeysFile     %h/.ssh/authorized_keys
</code></pre><pre tabindex="0"><code>systemctl restart sshd
</code></pre><h3 id="add-vagrant-ssh-keys-to-vagrant-user">Add vagrant ssh keys to vagrant user</h3>
<pre tabindex="0"><code>su - vagrant

mkdir -p /home/vagrant/.ssh
chmod 0700 /home/vagrant/.ssh
</code></pre><pre tabindex="0"><code>wget --no-check-certificate \
    https://raw.github.com/mitchellh/vagrant/master/keys/vagrant.pub \
    -O /home/vagrant/.ssh/authorized_keys
</code></pre><pre tabindex="0"><code>chmod 0600 /home/vagrant/.ssh/authorized_keys
chown –R vagrant /home/vagrant/.ssh
</code></pre><h3 id="add-virtualboxadditions">Add VirtualBoxadditions</h3>
<p>Go to your virtualbox menu for the VM and select <code>Devices / Insert Guest Additions CD Image</code></p>
<p><img src="/img/include/photon_base_1.png" alt="Example image"  class="zoomable" /><!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<pre tabindex="0"><code>mount /dev/cdrom /mnt/cdrom
cd /mnt/cdrom
./VBoxLinuxAdditions.run
</code></pre><h3 id="compress-vm">Compress vm</h3>
<pre tabindex="0"><code>sudo dd if=/dev/zero of=/EMPTY bs=1M
sudo rm -f /EMPTY
</code></pre><h3 id="package-box">Package box</h3>
<pre tabindex="0"><code>vagrant package --base &quot;Photon-base&quot;

vagrant box add &lt;user-name&gt;/photon3 package.box
vagrant box add devopstales/photon3 package.box
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[OpenShift 4.2 with Red Hat CodeReady Containers]]></title>
            <link href="https://devopstales.github.io/home/openshift_4/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-secondary-router/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift secondary route" />
                <link href="https://devopstales.github.io/home/openshift-gitlab-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Installing GitLab on OpenShift" />
                <link href="https://devopstales.github.io/home/openshift-kompose/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Run docker-compoe in Openshift" />
                <link href="https://devopstales.github.io/home/openshift-add-node/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Add Nodes to a Cluster" />
                <link href="https://devopstales.github.io/home/openshift-letsencrypt/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Letsencrypt certificates" />
            
                <id>https://devopstales.github.io/home/openshift_4/</id>
            
            
            <published>2020-03-02T00:00:00+00:00</published>
            <updated>2020-03-02T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>The Red Hat CodeReady Containers enables you to run a minimal OpenShift 4.2 or newer cluster on your local laptop or desktop computer.</p>
<h3 id="download-crc">Download CRC</h3>
<pre tabindex="0"><code>wget https://mirror.openshift.com/pub/openshift-v4/clients/crc/latest/crc-linux-amd64.tar.xz
tar xvf crc-linux-amd64.tar.xz
sudo cp crc-linux-*-amd64/crc /usr/local/sbin/
crc version
</code></pre><h3 id="setup-crs">Setup CRS</h3>
<pre tabindex="0"><code>crc setup
</code></pre><pre tabindex="0"><code>INFO Checking if running as non-root
INFO Caching oc binary
INFO Setting up virtualization
INFO Setting up KVM
INFO Installing libvirt service and dependencies
INFO Adding user to libvirt group
INFO Enabling libvirt
INFO Starting libvirt service
INFO Will use root access: start libvirtd service
[sudo] password for devopstales:
INFO Checking if a supported libvirt version is installed
INFO Installing crc-driver-libvirt
INFO Removing older system-wide crc-driver-libvirt
INFO Setting up libvirt 'crc' network
INFO Starting libvirt 'crc' network
INFO Checking if NetworkManager is installed
INFO Checking if NetworkManager service is running
INFO Writing Network Manager config for crc
INFO Will use root access: write NetworkManager config in /etc/NetworkManager/conf.d/crc-nm-dnsmasq.conf
INFO Will use root access: execute systemctl daemon-reload command
INFO Will use root access: execute systemctl stop/start command
INFO Writing dnsmasq config for crc
INFO Will use root access: write dnsmasq configuration in /etc/NetworkManager/dnsmasq.d/crc.conf
INFO Will use root access: execute systemctl daemon-reload command
INFO Will use root access: execute systemctl stop/start command
INFO Unpacking bundle from the CRC binary
Setup is complete, you can now run 'crc start' to start the OpenShift cluster
</code></pre><p>Thec configuration creats two dnsmasq config:</p>
<pre tabindex="0"><code>cat /etc/NetworkManager/conf.d/crc-nm-dnsmasq.conf
[main]
dns=dnsmasq

cat /etc/NetworkManager/dnsmasq.d/crc.conf
server=/apps-crc.testing/192.168.130.11
server=/crc.testing/192.168.130.11
</code></pre><p>The first enable the NetworkManager&rsquo;s dnsmasq plugin to be used as a dns server and the second points two dns zone the <code>*.apps-crc.testing</code> and the <code>*.crc.testing</code> to the ip of the new vm&rsquo;s ip. The crc creats a kvm network for the vm whit this ip range.</p>
<pre tabindex="0"><code>sudo virsh net-list
 Name      State    Autostart   Persistent
--------------------------------------------
 crc       active   yes         yes
 default   active   yes         yes
</code></pre><pre tabindex="0"><code>sudo virsh net-edit
 &lt;network&gt;
  &lt;name&gt;crc&lt;/name&gt;
  &lt;uuid&gt;49eee855-d342-46c3-9ed3-b8d1758814cd&lt;/uuid&gt;
  &lt;forward mode='nat'&gt;
    &lt;nat&gt;
      &lt;port start='1024' end='65535'/&gt;
    &lt;/nat&gt;
  &lt;/forward&gt;
  &lt;bridge name='crc' stp='on' delay='0'/&gt;

  &lt;mac address='52:54:00:fd:be:d0'/&gt;
  &lt;ip family='ipv4' address='192.168.130.1' prefix='24'&gt;
    &lt;dhcp&gt;
      &lt;host mac='52:fd:fc:07:21:82' ip='192.168.130.11'/&gt;
    &lt;/dhcp&gt;
  &lt;/ip&gt;
&lt;/network&gt;
</code></pre><h3 id="start-crs">Start CRS</h3>
<pre tabindex="0"><code>crc start
</code></pre><pre tabindex="0"><code>INFO Checking if running as non-root
INFO Checking if oc binary is cached
INFO Checking if Virtualization is enabled
INFO Checking if KVM is enabled
INFO Checking if libvirt is installed
INFO Checking if user is part of libvirt group
INFO Checking if libvirt is enabled
INFO Checking if libvirt daemon is running
INFO Checking if a supported libvirt version is installed
INFO Checking if crc-driver-libvirt is installed
INFO Checking if libvirt 'crc' network is available
INFO Checking if libvirt 'crc' network is active
INFO Checking if NetworkManager is installed
INFO Checking if NetworkManager service is running
INFO Checking if /etc/NetworkManager/conf.d/crc-nm-dnsmasq.conf exists
INFO Checking if /etc/NetworkManager/dnsmasq.d/crc.conf exists
? Image pull secret [? for help]
</code></pre><p>Please note that a valid OpenShift user pull secret is required during installation. The pull secret can be copied or downloaded from the Pull Secret section of the <a href="https://cloud.redhat.com/openshift/install/crc/installer-provisioned">Install on Laptop: Red Hat CodeReady Containers</a> page on cloud.redhat.com.</p>
<pre tabindex="0"><code>INFO To access the cluster, first set up your environment by following 'crc oc-env' instructions
INFO Then you can access it by running 'oc login -u developer -p developer https://api.crc.testing:6443'
INFO To login as an admin, run 'oc login -u kubeadmin -p 7z6T5-qmTth-oxaoD-p3xQF https://api.crc.testing:6443'
INFO
INFO You can now run 'crc console' and use these credentials to access the OpenShift web console
</code></pre><p>Go to the console:</p>
<pre tabindex="0"><code>crc console
</code></pre><p><img src="/img/include/okd4.png" alt="Example image"  class="zoomable" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[CEPH backup with Benji]]></title>
            <link href="https://devopstales.github.io/home/ceph_backup_benji/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/ceph_backup_benji/?utm_source=atom_feed" rel="related" type="text/html" title="CEPH backup with Benji" />
                <link href="https://devopstales.github.io/home/k8s-ceph-storage-with-csi-driver/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD volume with CSI driver" />
                <link href="https://devopstales.github.io/home/who-mapping-rbd-device/?utm_source=atom_feed" rel="related" type="text/html" title="Ceph: who&#39;s mapping a RBD device" />
                <link href="https://devopstales.github.io/home/k8s-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/install-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ceph cluster" />
            
                <id>https://devopstales.github.io/home/ceph_backup_benji/</id>
            
            
            <published>2020-02-24T00:00:00+00:00</published>
            <updated>2020-02-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this article I will show you how to use benji to backup CEPH RBD incrementally.</p>
<p>Benji Backup is a block based deduplicating backup software. It builds on the excellent foundations and concepts of backy² by Daniel Kraft.</p>
<p>Benji requires <code>Python 3.6.5</code> or newer because older Python versions have some shortcomings in the <code>concurrent.futures</code> implementation which lead to an excessive memory usage. So in this example I will use docker to make easier to implement.</p>
<pre tabindex="0"><code>mkdir -p /opt/benji/config/
mkdir -p /opt/benji/postgresql/data

nano /opt/benji/docker-compose.yaml
version: '3.5'

services:
  benji:
    image: elementalnet/benji-k8s:0.5.0
    restart: always
    volumes:
      - /etc/ceph:/etc/ceph
      - /backup/benji-ceph:/data
      - /opt/benji/config/benji.yaml:/benji/etc/benji.yaml
      - /opt/benji/config/backup-ceph:/benji/scripts/backup-ceph
      - /opt/benji/config/crontab:/benji/etc/crontab

  postgres:
    image: postgres:11
    restart: always
    environment:
      - &quot;POSTGRES_DB=benji-prod&quot;
      - &quot;POSTGRES_USER=benji-prod&quot;
      - &quot;POSTGRES_PASSWORD=Password1&quot;
    volumes:
      - /opt/benji/postgresql/data:/var/lib/postgresql/data
</code></pre><h3 id="create-benji-config">Create benji config</h3>
<pre tabindex="0"><code>nano /opt/benji/config/benji.yaml
configurationVersion: '1'
databaseEngine: postgresql://benji-prod:Password1@postgres:5432/benji-prod
defaultStorage: storage-1
storages:
  - name: storage-1
    storageId: 1
    module: file
    configuration:
      path: /data
ios:
  - name: rbd
    module: rbd
</code></pre><h3 id="create-backup-script">Create backup script</h3>
<pre tabindex="0"><code>nano /opt/benji/config/backup-ceph
#!/bin/bash

. common.sh
. prometheus.sh
. ceph.sh
. hooks.sh

FSFREEZE=no
BENJI_INSTANCE:devopstales
POOL=testpool
EXCLUDE=&quot;test1&quot;

# benji::backup::ceph test/test test test test/test

echo &quot;POOL :&quot; $POOL;
for IMAGE in `rbd ls $POOL | grep -v @ | grep -v $EXCLUDE`;
do
#   echo $POOL/$IMAGE
   benji::backup::ceph $POOL/$IMAGE $POOL $IMAGE $POOL/$IMAGE
done
</code></pre><h3 id="create-crontab">Create crontab</h3>
<pre tabindex="0"><code>nano /opt/benji/config/crontab
BENJI_INSTANCE:devopstales
00 00 * * * root backup-ceph
00 04 * * * root benji-command enforce days8,weeks3,months2
00 05 * * * root benji-command cleanup
30 05 * * * root benji-versions-status
00 06 * * * root benji-command batch-deep-scrub
</code></pre><h3 id="perform-a-backup">Perform a backup</h3>
<pre tabindex="0"><code>cd /opt/benji/
docker-compose up -d
docker exec -it benji_benji_1 bash
benji database-init
benji backup rbd:testpool/test2 devopstales
benji ls
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Using the NetworkManager’s DNSMasq plugin]]></title>
            <link href="https://devopstales.github.io/home/networkmanagger-dnsmasq/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/ceph_backup_benji/?utm_source=atom_feed" rel="related" type="text/html" title="CEPH backup with Benji" />
                <link href="https://devopstales.github.io/linux/networkmanagger-dnsmasq/?utm_source=atom_feed" rel="related" type="text/html" title="Using the NetworkManager’s DNSMasq plugin" />
                <link href="https://devopstales.github.io/home/pgbackrest_backup_to_s3/?utm_source=atom_feed" rel="related" type="text/html" title="pgBackRest Backup to S3" />
                <link href="https://devopstales.github.io/home/pgbackrest_backup_server/?utm_source=atom_feed" rel="related" type="text/html" title="pgBackRest Backup server" />
                <link href="https://devopstales.github.io/home/postgresql_pgbackrest/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL backup with pgBackRest" />
            
                <id>https://devopstales.github.io/home/networkmanagger-dnsmasq/</id>
            
            
            <published>2020-02-24T00:00:00+00:00</published>
            <updated>2020-02-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Imagine you want to test something in a demo setup with 5 machines. You create the necessary VMs in your local environment – but you cannot address them properly by name. With 5 machines you also need to write down the appropriate IP addresses – that’s hardly practical.</p>
<p>Luckily, there is an elegant solution: The dnsmasq plugin is a hidden gem of NetworkManager.</p>
<h3 id="enable-networkmanagers-dnsmasq">Enable NetworkManager&rsquo;s dnsmasq</h3>
<pre tabindex="0"><code># /etc/NetworkManager/conf.d/00-use-dnsmasq.conf
#
# This enabled the dnsmasq plugin.
[main]
dns=dnsmasq
</code></pre><pre tabindex="0"><code># /etc/NetworkManager/dnsmasq.d/00-homelab.conf
# This file sets up the local lablab domain and
# defines some aliases and a wildcard.
local=/homelab/

# The below defines a Wildcard DNS Entry.
address=/.ose.homelab/192.168.101.125

# Below I define some host names.  I also pull in   
address=/openshift.homelab/192.168.101.120
address=/openshift-int.homelab/192.168.101.120
</code></pre><pre tabindex="0"><code># /etc/NetworkManager/dnsmasq.d/02-add-hosts.conf
# By default, the plugin does not read from /etc/hosts.  
# This forces the plugin to slurp in the file.
#
# If you didn't want to write to the /etc/hosts file.  This could
# be pointed to another file.
#
addn-hosts=/etc/hosts
</code></pre><p>Restart your network managger <code>systemctl restart NetworkManager</code>. If everything is working right, you should see that your resolv.conf points to 127.0.0.1 and a new dnsmasq process spawned.</p>
<pre tabindex="0"><code>cat /etc/resolv.conf
# Generated by NetworkManager
nameserver 127.0.0.1
</code></pre><h3 id="configurate-networkmanager-fo-libvirts-domain">Configurate NetworkManager fo libvirt&rsquo;s domain</h3>
<p>Libvirt comes with its own in-build DNS server, dnsmasq to serve DHCP and DNS to servers for vms.
Additionally, NetworkManager can be configured to use its dnsmasq plugin to forwarding DNS requests to the libvirt instance if needed.</p>
<pre tabindex="0"><code># /etc/NetworkManager/dnsmasq.d/01-libvirt_dnsmasq.conf
server=/qxyz.intra/192.168.122.1
</code></pre><h3 id="configuring-libvirt">Configuring libvirt</h3>
<p>First of all, libvirt needs to be configured. Given that the network &ldquo;default&rdquo; is assigned to the relevant VMs, the configuration should look like this:</p>
<pre tabindex="0"><code>sudo virsh net-edit default
&lt;network connections='1'&gt;
  &lt;name&gt;default&lt;/name&gt;
  &lt;uuid&gt;158880c3-9adb-4a44-ab51-d0bc1c18cddc&lt;/uuid&gt;
  &lt;forward mode='nat'&gt;
    &lt;nat&gt;
      &lt;port start='1024' end='65535'/&gt;
    &lt;/nat&gt;
  &lt;/forward&gt;
  &lt;bridge name='virbr0' stp='on' delay='0'/&gt;
  &lt;mac address='52:54:00:fa:cb:e5'/&gt;
  &lt;domain name='qxyz.de' localOnly='yes'/&gt;
  &lt;ip address='192.168.122.1' netmask='255.255.255.0'&gt;
    &lt;dhcp&gt;
      &lt;range start='192.168.122.128' end='192.168.122.254'/&gt;
    &lt;/dhcp&gt;
  &lt;/ip&gt;
&lt;/network&gt;
</code></pre><h3 id="configuring-the-vm-guests">Configuring the VM guests</h3>
<pre tabindex="0"><code>sudo hostnamectl set-hostname neon.qxyz.intra
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[pgBackRest Backup to S3]]></title>
            <link href="https://devopstales.github.io/home/pgbackrest_backup_to_s3/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/pgbackrest_backup_server/?utm_source=atom_feed" rel="related" type="text/html" title="pgBackRest Backup server" />
                <link href="https://devopstales.github.io/home/postgresql_pgbackrest/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL backup with pgBackRest" />
                <link href="https://devopstales.github.io/linux/pgbackrest_backup_to_s3/?utm_source=atom_feed" rel="related" type="text/html" title="pgBackRest Backup to S3" />
                <link href="https://devopstales.github.io/linux/pgbackrest_backup_server/?utm_source=atom_feed" rel="related" type="text/html" title="pgBackRest Backup server" />
                <link href="https://devopstales.github.io/linux/postgresql_pgbackrest/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL backup with pgBackRest" />
            
                <id>https://devopstales.github.io/home/pgbackrest_backup_to_s3/</id>
            
            
            <published>2020-02-21T00:00:00+00:00</published>
            <updated>2020-02-21T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this article I will show you how to use pgBackRest to backup PostgreSQL servers to S3 buckets.</p>
<p>For the purpose of this demo setup, we’ll use MinIO server, which is an Amazon S3 Compatible Object Storage.</p>
<h3 id="s3cmd">s3cmd</h3>
<pre tabindex="0"><code>yum install -y epel-release
yum --enablerepo epel-testing install -y s3cmd
</code></pre><pre tabindex="0"><code>nano ~/.s3cfg
host_base = minio.mydomain.intra:9000
host_bucket = minio.mydomain.intra:9000
bucket_location = us-east-1
use_https = false
access_key = &lt;minop_access_key&gt;
secret_key = &lt;minio_secret_key&gt;
signature_v2 = False
</code></pre><p>Create bucket and folders</p>
<pre tabindex="0"><code>s3cmd mb --no-check-certificate s3://pgbackrest
mkdir postgresql12
s3cmd cp postgresql1 --no-check-certificate s3://pgbackrest
s3cmd ls --no-check-certificate s3://pgbackrest/postgresql12
                       DIR   s3://pgbackrest/postgresql12/
</code></pre><h3 id="installation">Installation</h3>
<pre tabindex="0"><code>yum install epel-release -y
yum update -y
rpm -Uvh https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-centos10-10-2.noarch.rpm
yum install -y pgbackrest
</code></pre><h3 id="configure-pgbackrest">Configure pgBackRest</h3>
<pre tabindex="0"><code>nano /etc/pgbackrest.conf

[global]
repo1-path=/postgresql12
repo1-type=s3
repo1-s3-endpoint=minio.mydomain.intra
repo1-s3-port=9000
repo1-s3-bucket=pgbackrest
repo1-s3-verify-tls=n
repo1-s3-key=&lt;minop_access_key&gt;
repo1-s3-key-secret=&lt;minio_secret_key&gt;
repo1-s3-region=us-east-1

repo1-retention-full=1
process-max=2
log-level-console=info
log-level-file=debug
start-fast=y
delta=y

[postgresql12]
pg1-path=/var/lib/pgsql/12/data
</code></pre><h3 id="configure-postgresql-on-postgresql12-server">Configure PostgreSQL on postgresql12 server</h3>
<pre tabindex="0"><code>nano /var/lib/pgsql/12/data/postgresql.conf
...
archive_mode = on
archive_command = 'pgbackrest --stanza=postgresql12 archive-push %p'
...

systemctl restart postgresql12
</code></pre><h3 id="perform-a-backup">Perform a backup</h3>
<p>Before we can start a backup we need to initialize the backup repository.</p>
<pre tabindex="0"><code>sudo -iu postgres pgbackrest --stanza=postgresql12 stanza-create
sudo -iu postgres pgbackrest --stanza=postgresql12 check
</code></pre><pre tabindex="0"><code>sudo -iu postgres pgbackrest --stanza=postgresql12 --type=full backup
</code></pre><h3 id="show-backup-information">Show backup information</h3>
<pre tabindex="0"><code>$ sudo -iu postgres pgbackrest info
stanza: postgresql12
    status: ok
    cipher: none

    db (current)
        wal archive min/max (11-1): 000000010000000000000005/000000010000000000000005

        full backup: 20200219-091209F
            timestamp start/stop: 2020-02-19 09:12:09 / 2020-02-19 09:12:21
            wal start/stop: 000000010000000000000005 / 000000010000000000000005
            database size: 23.5MB, backup size: 23.5MB
            repository size: 2.8MB, repository backup size: 2.8MB
</code></pre><h3 id="restore-a-backup">Restore a backup</h3>
<p>The restore command can then be used on the postgresql host.</p>
<pre tabindex="0"><code>sudo systemctl stop postgresql12

sudo -iu postgres pgbackrest --stanza=postgresql12 restore
</code></pre><p>Restore only test database:</p>
<pre tabindex="0"><code>sudo -iu postgres pgbackrest --stanza=postgresql12 --delta --db-include=test restore
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[pgBackRest Backup server]]></title>
            <link href="https://devopstales.github.io/home/pgbackrest_backup_server/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/postgresql_pgbackrest/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL backup with pgBackRest" />
                <link href="https://devopstales.github.io/linux/pgbackrest_backup_server/?utm_source=atom_feed" rel="related" type="text/html" title="pgBackRest Backup server" />
                <link href="https://devopstales.github.io/linux/postgresql_pgbackrest/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL backup with pgBackRest" />
                <link href="https://devopstales.github.io/home/postgresql_replication/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL Streamin replication" />
                <link href="https://devopstales.github.io/home/install-postgresql/?utm_source=atom_feed" rel="related" type="text/html" title="Install PostgreSQL" />
            
                <id>https://devopstales.github.io/home/pgbackrest_backup_server/</id>
            
            
            <published>2020-02-20T00:00:00+00:00</published>
            <updated>2020-02-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this article I will show you how to use pgBackRest in a dedicated backup server to backup remote PostgreSQL servers.</p>
<h3 id="installation">Installation</h3>
<pre tabindex="0"><code>yum install epel-release -y
yum update -y
rpm -Uvh https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-centos10-10-2.noarch.rpm
yum install -y pgbackrest
</code></pre><h3 id="allow-passwordles-ssh-for-hosts">Allow passwordles ssh for hosts</h3>
<p>Generate ssh key on all hosts and copy ssh keys</p>
<pre tabindex="0"><code>sudo -u postgres ssh-keygen -f /var/lib/pgsql/.ssh/id_rsa
sudo -u postgres restorecon -R /var/lib/pgsql/.ssh
</code></pre><pre tabindex="0"><code>sudo -u postgres ssh-copy-id -i /var/lib/pgsql/.ssh/id_rsa postgres@backup-server

sudo -u postgres ssh-copy-id -i /var/lib/pgsql/.ssh/id_rsa postgres@postgresql1
</code></pre><h3 id="configure-pgbackrest-on-postgresql12-server">Configure pgBackRest on postgresql12 server</h3>
<pre tabindex="0"><code>nano /etc/pgbackrest.conf

[global]
repo1-host=backup-server
repo1-host-user=postgres
process-max=2
log-level-console=info
log-level-file=debug

[postgresql12]
pg1-path=/var/lib/pgsql/12/data
</code></pre><h3 id="configure-postgresql-on-postgresql12-server">Configure PostgreSQL on postgresql12 server</h3>
<pre tabindex="0"><code>nano /var/lib/pgsql/12/data/postgresql.conf
...
archive_mode = on
archive_command = 'pgbackrest --stanza=postgresql1 archive-push %p'
...

systemctl restart postgresql12
</code></pre><h3 id="configure-pgbackrest-on-backup-server">Configure pgBackRest on backup-server</h3>
<pre tabindex="0"><code>nano /etc/pgbackrest.conf

[global]
repo1-path=/var/lib/pgbackrest
repo1-retention-full=1
process-max=2
log-level-console=info
log-level-file=debug
start-fast=y
stop-auto=y

[postgresql12]
pg1-path=/var/lib/pgsql/12/data
pg1-host=postgresql1
pg1-host-user=postgres
</code></pre><h3 id="perform-a-backup">Perform a backup</h3>
<p>Before we can start a backup we need to initialize the backup repository on backup-server.</p>
<pre tabindex="0"><code>sudo -iu postgres pgbackrest --stanza=postgresql12 stanza-create
</code></pre><p>Finally, check the configuration on all hosts.</p>
<pre tabindex="0"><code>sudo -iu postgres pgbackrest --stanza=postgresql12 check
</code></pre><pre tabindex="0"><code>sudo -iu postgres pgbackrest --stanza=postgresql12 --type=full backup
</code></pre><h3 id="show-backup-information">Show backup information</h3>
<pre tabindex="0"><code>$ sudo -iu postgres pgbackrest info
stanza: postgresql12
    status: ok
    cipher: none

    db (current)
        wal archive min/max (11-1): 000000010000000000000005/000000010000000000000005

        full backup: 20200219-091209F
            timestamp start/stop: 2020-02-19 09:12:09 / 2020-02-19 09:12:21
            wal start/stop: 000000010000000000000005 / 000000010000000000000005
            database size: 23.5MB, backup size: 23.5MB
            repository size: 2.8MB, repository backup size: 2.8MB
</code></pre><h3 id="restore-a-backup">Restore a backup</h3>
<p>The restore command can then be used on the postgresql host.</p>
<pre tabindex="0"><code>sudo systemctl stop postgresql12

sudo -iu postgres pgbackrest --stanza=postgresql12 restore
</code></pre><p>Restore only test database:</p>
<pre tabindex="0"><code>sudo -iu postgres pgbackrest --stanza=postgresql12 --delta --db-include=test restore
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[PostgreSQL backup with pgBackRest]]></title>
            <link href="https://devopstales.github.io/home/postgresql_pgbackrest/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/postgresql_pgbackrest/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL backup with pgBackRest" />
                <link href="https://devopstales.github.io/home/postgresql_replication/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL Streamin replication" />
                <link href="https://devopstales.github.io/home/install-postgresql/?utm_source=atom_feed" rel="related" type="text/html" title="Install PostgreSQL" />
                <link href="https://devopstales.github.io/home/postgresql_pg_rewind/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL: pg_rewind" />
                <link href="https://devopstales.github.io/home/alerta-on-centos8/?utm_source=atom_feed" rel="related" type="text/html" title="Install alerta on Centos8" />
            
                <id>https://devopstales.github.io/home/postgresql_pgbackrest/</id>
            
            
            <published>2020-02-19T00:00:00+00:00</published>
            <updated>2020-02-19T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this article I will show you how to use pgBackRest as a backup program for PostgreSQL.</p>
<p>pgBackRest aims to be a simple, reliable backup and restore solution that can seamlessly scale up to the largest databases and workloads by utilizing algorithms that are optimized for database-specific requirements.</p>
<blockquote>
<p>A backup is a consistent copy of a database cluster that can be restored to recover from a hardware failure, to perform Point-In-Time Recovery, or to bring up a new standby. For individual database backups, a tool such as <code>pg_dump</code> must be used.</p>
</blockquote>
<h3 id="installation">Installation</h3>
<pre tabindex="0"><code>yum install epel-release -y
yum update -y
rpm -Uvh https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-centos10-10-2.noarch.rpm
yum install -y pgbackrest
</code></pre><h3 id="configure-pgbackrest">Configure pgBackRest</h3>
<pre tabindex="0"><code>cp /etc/pgbackrest.conf /etc/pgbackrest.conf.bck

nano /etc/pgbackrest.conf
[global]
repo1-path=/var/lib/pgsql/12/backups
log-level-console=info
log-level-file=debug
start-fast=y

[backup_stanza]
pg1-path=/var/lib/pgsql/12/data
repo1-retention-full=1
</code></pre><p>The configuration is based on two section the <code>global</code> what is specify the repository where you stores the backups and WAL segments archives and a <code>stanza</code>, in my case <code>backup_stanza</code>.</p>
<p>A stanza defines the backup configuration for a specific PostgreSQL database cluster. The stanza section must define the database cluster path and host/user if the database cluster is remote. Also, any global configuration sections can be overridden to define stanza-specific settings.</p>
<h3 id="configure-postgresql">Configure PostgreSQL</h3>
<pre tabindex="0"><code>nano /var/lib/pgsql/12/data/postgresql.conf
...
archive_mode = on
archive_command = 'pgbackrest --stanza=backup_stanza archive-push %p'
...

systemctl restart postgresql12
</code></pre><h3 id="perform-a-backup">Perform a backup</h3>
<p>Before we can start a backup we need to initialize the backup repository.</p>
<pre tabindex="0"><code>$ sudo -iu postgres pgbackrest --stanza=backup_stanza stanza-create
P00   INFO: stanza-create command begin 2.23: ...
P00   INFO: stanza-create command end: completed successfully

$ sudo -iu postgres pgbackrest --stanza=backup_stanza check
P00   INFO: check command begin 2.23: ...
P00   INFO: WAL segment ... successfully stored in the archive at ...
P00   INFO: check command end: completed successfully
</code></pre><pre tabindex="0"><code>$ sudo -iu postgres pgbackrest --stanza=backup_stanza --type=full backup
P00   INFO: backup command begin 2.23: ...
P00   INFO: execute non-exclusive pg_start_backup() with label
        &quot;pgBackRest backup started at ...&quot;: backup begins after the requested immediate checkpoint completes
P00   INFO: backup start archive = 000000010000000000000005, lsn = 0/5000028
P00   INFO: full backup size = 23.5MB
P00   INFO: execute non-exclusive pg_stop_backup() and wait for all WAL segments to archive
P00   INFO: backup stop archive = 000000010000000000000005, lsn = 0/5000130
P00   INFO: new backup label = 20200219-091209F
P00   INFO: backup command end: completed successfully
P00   INFO: expire command begin
P00   INFO: expire full backup 20200219-090152F
P00   INFO: remove expired backup 20200219-090152F
P00   INFO: expire command end: completed successfully
</code></pre><h3 id="show-backup-information">Show backup information</h3>
<pre tabindex="0"><code>$ sudo -iu postgres pgbackrest info
stanza: backup_stanza
    status: ok
    cipher: none

    db (current)
        wal archive min/max (11-1): 000000010000000000000005/000000010000000000000005

        full backup: 20200219-091209F
            timestamp start/stop: 2020-02-19 09:12:09 / 2020-02-19 09:12:21
            wal start/stop: 000000010000000000000005 / 000000010000000000000005
            database size: 23.5MB, backup size: 23.5MB
            repository size: 2.8MB, repository backup size: 2.8MB
</code></pre><h3 id="restore-a-backup">Restore a backup</h3>
<pre tabindex="0"><code>sudo systemctl stop postgresql12

sudo -iu postgres pgbackrest --stanza=backup_stanza restore
P00   INFO: restore command begin 2.15: ...
P00   INFO: restore backup set 20200219-091209F
P00   INFO: write /var/lib/pgsql/11/data/recovery.conf
P00   INFO: restore global/pg_control (performed last to ensure aborted restores cannot be started)
P00   INFO: restore command end: completed successfully
</code></pre><p>Restore only test database:</p>
<pre tabindex="0"><code># restor to the lastert backup
sudo -iu postgres pgbackrest --stanza=backup_stanza --delta --db-include=test restore
# restore to a specific verion
sudo -Hiu postgres pgbackrest --stanza=backup_stanza --delta --db-include=test --set=20200219-091209F restore
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Free sso for Mattermost Teams Edition]]></title>
            <link href="https://devopstales.github.io/home/mattermost-keycloak-sso/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/sso/mattermost-keycloak-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Free sso for Mattermost Teams Edition" />
                <link href="https://devopstales.github.io/home/k8s-kuberos/?utm_source=atom_feed" rel="related" type="text/html" title="Kubectl authentication with Kuberos" />
                <link href="https://devopstales.github.io/home/k8s-gangway/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak and gangway" />
                <link href="https://devopstales.github.io/home/k8s-dasboard-auth/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak and gatekeeper" />
                <link href="https://devopstales.github.io/home/grafana-sso/?utm_source=atom_feed" rel="related" type="text/html" title="SSO login to Grafana" />
            
                <id>https://devopstales.github.io/home/mattermost-keycloak-sso/</id>
            
            
            <published>2020-02-16T00:00:00+00:00</published>
            <updated>2020-02-16T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this article I will show you how to use Keycloak as an authentication provider for  Mattermost Teams Edition.</p>
<p>Slack and Mattermost are very similar products. With Mattermost Teams Edition you can get almost all the functionality of the payed Slack. The only problem is sso. In Mattermost you can use google sso login in the E20 licensing what is more costly the Slack. (Slack: 8$/user Mattermost E20: 8.5$/user) In  Mattermost Teams Edition (the free edition of Mattermost) the only authentication provider you can use is gitlab, but thanks to <a href="https://qiita.com/wadahiro/items/8b118c34aae904353865">wadahiro</a> we can use Keycloak instead of gitlab.</p>
<h3 id="configurate-mattermost">Configurate MAttermost</h3>
<pre tabindex="0"><code>nano /etc/mattermost/config.json
...
&quot;GitLabSettings&quot;: {
    &quot;Enable&quot;: false,
    &quot;Secret&quot;: &quot;&lt;secret&gt;&quot;,
    &quot;Id&quot;: &quot;mattermost&quot;,
    &quot;Scope&quot;: &quot;&quot;,
    &quot;AuthEndpoint&quot;: &quot;https://keycloak.mydomain.intra/auth/realms/mydomain/protocol/openid-connect/auth&quot;,
    &quot;TokenEndpoint&quot;: &quot;https://keycloak.mydomain.intra/auth/realms/mydomain/protocol/openid-connect/token&quot;,
    &quot;UserApiEndpoint&quot;: &quot;https://keycloak.mydomain.intra/auth/realms/mydomain/protocol/openid-connect/userinfo&quot;
},
</code></pre><h3 id="create-client-on-keycloak">Create Client on Keycloak</h3>
<p>In keycloak go to <code>Configure &gt; Clients</code> and create a new client for Mattermostw With the fallowing data:</p>
<ul>
<li>Standard Flow Enabled: <code>ON</code></li>
<li>Access Type: <code>confidential</code></li>
<li>Valid Redirect URIs: <code>http://&lt;Mattermost-FQDN&gt;/signup/gitlab/complete</code></li>
</ul>
<p><img src="/img/include/mattermost_sso1.png" alt="image"  class="zoomable" /> <!-- raw HTML omitted --></p>
<h3 id="create-mapper-for-correct-data">Create mapper for correct data</h3>
<p>Mattermost <a href="https://github.com/mattermost/mattermost-server/blob/v4.10.0/model/gitlab/gitlab.go#L19-L25">want</a> the fallowing data from the authentication provider:</p>
<pre tabindex="0"><code>type GitLabUser struct {
	Id       int64  `json:&quot;id&quot;`
	Username string `json:&quot;username&quot;`
	Login    string `json:&quot;login&quot;`
	Email    string `json:&quot;email&quot;`
	Name     string `json:&quot;name&quot;`
}
</code></pre><p>Create mapping for username:</p>
<p><img src="/img/include/mattermost_sso2.png" alt="image"  class="zoomable" /> <!-- raw HTML omitted --></p>
<p>The only problematic data is the ID. For the test run you can use a self generated id like this:</p>
<p><img src="/img/include/mattermost_sso3.png" alt="image"  class="zoomable" /> <!-- raw HTML omitted --></p>
<h3 id="create-user-for-test">Create user for test</h3>
<p>For testing purposes we’re going to create a local user in Keycloak.</p>
<p><img src="/img/include/mattermost_sso4.png" alt="image"  class="zoomable" /> <!-- raw HTML omitted --></p>
<p>Now we need to create an attribute for this user with the mattermost id and the value should be an integer between 1 and 9999999999999999999.</p>
<p><img src="/img/include/mattermost_sso5.png" alt="image"  class="zoomable" /> <!-- raw HTML omitted --></p>
<p>If you have many users you didn&rsquo;t want to create id for them manually. If you use LDAP for users the best fit for this need was the employeeNumber or EmployeeId ldap attribute.</p>
<p><img src="/img/include/mattermost_sso6.png" alt="image"  class="zoomable" /> <!-- raw HTML omitted --></p>
<p>If you use ActiveDirectory this attribute maybe null so we need to generate it with a powershell script.</p>
<pre tabindex="0"><code>ForEach ($User in ((Get-ADUser -Filter * -Properties SamAccountName,EmployeeId)))
{
if ( ([string]::IsNullOrEmpty($User.EmployeeId)))
{
$DATE = (Get-ADuser $User.SamAccountName -Properties whencreated).whencreated.ToString('yyMMddHHmmss')
$RANDOM = (Get-Random -Maximum 9999)
$DATA = -join ($DATE, $RANDOM)
$User.SamAccountName
$DATA
Set-ADUser $User.SamAccountName -employeeID $DATA
}
}
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[PostgreSQL: pg_rewind]]></title>
            <link href="https://devopstales.github.io/home/postgresql_pg_rewind/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/postgresql_replication/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL Streamin replication" />
                <link href="https://devopstales.github.io/linux/postgresql_pg_rewind/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL: pg_rewind" />
                <link href="https://devopstales.github.io/home/install-postgresql/?utm_source=atom_feed" rel="related" type="text/html" title="Install PostgreSQL" />
                <link href="https://devopstales.github.io/linux/postgresql_replication/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL Streamin replication" />
                <link href="https://devopstales.github.io/home/k8s-longhorn/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes project longhorn" />
            
                <id>https://devopstales.github.io/home/postgresql_pg_rewind/</id>
            
            
            <published>2020-02-05T00:00:00+00:00</published>
            <updated>2020-02-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this pos I will show you how to perform a rewind on a broken Streamin replication.</p>
<h3 id="create-test-db">create test db</h3>
<pre tabindex="0"><code># master:
sudo -iu postgres psql -c &quot;create database test1;&quot;
</code></pre><h3 id="break-the-replication">break the replication</h3>
<pre tabindex="0"><code># slaeve:
sudo -u postgres /usr/pgsql-12/bin/pg_ctl stop -m fast -D /var/lib/pgsql/12/data/
</code></pre><p>generate data in master</p>
<pre tabindex="0"><code># master:
sudo -iu postgres /usr/pgsql-12/bin/pgbench -i -s 100 -d test1
</code></pre><h3 id="perform-the-rewind">perform the rewind</h3>
<pre tabindex="0"><code># slaeve:
sudo -u postgres /usr/pgsql-12/bin/pg_rewind  --target-pgdata=/var/lib/pgsql/12/data/ --source-server=&quot;host=192.168.0.110 user=admin password=Password1 dbname=test1&quot; -P
systemctl start postgresql-12
systemctl status postgresql-12
</code></pre><h3 id="test-the-replication-status">test the replication status</h3>
<pre tabindex="0"><code>/usr/pgsql-12/bin/pg_controldata -D /var/lib/pgsql/12/data/ | grep cluster
Database cluster state:               in archive recovery
</code></pre><pre tabindex="0"><code># master:
sudo -u postgres psql -x -c &quot;select * from pg_stat_replication&quot;
could not change directory to &quot;/root&quot;: Permission denied
-[ RECORD 1 ]----+------------------------------
pid              | 11548
usesysid         | 16384
usename          | replica_user
application_name | walreceiver
client_addr      | 192.168.0.111
client_hostname  |
client_port      | 48592
backend_start    | 2020-02-08 10:49:45.449591+01
backend_xmin     |
state            | streaming
sent_lsn         | 1/448638D0
write_lsn        | 1/448638D0
flush_lsn        | 1/448638D0
replay_lsn       | 1/448638D0
write_lag        |
flush_lag        |
replay_lag       |
sync_priority    | 0
sync_state       | async
reply_time       | 2020-02-08 10:50:03.026266+01
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[PostgreSQL Streamin replication]]></title>
            <link href="https://devopstales.github.io/home/postgresql_replication/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/install-postgresql/?utm_source=atom_feed" rel="related" type="text/html" title="Install PostgreSQL" />
                <link href="https://devopstales.github.io/linux/postgresql_replication/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL Streamin replication" />
                <link href="https://devopstales.github.io/home/k8s-longhorn/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes project longhorn" />
                <link href="https://devopstales.github.io/home/speed_up_zfs/?utm_source=atom_feed" rel="related" type="text/html" title="How to speed up zfs resilver?" />
                <link href="https://devopstales.github.io/home/k8s-local-pv/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
            
                <id>https://devopstales.github.io/home/postgresql_replication/</id>
            
            
            <published>2020-02-03T00:00:00+00:00</published>
            <updated>2020-02-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this pos I will show you how to configure Postgresql Streamin replication.</p>
<h3 id="wal-segments">WAL segments</h3>
<p>At all times, PostgreSQL maintains a write ahead log (WAL) in the <code>pg_wal/</code> subdirectory of the cluster&rsquo;s data directory. The log describes every change made to the database&rsquo;s data files. This log exists primarily for crash-safety purposes: if the system crashes, the database can be restored to consistency by “replaying&quot; the log entries made since the last checkpoint. This wal segments san be used to repliyate the transactions to a second read database server.</p>
<h3 id="requirements">requirements</h3>
<pre tabindex="0"><code>yum install https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm -y
yum install postgresql12 postgresql12-server postgresql12-contrib -y
</code></pre><pre tabindex="0"><code>/usr/pgsql-12/bin/postgresql-12-setup initdb
</code></pre><p>Set up the streaming replication related parameters on the two servers</p>
<pre tabindex="0"><code>$ nano /var/lib/pgsql/12/data/postgresql.conf
--------------------------------------------------------
listen_addresses = '*'

wal_level = hot_standby

max_wal_senders = 6

#16 Mb * 32 = 512 Mb
wal_keep_segments = 32

max_wal_size = 2GB

# for pg dump on slave
hot_standby_feedback = on

# for pg_rewind
full_page_writes = on
wal_log_hints = on
</code></pre><h3 id="master-config">master config</h3>
<pre tabindex="0"><code>$ nano /var/lib/pgsql/12/data/postgresql.conf
--------------------------------------------------------
archive_mode = on
archive_command = 'cp %p /var/lib/pgsql/archive/%f'
</code></pre><pre tabindex="0"><code>systemctl enable postgresql-12
systemctl start postgresql-12
</code></pre><pre tabindex="0"><code>su - postgres
pgsql
ALTER SYSTEM SET listen_addresses TO '*';
CREATE ROLE replica_user WITH REPLICATION LOGIN;
\du
\q
exit
</code></pre><pre tabindex="0"><code>echo &quot;host    replication     replica_user    192.168.0.111/32      trust&quot;&gt;&gt;/var/lib/pgsql/12/data/pg_hba.conf
systemctl restart postgresql-12
</code></pre><h3 id="slave-config">slave config</h3>
<pre tabindex="0"><code>rm -rf /var/lib/pgsql/12/data/*

su - postgres
/usr/pgsql-12/bin/pg_basebackup --host=192.168.0.110 --pgdata=/var/lib/pgsql/12/data/ --username=replica_user --verbose --progress --wal-method=stream --write-recovery-conf --checkpoint=fast

ll /var/lib/pgsql/12/data/standby.signal
cat /var/lib/pgsql/12/data/postgresql.auto.conf

echo &quot;restore_command = 'cp /var/lib/pgsql/archive/%f %p'&quot; &gt;&gt; /var/lib/pgsql/12/data/postgresql.auto.conf

systemctl start postgresql-12
</code></pre><h3 id="slave--test">slave  test</h3>
<pre tabindex="0"><code>SELECT * FROM pg_stat_wal_receiver;
sudo -u postgres psql -x -c &quot;select * from pg_stat_wal_receiver&quot;
</code></pre><h3 id="master-test">master test</h3>
<pre tabindex="0"><code>SELECT * FROM pg_stat_replication;
sudo -u postgres psql -x -c &quot;select * from pg_stat_replication&quot;
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes project longhorn]]></title>
            <link href="https://devopstales.github.io/home/k8s-longhorn/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-longhorn/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes project longhorn" />
                <link href="https://devopstales.github.io/home/k8s-gangway/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak and gangway" />
                <link href="https://devopstales.github.io/home/k8s-dasboard-auth/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak and gatekeeper" />
                <link href="https://devopstales.github.io/home/speed_up_zfs/?utm_source=atom_feed" rel="related" type="text/html" title="How to speed up zfs resilver?" />
                <link href="https://devopstales.github.io/home/k8s-local-pv/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
            
                <id>https://devopstales.github.io/home/k8s-longhorn/</id>
            
            
            <published>2020-01-18T00:00:00+00:00</published>
            <updated>2020-01-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Longhorn is lightweight, reliable, and powerful distributed block storage system for Kubernetes..</p>
<H3>Parst of the Kubernetes series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
     <li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
     <li>Part1c: <a href="../../kubernetes/k8s-install-containerd/">Install K8S with containerd and kubeadm</a></li>
     <li>Part1d: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
     <li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a></li>
<!--      <li>Part3b: k8s-multiple-nginx-ingress -->
<!--      <li>Part3c: k8s-contour-ingress -->
     <li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a></li>
<!-- local folder (+ autoprovisioning ) -->
     <li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
     <li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
     <li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
     <li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
     <li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
     <li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a></li>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... -->
     <li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
     <li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
     <li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
     <li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
     <li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
     <li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
     <li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a></li>
<!-- istio-install.md -->
     <li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a></li>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog -->
</ul>


<p>You can install Longhorn on an existing Kubernetes cluster with one <code>kubectl apply</code> command or using Helm charts. Once Longhorn is installed, it adds persistent volume support to the Kubernetes cluster.</p>
<h3 id="install-dependency">Install dependency</h3>
<pre tabindex="0"><code>yum install iscsi-initiator-utils

modprobe iscsi_tcp
echo &quot;iscsi_tcp&quot; &gt;/etc/modules-load.d/iscsi-tcp.conf
</code></pre><h3 id="deploy-longhorn-and-storageclass">Deploy Longhorn and storageclass</h3>
<pre tabindex="0"><code>kubectl apply -f https://raw.githubusercontent.com/rancher/longhorn/master/deploy/longhorn.yaml
kubectl apply -f https://raw.githubusercontent.com/rancher/longhorn/master/examples/storageclass.yaml

kubectl get storageclass
NAME       PROVISIONER          RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
longhorn   driver.longhorn.io   Delete          Immediate           false                  11m
</code></pre><p>Patch longhorn storageclass to be the default storageclass.</p>
<pre tabindex="0"><code>kubectl patch storageclass longhorn -p \
  '{&quot;metadata&quot;: {&quot;annotations&quot;:{&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;true&quot;}}}'

kubectl get storageclass
NAME                 PROVISIONER          RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
longhorn (default)   driver.longhorn.io   Delete          Immediate           false                  12m
</code></pre><h3 id="deploy-admin-gui">Deploy admin gui</h3>
<pre tabindex="0"><code>kubectl -n longhorn-system get svc

cat minio-sec.yaml
---
apiVersion: v1
kind: Secret
metadata:
  namespace: longhorn-system
  name: longhorn-minio
type: Opaque
data:
  AWS_ACCESS_KEY_ID: bWluaW8=
  AWS_SECRET_ACCESS_KEY: bWluaW8xMjM=
  AWS_ENDPOINTS: aHR0cDovL21pbmlvLmxvbmdob3JuLXN5c3RlbTo5MDAw
</code></pre><p><img src="/img/include/longhorn0.png" alt="Example image"  class="zoomable" /><!-- raw HTML omitted -->
<img src="/img/include/longhorn2.png" alt="Example image"  class="zoomable" /><!-- raw HTML omitted -->
<img src="/img/include/longhorn1.png" alt="Example image"  class="zoomable" /><!-- raw HTML omitted --></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to speed up zfs resilver?]]></title>
            <link href="https://devopstales.github.io/home/speed_up_zfs/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-local-pv/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
                <link href="https://devopstales.github.io/home/k8s-kuberos/?utm_source=atom_feed" rel="related" type="text/html" title="Kubectl authentication with Kuberos" />
                <link href="https://devopstales.github.io/home/k8s-gangway/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak and gangway" />
                <link href="https://devopstales.github.io/home/k8s-dasboard-auth/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak and gatekeeper" />
                <link href="https://devopstales.github.io/home/helm3-loki/?utm_source=atom_feed" rel="related" type="text/html" title="Install Grafana Loki with Helm3" />
            
                <id>https://devopstales.github.io/home/speed_up_zfs/</id>
            
            
            <published>2020-01-12T00:00:00+00:00</published>
            <updated>2020-01-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this article I will show you how speed up zfs.</p>
<p>First we need to understand there is two type of zfs the FreeBSD/Solaris based and Linux based cald Zfs On Linux or ZOL.</p>
<p>When a device is replaced, a resilvering operation is initiated to move data from the good copies to the new device. This action is a form of disk scrubbing. RAIDz resilvering is very slow in OpenZFS-based zpools. Basically, it starts with every transaction that’s ever happened in the pool and plays them back one-by-one to the new drive. This is very IO-intensive. Some say if you’re using hard drives larger than 1TB and you are using OpenZFS, use mirror, not RAIDz* or use only SSD for RAIDz*.</p>
<p>In ZOL 0.8.0 they changed to a new resilvering solution. The previous resilvering algorithm repairs blocks from oldest to newest, which can degrade into a lot of small random I/O. The new resilvering algorithm uses a two-step process to sort and resilver blocks in LBA order. If you have an older version of ZOL or want even better performance you can tweak with the zfs configuration.</p>
<h3 id="speed-up-resilvering">Speed up resilvering</h3>
<pre tabindex="0"><code>echo 0 &gt; /sys/module/zfs/parameters/zfs_resilver_delay
echo 0 &gt; /sys/module/zfs/parameters/zfs_scrub_delay
echo 512 &gt; /sys/module/zfs/parameters/zfs_top_maxinflight
echo 8192 &gt; /sys/module/zfs/parameters/zfs_top_maxinflight
echo 8000 &gt; /sys/module/zfs/parameters/zfs_resilver_min_time_ms
</code></pre><h3 id="speed-up-on-pool">Speed up on pool</h3>
<pre tabindex="0"><code>zpool set autoreplace=on zpool

echo 0 &gt; /sys/module/zfs/parameters/zfs_scan_idle
echo 24 &gt; /sys/module/zfs/parameters/zfs_vdev_scrub_min_active
echo 64 &gt; /sys/module/zfs/parameters/zfs_vdev_scrub_max_active
echo 64 &gt; /sys/module/zfs/parameters/zfs_vdev_sync_write_min_active
echo 32 &gt; /sys/module/zfs/parameters/zfs_vdev_sync_write_max_active
echo 8 &gt; /sys/module/zfs/parameters/zfs_vdev_sync_read_min_active
echo 32 &gt; /sys/module/zfs/parameters/zfs_vdev_sync_read_max_active
echo 8 &gt; /sys/module/zfs/parameters/zfs_vdev_async_read_min_active
echo 32 &gt; /sys/module/zfs/parameters/zfs_vdev_async_read_max_active
echo 8 &gt; /sys/module/zfs/parameters/zfs_vdev_async_write_min_active
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes nginx ingress with helm]]></title>
            <link href="https://devopstales.github.io/home/k8s-local-pv/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-velero-backup/?utm_source=atom_feed" rel="related" type="text/html" title="Backup your Kubernetes Cluster with Velero" />
                <link href="https://devopstales.github.io/home/k8s-cert-manager/?utm_source=atom_feed" rel="related" type="text/html" title="Install cert-manager for Kubernetes" />
                <link href="https://devopstales.github.io/home/kind-install/?utm_source=atom_feed" rel="related" type="text/html" title="Starting local Kubernetes using kind" />
                <link href="https://devopstales.github.io/home/k8s-metallb/?utm_source=atom_feed" rel="related" type="text/html" title="Install MetalLB load balancer for Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-tillerless-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Tillerless helm2 install" />
            
                <id>https://devopstales.github.io/home/k8s-local-pv/</id>
            
            
            <published>2020-01-08T00:00:00+00:00</published>
            <updated>2020-01-08T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how to use a local folder as a persistent volume in Kubernetes.</p>
<H3>Parst of the Kubernetes series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
     <li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
     <li>Part1c: <a href="../../kubernetes/k8s-install-containerd/">Install K8S with containerd and kubeadm</a></li>
     <li>Part1d: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
     <li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a></li>
<!--      <li>Part3b: k8s-multiple-nginx-ingress -->
<!--      <li>Part3c: k8s-contour-ingress -->
     <li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a></li>
<!-- local folder (+ autoprovisioning ) -->
     <li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
     <li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
     <li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
     <li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
     <li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
     <li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a></li>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... -->
     <li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
     <li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
     <li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
     <li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
     <li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
     <li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
     <li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a></li>
<!-- istio-install.md -->
     <li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a></li>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog -->
</ul>


<p>For a production environment this is not an ideal structure because if you store the data on a single host if the host dies your data will be lost. For this Demo I will use a separate disk for storing the PV&rsquo;s folders. So you can backup or replicate this disk separately.</p>
<h3 id="configure-the-disk">Configure the disk</h3>
<pre tabindex="0"><code>vgcreate local-vg /dev/sdd
lvcreate -l 100%FREE -n local-lv local-vg /dev/sdd
mkfs.xfs -f /dev/local-vg/local-lv
mkdir -p /mnt/local-storage/
mount /dev/local-vg/local-lv /mnt/local-storage
echo &quot;/dev/local-vg/local-lv        /mnt/local-storage              xfs defaults 0 0&quot; &gt;&gt; /etc/fstab
rm -rf /mnt/local-storage/lost+found
</code></pre><p>Now you can create every PV and PVC manually.</p>
<pre tabindex="0"><code>mkdir /mnt/local-storage/pv-tst

cat pv-tst.yaml
---
kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv-tst
spec:
  capacity:
    storage: 1Gi
  local:
    path: /mnt/local-storage/pv-tst
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local
  nodeAffinity:
    required:
      nodeSelectorTerms:
        - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
                - kubernetes03.devopstales.intra
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pv-tst
  namespace: tst
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  volumeName: pv-tst
  storageClassName: local
</code></pre><h3 id="add-automated-hostpath-provisioner">Add automated hostpath-provisioner</h3>
<p>This is a Persistent Volume Claim (PVC) provisioner for Kubernetes. It dynamically provisions hostPath volumes to provide storage for PVCs.</p>
<pre tabindex="0"><code>git clone https://github.com/torchbox/k8s-hostpath-provisioner
cd k8s-hostpath-provisioner
kubectl apply -f deployment.yaml

nano local-sc.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: auto-local
  annotations:
    storageclass.kubernetes.io/is-default-class: &quot;true&quot;
provisioner: torchbox.com/hostpath
parameters:
  pvDir: /mnt/local-storage
</code></pre><p>Test the provisioner by creating a new PVC:</p>
<pre tabindex="0"><code>cat testpvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: testpvc
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 50Gi

kubectl create -f testpvc.yaml
kubectl get pvc
NAME      STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
testpvc   Bound     pvc-145c785e-ab83-11e7-9432-4201ac1fd019   50Gi       RWX            auto-local     10s
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubectl authentication with Kuberos]]></title>
            <link href="https://devopstales.github.io/home/k8s-kuberos/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/sso/k8s-kuberos/?utm_source=atom_feed" rel="related" type="text/html" title="Kubectl authentication with Kuberos" />
                <link href="https://devopstales.github.io/home/k8s-gangway/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak and gangway" />
                <link href="https://devopstales.github.io/home/k8s-dasboard-auth/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak and gatekeeper" />
                <link href="https://devopstales.github.io/home/grafana-sso/?utm_source=atom_feed" rel="related" type="text/html" title="SSO login to Grafana" />
                <link href="https://devopstales.github.io/home/openshift-sso-2/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift SSO with Gitlab" />
            
                <id>https://devopstales.github.io/home/k8s-kuberos/</id>
            
            
            <published>2020-01-07T00:00:00+00:00</published>
            <updated>2020-01-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Kuberos is an OIDC authentication helper for Kubernetes' kubectl</p>
<H3>Parst of the Kubernetes series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
     <li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
     <li>Part1c: <a href="../../kubernetes/k8s-install-containerd/">Install K8S with containerd and kubeadm</a></li>
     <li>Part1d: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
     <li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a></li>
<!--      <li>Part3b: k8s-multiple-nginx-ingress -->
<!--      <li>Part3c: k8s-contour-ingress -->
     <li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a></li>
<!-- local folder (+ autoprovisioning ) -->
     <li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
     <li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
     <li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
     <li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
     <li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
     <li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a></li>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... -->
     <li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
     <li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
     <li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
     <li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
     <li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
     <li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
     <li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a></li>
<!-- istio-install.md -->
     <li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a></li>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog -->
</ul>


<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano /etc/kubernetes/manifests/kube-apiserver.yaml
...
    command:
    - /hyperkube
    - apiserver
    - --advertise-address<span style="color:#f92672">=</span>10.10.40.30
...

    - --oidc-issuer-url<span style="color:#f92672">=</span>https://keycloak.devopstales.intra/auth/realms/mydomain
    - --oidc-client-id<span style="color:#f92672">=</span>k8s
    - --oidc-username-claim<span style="color:#f92672">=</span>email
    - --oidc-groups-claim<span style="color:#f92672">=</span>groups
...

systemctl restart docker kubelet
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">cat &lt;&lt;&#39;EOF&#39;&gt; values.yaml</span>
<span style="color:#f92672">replicaCount</span>: <span style="color:#ae81ff">1</span>

<span style="color:#f92672">kuberos</span>:
  <span style="color:#f92672">oidcClientURL</span>: <span style="color:#ae81ff">https://keycloak.devopstales.intra/auth/realms/mydomain</span>
  <span style="color:#f92672">oidcClientID</span>: <span style="color:#ae81ff">k8s</span>
  <span style="color:#f92672">oidcSecret</span>: <span style="color:#ae81ff">43219919-0904-4338</span>-<span style="color:#ae81ff">bc0f-c986e1891a7a</span>
  <span style="color:#f92672">clusters</span>:
  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">openshift</span>
    <span style="color:#f92672">apiServer</span>: <span style="color:#ae81ff">https://192.168.0.106:6443</span>
    <span style="color:#75715e"># `apiServer` is the url for kubectl</span>
    <span style="color:#75715e">#   This is typically  https://api.fqdn</span>
    <span style="color:#f92672">caCrt</span>: |-<span style="color:#e6db74">
</span><span style="color:#e6db74">      -----BEGIN CERTIFICATE-----
</span><span style="color:#e6db74">      MIIDZDCCAkygAwIBAgIIe/R9sc8oJiAwDQYJKoZIhvcNAQELBQAwFTETMBEGA1UE
</span><span style="color:#e6db74">      AxMKa3ViZXJuZXRlczAeFw0xOTEyMjcxNzM3MzlaFw0yMDEyMjYxNzM3MzlaMBkx
</span><span style="color:#e6db74">      FzAVBgNVBAMTDmt1YmUtYXBpc2VydmVyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8A
</span><span style="color:#e6db74">      MIIBCgKCAQEAzAg7MflA/HVTVcZPsGJH71cfcJ/U1CtEYbXfu/AQbhGg09XKmeK9
</span><span style="color:#e6db74">      aGEK3kSgi/Hyoi7M+e/ntx1+Gp/jwc8kanMFRLgxdKCxxi4MOswZF/q2loUdNoE/
</span><span style="color:#e6db74">      OQVPWQi8Hgznubw/0gINUkIq8mRx9Bb+RcRnJEfD3CXkxDhUNeCvvjeTrujguF0h
</span><span style="color:#e6db74">      pgfzrLoc2kGdJYpHiLqow8jRq7XXk0RzZaqCQjAEZgqWamwbTTqFZh3v+1gF/2s0
</span><span style="color:#e6db74">      EbFVVL2Ctu1dOGe1FkZxte7/Po1XBkPLQuRXbH3QRiJkPfyOW16T1nWk1QTcpCdH
</span><span style="color:#e6db74">      HO/l+CY2nLPFZL1BM83QuVmPgR1T1p+5tQIDAQABo4GzMIGwMA4GA1UdDwEB/wQE
</span><span style="color:#e6db74">      AwIFoDATBgNVHSUEDDAKBggrBgEFBQcDATCBiAYDVR0RBIGAMH6CEms4cy5teWRv
</span><span style="color:#e6db74">      bWFpbi5pbnRyYYIKa3ViZXJuZXRlc4ISa3ViZXJuZXRlcy5kZWZhdWx0ghZrdWJl
</span><span style="color:#e6db74">      cm5ldGVzLmRlZmF1bHQuc3ZjgiRrdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0
</span><span style="color:#e6db74">      ZXIubG9jYWyHBApgAAGHBMCoAGowDQYJKoZIhvcNAQELBQADggEBAJZ7jxPR72V6
</span><span style="color:#e6db74">      PYL3SKCWaS+RgTuGuSm0pYu26cBmPOjsugd8DUrJ7+iAnKDHUmmw22sWheLLCokc
</span><span style="color:#e6db74">      YU/AIfdbbsz0+f+/qthkO7zJmAJgdIAOMJ5MQCbxMBt+6L813r1R3QI7kAGxHvzV
</span><span style="color:#e6db74">      loKJVIIHq/6K3gFEZDfo0myvNvtOIpBCeMnZRK+8hx3UNcHckZbhkan1Z1j9t9iw
</span><span style="color:#e6db74">      b6Vv5jY1+9t2Iltd2wuNaUvHicx+3X6JPAqVR6H0jI3i+QSyT1EHXtBtbQBBpP4T
</span><span style="color:#e6db74">      5WDz+9uDa1mIDHtww7DTnJwY+hGI7fVF2H7XQaM4xwhGnwIwbkSh45JWVtUEHMou
</span><span style="color:#e6db74">      Q7T4bTyrwuQ=
</span><span style="color:#e6db74">      -----END CERTIFICATE-----</span>      
    <span style="color:#75715e"># `caCrt` is the public / CA cert for the cluster</span>
    <span style="color:#75715e"># cat /etc/kubernetes/pki/apiserver.crt</span>

<span style="color:#f92672">ingress</span>:
  <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">true</span>
  <span style="color:#f92672">annotations</span>:
    <span style="color:#f92672">kubernetes.io/ingress.class</span>: <span style="color:#ae81ff">nginx</span>
    <span style="color:#f92672">nginx.ingress.kubernetes.io/proxy-buffer-size</span>: <span style="color:#e6db74">&#34;64k&#34;</span>
    <span style="color:#f92672">cert-manager.io/cluster-issuer</span>: <span style="color:#ae81ff">ca-issuer</span>
    <span style="color:#f92672">ingress.kubernetes.io/force-ssl-redirect</span>: <span style="color:#e6db74">&#34;true&#34;</span>
  <span style="color:#f92672">path</span>: <span style="color:#ae81ff">/</span>
  <span style="color:#f92672">hosts</span>:
    - <span style="color:#ae81ff">kubectl.devopstales.intra</span>
  <span style="color:#f92672">tls</span>:
    - <span style="color:#f92672">secretName</span>: <span style="color:#ae81ff">default-cert</span>
      <span style="color:#f92672">hosts</span>:
        - <span style="color:#ae81ff">kubectl.devopstales.intra</span>

<span style="color:#f92672">image</span>:
  <span style="color:#f92672">repository</span>: <span style="color:#ae81ff">negz/kuberos</span>
  <span style="color:#f92672">tag</span>: <span style="color:#ae81ff">ede4085</span>
  <span style="color:#f92672">pullPolicy</span>: <span style="color:#ae81ff">IfNotPresent</span>

<span style="color:#f92672">service</span>:
  <span style="color:#f92672">type</span>: <span style="color:#ae81ff">ClusterIP</span>
  <span style="color:#f92672">port</span>: <span style="color:#ae81ff">80</span>
  <span style="color:#f92672">annotations</span>: {}
  <span style="color:#75715e"># Add your service annotations here.</span>

<span style="color:#f92672">resources</span>: {}
<span style="color:#ae81ff">EOF</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">helm repo add devopstales https://devopstales.github.io/helm-charts
helm upgrade --install kuberos devopstales/kuberos --namespace kuberos -f values.yaml
</code></pre></div><p><img src="/img/include/kuberos.png" alt="kuberos"  class="zoomable" /></p>
<p>Below is an example of a <code>ClusterRoleBinding</code> that binds the role <code>cluster-admin</code> to the Keycloak group <code>devops-team</code>. (In my case it came from ldap) Create your own role bindings to fit your needs and apply them to the cluster.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano devops-team_ClusterRoleBinding.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRoleBinding</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cluster-admin-it-afdeling</span>
<span style="color:#f92672">roleRef</span>:
  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRole</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cluster-admin</span>
<span style="color:#f92672">subjects</span>:
  - <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
    <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Group</span>
    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">devops-team</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano user_ClusterRoleBinding.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRoleBinding</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">devopstales-admin</span>
<span style="color:#f92672">subjects</span>:
  - <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">User</span>
    <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">devopstales</span>
<span style="color:#f92672">roleRef</span>:
  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRole</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cluster-admin</span>
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Dashboard authentication with Keycloak and gangway]]></title>
            <link href="https://devopstales.github.io/home/k8s-gangway/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-dasboard-auth/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak and gatekeeper" />
                <link href="https://devopstales.github.io/sso/k8s-gangway/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak and gangway" />
                <link href="https://devopstales.github.io/sso/k8s-dasboard-auth/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak and gatekeeper" />
                <link href="https://devopstales.github.io/home/grafana-sso/?utm_source=atom_feed" rel="related" type="text/html" title="SSO login to Grafana" />
                <link href="https://devopstales.github.io/home/openshift-sso-2/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift SSO with Gitlab" />
            
                <id>https://devopstales.github.io/home/k8s-gangway/</id>
            
            
            <published>2020-01-06T00:00:00+00:00</published>
            <updated>2020-01-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Kuberos is an OIDC authentication helper for Kubernetes' kubectl</p>
<H3>Parst of the Kubernetes series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
     <li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
     <li>Part1c: <a href="../../kubernetes/k8s-install-containerd/">Install K8S with containerd and kubeadm</a></li>
     <li>Part1d: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
     <li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a></li>
<!--      <li>Part3b: k8s-multiple-nginx-ingress -->
<!--      <li>Part3c: k8s-contour-ingress -->
     <li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a></li>
<!-- local folder (+ autoprovisioning ) -->
     <li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
     <li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
     <li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
     <li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
     <li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
     <li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a></li>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... -->
     <li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
     <li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
     <li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
     <li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
     <li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
     <li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
     <li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a></li>
<!-- istio-install.md -->
     <li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a></li>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog -->
</ul>


<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano /etc/kubernetes/manifests/kube-apiserver.yaml
...
    command:
    - /hyperkube
    - apiserver
    - --advertise-address<span style="color:#f92672">=</span>10.10.40.30
...

    - --oidc-issuer-url<span style="color:#f92672">=</span>https://keycloak.devopstales.intra/auth/realms/mydomain
    - --oidc-client-id<span style="color:#f92672">=</span>k8s
    - --oidc-username-claim<span style="color:#f92672">=</span>email
    - --oidc-groups-claim<span style="color:#f92672">=</span>groups
...

systemctl restart docker kubelet
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano gangway.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Namespace</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gangway</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Secret</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gangway-key</span>
  <span style="color:#f92672">labels</span>:
    <span style="color:#f92672">app.kubernetes.io/name</span>: <span style="color:#ae81ff">gangway</span>
<span style="color:#f92672">type</span>: <span style="color:#ae81ff">Opaque</span>
<span style="color:#f92672">data</span>:
  <span style="color:#f92672">sessionkey</span>: <span style="color:#e6db74">&#34;ZTZNYlJUbDdHcHlSeXVFU0J6ZDZmbUs5Mks5a21NWEo=&#34;</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ConfigMap</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gangway</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">gangway</span>
<span style="color:#f92672">data</span>:
  <span style="color:#f92672">gangway.yaml</span>: |<span style="color:#e6db74">
</span><span style="color:#e6db74">    clusterName: &#34;minikube&#34;
</span><span style="color:#e6db74">    authorizeURL: &#34;https://keycloak.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/auth&#34;
</span><span style="color:#e6db74">    tokenURL: &#34;https://keycloak.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/token&#34;
</span><span style="color:#e6db74">    audience: &#34;https://keycloak.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/userinfo&#34;
</span><span style="color:#e6db74">    # Used to specify the scope of the requested Oauth authorization.
</span><span style="color:#e6db74">    # scopes: [&#34;openid&#34;, &#34;profile&#34;, &#34;email&#34;, &#34;offline_access&#34;]
</span><span style="color:#e6db74">    # scopes: [&#34;groups&#34;]
</span><span style="color:#e6db74">    # scopes: [&#34;openid&#34;, &#34;profile&#34;, &#34;email&#34;, &#34;offline_access&#34;, &#34;groups&#34;]
</span><span style="color:#e6db74">    # Where to redirect back to. This should be a URL where gangway is reachable.
</span><span style="color:#e6db74">    # Typically this also needs to be registered as part of the oauth application
</span><span style="color:#e6db74">    # with the oAuth provider.
</span><span style="color:#e6db74">    # Env var: GANGWAY_REDIRECT_URL
</span><span style="color:#e6db74">    redirectURL: &#34;https://gangway.devopstales.intra/callback&#34;
</span><span style="color:#e6db74">    clientID: &#34;k8s&#34;
</span><span style="color:#e6db74">    clientSecret: &#34;43219919-0904-4338-bc0f-c986e1891a7a&#34;
</span><span style="color:#e6db74">    # The JWT claim to use as the username. This is used in UI.
</span><span style="color:#e6db74">    # Default is &#34;nickname&#34;. This is combined with the clusterName
</span><span style="color:#e6db74">    # for the &#34;user&#34; portion of the kubeconfig.
</span><span style="color:#e6db74">    # Env var: GANGWAY_USERNAME_CLAIM
</span><span style="color:#e6db74">    # usernameClaim: &#34;sub&#34;
</span><span style="color:#e6db74">    usernameClaim: &#34;preferred_username&#34;
</span><span style="color:#e6db74">    emailClaim: &#34;email&#34;
</span><span style="color:#e6db74">    # The API server endpoint used to configure kubectl
</span><span style="color:#e6db74">    # Env var: GANGWAY_APISERVER_URL
</span><span style="color:#e6db74">    # apiServerURL: &#34;https://kube.codeformuenster.org:6443&#34;
</span><span style="color:#e6db74">    apiServerURL: &#34;https://192.168.0.106:8443&#34;
</span><span style="color:#e6db74">    # The path to find the CA bundle for the API server. Used to configure kubectl.
</span><span style="color:#e6db74">    # This is typically mounted into the default location for workloads running on
</span><span style="color:#e6db74">    # a Kubernetes cluster and doesn&#39;t need to be set.
</span><span style="color:#e6db74">    # Env var: GANGWAY_CLUSTER_CA_PATH
</span><span style="color:#e6db74">    # cluster_ca_path: &#34;/var/run/secrets/kubernetes.io/serviceaccount/ca.crt&#34;
</span><span style="color:#e6db74">    # The path to a root CA to trust for self signed certificates at the Oauth2 URLs
</span><span style="color:#e6db74">    # Env var: GANGWAY_TRUSTED_CA_PATH
</span><span style="color:#e6db74">    # for self signd certificate:
</span><span style="color:#e6db74">    trustedCAPath: /gangway/rootca.crt
</span><span style="color:#e6db74">    # The path gangway uses to create urls (defaults to &#34;&#34;)
</span><span style="color:#e6db74">    # Env var: GANGWAY_HTTP_PATH
</span><span style="color:#e6db74">    # httpPath: &#34;https://${GANGWAY_HTTP_PATH}&#34;
</span><span style="color:#e6db74">    # for self signd certificate:</span>    
  <span style="color:#f92672">rootca.crt</span>: |<span style="color:#e6db74">
</span><span style="color:#e6db74">    -----BEGIN CERTIFICATE-----
</span><span style="color:#e6db74">    MIIEATCCAumgAwIBAgIUXHOQsGW+UHpCphHViHSvyBny8BwwDQYJKoZIhvcNAQEL
</span><span style="color:#e6db74">    BQAwgY4xCzAJBgNVBAYTAkhVMQ0wCwYDVQQIDARQZXN0MREwDwYDVQQHDAhCdWRh
</span><span style="color:#e6db74">    cGVzdDETMBEGA1UECgwKTXkgQ29tcGFueTELMAkGA1UECwwCT1UxFzAVBgNVBAMM
</span><span style="color:#e6db74">    Dm15ZG9tYWluLmludHJhMSIwIAYJKoZIhvcNAQkBFhNyb290QG15ZG9tYWluLmlu
</span><span style="color:#e6db74">    dHJhMCAXDTE5MTIyNzE3MTk0OVoYDzIxMTkxMjAzMTcxOTQ5WjCBjjELMAkGA1UE
</span><span style="color:#e6db74">    BhMCSFUxDTALBgNVBAgMBFBlc3QxETAPBgNVBAcMCEJ1ZGFwZXN0MRMwEQYDVQQK
</span><span style="color:#e6db74">    DApNeSBDb21wYW55MQswCQYDVQQLDAJPVTEXMBUGA1UEAwwObXlkb21haW4uaW50
</span><span style="color:#e6db74">    cmExIjAgBgkqhkiG9w0BCQEWE3Jvb3RAbXlkb21haW4uaW50cmEwggEiMA0GCSqG
</span><span style="color:#e6db74">    SIb3DQEBAQUAA4IBDwAwggEKAoIBAQDevUZouW101hAu68qojvfmnC3fUIA9L5nj
</span><span style="color:#e6db74">    J+OTbgwDhxYduHVcmwFcrZJzBn/udm72sAUsjmoc34ZoEQVr1mCjrnDdb3NtOWBI
</span><span style="color:#e6db74">    XYmN7/RySzu5DSFLFv8Sj+27VvGLpYTXgDEt+IQpV4EgosX6DzjYK7BtmqaWCY3t
</span><span style="color:#e6db74">    aClGnzxEotlxMakTCt9eALD+l/ffV4NbiS6sPNaOFHbG8CKRnfzDzqh78qYaSH8d
</span><span style="color:#e6db74">    wWxGLGAvciNm1wv1G3NIkjMIZlkMqAv6uTzEtfOPQrHigG8sbb4hHAg8a9RtH2Sk
</span><span style="color:#e6db74">    nXjZRb3Wfo+XJ2eUCZyC6pwvZfEuZBuRAAo12Ycp/Ve2FC3kvpzLAgMBAAGjUzBR
</span><span style="color:#e6db74">    MB0GA1UdDgQWBBTRPLiCReojvBQXna2zkBBTsKdsnzAfBgNVHSMEGDAWgBTRPLiC
</span><span style="color:#e6db74">    ReojvBQXna2zkBBTsKdsnzAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUA
</span><span style="color:#e6db74">    A4IBAQBXp676KZ7VTPc/RPI+QIyb0ugPE+w6fkpw1PIzG+y0h53AsOu15tXxKX5L
</span><span style="color:#e6db74">    SwotjXoDuTnqQqLZ5wTFSiolscay+MpEDnoIdo+Pw7u3q3bpn6GmDjae1BaIL/En
</span><span style="color:#e6db74">    wvxvvJQsOJrXfEUQeC6M75i/MrYPSwhWNDAbqJTY2qEuRXcj/AALGrnlF5DEEd+O
</span><span style="color:#e6db74">    RYw79sj+xU88/kCOVWI35LwiH+/0QWFyKcPQvY8nER69nt5evFGqUQPE6qlOJKg/
</span><span style="color:#e6db74">    YD8dK+OF26Ta/qz0iKNAfh3WDgYU4lHAawKtwAbpBVBLlzLl+bD11BQvn6zDWVWA
</span><span style="color:#e6db74">    rfrwKUIO+dwSL3ZKS0kA0OlN3dyy
</span><span style="color:#e6db74">    -----END CERTIFICATE-----</span>    
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">apps/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Deployment</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gangway</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">gangway</span>
  <span style="color:#f92672">labels</span>:
    <span style="color:#f92672">app</span>: <span style="color:#ae81ff">gangway</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">replicas</span>: <span style="color:#ae81ff">1</span>
  <span style="color:#f92672">selector</span>:
    <span style="color:#f92672">matchLabels</span>:
      <span style="color:#f92672">app</span>: <span style="color:#ae81ff">gangway</span>
  <span style="color:#f92672">strategy</span>:
  <span style="color:#f92672">template</span>:
    <span style="color:#f92672">metadata</span>:
      <span style="color:#f92672">labels</span>:
        <span style="color:#f92672">app</span>: <span style="color:#ae81ff">gangway</span>
    <span style="color:#f92672">spec</span>:
      <span style="color:#f92672">containers</span>:
      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gangway</span>
        <span style="color:#f92672">image</span>: <span style="color:#ae81ff">gcr.io/heptio-images/gangway:v3.2.0</span>
        <span style="color:#f92672">command</span>: [<span style="color:#e6db74">&#34;gangway&#34;</span>, <span style="color:#e6db74">&#34;-config&#34;</span>, <span style="color:#e6db74">&#34;/gangway/gangway.yaml&#34;</span>]
        <span style="color:#f92672">env</span>:
        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">GANGWAY_SESSION_SECURITY_KEY</span>
          <span style="color:#f92672">valueFrom</span>:
            <span style="color:#f92672">secretKeyRef</span>:
              <span style="color:#f92672">key</span>: <span style="color:#ae81ff">sessionkey</span>
              <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gangway-key</span>
        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">GANGWAY_PORT</span>
          <span style="color:#f92672">value</span>: <span style="color:#e6db74">&#34;8080&#34;</span>
        <span style="color:#f92672">ports</span>:
        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">http</span>
          <span style="color:#f92672">containerPort</span>: <span style="color:#ae81ff">8080</span>
          <span style="color:#f92672">protocol</span>: <span style="color:#ae81ff">TCP</span>
        <span style="color:#f92672">resources</span>:
          <span style="color:#f92672">requests</span>:
            <span style="color:#f92672">cpu</span>: <span style="color:#e6db74">&#34;100m&#34;</span>
            <span style="color:#f92672">memory</span>: <span style="color:#e6db74">&#34;128Mi&#34;</span>
          <span style="color:#f92672">limits</span>:
            <span style="color:#f92672">cpu</span>: <span style="color:#e6db74">&#34;200m&#34;</span>
            <span style="color:#f92672">memory</span>: <span style="color:#e6db74">&#34;512Mi&#34;</span>
        <span style="color:#f92672">volumeMounts</span>:
        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gangway</span>
          <span style="color:#f92672">mountPath</span>: <span style="color:#ae81ff">/gangway/</span>
        <span style="color:#f92672">livenessProbe</span>:
          <span style="color:#f92672">httpGet</span>:
            <span style="color:#f92672">path</span>: <span style="color:#ae81ff">/</span>
            <span style="color:#f92672">port</span>: <span style="color:#ae81ff">8080</span>
          <span style="color:#f92672">initialDelaySeconds</span>: <span style="color:#ae81ff">20</span>
          <span style="color:#f92672">timeoutSeconds</span>: <span style="color:#ae81ff">1</span>
          <span style="color:#f92672">periodSeconds</span>: <span style="color:#ae81ff">60</span>
          <span style="color:#f92672">failureThreshold</span>: <span style="color:#ae81ff">3</span>
        <span style="color:#f92672">readinessProbe</span>:
          <span style="color:#f92672">httpGet</span>:
            <span style="color:#f92672">path</span>: <span style="color:#ae81ff">/</span>
            <span style="color:#f92672">port</span>: <span style="color:#ae81ff">8080</span>
          <span style="color:#f92672">timeoutSeconds</span>: <span style="color:#ae81ff">1</span>
          <span style="color:#f92672">periodSeconds</span>: <span style="color:#ae81ff">10</span>
          <span style="color:#f92672">failureThreshold</span>: <span style="color:#ae81ff">3</span>
      <span style="color:#f92672">volumes</span>:
      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gangway</span>
        <span style="color:#f92672">configMap</span>:
          <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gangway</span>
---
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Service</span>
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gangway</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">gangway</span>
  <span style="color:#f92672">labels</span>:
    <span style="color:#f92672">app</span>: <span style="color:#ae81ff">gangway</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">type</span>: <span style="color:#ae81ff">ClusterIP</span>
  <span style="color:#f92672">ports</span>:
    - <span style="color:#f92672">name</span>: <span style="color:#e6db74">&#34;http&#34;</span>
      <span style="color:#f92672">protocol</span>: <span style="color:#ae81ff">TCP</span>
      <span style="color:#f92672">port</span>: <span style="color:#ae81ff">80</span>
      <span style="color:#f92672">targetPort</span>: <span style="color:#e6db74">&#34;http&#34;</span>
  <span style="color:#f92672">selector</span>:
    <span style="color:#f92672">app</span>: <span style="color:#ae81ff">gangway</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">extensions/v1beta1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Ingress</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gangway</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">gangway</span>
  <span style="color:#f92672">annotations</span>:
    <span style="color:#f92672">ingress.kubernetes.io/rewrite-target</span>: <span style="color:#ae81ff">/</span>
    <span style="color:#f92672">cert-manager.io/cluster-issuer</span>: <span style="color:#ae81ff">ca-issuer</span>
    <span style="color:#f92672">ingress.kubernetes.io/force-ssl-redirect</span>: <span style="color:#e6db74">&#34;true&#34;</span>
    <span style="color:#f92672">kubernetes.io/ingress.class</span>: <span style="color:#ae81ff">nginx</span>
    <span style="color:#f92672">nginx.ingress.kubernetes.io/proxy-buffer-size</span>: <span style="color:#e6db74">&#34;64k&#34;</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">rules</span>:
  - <span style="color:#f92672">host</span>: <span style="color:#ae81ff">gangway.devopstales.intra</span>
    <span style="color:#f92672">http</span>:
      <span style="color:#f92672">paths</span>:
      - <span style="color:#f92672">backend</span>:
          <span style="color:#f92672">serviceName</span>: <span style="color:#ae81ff">gangway</span>
          <span style="color:#f92672">servicePort</span>: <span style="color:#ae81ff">http</span>
  <span style="color:#f92672">tls</span>:
  - <span style="color:#f92672">secretName</span>: <span style="color:#ae81ff">gangway-tls</span>
    <span style="color:#f92672">hosts</span>:
    - <span style="color:#ae81ff">gangway.devopstales.intra</span>
</code></pre></div><p><img src="/img/include/gangway.png" alt="gangway"  class="zoomable" /></p>
<p>Below is an example of a <code>ClusterRoleBinding</code> that binds the role <code>cluster-admin</code> to the Keycloak group <code>devops-team</code>. (In my case it came from ldap) Create your own role bindings to fit your needs and apply them to the cluster.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano devops-team_ClusterRoleBinding.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRoleBinding</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cluster-admin-it-afdeling</span>
<span style="color:#f92672">roleRef</span>:
  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRole</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cluster-admin</span>
<span style="color:#f92672">subjects</span>:
  - <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
    <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Group</span>
    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">devops-team</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano user_ClusterRoleBinding.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRoleBinding</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">devopstales-admin</span>
<span style="color:#f92672">subjects</span>:
  - <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">User</span>
    <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">devopstales</span>
<span style="color:#f92672">roleRef</span>:
  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRole</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cluster-admin</span>
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Dashboard authentication with Keycloak and gatekeeper]]></title>
            <link href="https://devopstales.github.io/home/k8s-dasboard-auth/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/sso/k8s-dasboard-auth/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak and gatekeeper" />
                <link href="https://devopstales.github.io/home/grafana-sso/?utm_source=atom_feed" rel="related" type="text/html" title="SSO login to Grafana" />
                <link href="https://devopstales.github.io/home/openshift-sso-2/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift SSO with Gitlab" />
                <link href="https://devopstales.github.io/home/nextcloud-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Nextcloud SSO" />
                <link href="https://devopstales.github.io/home/nexus-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Sonatype Nexus SSO" />
            
                <id>https://devopstales.github.io/home/k8s-dasboard-auth/</id>
            
            
            <published>2020-01-03T00:00:00+00:00</published>
            <updated>2020-01-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how to add a keycloak gatekeeper authentication proxy for Kubernetes Dashboard.</p>
<H3>Parst of the Kubernetes series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
     <li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
     <li>Part1c: <a href="../../kubernetes/k8s-install-containerd/">Install K8S with containerd and kubeadm</a></li>
     <li>Part1d: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
     <li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a></li>
<!--      <li>Part3b: k8s-multiple-nginx-ingress -->
<!--      <li>Part3c: k8s-contour-ingress -->
     <li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a></li>
<!-- local folder (+ autoprovisioning ) -->
     <li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
     <li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
     <li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
     <li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
     <li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
     <li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a></li>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... -->
     <li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
     <li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
     <li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
     <li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
     <li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
     <li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
     <li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a></li>
<!-- istio-install.md -->
     <li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a></li>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog -->
</ul>


<p>Kubernetes does not have its own user management and relies on external providers like Keycloak. First we need to integrate an OpeniD prodiver (for me keycloak) with the kubernetes api server.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">nano /etc/kubernetes/manifests/kube-apiserver.yaml
...
    command:
    - /hyperkube
    - apiserver
    - --advertise-address<span style="color:#f92672">=</span>10.10.40.30
...

    - --oidc-issuer-url<span style="color:#f92672">=</span>https://keycloak.devopstales.intra/auth/realms/mydomain
    - --oidc-client-id<span style="color:#f92672">=</span>k8s
    - --oidc-username-claim<span style="color:#f92672">=</span>email
    - --oidc-groups-claim<span style="color:#f92672">=</span>groups
    <span style="color:#75715e"># for self sign cert or custom ca</span>
    <span style="color:#75715e">#- --oidc-ca-file=/etc/kubernetes/pki/rootca.pem</span>
...

systemctl restart docker kubelet
</code></pre></div><p>We need an authentication proxy before the dasboard. I will use keycloak-gatekeeper for that purpose.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano proxy-deplayment.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">apps/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Deployment</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">dasboard-proxy</span>
  <span style="color:#f92672">labels</span>:
    <span style="color:#f92672">app.kubernetes.io/name</span>: <span style="color:#ae81ff">dasboard-proxy</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">kubernetes-dashboard</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">replicas</span>: <span style="color:#ae81ff">1</span>
  <span style="color:#f92672">selector</span>:
    <span style="color:#f92672">matchLabels</span>:
      <span style="color:#f92672">app.kubernetes.io/name</span>: <span style="color:#ae81ff">dasboard-proxy</span>
  <span style="color:#f92672">template</span>:
    <span style="color:#f92672">metadata</span>:
      <span style="color:#f92672">labels</span>:
        <span style="color:#f92672">app.kubernetes.io/name</span>: <span style="color:#ae81ff">dasboard-proxy</span>
    <span style="color:#f92672">spec</span>:
      <span style="color:#f92672">containers</span>:
        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">dasboard-proxy</span>
          <span style="color:#f92672">image</span>: <span style="color:#e6db74">&#34;keycloak/keycloak-gatekeeper:latest&#34;</span>
          <span style="color:#f92672">command</span>:
            - <span style="color:#ae81ff">/opt/keycloak-gatekeeper</span>
            - --<span style="color:#ae81ff">discovery-url=https://keycloak.devopstales.intra/auth/realms/mydomain/.well-known/openid-configuration</span>
            - --<span style="color:#ae81ff">client-id=k8s</span>
            - --<span style="color:#ae81ff">client-secret=43219919-0904-4338-bc0f-c986e1891a7a</span>
            - --<span style="color:#ae81ff">listen=0.0.0.0:3000</span>
            - --<span style="color:#ae81ff">encryption-key=AgXa7xRcoClDEU0ZDSH4X0XhL5Qy2Z2j</span>
            - --<span style="color:#ae81ff">redirection-url=https://dashboard.devopstales.intra</span>
            - --<span style="color:#ae81ff">enable-refresh-tokens=true</span>
            - --<span style="color:#ae81ff">upstream-url=https://kubernetes-dashboard</span>
            <span style="color:#75715e"># debug:</span>
            <span style="color:#75715e">#- --upstream-url=http://echo:8080</span>
            <span style="color:#75715e"># for self sign cert or custom ca</span>
            <span style="color:#75715e">#- --skip-upstream-tls-verify</span>
            <span style="color:#75715e">#- --skip-openid-provider-tls-verify</span>
          <span style="color:#f92672">ports</span>:
            - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">http</span>
              <span style="color:#f92672">containerPort</span>: <span style="color:#ae81ff">3000</span>
              <span style="color:#f92672">protocol</span>: <span style="color:#ae81ff">TCP</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Service</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">dasboard-proxy</span>
  <span style="color:#f92672">labels</span>:
    <span style="color:#f92672">app.kubernetes.io/name</span>: <span style="color:#ae81ff">dasboard-proxy</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">kubernetes-dashboard</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">type</span>: <span style="color:#ae81ff">ClusterIP</span>
  <span style="color:#f92672">ports</span>:
    - <span style="color:#f92672">port</span>: <span style="color:#ae81ff">3000</span>
      <span style="color:#f92672">targetPort</span>: <span style="color:#ae81ff">http</span>
      <span style="color:#f92672">protocol</span>: <span style="color:#ae81ff">TCP</span>
      <span style="color:#f92672">name</span>: <span style="color:#ae81ff">http</span>
  <span style="color:#f92672">selector</span>:
    <span style="color:#f92672">app.kubernetes.io/name</span>: <span style="color:#ae81ff">dasboard-proxy</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">extensions/v1beta1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Ingress</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">dasboard-proxy</span>
  <span style="color:#f92672">annotations</span>:
    <span style="color:#f92672">kubernetes.io/ingress.class</span>: <span style="color:#ae81ff">nginx</span>
    <span style="color:#f92672">nginx.ingress.kubernetes.io/proxy-buffer-size</span>: <span style="color:#e6db74">&#34;64k&#34;</span>
    <span style="color:#f92672">cert-manager.io/cluster-issuer</span>: <span style="color:#ae81ff">ca-issuer</span>
  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">kubernetes-dashboard</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">tls</span>:
    - <span style="color:#f92672">hosts</span>:
        - <span style="color:#ae81ff">dashboard.devopstales.intra</span>
      <span style="color:#f92672">secretName</span>: <span style="color:#ae81ff">dasboard-proxy-tls</span>
  <span style="color:#f92672">rules</span>:
    - <span style="color:#f92672">host</span>: <span style="color:#ae81ff">dashboard.devopstales.intra</span>
      <span style="color:#f92672">http</span>:
        <span style="color:#f92672">paths</span>:
          - <span style="color:#f92672">backend</span>:
             <span style="color:#f92672">serviceName</span>: <span style="color:#ae81ff">dasboard-proxy</span>
             <span style="color:#f92672">servicePort</span>: <span style="color:#ae81ff">3000</span>
</code></pre></div><p>Now you can login at dashboard.devopstales.intra but you haven&rsquo;t got any privileges so lets create. some.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano devops-team_ClusterRoleBinding.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRoleBinding</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cluster-admin-it-afdeling</span>
<span style="color:#f92672">roleRef</span>:
  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRole</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cluster-admin</span>
<span style="color:#f92672">subjects</span>:
  - <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
    <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Group</span>
    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">devops-team</span>
</code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#ae81ff">nano user_ClusterRoleBinding.yaml</span>
---
<span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRoleBinding</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">devopstales-admin</span>
<span style="color:#f92672">subjects</span>:
  - <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">User</span>
    <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">devopstales</span>
<span style="color:#f92672">roleRef</span>:
  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRole</span>
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cluster-admin</span>
</code></pre></div>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Grafana Loki with Helm3]]></title>
            <link href="https://devopstales.github.io/home/helm3-loki/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/grafana-loki/?utm_source=atom_feed" rel="related" type="text/html" title="Grafana Loki" />
                <link href="https://devopstales.github.io/kubernetes/helm3-loki/?utm_source=atom_feed" rel="related" type="text/html" title="Install Grafana Loki with Helm3" />
                <link href="https://devopstales.github.io/monitoring/grafana-loki/?utm_source=atom_feed" rel="related" type="text/html" title="Grafana Loki" />
                <link href="https://devopstales.github.io/home/graylog-pfsense-squid/?utm_source=atom_feed" rel="related" type="text/html" title="Analyzing PFsense squid logs in Graylog" />
                <link href="https://devopstales.github.io/home/k8s-ceph-storage-with-csi-driver/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD volume with CSI driver" />
            
                <id>https://devopstales.github.io/home/helm3-loki/</id>
            
            
            <published>2020-01-03T00:00:00+00:00</published>
            <updated>2020-01-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Helm is a template based package management system for kubernetes applications.</p>
<h3 id="wath-is-new-in-helm3">Wath is new in Helm3</h3>
<p>The most important change in Helm3, tiller was removed completely. Tiller was the server component (rinninf in a pod on the Kubernetes cluster) for helm&rsquo;s cli. When Helm 2 was developed, Kubernetes did not yet have role-based access control (RBAC) therefore to achieve mentioned goal, Helm had to take care of that itself. After Kubernetes 1.6 RBAC is enabled by default so you had to create a serviceaccount for tiller. With Tiller gone, Helm permissions are now simply evaluated using kubeconfig file.</p>
<p>Tiller was also used as a central hub for Helm release information and for maintaining the Helm state. In Helm 3 the same information are fetched directly from Kubernetes API Server and Charts are rendered client-side.</p>
<p>Helm 2 stored the informations of the releases in configmaps now in Helm 3 that is stored in secrets for better security.</p>
<h3 id="install-chart-with-helm3">Install Chart with Helm3</h3>
<p>The removal of Tiller means you didn&rsquo;t need a <code>helm init</code> for initializing the tiller.</p>
<pre tabindex="0"><code>helm repo add loki https://grafana.github.io/loki/charts
helm repo add stable https://kubernetes-charts.storage.googleapis.com
helm repo update

kubectl create namespace loki-stackhtop

helm upgrade --install loki --namespace=loki-stack loki/loki-stack
elm3 upgrade --install grafana --namespace=loki-stack stable/grafana
</code></pre><p>Namespaces are important now. <code>helm ls</code> won’t show anything, we have to specify the namespace with it:</p>
<pre tabindex="0"><code>helm -n loki-stack ls
</code></pre><h3 id="access-grafana-interface">Access Grafana Interface</h3>
<pre tabindex="0"><code>kubectl get secret -n loki-stack grafana -o jsonpath=&quot;{.data.admin-password}&quot; | base64 --decode
kubectl port-forward -n loki-stack service/grafana 3000:80

# go to localhost:3000
# add loki datasource URL http://loki:3100 and press Save &amp; Test
</code></pre><p><img src="/img/include/loki_1.png" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/loki_2.png" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/loki_3.png" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/loki_4.png" alt="Example image"  class="zoomable" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Backup your Kubernetes Cluster with Velero]]></title>
            <link href="https://devopstales.github.io/home/k8s-velero-backup/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-velero-backup/?utm_source=atom_feed" rel="related" type="text/html" title="Backup your Kubernetes Cluster with Velero" />
                <link href="https://devopstales.github.io/home/k8s-cert-manager/?utm_source=atom_feed" rel="related" type="text/html" title="Install cert-manager for Kubernetes" />
                <link href="https://devopstales.github.io/home/kind-install/?utm_source=atom_feed" rel="related" type="text/html" title="Starting local Kubernetes using kind" />
                <link href="https://devopstales.github.io/home/k8s-metallb/?utm_source=atom_feed" rel="related" type="text/html" title="Install MetalLB load balancer for Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-tillerless-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Tillerless helm2 install" />
            
                <id>https://devopstales.github.io/home/k8s-velero-backup/</id>
            
            
            <published>2020-01-02T00:00:00+00:00</published>
            <updated>2020-01-02T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Velero (formerly Heptio Ark) gives you tools to back up and restore your Kubernetes cluster resources and persistent volumes. You can run Velero with a cloud provider or on-premises.</p>
<H3>Parst of the Kubernetes series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
     <li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
     <li>Part1c: <a href="../../kubernetes/k8s-install-containerd/">Install K8S with containerd and kubeadm</a></li>
     <li>Part1d: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
     <li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a></li>
<!--      <li>Part3b: k8s-multiple-nginx-ingress -->
<!--      <li>Part3c: k8s-contour-ingress -->
     <li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a></li>
<!-- local folder (+ autoprovisioning ) -->
     <li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
     <li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
     <li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
     <li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
     <li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
     <li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a></li>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... -->
     <li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
     <li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
     <li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
     <li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
     <li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
     <li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
     <li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a></li>
<!-- istio-install.md -->
     <li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a></li>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog -->
</ul>


<h3 id="how-its-work">How it&rsquo;s work</h3>
<p>Each Velero operation (on-demand backup, scheduled backup, restore) is a custom resource, stored in etcd. A backup opertaion is uploads a tarball of copied Kubernetes objects into cloud object storage. After that calls the cloud provider API to make disk snapshots of persistent volumes, if specified. Optionally you can specify hooks to be executed during the backup. When you create a backup, you can specify a TTL by adding the flag <code>--ttl &lt;DURATION&gt;</code>.</p>
<h3 id="velero-supported-providers">Velero supported providers</h3>
<table>
<thead>
<tr>
<th>Object Store</th>
<th>Volume Snapshotter</th>
</tr>
</thead>
<tbody>
<tr>
<td>AWS S3</td>
<td>AWS EBS</td>
</tr>
<tr>
<td>Google Cloud Storage</td>
<td>Google Compute Engine Disks</td>
</tr>
<tr>
<td>Azure Blob Storage</td>
<td>Azure Managed Disks</td>
</tr>
<tr>
<td>-</td>
<td>Portworx Volume</td>
</tr>
<tr>
<td>-</td>
<td>OpenEBS CStor Volume</td>
</tr>
</tbody>
</table>
<h3 id="install-cli">Install cli</h3>
<pre tabindex="0"><code>wget https://github.com/vmware-tanzu/velero/releases/download/v1.2.0/velero-v1.2.0-linux-amd64.tar.gz
tar -xzf velero-v1.2.0-linux-amd64.tar.gz
sudo cp velero-v1.2.0-linux-amd64/velero /usr/local/sbin
</code></pre><h2 id="deploy-minio-and-deno-app">Deploy minio and deno app</h2>
<pre tabindex="0"><code>kubctl apply -f velero-v1.2.0-linux-amd64/examples/minio/00-minio-deployment.yaml
kubctl apply -f velero-v1.2.0-linux-amd64/examples/nginx-app/base.yaml
</code></pre><h3 id="deploy-server-component">Deploy server component</h3>
<pre tabindex="0"><code>nano velero.yaml
image:
  repository: velero/velero
  tag: v1.2.0
  pullPolicy: IfNotPresent

initContainers:
  - name: aws
    image: velero/velero-plugin-for-aws:v1.0.0
    imagePullPolicy: IfNotPresent
    volumeMounts:
      - mountPath: /target
        name: plugins

metrics:
  enabled: true
  scrapeInterval: 30s

  # Pod annotations for Prometheus
  podAnnotations:
    prometheus.io/scrape: &quot;true&quot;
    prometheus.io/port: &quot;8085&quot;
    prometheus.io/path: &quot;/metrics&quot;

  serviceMonitor:
    enabled: false
    additionalLabels: {}



configuration:
  provider: aws
  backupStorageLocation:
    name: aws
    bucket: velero
    config:
      region: minio
      s3ForcePathStyle: true
      publicUrl: https://minio.devopstales.intra
      s3Url: http://minio:9000
  volumeSnapshotLocation:
    name: aws
    bucket: kubernetes-pv
    config:
      region: minio
      s3ForcePathStyle: true
      publicUrl: https://minio.devopstales.intra
      s3Url: http://minio:9000

credentials:
  useSecret: true
  secretContents:
    cloud: |
      [default]
      aws_access_key_id = minio
      aws_secret_access_key = minio123

snapshotsEnabled: true
deployRestic: true
</code></pre><pre tabindex="0"><code>helm repo add vmware-tanzu https://vmware-tanzu.github.io/helm-charts
helm repo update

helm install velero vmware-tanzu/velero --namespace velero -f velero.yaml
</code></pre><h3 id="create-backup">Create Backup</h3>
<pre tabindex="0"><code>velero backup create nginx-backup --selector app=nginx
velero backup describe nginx-backup
velero backup logs nginx-backup
velero backup get

velero schedule create nginx-daily --schedule=&quot;0 1 * * *&quot; --selector app=nginx
velero schedule get
velero backup get
</code></pre><h3 id="restore-test">Restore test</h3>
<pre tabindex="0"><code>kubectl delete ns nginx-example

velero restore create --from-backup nginx-backup
velero restore get

kubectl get po -n nginx-example
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install cert-manager for Kubernetes]]></title>
            <link href="https://devopstales.github.io/home/k8s-cert-manager/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/kind-install/?utm_source=atom_feed" rel="related" type="text/html" title="Starting local Kubernetes using kind" />
                <link href="https://devopstales.github.io/home/k8s-metallb/?utm_source=atom_feed" rel="related" type="text/html" title="Install MetalLB load balancer for Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-tillerless-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Tillerless helm2 install" />
                <link href="https://devopstales.github.io/home/k8s-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/k8s-nginx-ingress/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
            
                <id>https://devopstales.github.io/home/k8s-cert-manager/</id>
            
            
            <published>2019-12-29T00:00:00+00:00</published>
            <updated>2019-12-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this tutorial I will show you how to install  cert-manager running on Kubernetes (k8s).</p>
<H3>Parst of the Kubernetes series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
     <li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
     <li>Part1c: <a href="../../kubernetes/k8s-install-containerd/">Install K8S with containerd and kubeadm</a></li>
     <li>Part1d: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
     <li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a></li>
<!--      <li>Part3b: k8s-multiple-nginx-ingress -->
<!--      <li>Part3c: k8s-contour-ingress -->
     <li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a></li>
<!-- local folder (+ autoprovisioning ) -->
     <li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
     <li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
     <li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
     <li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
     <li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
     <li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a></li>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... -->
     <li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
     <li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
     <li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
     <li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
     <li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
     <li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
     <li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a></li>
<!-- istio-install.md -->
     <li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a></li>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog -->
</ul>


<p>cert-manager is a native Kubernetes certificate management controller. It can help with issuing certificates from a variety of sources, such as Let’s Encrypt, HashiCorp Vault, Venafi, a simple signing key pair, or self signed.</p>
<h3 id="install-cert-managger">Install cert-managger</h3>
<p>In order to install cert-manager, we must first create a namespace to run it in.</p>
<pre tabindex="0"><code>kubectl create namespace cert-manager
</code></pre><p>Install the <code>CustomResourceDefinitions</code> and cert-manager itself</p>
<pre tabindex="0"><code>kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.12.0/cert-manager.yaml
</code></pre><p>Verifying the installation</p>
<pre tabindex="0"><code>kubectl get pods --namespace cert-manager

NAME                                       READY   STATUS    RESTARTS   AGE
cert-manager-5c6866597-zw7kh               1/1     Running   0          2m
cert-manager-cainjector-577f6d9fd7-tr77l   1/1     Running   0          2m
cert-manager-webhook-787858fcdb-nlzsq      1/1     Running   0          2m
</code></pre><h3 id="create-a-clusterissuer">Create a ClusterIssuer</h3>
<p>Before you can begin issuing certificates, you must configure at least one <code>Issuer</code> or <code>ClusterIssuer</code> resource in your cluster. These resources represent a particular signing authority and detail how the certificate requests are going to be honored. For this Demo I will use my own CA as an Issuer.</p>
<pre tabindex="0"><code>cat issuer.yaml
apiVersion: cert-manager.io/v1alpha2
kind: ClusterIssuer
metadata:
  name: ca-issuer
  namespace: cert-manager
spec:
  ca:
    secretName: ca-key-pair

kubectl apply -f issuer.yaml
</code></pre><p>In order to create my certs, I must submit my CA certificate and singing private key to the Kubernetes Cluster so that cert-manager is able to use them and sign certificates.</p>
<pre tabindex="0"><code>cat  rootCA.key | base64
LS0tLS1CRUdJTiB...
cat rootCA.crt | base64
LS0tLSD5DUdJTiB...

cat ca-key-pair.yaml
apiVersion: v1
kind: Secret
metadata:
  name: ca-key-pair
  namespace: cert-manager
data:
  tls.key: LS0tLS1CRUdJTiB...
  tls.crt: LS0tLSD5DUdJTiB...

kubectl apply -f ca-key-pair.yaml
</code></pre><h3 id="demo">demo</h3>
<p>Create cert for test</p>
<pre tabindex="0"><code>cat test-resources.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: cert-manager-test
---
apiVersion: cert-manager.io/v1alpha2
kind: Issuer
metadata:
  name: test-selfsigned
  namespace: cert-manager-test
spec:
  selfSigned: {}
---
apiVersion: cert-manager.io/v1alpha2
kind: Certificate
metadata:
  name: selfsigned-cert
  namespace: cert-manager-test
spec:
  commonName: example.com
  secretName: selfsigned-cert-tls
  issuerRef:
    name: test-selfsigned
---
apiVersion: cert-manager.io/v1alpha2
kind: Certificate
metadata:
  name: ca-cert
  namespace: cert-manager-test
spec:
  commonName: example.com
  secretName: ca-cert-tls
  issuerRef:
    name: ca-issuer
    kind: ClusterIssuer

kubectl apply -f test-resources.yaml
</code></pre><pre tabindex="0"><code>kubectl describe certificate -n cert-manager-test

...
Spec:
  Common Name:  example.com
  Issuer Ref:
    Name:       test-selfsigned
  Secret Name:  selfsigned-cert-tls
Status:
  Conditions:
    Last Transition Time:  2019-12-29T17:34:30Z
    Message:               Certificate is up to date and has not expired
    Reason:                Ready
    Status:                True
    Type:                  Ready
  Not After:               2019-12-29T17:34:29Z
Events:
  Type    Reason      Age   From          Message
  ----    ------      ----  ----          -------
  Normal  CertIssued  4s    cert-manager  Certificate issued successfully
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift secondary route]]></title>
            <link href="https://devopstales.github.io/home/openshift-secondary-router/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-gitlab-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Installing GitLab on OpenShift" />
                <link href="https://devopstales.github.io/home/openshift-kompose/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Run docker-compoe in Openshift" />
                <link href="https://devopstales.github.io/home/openshift-add-node/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Add Nodes to a Cluster" />
                <link href="https://devopstales.github.io/home/openshift-letsencrypt/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Letsencrypt certificates" />
                <link href="https://devopstales.github.io/home/openshift-hostalreadyclaimed/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Error: HostAlreadyClaimed" />
            
                <id>https://devopstales.github.io/home/openshift-secondary-router/</id>
            
            
            <published>2019-12-20T00:00:00+00:00</published>
            <updated>2019-12-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I this tutorial I will show you how to create a secondari router for Openshift.</p>
<H3>Parst of the Openshift series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
     <li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
     <li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
     <li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
     <li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
     <li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
     <li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
     <li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
     <li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
     <li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
     <li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
     <li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
     <li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
     <li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
     <li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
     <li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
     <li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
     <li>Part16a: <a href="../../kubernetes/openshift-elasticsearch-error/">Opeshift elasticsearch search-guard error</a></li>
     <li>Part16b: <a href="../../kubernetes/openshift-log4shell/">Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)</a></li>
</ul>


<h3 id="environment">Environment</h3>
<pre tabindex="0"><code>192.168.1.40    deployer
192.168.1.41    openshift01 # master node
192.168.1.42    openshift02 # infra node
192.168.1.43    openshift03 # worker node with second router
192.168.1.44    openshift04 # worker node
</code></pre><h3 id="deploy-route">Deploy route</h3>
<pre tabindex="0"><code>oc adm router router-public --replicas=2 --ports=&quot;8080:8080,8443:8443&quot; \
--stats-port=1937 --selector=&quot;router=public&quot; --labels=&quot;router=public&quot;

oc set env dc/router-public \
DEFAULT_CERTIFICATE_PATH=/etc/pki/tls/private/tls.crt \
NAMESPACE_LABELS=&quot;router=public&quot; \
ROUTER_ALLOW_WILDCARD_ROUTES=true \
ROUTER_ENABLE_HTTP2=true \
ROUTER_HAPROXY_CONFIG_MANAGER=true \
ROUTER_SERVICE_HTTP_PORT=8080 \
ROUTER_SERVICE_HTTPS_PORT=8443 \
ROUTER_TCP_BALANCE_SCHEME=roundrobin

oc label node openshift03 &quot;router=public&quot;
</code></pre><p>Configurate your firewall to create a NAT rule from publicIP:80 to openshift03:8080 and publicIP:443 to openshift03:8443</p>
<h3 id="demo">Demo</h3>
<pre tabindex="0"><code>oc new-project test
oc label namespace test router=public
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Starting local Kubernetes using kind]]></title>
            <link href="https://devopstales.github.io/home/kind-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-metallb/?utm_source=atom_feed" rel="related" type="text/html" title="Install MetalLB load balancer for Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-tillerless-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Tillerless helm2 install" />
                <link href="https://devopstales.github.io/home/k8s-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/k8s-nginx-ingress/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
                <link href="https://devopstales.github.io/home/k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install kubernetes with kubeadm" />
            
                <id>https://devopstales.github.io/home/kind-install/</id>
            
            
            <published>2019-12-20T00:00:00+00:00</published>
            <updated>2019-12-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this article, I will show you how to run a cluster in single Docker container using kind.</p>
<h3 id="what-is-kind">What is kind?</h3>
<p>Kind (Kubernetes IN Docker) is a tool to start kubernetes nodes as a docker container. It is a cross-platform tool you can run with Docker for Windows too.</p>
<h3 id="install-kind-binary">Install kind binary</h3>
<pre tabindex="0"><code>wget https://github.com/kubernetes-sigs/kind/releases/latest/download/kind-linux-amd64
chmod +x kind-linux-amd64
sudo mv kind-linux-amd64 /usr/local/sbin/kind
</code></pre><h3 id="start-a-cluster-for-ingress">Start a cluster for Ingress</h3>
<pre tabindex="0"><code>cat &lt;&lt;EOF | kind create cluster --config=-
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  kubeadmConfigPatches:
  - |
    apiVersion: kubeadm.k8s.io/v1beta2
    kind: InitConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        node-labels: &quot;ingress-ready=true&quot;
        authorization-mode: &quot;AlwaysAllow&quot;
  extraPortMappings:
  - containerPort: 80
    hostPort: 80
  - containerPort: 443
    hostPort: 443
EOF
</code></pre><h3 id="install-ingress">Install ingress</h3>
<pre tabindex="0"><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml

# patch ingress for kind
kubectl patch deployments -n ingress-nginx nginx-ingress-controller -p '{&quot;spec&quot;:{&quot;template&quot;:{&quot;spec&quot;:{&quot;containers&quot;:[{&quot;name&quot;:&quot;nginx-ingress-controller&quot;,&quot;ports&quot;:[{&quot;containerPort&quot;:80,&quot;hostPort&quot;:80},{&quot;containerPort&quot;:443,&quot;hostPort&quot;:443}]}],&quot;nodeSelector&quot;:{&quot;ingress-ready&quot;:&quot;true&quot;}}}}}'
</code></pre><h3 id="demo">Demo</h3>
<pre tabindex="0"><code>kubectl apply -f https://kind.sigs.k8s.io/examples/ingress/usage.yaml

curl localhost/foo
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install icinga director modules to Icingaweb2]]></title>
            <link href="https://devopstales.github.io/home/icinga2_director/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/icinga2_nrpe/?utm_source=atom_feed" rel="related" type="text/html" title="Install nrpe tp Icinga2" />
                <link href="https://devopstales.github.io/home/icinga2_add_host/?utm_source=atom_feed" rel="related" type="text/html" title="Add host to Icinga2" />
                <link href="https://devopstales.github.io/home/icinga2_install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Icinga2" />
                <link href="https://devopstales.github.io/monitoring/icinga2_director/?utm_source=atom_feed" rel="related" type="text/html" title="Install icinga director modules to Icingaweb2" />
                <link href="https://devopstales.github.io/monitoring/icinga2_nrpe/?utm_source=atom_feed" rel="related" type="text/html" title="Install nrpe tp Icinga2" />
            
                <id>https://devopstales.github.io/home/icinga2_director/</id>
            
            
            <published>2019-12-13T00:00:00+00:00</published>
            <updated>2019-12-13T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this tutorial I will show you how to install Icingaweb2 module director.</p>
<p>Icinga irector is designed for those who want to automate their configuration deployment and those who want to grant easy access for there users to the Icinga2 configuration.</p>
<h3 id="install-dependency">Install dependency</h3>
<pre tabindex="0"><code>yum install git -y
yum install rh-php71-php-curl rh-php71-php-pcntl rh-php71-php-posix rh-php71-php-sockets rh-php71-php-xml rh-php71-php-zip -y
</code></pre><pre tabindex="0"><code>nano
local   director      director                        md5
host    director      director      127.0.0.1/32      md5
host    director      director      ::1/128           md5

systemctl restart postgresql-10
</code></pre><h3 id="install-icingaweb2-modules">Install Icingaweb2 modules</h3>
<pre tabindex="0"><code>MODULE_NAME=ipl
MODULE_VERSION=v0.4.0
REPO=&quot;https://github.com/Icinga/icingaweb2-module-${MODULE_NAME}&quot;
MODULES_PATH=&quot;/usr/share/icingaweb2/modules&quot;
git clone ${REPO} &quot;${MODULES_PATH}/${MODULE_NAME}&quot; --branch &quot;${MODULE_VERSION}&quot;
icingacli module enable &quot;${MODULE_NAME}&quot;

MODULE_NAME=incubator
MODULE_VERSION=v0.5.0
REPO=&quot;https://github.com/Icinga/icingaweb2-module-${MODULE_NAME}&quot;
MODULES_PATH=&quot;/usr/share/icingaweb2/modules&quot;
git clone ${REPO} &quot;${MODULES_PATH}/${MODULE_NAME}&quot; --branch &quot;${MODULE_VERSION}&quot;
icingacli module enable &quot;${MODULE_NAME}&quot;

MODULE_NAME=reactbundle
MODULE_VERSION=v0.7.0
REPO=&quot;https://github.com/Icinga/icingaweb2-module-${MODULE_NAME}&quot;
MODULES_PATH=&quot;/usr/share/icingaweb2/modules&quot;
git clone ${REPO} &quot;${MODULES_PATH}/${MODULE_NAME}&quot; --branch &quot;${MODULE_VERSION}&quot;
icingacli module enable &quot;${MODULE_NAME}&quot;

ICINGAWEB_MODULEPATH=&quot;/usr/share/icingaweb2/modules&quot;
REPO_URL=&quot;https://github.com/icinga/icingaweb2-module-director&quot;
TARGET_DIR=&quot;${ICINGAWEB_MODULEPATH}/director&quot;
MODULE_VERSION=&quot;1.7.2&quot;
git clone &quot;${REPO_URL}&quot; &quot;${TARGET_DIR}&quot;
cd &quot;${TARGET_DIR}&quot;
git fetch &amp;&amp; git fetch --tags
git checkout &quot;{MODULE_VERSION}&quot;
restorecon -R &quot;${TARGET_DIR}&quot;

MODULE_NAME=fileshipper
MODULE_VERSION=v1.1.0
REPO=&quot;https://github.com/Icinga/icingaweb2-module-${MODULE_NAME}&quot;
MODULES_PATH=&quot;/usr/share/icingaweb2/modules&quot;
git clone ${REPO} &quot;${MODULES_PATH}/${MODULE_NAME}&quot; --branch &quot;${MODULE_VERSION}&quot;
icingacli module enable &quot;${MODULE_NAME}&quot;
</code></pre><h3 id="create-db-for-director">Create db for director</h3>
<pre tabindex="0"><code>sudo -u postgres psql -c &quot;CREATE DATABASE director WITH ENCODING 'utf8';&quot;
sudo -u postgres psql director -q -c &quot;CREATE USER director WITH PASSWORD 'director';
GRANT ALL PRIVILEGES ON DATABASE director TO director;
CREATE EXTENSION pgcrypto;&quot;
sudo -u postgres psql director &lt; /usr/share/icingaweb2/modules/director/schema/pgsql.sql
</code></pre><h3 id="edit-director-configuration">Edit director configuration</h3>
<pre tabindex="0"><code>cat &lt;&lt;EOF &gt;&gt; /etc/icinga2/zones.conf

object Zone &quot;director-global&quot; {
  global = true
}
EOF

cat &lt;&lt;EOF &gt;&gt; /etc/icinga2/conf.d/api-users.conf

object ApiUser &quot;director&quot; {
        password = &quot;director&quot;
        permissions = [ &quot;*&quot; ]
}
EOF

cat &lt;&lt;EOF &gt;&gt; /etc/icingaweb2/resources.ini

[icinga_director]
type = &quot;db&quot;
db = &quot;pgsql&quot;
host = &quot;localhost&quot;
port = &quot;5432&quot;
dbname = &quot;director&quot;
username = &quot;director&quot;
password = &quot;director&quot;
charset = &quot;utf8&quot;
use_ssl = &quot;0&quot;
EOF

mkdir /etc/icingaweb2/modules/director/
cat &lt;&lt;EOF &gt;&gt; /etc/icingaweb2/modules/director/config.ini
[db]
resource = &quot;icinga_director&quot;
EOF

cat &lt;&lt;EOF &gt;&gt; /etc/icingaweb2/modules/director/kickstart.ini
[config]
endpoint = icinga.devopstales.intra
host = 127.0.0.1
port = 5665
username = director
password = director
EOF
</code></pre><pre tabindex="0"><code>icingacli module enable director
icingacli director kickstart run
icingacli director migration run --verbose
icingacli director migration pending --verbose
</code></pre><pre tabindex="0"><code>mkdir /etc/icingaweb2/modules/fileshipper
cat &lt;&lt;EOF &gt;&gt; /etc/icingaweb2/modules/fileshipper/imports.ini
[icinga2 - groups]
basedir = &quot;/etc/icinga2/conf.d/groups/

[icinga2 - hosts]
basedir = &quot;/etc/icinga2/conf.d/hosts/
EOF
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install nrpe tp Icinga2]]></title>
            <link href="https://devopstales.github.io/home/icinga2_nrpe/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/icinga2_add_host/?utm_source=atom_feed" rel="related" type="text/html" title="Add host to Icinga2" />
                <link href="https://devopstales.github.io/home/icinga2_install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Icinga2" />
                <link href="https://devopstales.github.io/monitoring/icinga2_nrpe/?utm_source=atom_feed" rel="related" type="text/html" title="Install nrpe tp Icinga2" />
                <link href="https://devopstales.github.io/home/nagios-nrpe/?utm_source=atom_feed" rel="related" type="text/html" title="Nagios Remote Plugin Executor" />
                <link href="https://devopstales.github.io/monitoring/icinga2_add_host/?utm_source=atom_feed" rel="related" type="text/html" title="Add host to Icinga2" />
            
                <id>https://devopstales.github.io/home/icinga2_nrpe/</id>
            
            
            <published>2019-12-12T00:00:00+00:00</published>
            <updated>2019-12-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this tutorial I will show you how to nrpe check in Icinga2.</p>
<h3 id="install-nrpe-on-the-clients">install nrpe on the clients</h3>
<pre tabindex="0"><code>yum install nrpe nagios-plugins-all -y

firewall-cmd --permanent --add-port=5665/tcp
firewall-cmd --reload
</code></pre><h3 id="install-nrpe-on-the-server">install nrpe on the server</h3>
<pre tabindex="0"><code>yum install nrpe nagios-plugins-all -y

firewall-cmd --permanent --add-port=5665/tcp
firewall-cmd --reload
</code></pre><h3 id="configurate-icinga-to-use-nrpe">configurate icinga to use nrpe</h3>
<pre tabindex="0"><code>vi /etc/icinga2/conf.d/linux_services.conf
apply Service &quot;nrpe-disk-root&quot; {
  import &quot;generic-service&quot;
  check_command = &quot;nrpe&quot;
  vars.nrpe_command = &quot;check_disk&quot;
  vars.nrpe_arguments = [ &quot;20%&quot;, &quot;10%&quot;, &quot;/&quot; ]
  assign where &quot;linux-servers&quot; in host.groups
  ignore where match(&quot;*icinga*&quot;, host.name)
}

vi /etc/icinga2/conf.d/CLIENTS/server1.conf
object Host &quot;server1&quot; {
  address = &quot;192.168.10.60&quot;
  check_command = &quot;ping&quot;
  vars.os = &quot;Linux&quot;
}
</code></pre><p>Test the config and restart:</p>
<pre tabindex="0"><code>icinga2 daemon -C
systemctl restart icinga2
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Add host to Icinga2]]></title>
            <link href="https://devopstales.github.io/home/icinga2_add_host/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/icinga2_install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Icinga2" />
                <link href="https://devopstales.github.io/monitoring/icinga2_add_host/?utm_source=atom_feed" rel="related" type="text/html" title="Add host to Icinga2" />
                <link href="https://devopstales.github.io/monitoring/icinga2_install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Icinga2" />
                <link href="https://devopstales.github.io/home/nagios-ncpa/?utm_source=atom_feed" rel="related" type="text/html" title="Nagios Cross Platform Agent" />
                <link href="https://devopstales.github.io/home/nagios-nrpe/?utm_source=atom_feed" rel="related" type="text/html" title="Nagios Remote Plugin Executor" />
            
                <id>https://devopstales.github.io/home/icinga2_add_host/</id>
            
            
            <published>2019-12-11T00:00:00+00:00</published>
            <updated>2019-12-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this tutorial I will show you how to add hosts to Icinga2.</p>
<h3 id="create-new-host">Create new host</h3>
<pre tabindex="0"><code>nano /etc/icinga2/conf.d/my_router.conf
object Host &quot;Router&quot; {
  address = &quot;192.168.1.1&quot;
  check_command = &quot;hostalive&quot;
}

</code></pre><p>If we want to check the web admin interface of the router we can add a http check to see if the HTTP server is alive and responds with the proper HTTP codes</p>
<pre tabindex="0"><code>nano /etc/icinga2/conf.d/my_router.conf
...
object Service &quot;http&quot; {
  host_name = &quot;Router&quot;
  check_command = &quot;http&quot;
}
</code></pre><h3 id="ad-custom-check">Ad custom check</h3>
<p>We will us the check_udpport script. This is the syntax.</p>
<pre tabindex="0"><code>/usr/lib64/nagios/plugins/check_udpport –H 127.0.0.1 –p 69
</code></pre><p>Now we need to create a nwe command in icinga2:</p>
<pre tabindex="0"><code>nano /etc/icinga2/conf.d/commands.conf
object CheckCommand &quot;myudp&quot; {
  command = [ PluginDir + &quot;/check_udpport&quot; ]

    arguments = {
    &quot;-H&quot; = &quot;$addr$&quot;
    &quot;-p&quot; = &quot;$port$&quot;
  }
  vars.addr = &quot;$address$&quot;
}
</code></pre><p>We have a nwe command so we can create a service to use rhis command:</p>
<pre tabindex="0"><code>nano /etc/icinga2/conf.d/my_router.conf
...
object Service &quot;dhcp&quot; {
  host_name = Router&quot;
  check_command = &quot;myudp&quot;
vars.port = &quot;67&quot;
}
</code></pre><p>Test the config and restart:</p>
<pre tabindex="0"><code>icinga2 daemon -C
systemctl restart icinga2
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configure Rundeck ACL]]></title>
            <link href="https://devopstales.github.io/home/rundeck-acl/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/rundeck-acl/</id>
            
            
            <published>2019-12-10T00:00:00+00:00</published>
            <updated>2019-12-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will configure access control in Rundeck.</p>
<h3 id="configurate-ad-groups-in-rundeck">Configurate AD groups in rundeck</h3>
<p>The modification of the <code>web.xml</code> no longer needed after 3.0.x.</p>
<pre tabindex="0"><code>nano /var/lib/rundeck/exp/webapp/WEB-INF/web.xml
        &lt;security-role&gt;
               &lt;role-name&gt;rundeck-administrators&lt;/role-name&gt;
               &lt;role-name&gt;rundeck-project&lt;/role-name&gt;
        &lt;/security-role&gt;
</code></pre><h3 id="configure-the-privilege-for-ad-group">Configure the privilege for AD group</h3>
<pre tabindex="0"><code>nano /etc/rundec/admin.aclpolicy

description: Admin, all access.
context:
  project: '.*' # all projects
for:
  resource:
    - allow: '*' # allow read/create all kinds
  adhoc:
    - allow: '*' # allow read/running/killing adhoc jobs
  job:
    - allow: '*' # allow read/write/delete/run/kill of all jobs
  node:
    - allow: '*' # allow read/run for all nodes
by:
  group: rundeck-administrators

---

description: Admin, all access.
context:
  application: 'rundeck'
for:
  resource:
    - allow: '*' # allow create of projects
  project:
    - allow: '*' # allow view/admin of all projects
  project_acl:
    - allow: '*' # allow admin of all project-level ACL policies
  storage:
    - allow: '*' # allow read/create/update/delete for all /keys/* storage content
by:
  group: rundeck-administrators
---

description: rundeck-project  PROJECT all access.
context:
  project: 'PROJECT'
for:
  resource:
    - allow: '*' # allow read/create all kinds
  adhoc:
    - allow: '*' # allow read/running/killing adhoc jobs
  job:
    - allow: '*' # allow read/write/delete/run/kill of all jobs
  node:
    - allow: '*' # allow read/run for all nodes
by:
  group: rundeck-project

---

description: rundeck-project, all access.
context:
  application: 'rundeck'
for:
  project:
    - match:
        name: 'PROJECT'
      allow: [read]
  system:
    - match:
        name: '.*'
      allow: [read]
  storage:
    - equals:
        path: 'keys'
      allow: [read]
    - match:
        path: 'keys/id_rsa*'
      allow: [read]
by:
  group: rundeck-project
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Icinga2]]></title>
            <link href="https://devopstales.github.io/home/icinga2_install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/monitoring/icinga2_install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Icinga2" />
                <link href="https://devopstales.github.io/home/nagios-ncpa/?utm_source=atom_feed" rel="related" type="text/html" title="Nagios Cross Platform Agent" />
                <link href="https://devopstales.github.io/home/nagios-nrpe/?utm_source=atom_feed" rel="related" type="text/html" title="Nagios Remote Plugin Executor" />
                <link href="https://devopstales.github.io/home/centreon-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Centreon on Centos 7" />
                <link href="https://devopstales.github.io/home/naemon-influxdb/?utm_source=atom_feed" rel="related" type="text/html" title="Install Nemon with Influxdb storage" />
            
                <id>https://devopstales.github.io/home/icinga2_install/</id>
            
            
            <published>2019-12-10T00:00:00+00:00</published>
            <updated>2019-12-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this tutorial I will show you how to install Icinga2 and Icingaweb2 webinterface.</p>
<p>Icinga 2 is an open source, scalable and extensible monitoring tool which checks the availability of your network resources, notifies users of outages, and generates performance data for reporting.</p>
<h3 id="configure-syslinux-and-firewall-for-the-install">Configure syslinux and Firewall for the install</h3>
<pre tabindex="0"><code>setsebool -P httpd_can_network_connect_db 1
setsebool -P httpd_can_network_connect 1

firewall-cmd --permanent --add-service=http
firewall-cmd --permanent --add-service=https
firewall-cmd --permanent --add-service=postgresql
firewall-cmd --permanent --add-port=5665/tcp
firewall-cmd --reload
</code></pre><h3 id="install-postgresql">Install postgresql</h3>
<pre tabindex="0"><code>yum install postgresql-server postgresql
postgresql-setup initdb

nano /var/lib/pgsql/data/pg_hba.conf
# icinga
local   icinga      icinga                            md5
host    icinga      icinga      127.0.0.1/32          md5
host    icinga      icinga      ::1/128               md5
local   icinga_web  icinga_web                        md5
host    icinga_web  icinga_web  127.0.0.1/32          md5
host    icinga_web  icinga_web  ::1/128               md5

# &quot;local&quot; is for Unix domain socket connections only
local   all         all                               ident
# IPv4 local connections:
host    all         all         127.0.0.1/32          ident
# IPv6 local connections:
host    all         all         ::1/128               ident

systemctl enable  --now postgresql-10
</code></pre><h3 id="install-icinga2">Install Icinga2</h3>
<pre tabindex="0"><code>yum install https://packages.icinga.com/epel/icinga-rpm-release-7-latest.noarch.rpm
yum install epel-release

yum install icinga2 icinga2-selinux nagios-plugins-all nano-icinga2

systemctl enable --now httpd

echo 'include &quot;/usr/share/nano/icinga2.nanorc&quot;' &gt;&gt; /etc/nanorc
cp /etc/nanorc ~/.nanorc

yum install icinga2-ido-pgsql

cd /tmp
sudo -u postgres psql -c &quot;CREATE ROLE icinga WITH LOGIN PASSWORD 'icinga'&quot;
sudo -u postgres createdb -O icinga -E UTF8 icinga

export PGPASSWORD=icinga
psql -U icinga -d icinga &lt; /usr/share/icinga2-ido-pgsql/schema/pgsql.sql

icinga2 feature enable ido-pgsql

cat &lt;&lt; EOF | sudo tee /etc/icinga2/features-enabled/ido-pgsql.conf
/**
 * The db_ido_pgsql library implements IDO functionality
 * for PostgreSQL.
 */

library &quot;db_ido_pgsql&quot;

object IdoPgsqlConnection &quot;ido-pgsql&quot; {
  user = &quot;icinga&quot;,
  password = &quot;icinga&quot;,
  host = &quot;localhost&quot;,
  database = &quot;icinga&quot;
}
EOF

icinga2 api setup


ecjo '
object ApiUser &quot;icingaweb2&quot; {
  password = &quot;Wijsn8Z9eRs5E25d&quot;
  permissions = [ &quot;status/query&quot;, &quot;actions/*&quot;, &quot;objects/modify/*&quot;, &quot;objects/query/*&quot; ]
}' &gt;&gt; /etc/icinga2/conf.d/api-users.conf

icinga2 feature enable command
</code></pre><pre tabindex="0"><code>icinga2 node wizard
Welcome to the Icinga 2 Setup Wizard!

We'll guide you through all required configuration details.

Please specify if this is a satellite setup ('n' installs a master setup) [Y/n]: n
Starting the Master setup routine...
Please specifiy the common name (CN) [icinga.devopstales.intra]:
Checking for existing certificates for common name 'icinga.devopstales.intra'...
Certificates not yet generated. Running 'api setup' now.
information/cli: Generating new CA.
information/base: Writing private key to '/var/lib/icinga2/ca/ca.key'.
information/base: Writing X509 certificate to '/var/lib/icinga2/ca/ca.crt'.
information/cli: Generating new CSR in '/etc/icinga2/pki/icinga.devopstales.intra.csr'.
information/base: Writing private key to '/etc/icinga2/pki/icinga.devopstales.intra.key'.
information/base: Writing certificate signing request to '/etc/icinga2/pki/icinga.devopstales.intra.csr'.
information/cli: Signing CSR with CA and writing certificate to '/etc/icinga2/pki/icinga.devopstales.intra.crt'.
information/cli: Copying CA certificate to '/etc/icinga2/pki/ca.crt'.
Generating master configuration for Icinga 2.
information/cli: Adding new ApiUser 'root' in '/etc/icinga2/conf.d/api-users.conf'.
information/cli: Enabling the 'api' feature.
Enabling feature api. Make sure to restart Icinga 2 for these changes to take effect.
information/cli: Dumping config items to file '/etc/icinga2/zones.conf'.
information/cli: Created backup file '/etc/icinga2/zones.conf.orig'.
Please specify the API bind host/port (optional):
Bind Host []: Hit Enter
Bind Port []: Hit Enter
information/cli: Created backup file '/etc/icinga2/features-available/api.conf.orig'.
information/cli: Updating constants.conf.
information/cli: Created backup file '/etc/icinga2/constants.conf.orig'.
information/cli: Updating constants file '/etc/icinga2/constants.conf'.
information/cli: Updating constants file '/etc/icinga2/constants.conf'.
information/cli: Updating constants file '/etc/icinga2/constants.conf'.
Done.

Now restart your Icinga 2 daemon to finish the installation!

systemctl restart icinga2
</code></pre><h3 id="install-icingaweb2">Install IcingaWeb2</h3>
<pre tabindex="0"><code>yum install centos-release-scl
yum install icingaweb2 icingacli icingaweb2-selinux

yum install httpd
systemctl start httpd.service
systemctl enable httpd.service

icingacli setup config webserver apache

## OR

yum install nginx rh-php71-php-fpm rh-php71-php-pgsql
systemctl enable --now rh-php71-php-fpm.service

## config
icingacli setup config webserver nginx &gt; /etc/nginx/default.d//icinga.conf
systemctl enable --now nginx

sudo -u postgres psql -c &quot;CREATE ROLE icinga_web WITH LOGIN PASSWORD 'icinga_web'&quot;
sudo -u postgres createdb -O icinga_web -E UTF8 icinga_web

icingacli setup token create

# go to
https://icinga.devopstales.intra/icingaweb2/
</code></pre><p><img src="/img/include/icingaweb1.png" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/icingaweb2.png" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/icingaweb3.png" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/icingaweb4.png" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/icingaweb5.png" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/icingaweb6.png" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/icingaweb7.png" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/icingaweb8.png" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/icingaweb9.png" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/icingaweb10.png" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/icingaweb11.png" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/icingaweb12.png" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/icingaweb13.png" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/icingaweb14.png" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/icingaweb15.png" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/icingaweb16.png" alt="Example image"  class="zoomable" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configure Rundeck LADAP]]></title>
            <link href="https://devopstales.github.io/home/rundeck-ldap/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/rundeck-ldap/</id>
            
            
            <published>2019-12-09T00:00:00+00:00</published>
            <updated>2019-12-09T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will configure Rundeck to use LDAP as a User backend.</p>
<h3 id="rundeck-ldap-config-file">Rundeck LDAP config file</h3>
<pre tabindex="0"><code>nano /etc/rundeck/jaas-ldap.conf

# openldap
ldap {
      com.dtolabs.rundeck.jetty.jaas.JettyCachingLdapLoginModule required
      contextFactory=&quot;com.sun.jndi.ldap.LdapCtxFactory&quot;
      debug=&quot;true&quot;
      providerUrl=&quot;ldap://openldap:389&quot;
      bindDn=&quot;cn=admin,dc=mydomain,dc=intra&quot;
      bindPassword=&quot;Password1&quot;
      authenticationMethod=&quot;simple&quot;
      forceBindingLogin=&quot;true&quot;
      userBaseDn=&quot;dc=mydomain,dc=intra&quot;
      userRdnAttribute=&quot;cn&quot;
      userIdAttribute=&quot;cn&quot;
      userPasswordAttribute=&quot;userPassword&quot;
      userObjectClass=&quot;inetOrgPerson&quot;
      roleBaseDn=&quot;dc=mydomain,dc=intra&quot;
      roleNameAttribute=&quot;cn&quot;
      roleMemberAttribute=&quot;uniqueMember&quot;
      roleObjectClass=&quot;groupOfUniqueNames&quot;
      supplementalRoles=&quot;admin, user&quot;;
      };

# windows AD
ldap {
      com.dtolabs.rundeck.jetty.jaas.JettyCachingLdapLoginModule required
      contextFactory=&quot;com.sun.jndi.ldap.LdapCtxFactory&quot;
      debug=&quot;true&quot;
      providerUrl=&quot;ldap://devopstales.intra:389&quot;
      bindDn=&quot;cn=admin,dc=mydomain,dc=intra&quot;
      bindPassword=&quot;Password1&quot;
      authenticationMethod=&quot;simple&quot;
      forceBindingLogin=&quot;true&quot;
      userBaseDn=&quot;dc=mydomain,dc=intra&quot;
      userRdnAttribute=&quot;sAMAccountName&quot;
      userIdAttribute=&quot;sAMAccountName&quot;
      userPasswordAttribute=&quot;unicodePwd&quot;
      userObjectClass=&quot;user&quot;
      roleBaseDn=&quot;dc=mydomain,dc=intra&quot;
      roleNameAttribute=&quot;cn&quot;
      roleMemberAttribute=&quot;member&quot;
      roleObjectClass=&quot;group&quot;
      supplementalRoles=&quot;admin, user&quot;;
      };
</code></pre><h3 id="rundeck-multibackend-config-file">Rundeck multibackend config file</h3>
<pre tabindex="0"><code>nano /etc/rundeck/jaas-multiauth.conf

multiauth {
      com.dtolabs.rundeck.jetty.jaas.JettyCombinedLdapLoginModule required
      contextFactory=&quot;com.sun.jndi.ldap.LdapCtxFactory&quot;
      debug=&quot;true&quot;
      providerUrl=&quot;ldap://ad1:389&quot;
      bindDn=&quot;cn=admin,dc=mydomain,dc=intra&quot;
      bindPassword=&quot;Password1&quot;
      authenticationMethod=&quot;simple&quot;
      forceBindingLogin=&quot;true&quot;
      userBaseDn=&quot;ou=Users,dc=mydomain,dc=intra&quot;
      userRdnAttribute=&quot;cn&quot;
      userIdAttribute=&quot;cn&quot;
      userPasswordAttribute=&quot;userPassword&quot;
      userObjectClass=&quot;inetOrgPerson&quot;
      roleBaseDn=&quot;ou=Groups,dc=mydomain,dc=intra&quot;
      roleNameAttribute=&quot;cn&quot;
      roleMemberAttribute=&quot;uniqueMember&quot;
      roleObjectClass=&quot;groupOfUniqueNames&quot;

      ignoreRoles=&quot;true&quot;
      storePass=&quot;true&quot;
      clearPass=&quot;true&quot;
      useFirstPass=&quot;false&quot;
      tryFirstPass=&quot;false&quot;
      supplementalRoles=&quot;admin, user&quot;;

      org.rundeck.jaas.jetty.JettyRolePropertyFileLoginModule required
      debug=&quot;true&quot;
      useFirstPass=&quot;true&quot;
      file=&quot;/etc/rundeck/realm.properties&quot;;
      };
</code></pre><h3 id="configure-rundeck-to-use-multibackend-config-file">Configure Rundeck to use multibackend config file</h3>
<pre tabindex="0"><code>nano /etc/rundeck/profile

JAAS_CONF=&quot;${JAAS_CONF:-$RDECK_CONFIG/jaas-ldap.conf}&quot;
LOGIN_MODULE=&quot;ldap&quot;

# OR based on your distro
nano /etc/default/rundeckd

export JAAS_CONF=&quot;/etc/rundeck/jaas-ldap.conf&quot;
export LOGIN_MODULE=&quot;ldap&quot;
</code></pre><h3 id="configure-rundeck-to-use-ldap-config-file">Configure Rundeck to use LDAP config file</h3>
<pre tabindex="0"><code>nano /etc/rundeck/profile

JAAS_CONF=&quot;${JAAS_CONF:-$RDECK_CONFIG/jaas-multiauth.conf}&quot;
LOGIN_MODULE=&quot;multiauth&quot;

# OR based on your distro
nano /etc/default/rundeckd

export JAAS_CONF=&quot;/etc/rundeck/jaas-multiauth.conf&quot;
export LOGIN_MODULE=&quot;multiauth&quot;
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install docker on fedora 31]]></title>
            <link href="https://devopstales.github.io/home/docker-on-fedora31/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/docker-on-fedora31/?utm_source=atom_feed" rel="related" type="text/html" title="Install docker on fedora 31" />
                <link href="https://devopstales.github.io/home/graylog-pfsense-squid/?utm_source=atom_feed" rel="related" type="text/html" title="Analyzing PFsense squid logs in Graylog" />
                <link href="https://devopstales.github.io/home/openshift-gitlab-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Installing GitLab on OpenShift" />
                <link href="https://devopstales.github.io/home/proxmox-backup-error/?utm_source=atom_feed" rel="related" type="text/html" title="Solution for: Proxmox backup error due to iothread" />
                <link href="https://devopstales.github.io/home/foreman-pxe/?utm_source=atom_feed" rel="related" type="text/html" title="Install Foreman PXE boot" />
            
                <id>https://devopstales.github.io/home/docker-on-fedora31/</id>
            
            
            <published>2019-12-04T00:00:00+00:00</published>
            <updated>2019-12-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>On Fedora 31 after starting docker container some error as follows because of cgroups v2.</p>
<pre tabindex="0"><code>docker: Error response from daemon: OCI runtime create failed: container_linux.go:346: starting container process caused &quot;process_linux.go:297: applying cgroup configuration for process caused \&quot;open /sys/fs/cgroup/docker/cpuset.cpus.effective: no such file or directory\&quot;&quot;: unknown.
</code></pre><p>This is beacaue Fedora 31 uses cgroups v2 by default and docker doesn’t yet support cgroupsv2. In this tutorial I am going to show you how to change cgroups to v1 as a quick fix to run docker.</p>
<pre tabindex="0"><code>sudo nano /etc/default/grub
GRUB_CMDLINE_LINUX=&quot;systemd.unified_cgroup_hierarchy=0&quot;

grub2-mkconfig -o /boot/efi/EFI/fedora/grub.cfg
reboot
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Analyzing PFsense squid logs in Graylog]]></title>
            <link href="https://devopstales.github.io/home/graylog-pfsense-squid/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/graylog3-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Analyzing PFsense logs in Graylog3" />
                <link href="https://devopstales.github.io/home/graylog3-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Graylog3" />
                <link href="https://devopstales.github.io/linux/graylog-pfsense-squid/?utm_source=atom_feed" rel="related" type="text/html" title="Analyzing PFsense squid logs in Graylog" />
                <link href="https://devopstales.github.io/linux/graylog3-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Analyzing PFsense logs in Graylog3" />
                <link href="https://devopstales.github.io/linux/graylog3-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Graylog3" />
            
                <id>https://devopstales.github.io/home/graylog-pfsense-squid/</id>
            
            
            <published>2019-11-24T00:00:00+00:00</published>
            <updated>2019-11-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>We will parse the access log records generated by the PfSense&rsquo;s squd plugin. We already have our graylog server running and we will start preparing the terrain to capture those logs records.</p>
<p>Many thanks to opc40772 developed the original contantpack for pfsense squid log agregation what I updated for the new Graylog3 and Elasticsearch 6.</p>
<h3 id="celebro-localinstall">Celebro localinstall</h3>
<pre tabindex="0"><code># celebro van to use port 9000 so change graylog3 bindport
nano /etc/graylog/server/server.conf
http_bind_address = 127.0.0.1:9400
nano /etc/nginx/conf.d/graylog.conf

systemctl restart graylog-server.service
systemctl restart nginx

wget https://github.com/lmenezes/cerebro/releases/download/v0.8.3/cerebro-0.8.3-1.noarch.rpm
yum localinstall cerebro-0.8.3-1.noarch.rpm
</code></pre><h3 id="create-indices">Create indices</h3>
<p>We now create the Pfsense indice on Graylog at <code>System / Indexes</code> <!-- raw HTML omitted -->
<img src="/img/include/squid_pfsense1.png" alt="image"  class="zoomable" /> <!-- raw HTML omitted --></p>
<h3 id="import-index-template-for-elasticsearch-6x">Import index template for elasticsearch 6.x</h3>
<pre tabindex="0"><code>systemctl stop graylog-server.service
</code></pre><p>Go to <code>celebro &gt; more &gt; index templates</code>
Create new with name: <code>pfsense-custom</code> and copy the template from file <code>squid_custom_template_el6.json</code>
Edit other pfsense template to (sorrend 0)</p>
<p>In Cerebro we stand on top of the pfsense index and unfold the options and select delete index.</p>
<h3 id="geoip-database">Geoip database</h3>
<pre tabindex="0"><code>wget -t0 -c http://geolite.maxmind.com/download/geoip/database/GeoLite2-City.tar.gz
tar -xvf GeoLite2-City.tar.gz
cp GeoLite2-City_*/GeoLite2-City.mmdb /etc/graylog/server

systemctl start graylog-server.service
</code></pre><p>Enable geoip database at <code>System \ Imput &gt; Configurations &gt; Plugins &gt; Geo-Location Processor &gt; update</code>
Chane the order of the <code>Message Processors Configuration</code></p>
<ul>
<li>AWS Instance Name Lookup</li>
<li>Message Filter Chain</li>
<li>Pipeline Processor</li>
<li>GeoIP Resolver</li>
</ul>
<p>Enable geoip database</p>
<h3 id="import-contantpack">Import contantpack</h3>
<pre tabindex="0"><code>git clone https://github.com/devopstales/Squid-Graylog.git
</code></pre><p>import
chaneg date timezone in Pipeline rule
Go tu Stream menu and edit stream <!-- raw HTML omitted -->
<img src="/img/include/squid_pfsense7.png" alt="image"  class="zoomable" /> <!-- raw HTML omitted --></p>
<p><code>System &gt; Pipelines</code>
Manage rules and then Edit rule (Change the timezone)</p>
<pre tabindex="0"><code>rule &quot;timestamp_pfsense_for_grafana&quot;
 when
 has_field(&quot;timestamp&quot;)
then
// the following date format assumes there's no time zone in the string
 let source_timestamp = parse_date(substring(to_string(now(&quot;Europe/Budapest&quot;)),0,23), &quot;yyyy-MM-dd'T'HH:mm:ss.SSS&quot;);
 let dest_timestamp = format_date(source_timestamp,&quot;yyyy-MM-dd HH:mm:ss&quot;);
 set_field(&quot;real_timestamp&quot;, dest_timestamp);
end
</code></pre><p>Add the existing pipeline to the squid stream by clicking the Edit connections at Pipeline connections.</p>
<p><img src="/img/include/squid_pfsense8.png" alt="image"  class="zoomable" /></p>
<h3 id="confifure-pfsense">Confifure pfsense</h3>
<pre tabindex="0"><code># http://pkg.freebsd.org/FreeBSD:11:amd64/latest/All/
pkg add http://pkg.freebsd.org/FreeBSD:11:amd64/latest/All/beats-6.7.1.txz

nano /usr/local/etc/filebeat.yml
filebeat.prospectors:
- input_type: log
  document_type: squid3
  paths:
    - /var/squid/logs/access.log

output.logstash:
  # The Logstash hosts
  hosts: [&quot;192.168.0.112:5044&quot;]

  # Optional SSL. By default is off.
  # List of root certificates for HTTPS server verifications
  bulk_max_size: 2048
 #ssl.certificate_authorities: [&quot;/etc/filebeat/logstash.crt&quot;]
  template.name: &quot;filebeat&quot;
  template.path: &quot;filebeat.template.json&quot;
  template.overwrite: false
  # Certificate for SSL client authentication
  #ssl.certificate: &quot;/etc/pki/client/cert.pem&quot;

  # Client Certificate Key
  #ssl.key: &quot;/etc/pki/client/cert.key&quot;

/usr/local/sbin/filebeat -c /usr/local/etc/filebeat.yml test config
cp /usr/local/etc/rc.d/filebeat /usr/local/etc/rc.d/filebeat.sh
echo &quot;filebeat_enable=yes&quot; &gt;&gt; /etc/rc.conf.local
echo &quot;filebeat_conf=/usr/local/etc/filebeat.yml&quot; &gt;&gt; /etc/rc.conf.local

/usr/local/etc/rc.d/filebeat.sh start
ps aux | grep beat
</code></pre><h3 id="install-grafana-dashboard">Install grafana Dashboard</h3>
<pre tabindex="0"><code># install nececery plugins
grafana-cli plugins install grafana-piechart-panel
grafana-cli plugins install grafana-worldmap-panel
grafana-cli plugins install savantly-heatmap-panel
grafana-cli plugins install briangann-datatable-panel
systemctl restart grafana-server
</code></pre><p>Create new datasource: <!-- raw HTML omitted -->
<img src="/img/include/squid_pfsense9.jpg" alt="image"  class="zoomable" /> <!-- raw HTML omitted --></p>
<p>Import dashboadr. <!-- raw HTML omitted --></p>
<hr>
<h5 id="contantpack">Contantpack:</h5>
<p><a href="https://github.com/devopstales/Squid-Graylog.git">https://github.com/devopstales/Squid-Graylog.git</a></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Squid proxy]]></title>
            <link href="https://devopstales.github.io/home/install-squid/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/install-squid/</id>
            
            
            <published>2019-11-18T00:00:00+00:00</published>
            <updated>2019-11-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Squid is the most popular Proxy server for Linux systems. The squid proxy server is also useful for the web packet filtering.</p>
<h3 id="install-squid">Install Squid</h3>
<p>Squid packages are available in default yum repositories.</p>
<pre tabindex="0"><code>yum install squid
</code></pre><h3 id="configuring-squid">Configuring Squid</h3>
<pre tabindex="0"><code>nano /etc/squid/squid.conf
    #
    # Recommended minimum configuration:
    ## Example rule allowing access from your local networks.
    # Adapt to list your (internal) IP networks from where browsing
    # should be allowed
    acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
    acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
    acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
    acl localnet src fc00::/7       # RFC 4193 local private network range
    acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)

    machinesacl SSL_ports port 443
    acl Safe_ports port 80          # http
    acl Safe_ports port 21          # ftp
    acl Safe_ports port 443         # https
    acl Safe_ports port 70          # gopher
    acl Safe_ports port 210         # wais
    acl Safe_ports port 1025-65535  # unregistered ports
    acl Safe_ports port 280         # http-mgmt
    acl Safe_ports port 488         # gss-http
    acl Safe_ports port 591         # filemaker
    acl Safe_ports port 777         # multiling http
    acl CONNECT method CONNECT#
    # Recommended minimum Access Permission configuration:
    #
    # Deny requests to certain unsafe ports
    http_access deny !Safe_ports# Deny CONNECT to other than secure SSL ports
    http_access deny CONNECT !SSL_ports# Only allow cachemgr access from localhost
    http_access allow localhost manager
    http_access deny manager# We strongly recommend the following be uncommented to protect innocent
    # web applications running on the proxy server who think the only
    # one who can access services on &quot;localhost&quot; is a local user
    #http_access deny to_localhost#
    # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
    ## Example rule allowing access from your local networks.
    # Adapt localnet in the ACL section to list your (internal) IP networks
    # from where browsing should be allowed
    http_access allow localnet
    http_access allow localhost# And finally deny all other access to this proxy
    http_access deny all# Squid normally listens to port 3128
    http_port 3128# Uncomment and adjust the following to add a disk cache directory.
    #cache_dir ufs /var/spool/squid 100 16 256# Leave coredumps in the first cache dir
    coredump_dir /var/spool/squid

# # Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%	1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%	0
refresh_pattern .               0       20%     4320
</code></pre><h3 id="allow-ip-address-to-use-the-internet-through-your-proxy-server">Allow IP Address to Use the Internet Through Your Proxy Server</h3>
<p>If your netwok is 110.220.330.0/24:</p>
<pre tabindex="0"><code>acl localnet src 110.220.330.0/24
</code></pre><p>For changes to take effect you will need to restart your Squid server, use the following command for same.</p>
<pre tabindex="0"><code>systemctl restart squid
</code></pre><h3 id="allow-a-specific-port-for-http-connections">Allow a Specific Port for HTTP Connections</h3>
<pre tabindex="0"><code>acl Safe_ports port 8080
</code></pre><h3 id="using-basic-authentication-with-squid">Using Basic Authentication with Squid</h3>
<pre tabindex="0"><code>yum -y install httpd-tools
touch /etc/squid/passwd &amp;&amp; chown squid /etc/squid/passwd

htpasswd /etc/squid/passwd proxyuser
    New password:
    Re-type new password:
    Adding password for user pxuser
</code></pre><pre tabindex="0"><code>nano /etc/squid/squid.conf
...
auth_param basic program /usr/lib64/squid/basic_ncsa_auth /etc/squid/passwd
auth_param basic children 5
auth_param basic realm Squid Basic Authentication
auth_param basic credentialsttl 2 hours
acl auth_users proxy_auth REQUIRED
http_access allow auth_users
</code></pre><h3 id="blocking-websites">Blocking Websites</h3>
<pre tabindex="0"><code>nano /etc/squid/blocked_sites
facebook.com
youtube.com
</code></pre><pre tabindex="0"><code>nano /etc/squid/squid.conf
acl blocked_sites dstdomain &quot;/etc/squid/blocked_sites&quot;
http_access deny blocked_sites
</code></pre><h3 id="block-specific-keyword-with-squid">Block Specific Keyword with Squid</h3>
<pre tabindex="0"><code>nano /etc/squid/blockkeywords.lst
yahoo
gmail
facebook
</code></pre><pre tabindex="0"><code>acl blockkeywordlist url_regex &quot;/etc/squid/blockkeywords.lst&quot;
http_access deny blockkeywordlist
</code></pre><h3 id="disable-caching">Disable caching</h3>
<pre tabindex="0"><code># Leave coredumps in the first cache dir
#coredump_dir /var/spool/squid
</code></pre><h3 id="changing-squid-port">Changing Squid Port</h3>
<pre tabindex="0"><code>nano /etc/squid/squid.conf
http_port 3128
</code></pre><h3 id="export-proxy-server-settings">Export Proxy Server Settings</h3>
<pre tabindex="0"><code>$ export http_proxy=&quot;http://PROXY_SERVER:PORT&quot;
$ export https_proxy=&quot;http://PROXY_SERVER:PORT&quot;
$ export ftp_proxy=&quot;http://PROXY_SERVER:PORT&quot;

$ export http_proxy=&quot;http://USER:PASSWORD@PROXY_SERVER:PORT&quot;
$ export https_proxy=&quot;http://USER:PASSWORD@PROXY_SERVER:PORT&quot;
$ export ftp_proxy=&quot;http://USER:PASSWORD@PROXY_SERVER:PORT&quot;
</code></pre><h3 id="test-caching">Test caching</h3>
<pre tabindex="0"><code>env | grep proxy

tailf /var/log/squid/access.log
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Installing GitLab on OpenShift]]></title>
            <link href="https://devopstales.github.io/home/openshift-gitlab-helm/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-gitlabrunner/?utm_source=atom_feed" rel="related" type="text/html" title="Install Gitlab runner on Openshift" />
                <link href="https://devopstales.github.io/kubernetes/openshift-gitlab-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Installing GitLab on OpenShift" />
                <link href="https://devopstales.github.io/home/openshift-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Helm" />
                <link href="https://devopstales.github.io/home/openshift-kompose/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Run docker-compoe in Openshift" />
                <link href="https://devopstales.github.io/home/openshift-add-node/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Add Nodes to a Cluster" />
            
                <id>https://devopstales.github.io/home/openshift-gitlab-helm/</id>
            
            
            <published>2019-11-18T00:00:00+00:00</published>
            <updated>2019-11-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I  had to install Gitlab to Openshift recently. Turned out getting GitLab up and running on OpenShift is not so easy.</p>
<h3 id="create-new-project">Create new project</h3>
<pre tabindex="0"><code>oc new-project gitlab-devopstales.intra
</code></pre><h3 id="deploy-helm">Deploy helm</h3>
<pre tabindex="0"><code>nano helm-namespace-account.yaml
kind: ServiceAccount
apiVersion: v1
metadata:
  name: tiller-gitlab-devopstales.intra
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tiller-gitlab-devopstales.intra
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller-gitlab-devopstales.intra
    namespace: kube-system
</code></pre><p>Now set up Helm, install the Tiller plugin and add the GitLab repository.</p>
<pre tabindex="0"><code>oc apply -f helm-namespace-account.yaml
oc get sa

helm init --service-account tiller-gitlab-devopstales.intra --tiller-namespace gitlab-mydomain-intra
oc get po -n kube-system

export TILLER_NAMESPACE=kube-system
echo $TILLER_NAMESPACE
helm version
</code></pre><h2 id="get-helmchart">Get helmchart</h2>
<pre tabindex="0"><code>helm repo add gitlab https://charts.gitlab.io/
helm repo update

oc adm policy add-scc-to-user anyuid -z default -n gitlab-devopstales.intra
oc adm policy add-scc-to-user anyuid -z gitlab-runner -n gitlab-devopstales.intra

# gitlab-tst is the name of the helm deployment
oc adm policy add-scc-to-user anyuid -z gitlab-tst-shared-secrets
oc adm policy add-scc-to-user anyuid -z gitlab-tst-gitlab-runner
oc adm policy add-scc-to-user anyuid -z gitlab-tst-prometheus-server
oc adm policy add-scc-to-user anyuid -z default
</code></pre><h3 id="create-chart-values">Create chart values</h3>
<pre tabindex="0"><code>nano gitlab-values.yml
certmanager:
  install: false
global:
  appConfig:
    enableUsagePing: true
    enableImpersonation: true
    defaultCanCreateGroup: true
    usernameChangingEnabled: true
    issueClosingPattern:
    defaultTheme:
    defaultProjectsFeatures:
      issues: true
      mergeRequests: true
      wiki: true
      snippets: true
      builds: true
      containerRegistry: true
    ldap:
      servers:
        main:
          base: dc=mydomain,dc=intra
          user_filter: (&amp;(objectClass=user)(memberof=cn=Users,dc=mydomain,dc=intra))
          bind_dn: Administrator@devopstales.intra
          host: 192.168.10.4
          label: devopstales.intra
          password:
            key: password
            secret: gitlab-ldap-secret
          port: 636
          encryption: simple_tls
          uid: sAMAccountName
          active_directory: true
          verify_certificates: false
          allow_username_or_email_login: true
    omniauth:
      enabled: true
      blockAutoCreatedUsers: false
      allowSingleSignOn: ['oauth2_generic']
      providers:
        - secret: gitlab-sso
          key: provider
    backups:
      bucket: gitlab-devopstales.intra
      tmpBucket: gitlab-devopstales.intra
      objectStorage:
        backend: s3
    lfs:
      bucket: gitlab-devopstales.intra
      connection:
        secret: ceph-storage
        key: gitlab
    artifacts:
      bucket: gitlab-devopstales.intra
      connection:
        secret: ceph-storage
        key: gitlab
    uploads:
      bucket: gitlab-devopstales.intra
      connection:
        secret: ceph-storage
        key: gitlab
    packages:
      bucket: gitlab-devopstales.intra
      connection:
        secret: ceph-storage
        key: gitlab
    externalDiffs:
      bucket: gitlab-devopstales.intra
      connection:
        secret: ceph-storage
        key: gitlab
    pseudonymizer:
      bucket: gitlab-devopstales.intra
      connection:
        secret: ceph-storage
        key: gitlab
  edition: ce
  email:
    from: gitlab@devopstales.intra
  hosts:
    domain: devopstales.intra
    externalIP: gitlab.devopstales.intra
    gitlab:
      name: gitlab.devopstales.intra
      https: false
    registry:
      name: gitlab-registry.devopstales.intra
      https: false
  ingress:
    enabled: false
    configureCertmanager: false
    tls:
      secretName: gitlab-certs
  smtp:
    address: mail.active.hu
    authentication: &quot;&quot;
    domain: devopstales.intra
    enabled: true
    port: 25
  gitlab-exporter:
    enabled: false
  registry:
    bucket: gitlab-registry
  minio:
    enabled: false
nginx-ingress:
  enabled: false
gitlab-runner:
  rbac:
    create: true
registry:
  enabled: true
  storage:
    secret: ceph-storage
    key: registry
  image:
    repository: docker.io/registry
    tag: 2.6.0
gitlab:
  task-runner:
    backups:
      objectStorage:
        config:
          secret: storage-config
          key: config
</code></pre><h3 id="create-secrets-for-deployment">Create secrets for deployment</h3>
<pre tabindex="0"><code>nano ceph.gitlab-data.yaml
provider: AWS
region: default
aws_access_key_id: W3MNDO373H6LQUNCG4SG
aws_secret_access_key: vVFEWx3hqbcrGJyaZVie9YoFG6rPoRYmqnDzRwrn
endpoint: &quot;https://s3.devopstales.intra&quot;
enable_signature_v4_streaming: false

# admin jog kell a cephez
nano ceph.gitlab-registry.yaml
cache:
  blobdescriptor: inmemory
s3:
  region: default
  bucket: gitlab-registry
  accesskey: PZIOIH63CENHPG15XY42
  secretkey: K6K1lWO7Jtyp5rZiCwj77JC5BFMEAZ4a2PAkg9fB
  regionendpoint: https://s3.devopstales.intra
  rootdirectory: /
  secure: true
  v4auth: false
  encrypt: false
  chunksize: 5242880
redirect:
  disable: true
</code></pre><pre tabindex="0"><code>nano ceph.backup.config
[default]
access_key = W3MNDO373H8LQUNCJ8QV
access_token = vVFEWx8hqbcrGJyaZVie8YoER8rPoRYmqnDzRwrn
host_base = s3.devopstales.intra
host_bucket = %(bucket)s.s3.devopstales.intra
bucket_location = US
use_https = True
check_ssl_certificate = False
</code></pre><pre tabindex="0"><code>nano keycloak.sso.yaml
name: 'oauth2_generic'
label: 'mydomain'
app_id: 'gitlab'
app_secret: 'f2514bd4-92e4-40fa-bec4-382838db25f0'
args:
  client_options:
    site: 'https://sso.devopstales.intra'
    user_info_url: '/auth/realms/mydomain/protocol/openid-connect/userinfo'
    authorize_url: '/auth/realms/mydomain/protocol/openid-connect/auth'
    token_url: '/auth/realms/mydomain/protocol/openid-connect/token'
  user_response_structure:
    attributes:
      email: 'email'
      first_name: 'given_name'
      last_name: 'family_name'
      name: 'name'
      nickname: 'preferred_username'
    id_path: 'preferred_username'
</code></pre><h3 id="deploy-secrets">Deploy secrets</h3>
<pre tabindex="0"><code># https ssl cert
oc create secret tls gitlab-certs --cert=tls.crt --key=tls.key

oc create secret generic storage-config --from-file=config=ceph.backup.config

oc create secret generic ceph-storage --from-file=registry=ceph.gitlab-registry.yaml --from-file=gitlab=ceph.gitlab-data.yaml

oc create secret generic gitlab-sso --from-file=provider=keycloak.sso.yaml
oc create secret generic gitlab-ldap-secret --from-literal=password=
</code></pre><h3 id="deploy-application-with-helm">Deploy application with helm</h3>
<pre tabindex="0"><code>helm upgrade --install -f gitlab-values.yml gitlab-tst gitlab/gitlab --debug --dry-run
helm upgrade --install -f gitlab-values.yml gitlab-tst gitlab/gitlab --timeout 600
helm upgrade -f gitlab-values.yml gitlab-tst gitlab/gitlab --timeout 600

# https://docs.gitlab.com/charts/installation/version_mappings.html
helm upgrade -f gitlab-values.yml gitlab-tst gitlab/gitlab --version 2.3.5 --timeout 600

# gitlab-tst
oc get secret gitlab-tst-gitlab-initial-root-password -o jsonpath='{.data.password}' | base64 -d
</code></pre><h3 id="heading"></h3>
<pre tabindex="0"><code>nano gitlab-ssh-nodeport-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: gitlab-shell-nodeport
  labels:
    app: gitlab-shell
    name: gitlab-shell-nodeport
spec:
  type: NodePort
  ports:
    - port: 2222
      nodePort: 32222
      name: ssh
  selector:
    app: gitlab-shell

</code></pre><pre tabindex="0"><code>oc create -f gitlab-ssh-nodeport-svc.yaml
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Solution for: Proxmox backup error due to iothread]]></title>
            <link href="https://devopstales.github.io/home/proxmox-backup-error/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/proxmox-backup-pve-zsync/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with pve-zsync" />
                <link href="https://devopstales.github.io/home/proxmox-backup-sanoid/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with sanoid" />
                <link href="https://devopstales.github.io/home/proxmox-backup-znapzend/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with znapzend" />
                <link href="https://devopstales.github.io/home/proxmox-node-remove/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox node removal" />
                <link href="https://devopstales.github.io/home/openshift-gitlab-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Installing GitLab on OpenShift" />
            
                <id>https://devopstales.github.io/home/proxmox-backup-error/</id>
            
            
            <published>2019-11-18T00:00:00+00:00</published>
            <updated>2019-11-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>If you see the following error when trying to backup a KVM VM image on Proxmox:</p>
<pre tabindex="0"><code>ERROR: Backup of VM 100 failed – disk ‘scsi0’ ‘zfsvols:vm-100-disk-1’ (iothread=on) can’t use backup feature currently. Please set backup=no for this drive at /usr/share/perl5/PVE/VZDump/QemuServer.pm line 77. INFO: Backup job finished with errors TASK ERROR: job errors
</code></pre><p>Posted on September 9, 2017 by Daniel Mettler
Solution for: Proxmox backup error due to iothread=1</p>
<p>If you see the following error when trying to backup a KVM VM image on Proxmox:</p>
<p>ERROR: Backup of VM 100 failed – disk ‘scsi0’ ‘zfsvols:vm-100-disk-1’ (iothread=on) can’t use backup feature currently. Please set backup=no for this drive at /usr/share/perl5/PVE/VZDump/QemuServer.pm line 77. INFO: Backup job finished with errors TASK ERROR: job errors</p>
<p>edit /etc/pve/qemu-server/100.conf, look for a line similar to</p>
<pre tabindex="0"><code>scsi0: zfsvols:vm-100-disk-1,iothread=1,size=70G
</code></pre><p>and change it to</p>
<pre tabindex="0"><code>scsi0: zfsvols:vm-100-disk-1,iothread=0,size=70G
# OR
scsi0: zfsvols:vm-100-disk-1,size=70G
</code></pre><p>After this you can backup the VM. This Problem was solvd in the proxmox 6 (pve-manager 6.0-11)</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Foreman PXE boot]]></title>
            <link href="https://devopstales.github.io/home/foreman-pxe/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/foreman-pxe/?utm_source=atom_feed" rel="related" type="text/html" title="Install Foreman PXE boot" />
                <link href="https://devopstales.github.io/home/clonedeploy/?utm_source=atom_feed" rel="related" type="text/html" title="Install clonedeploy pxeboot server" />
                <link href="https://devopstales.github.io/home/k8s-ceph-storage-with-csi-driver/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD volume with CSI driver" />
                <link href="https://devopstales.github.io/home/who-mapping-rbd-device/?utm_source=atom_feed" rel="related" type="text/html" title="Ceph: who&#39;s mapping a RBD device" />
                <link href="https://devopstales.github.io/home/gcp-vm-export/?utm_source=atom_feed" rel="related" type="text/html" title="Export GCP VM to S3" />
            
                <id>https://devopstales.github.io/home/foreman-pxe/</id>
            
            
            <published>2019-11-07T00:00:00+00:00</published>
            <updated>2019-11-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Foreman is a complete lifecycle management tool for physical and virtual servers. We give system administrators the power to easily automate repetitive tasks, quickly deploy applications, and proactively manage servers, on-premise or in the cloud.</p>
<p>I hawe a VM with two virtual interface the enp0s3 for NAT and enp0s9 with an internal network.</p>
<h3 id="install-dhcp-server">Install DHCP server</h3>
<pre tabindex="0"><code>yum install -y dhcp nano -y

echo &quot;DHCPDARGS=enp0s9&quot; &gt;&gt; /etc/sysconfig/dhcpd
</code></pre><pre tabindex="0"><code>cat &gt; /etc/dhcp/dhcpd.conf &lt;&lt; EOF
#DHCP configuration for PXE boot server
ddns-update-style interim;
ignore client-updates;
authoritative;
allow booting;
allow bootp;
allow unknown-clients;

# A slightly different configuration for an internal subnet.
subnet 192.168.100.0
netmask 255.255.255.0
{
range 192.168.100.101 192.168.100.200;
option domain-name-servers 192.168.100.100;
option routers 192.168.100.100;
default-lease-time 600;
max-lease-time 7200;

# PXE SERVER IP
next-server 192.168.100.100; #  PXE server ip
filename &quot;pxelinux.0&quot;;
}
EOF
</code></pre><pre tabindex="0"><code>systemctl start dhcpd.service
systemctl enable dhcpd.service
systemctl status dhcpd.service
</code></pre><h3 id="install-dns-server">Install DNS server</h3>
<pre tabindex="0"><code>yum -y install bind bind-utils

nano /etc/named.conf
options {
        ...
        // listen-on port 53 { 127.0.0.1; };
        // listen-on-v6 port 53 { ::1; };
        ...
        allow-query     { localhost; 192.168.100.0/24; };
        ...
        forwarders {
                8.8.8.8;
                8.8.4.4;
        };
};
include &quot;/etc/named.my.zones&quot;;
</code></pre><pre tabindex="0"><code>touch /etc/named.my.zones
chown root:named /etc/named.my.zones

nano /etc/named.my.zones
zone &quot;devopstales.intra&quot; IN {
         type master;
         file &quot;devopstales.intra.db&quot;;
         allow-update { none; };
};
</code></pre><pre tabindex="0"><code>nano /var/named/devopstales.intra.db
@   IN  SOA     primary.devopstales.intra. root.mydomain.intra. (
                                                1001    ;Serial
                                                3H      ;Refresh
                                                15M     ;Retry
                                                1W      ;Expire
                                                1D      ;Minimum TTL
                                                )

;Name Server Information
@      IN  NS      primary.devopstales.intra.

;IP address of Name Server
primary IN  A       192.168.100.100

;Mail exchanger
devopstales.intra. IN  MX 10   mail.mydomain.intra.

;A - Record HostName To IP Address
foreman IN  A       192.168.100.100
mail    IN  A       192.168.100.50
</code></pre><pre tabindex="0"><code>systemctl restart named
systemctl status named
systemctl enable named
</code></pre><h3 id="install-vtftp">Install vtftp</h3>
<pre tabindex="0"><code>yum install vsftpd -y

nano /etc/vsftpd/vsftpd.conf
anonymous_enable=YES
write_enable=NO

systemctl enable vsftpd
systemctl restart vsftpd
systemctl status vsftpd

cd /opt
wget http://ftp.bme.hu/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1908.iso
wget http://ftp.bme.hu/centos/7/isos/x86_64/sha256sum.txt

sha256sum CentOS-7-x86_64-Minimal-1908.iso
cat sha256sum.txt

mount -o loop /opt/CentOS-7-x86_64-Minimal-1908.iso  /mnt

mkdir /var/ftp/pub/CentOS_7_x86_64
rsync -rv --progress /mnt/ /var/ftp/pub/CentOS_7_x86_64/
umount /mnt
restorecon -Rv /var/ftp/pub/
</code></pre><h3 id="install-foreman">Install Foreman</h3>
<pre tabindex="0"><code>yum -y install https://yum.puppet.com/puppet6-release-el-7.noarch.rpm
yum -y install http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
yum -y install https://yum.theforeman.org/releases/1.23/el7/x86_64/foreman-release.rpm
yum -y install foreman-installer

foreman-installer \
--foreman-initial-organization &quot;mydomain&quot; \
--foreman-initial-location &quot;office&quot; \
--enable-foreman-plugin-ansible \
--enable-foreman-proxy-plugin-ansible \
--enable-foreman-plugin-remote-execution \
--enable-foreman-proxy-plugin-remote-execution-ssh \
--enable-foreman-plugin-cockpit \
--enable-foreman-plugin-openscap
</code></pre><h3 id="configure-hammer">Configure hammer</h3>
<pre tabindex="0"><code>nano ~/.hammer/cli.modules.d/foreman.yml
:foreman:
 :host: 'https://foreman.devopstales.intra/'
 :username: 'admin'
 :password: '**********'

hammer defaults add --param-name organization --param-value &quot;mydomain&quot;
hammer defaults add --param-name location --param-value &quot;office&quot;
hammer defaults list
</code></pre><h3 id="configurate-pxeboot">Configurate PXEboot</h3>
<pre tabindex="0"><code>sudo ss -lnup | grep 69
grep disa /etc/xinetd.d/tftp
ls -l /var/lib/tftpboot/

# create subnet
hammer subnet create \
--name PXEnet \
--network-type IPv4 \
--network 192.168.100.0 \
--mask 255.255.255.0 \
--dns-primary 192.168.100.100 \
--domains devopstales.intra \
--tftp-id 1 \
--httpboot-id 1 \
--ipam &quot;Internal DB&quot; \
--from 192.168.100.101 \
--to 192.168.100.200 \
--boot-mode Static

hammer medium create \
--name &quot;CentOS7_DVD_FTP&quot; \
--os-family &quot;Redhat&quot; \
--path &quot;ftp://foreman.devopstales.intra/pub/CentOS_7_x86_64/&quot;
</code></pre><p>Create a file hardened_ptable.txt with the content below.</p>
<pre tabindex="0"><code>&lt;%#
kind: ptable
name: Kickstart hardened
oses:
- CentOS
- Fedora
- RedHat
%&gt;

# System bootloader configuration
bootloader --location=mbr --boot-drive=sda --timeout=3
# Partition clearing information
clearpart --all --drives=sda
zerombr

# Disk partitioning information
part /boot --fstype=&quot;xfs&quot; --ondisk=sda --size=1024 --label=boot --fsoptions=&quot;rw,nodev,noexec,nosuid&quot;

# 30GB physical volume
part pv.01  --fstype=&quot;lvmpv&quot; --ondisk=sda --size=30720
volgroup vg_os pv.01

logvol /        --fstype=&quot;xfs&quot;  --size=4096 --vgname=vg_os --name=lv_root
logvol /home    --fstype=&quot;xfs&quot;  --size=512  --vgname=vg_os --name=lv_home --fsoptions=&quot;rw,nodev,nosuid&quot;
logvol /tmp     --fstype=&quot;xfs&quot;  --size=1024 --vgname=vg_os --name=lv_tmp  --fsoptions=&quot;rw,nodev,noexec,nosuid&quot;
logvol /var     --fstype=&quot;xfs&quot;  --size=6144 --vgname=vg_os --name=lv_var  --fsoptions=&quot;rw,nosuid&quot;
logvol /var/log --fstype=&quot;xfs&quot;  --size=512  --vgname=vg_os --name=lv_log  --fsoptions=&quot;rw,nodev,noexec,nosuid&quot;
logvol swap     --fstype=&quot;swap&quot; --size=2048 --vgname=vg_os --name=lv_swap --fsoptions=&quot;swap&quot;
</code></pre><pre tabindex="0"><code>hammer partition-table create \
  --name &quot;Kickstart hardened&quot; \
  --os-family &quot;Redhat&quot; \
  --operatingsystems &quot;CentOS 7.4.1708&quot; \
  --file &quot;hardened_ptable.txt&quot;

hammer os create \
  --name &quot;CentOS&quot; \
  --major &quot;7&quot; \
  --minor &quot;4.1708&quot; \
  --family &quot;Redhat&quot; \
  --password-hash &quot;SHA512&quot; \
  --architectures &quot;x86_64&quot; \
  --media &quot;CentOS7_DVD_FTP&quot; \
  --partition-tables &quot;Kickstart hardened&quot;

hammer hostgroup create \
  --name &quot;el7_group&quot; \
  --description &quot;Host group for CentOS 7 servers&quot; \
  --lifecycle-environment &quot;stable&quot; \
  --content-view &quot;el7_content&quot; \
  --content-source-id &quot;1&quot; \
  --environment &quot;homelab&quot; \
  --puppet-proxy &quot;foreman.devopstales.intra&quot; \
  --puppet-ca-proxy &quot;foreman.devopstales.intra&quot; \
  --domain &quot;devopstales.intra&quot; \
  --subnet &quot;PXEnet&quot; \
  --architecture &quot;x86_64&quot; \
  --operatingsystem &quot;CentOS 4.1708&quot; \
  --medium &quot;CentOS7_DVD_FTP&quot; \
  --partition-table &quot;Kickstart hardened&quot; \
  --pxe-loader &quot;PXELinux BIOS&quot; \
  --root-pass &quot;Password1&quot;

hammer hostgroup set-parameter  \
  --name &quot;selinux-mode&quot; \
  --value &quot;disabled&quot; \
  --hostgroup &quot;el7_group&quot;

hammer hostgroup set-parameter  \
  --name &quot;disable-firewall&quot; \
  --value &quot;true&quot; \
  --hostgroup &quot;el7_group&quot;

hammer hostgroup set-parameter  \
  --name &quot;bootloader-append&quot; \
  --value &quot;net.ifnames=0 biosdevname=0&quot; \
  --hostgroup &quot;el7_group&quot;

hammer host create \
  --name &quot;pxe-test&quot; \
  --hostgroup &quot;el7_group&quot; \
  --interface &quot;type=interface,mac=08:00:27:fb:ad:17,ip=192.168.100.110,managed=true,primary=true,provision=true&quot;
</code></pre><pre tabindex="0"><code>ll /var/lib/tftpboot/pxelinux.cfg/
cat /var/lib/tftpboot/pxelinux.cfg/01-08-00-27-fb-ad-17
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes Ceph RBD volume with CSI driver]]></title>
            <link href="https://devopstales.github.io/home/k8s-ceph-storage-with-csi-driver/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-ceph-storage-with-csi-driver/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD volume with CSI driver" />
                <link href="https://devopstales.github.io/home/who-mapping-rbd-device/?utm_source=atom_feed" rel="related" type="text/html" title="Ceph: who&#39;s mapping a RBD device" />
                <link href="https://devopstales.github.io/home/k8s-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/install-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ceph cluster" />
                <link href="https://devopstales.github.io/home/ceph-block-device/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph Block Device" />
            
                <id>https://devopstales.github.io/home/k8s-ceph-storage-with-csi-driver/</id>
            
            
            <published>2019-10-08T00:00:00+00:00</published>
            <updated>2019-10-08T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how can you use CEPH RBD with CSI driver for persistent storagi on Kubernetes.</p>
<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>


<H3>Parst of the Kubernetes series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
     <li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
     <li>Part1c: <a href="../../kubernetes/k8s-install-containerd/">Install K8S with containerd and kubeadm</a></li>
     <li>Part1d: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
     <li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a></li>
<!--      <li>Part3b: k8s-multiple-nginx-ingress -->
<!--      <li>Part3c: k8s-contour-ingress -->
     <li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a></li>
<!-- local folder (+ autoprovisioning ) -->
     <li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
     <li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
     <li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
     <li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
     <li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
     <li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a></li>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... -->
     <li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
     <li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
     <li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
     <li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
     <li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
     <li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
     <li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a></li>
<!-- istio-install.md -->
     <li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a></li>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog -->
</ul>


<p>The Container Storage Interface (CSI) is a standard for exposing arbitrary block and file storage storage systems to Kubernetes. Using CSI third-party storage providers can write and deploy plugins exposing storage systems in Kubernetes. Bbefore we begin lets ensure that we have the following requirements:</p>
<ul>
<li>Kubernetes cluster v1.14+</li>
<li>allow-privileged flag enabled for both kubelet and API server</li>
<li>Running Ceph cluster</li>
</ul>
<pre tabindex="0"><code>git clone https://github.com/ceph/ceph-csi.git
cd ceph-csi/deploy/rbd/kubernetes/v1.14+/

kubectl create -f csi-nodeplugin-rbac.yaml
kubectl create -f csi-provisioner-rbac.yaml
</code></pre><pre tabindex="0"><code>nano csi-config-map.yaml
---
apiVersion: v1
kind: ConfigMap
data:
  config.json: |-
    [
      {
        &quot;clusterID&quot;: &quot;k8s-ceph&quot;,
        &quot;monitors&quot;: [
          &quot;192.168.1.31:6789&quot;,
          &quot;192.168.1.32:6789&quot;,
          &quot;192.168.1.33:6789&quot;
        ]
      }
    ]
metadata:
  name: ceph-csi-config


kubectl create -fcsi-config-map.yaml
</code></pre><pre tabindex="0"><code>kubectl create -f csi-rbdplugin-provisioner.yaml
kubectl create -f csi-rbdplugin.yaml
</code></pre><pre tabindex="0"><code>ceph auth get-key client.admin|base64
QVFDTDliVmNEb21I32SHoPxXNGhmRkczTFNtcXM0ZW5VaXlTZEE977==

nano csi-rbd-secret.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: csi-rbd-secret
  namespace: default
data:
  userID: admin
  userKey: QVFDTDliVmNEb21I32SHoPxXNGhmRkczTFNtcXM0ZW5VaXlTZEE977==

nano rbd-csi-sc.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: csi-rbd
provisioner: rbd.csi.ceph.com
parameters:
   monitors: 192.168.1.31:6790,192.168.1.32:6790,192.168.1.33:6790
   clusterID: k8s-ceph
   pool: rbd
   imageFeatures: layering
   csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
   csi.storage.k8s.io/provisioner-secret-namespace: default
   csi.storage.k8s.io/node-publish-secret-name: csi-rbd-secret
   csi.storage.k8s.io/node-publish-secret-namespace: default
   adminid: admin
   csi.storage.k8s.io/fstype: ext4
reclaimPolicy: Delete
mountOptions:
   - discard

kubectl create -f csi-rbd-secret.yaml
kubectl create -f rbd-csi-sc.yaml

kubectl get storageclass
NAME      PROVISIONER        AGE
csi-rbd   rbd.csi.ceph.com   15s
</code></pre><pre tabindex="0"><code>nano raw-block-pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: raw-block-pvc
spec:
  accessModes:
    - ReadWriteMany
  volumeMode: Block
  resources:
    requests:
      storage: 1Gi
  storageClassName: csi-rbd

kubectl create -f raw-block-pvc.yaml

kubectl get pvc
NAME            STATUS    VOLUME                                  
raw-block-pvc   Bound     pvc-fd66b4d6-757d-22e9-8f9e-4f86e2356a59
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Ceph: who's mapping a RBD device]]></title>
            <link href="https://devopstales.github.io/home/who-mapping-rbd-device/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/install-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ceph cluster" />
                <link href="https://devopstales.github.io/home/ceph-block-device/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph Block Device" />
                <link href="https://devopstales.github.io/home/ceph-cephfs/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph CephFS" />
                <link href="https://devopstales.github.io/kubernetes/who-mapping-rbd-device/?utm_source=atom_feed" rel="related" type="text/html" title="Ceph: who&#39;s mapping a RBD device" />
            
                <id>https://devopstales.github.io/home/who-mapping-rbd-device/</id>
            
            
            <published>2019-10-05T00:00:00+00:00</published>
            <updated>2019-10-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<h3 id="main-problem">Main problem</h3>
<p>I get this error in Openshift:</p>
<pre tabindex="0"><code>  MountVolume.WaitForAttach failed for volume &quot;pvc-9fcd3d08-d14e-11e9-a958-66934f1af826&quot; :
  rbd image k8s-prod/kubernetes-dynamic-pvc-a0029613-d14e-11e9-aaf5-8611e6b1c395 is still being used
</code></pre><h3 id="solution">Solution</h3>
<p>So I Wanna know who is using this DBD device?</p>
<pre tabindex="0"><code>rbd status k8s-prod/kubernetes-dynamic-pvc-a0029613-d14e-11e9-aaf5-8611e6b1c395
Watchers:
	watcher=192.168.1.43:0/3614154426 client.165467009 cookie=18446462598732840961
</code></pre><p>To solve this problem you need to restart Openshift&rsquo;s Kubernetes components.</p>
<pre tabindex="0"><code>ssh 192.168.1.43
systemctl restart docker origin-node
</code></pre><p>Check if it is still mounted:</p>
<pre tabindex="0"><code>lsblk | grep &quot;pvc-9fcd3d08-d14e-11e9-a958-66934f1af826&quot;
rbd12                                252:192  0  100G  0 disk /var/lib/kubelet/plugins/kubernetes.io/csi/pv/pvc-9fcd3d08-d14e-11e9-a958-66934f1af826/globalmount/0001-0024-e285a458-7c95-4187-8129-fbd6c370c537-0000000000000006-12154609-2d98-11ec-9b66-66a9f6ca788c
</code></pre><p>With the restart of the docker and the kubelet the container dse not running anymore, so you can unmount it. Fot theat you need the rnd client.</p>
<pre tabindex="0"><code>rbd unmap k8s-prod/kubernetes-dynamic-pvc-a0029613-d14e-11e9-aaf5-8611e6b1c395
# or 
rbd unmap -o force k8s-prod/kubernetes-dynamic-pvc-a0029613-d14e-11e9-aaf5-8611e6b1c395
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Export GCP VM to S3]]></title>
            <link href="https://devopstales.github.io/home/gcp-vm-export/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/gcp-vm-export/?utm_source=atom_feed" rel="related" type="text/html" title="Export GCP VM to S3" />
                <link href="https://devopstales.github.io/home/ansible-k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Kubernetes" />
                <link href="https://devopstales.github.io/home/alerta-on-centos8/?utm_source=atom_feed" rel="related" type="text/html" title="Install alerta on Centos8" />
                <link href="https://devopstales.github.io/home/alerta-on-centos7/?utm_source=atom_feed" rel="related" type="text/html" title="Install alerta on Centos7" />
                <link href="https://devopstales.github.io/home/openshift-restrict-access/?utm_source=atom_feed" rel="related" type="text/html" title="Restrict access to OpenShift routes by IP address" />
            
                <id>https://devopstales.github.io/home/gcp-vm-export/</id>
            
            
            <published>2019-10-04T00:00:00+00:00</published>
            <updated>2019-10-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Step by step guide to export virtual machine running in Google cloud computer engine to your S3 bucket.</p>
<pre tabindex="0"><code>gcloud compute instances list
NAME        ZONE            MACHINE_TYPE               PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP  STATUS
demo1   europe-west1-b  g1-small                                10.132.0.3                TERMINATED
</code></pre><h3 id="create-snapshot">Create snapshot</h3>
<pre tabindex="0"><code>gcloud compute disks snapshot europe-west1-b/disks/demo1 --storage-locatio europe-west1

gcloud compute snapshots list
NAME              DISK_SIZE_GB  SRC_DISK                        STATUS
demo1-backup  30            europe-west1-b/disks/demo1  READY
</code></pre><h3 id="create-custom-image">Create custom image</h3>
<pre tabindex="0"><code>gcloud compute images create demo1-backup --source-snapshot demo1-backup
Created [https://www.googleapis.com/compute/v1/projects/demo-project-223110/global/images/demo1-backup].
NAME              PROJECT               FAMILY  DEPRECATED  STATUS
demo1-backup  demo-project-223110                      READY
</code></pre><h3 id="create-s3-storage">Create S3 storage</h3>
<pre tabindex="0"><code>gsutil mb gs://backup-demo-project-223110/ -l europe-west1
</code></pre><h3 id="export-to-s3-storage">Export to S3 storage</h3>
<pre tabindex="0"><code>gcloud compute images export --destination-uri gs://backup-demo-project-223110/demo1-beckup.tar.gz --image demo1-backup --export-format=vmdk
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Kubernetes]]></title>
            <link href="https://devopstales.github.io/home/ansible-k8s-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/ansible-operator-overview/?utm_source=atom_feed" rel="related" type="text/html" title="Ansible Operator Overview" />
                <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
                <link href="https://devopstales.github.io/home/openshift-restrict-access/?utm_source=atom_feed" rel="related" type="text/html" title="Restrict access to OpenShift routes by IP address" />
                <link href="https://devopstales.github.io/home/openshift-kompose/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Run docker-compoe in Openshift" />
                <link href="https://devopstales.github.io/home/openshift-add-node/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Add Nodes to a Cluster" />
            
                <id>https://devopstales.github.io/home/ansible-k8s-install/</id>
            
            
            <published>2019-10-03T00:00:00+00:00</published>
            <updated>2019-10-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Kubespray is a pre made ansible playbook for Kubernetes installation. In this Post I will show you how to use to install a new Kubernetes cluster.</p>
<H3>Parst of the Kubernetes series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
     <li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
     <li>Part1c: <a href="../../kubernetes/k8s-install-containerd/">Install K8S with containerd and kubeadm</a></li>
     <li>Part1d: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
     <li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a></li>
<!--      <li>Part3b: k8s-multiple-nginx-ingress -->
<!--      <li>Part3c: k8s-contour-ingress -->
     <li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a></li>
<!-- local folder (+ autoprovisioning ) -->
     <li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
     <li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
     <li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
     <li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
     <li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
     <li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a></li>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... -->
     <li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
     <li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
     <li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
     <li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
     <li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
     <li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
     <li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a></li>
<!-- istio-install.md -->
     <li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a></li>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog -->
</ul>


<h3 id="environment">Environment</h3>
<pre tabindex="0"><code>192.168.1.60    deployer.devopstales.intra (LB)
192.168.1.61    master0.devopstales.intra  (master)
192.168.1.62    master1.devopstales.intra  (master)
192.168.1.63    master2.devopstales.intra  (master)
192.168.1.64    worker0.devopstales.intra  (worker)
192.168.1.65    worker1.devopstales.intra  (worker)

# hardware requirement
4 CPU
16G RAM
</code></pre><h3 id="prerequirement">Prerequirement</h3>
<pre tabindex="0"><code># deployer

nano ~/.ssh/config
Host master0
    Hostname master0.devopstales.intra
    User ansible

Host master1
    Hostname master1.devopstales.intra
    User ansible

Host master2
    Hostname master2.devopstales.intra
    User ansible

Host worker0.devopstales.intra
    Hostname worker0.devopstales.intra
    User ansible

Host worker1
    Hostname worker1.devopstales.intra
    User ansible
</code></pre><pre tabindex="0"><code>yum install epel-release -y
yum update -y
yum install python-pip git tmux nano -y
git clone https://github.com/kubernetes-sigs/kubespray.git
cd kubespray
pip install --user -r requirements.txt

cp -rfp inventory/sample inventory/mycluster
</code></pre><h3 id="configurate-installer">Configurate Installer</h3>
<pre tabindex="0"><code>nano inventory/mycluster/inventory.ini
master0   ansible_host=192.168.1.61 ip=192.168.1.61
master1   ansible_host=192.168.1.62 ip=192.168.1.62
master2   ansible_host=192.168.1.63 ip=192.168.1.63
worker0   ansible_host=192.168.1.64 ip=192.168.1.64
worker1   ansible_host=192.168.1.65 ip=192.168.1.65

# ## configure a bastion host if your nodes are not directly reachable
# bastion ansible_host=x.x.x.x ansible_user=some_user

[kube-master]
master0
master1
master2

[etcd]
master0
master1
master2

[kube-node]
worker0
worker1

[calico-rr]

[k8s-cluster:children]
kube-master
kube-node
calico-rr
</code></pre><h3 id="run-the-installer">Run the Installer</h3>
<pre tabindex="0"><code>tmux new -s kubespray
ansible-playbook -i inventory/mycluster/inventory.ini --become \
--user=centos --become-user=root cluster.yml

test install on node:
sudo -i
kubectl get node
NAME      STATUS   ROLES    AGE   VERSION
master0   Ready    master   92m   v1.15.3
master1   Ready    master   91m   v1.15.3
master2   Ready    master   91m   v1.15.3
worker0   Ready    &lt;none&gt;   90m   v1.15.3
worker1   Ready    &lt;none&gt;   90m   v1.15.3

kubectl config get-clusters
</code></pre><h3 id="lets-configure-an-external-loadbalancer">Let’s configure an external loadbalancer</h3>
<pre tabindex="0"><code>sudo firewall-cmd --add-port=6443/tcp --permanent
sudo firewall-cmd --reload

yum -y install haproxy

nano ..
listen k8s-apiserver-https
  bind *:6443
  option ssl-hello-chk
  mode tcp
  balance roundrobin
  timeout client 3h
  timeout server 3h
  server master0 192.168.1.61:6443
  server master1 192.168.1.62:6443
  server master2 192.168.1.63:6443

systemctl enable --now haproxy
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install alerta on Centos8]]></title>
            <link href="https://devopstales.github.io/home/alerta-on-centos8/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/alerta-on-centos7/?utm_source=atom_feed" rel="related" type="text/html" title="Install alerta on Centos7" />
                <link href="https://devopstales.github.io/monitoring/alerta-on-centos8/?utm_source=atom_feed" rel="related" type="text/html" title="Install alerta on Centos8" />
                <link href="https://devopstales.github.io/monitoring/alerta-on-centos7/?utm_source=atom_feed" rel="related" type="text/html" title="Install alerta on Centos7" />
                <link href="https://devopstales.github.io/home/gitlab-install/?utm_source=atom_feed" rel="related" type="text/html" title="Gitlab Install" />
                <link href="https://devopstales.github.io/home/prometheus-alertmanagger/?utm_source=atom_feed" rel="related" type="text/html" title="Install Alertmanagger" />
            
                <id>https://devopstales.github.io/home/alerta-on-centos8/</id>
            
            
            <published>2019-09-28T00:00:00+00:00</published>
            <updated>2019-09-28T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>AIn this post I will show you how to install alerta monitoring dashboard on Centos 8.</p>
<pre tabindex="0"><code>yum install epel-release -y
yum upgrade -y
</code></pre><h3 id="install-and-configure-postgresql">Install and configure postgresql</h3>
<p>In a previous post I wrote about how to <a href="/linux/install-postgresql/#install-postgresql-10-on-centos-8">Install PostgreSQL 10</a></p>
<pre tabindex="0"><code>sudo su - postgres
createuser alerta
createdb -O alerta alerta
psql
ALTER USER &quot;alerta&quot; WITH PASSWORD 'alerta';
\q
exit
</code></pre><h3 id="install-python3-packages">install python3 packages</h3>
<pre tabindex="0"><code>yum install python3 python3-pip python3-setuptools python3-devel python3-psycopg2 gcc git tmux nginx
</code></pre><p>Create user for alerta and install python3 moduls with it.</p>
<pre tabindex="0"><code>useradd alerta
usermod -aG alerta nginx
su - alerta
pip3 install --user wheel alerta-server alerta uwsgi
</code></pre><h1 id="alerta-server">alerta server</h1>
<pre tabindex="0"><code>nano /etc/alertad.conf
DATABASE_URL = 'postgres://alerta:alerta@localhost:5432/alerta'
PLUGINS=['reject']
ALLOWED_ENVIRONMENTS=['Production', 'Development', 'Code']
</code></pre><p>We use uwsgi to create a unix socket wgere the alerta webgui can connect.</p>
<pre tabindex="0"><code>sudo mkdir -p /var/log/uwsgi
sudo chown -R alerta:alerta /var/log/uwsgi
mkdir /var/run/alerta
chown -R alerta.alerta /var/run/alerta/

echo &quot;from alerta import app&quot; &gt; /usr/share/nginx/wsgi.py
</code></pre><pre tabindex="0"><code>nano /etc/uwsgi.ini
[uwsgi]
chdir = /usr/share/nginx/
mount = /api=wsgi.py
callable = app
manage-script-name = true
env = BASE_URL=/api

master = true
processes = 5
#logger = syslog:alertad
logto = /var/log/uwsgi/%n.log

socket = /var/run/alerta/uwsgi.sock
chmod-socket = 664
uid = alerta
gid = alerta
vacuum = true

die-on-term = true
</code></pre><pre tabindex="0"><code>nano /etc/systemd/system/uwsgi.service
[Unit]
Description=uWSGI service
After=syslog.target

[Service]
ExecStart=/home/alerta/.local/bin/uwsgi --ini /etc/uwsgi.ini
RuntimeDirectory=uwsgi
Type=notify
StandardError=syslog
NotifyAccess=all

[Install]
WantedBy=multi-user.target
</code></pre><pre tabindex="0"><code>systemctl enable uwsgi
systemctl start uwsgi
systemctl status uwsgi
</code></pre><h1 id="alerta-webgui">alerta webgui</h1>
<pre tabindex="0"><code>wget https://github.com/alerta/alerta-webui/releases/latest/download/alerta-webui.tar.gz
tar zxvf alerta-webui.tar.gz
mv dist/ /usr/share/nginx/alerta
echo '{&quot;endpoint&quot;: &quot;/api&quot;}' &gt; /usr/share/nginx/dist/config.json
</code></pre><pre tabindex="0"><code>nano /etc/nginx/nginx.conf
    #server {
    #    listen       80 default_server;
    #    listen       [::]:80 default_server;
    #    server_name  _;
    #    root         /usr/share/nginx/html;

    #    # Load configuration files for the default server block.
    #    include /etc/nginx/default.d/*.conf;

     #   location / {
     #   }

     #   error_page 404 /404.html;
     #       location = /40x.html {
     #   }

     #   error_page 500 502 503 504 /50x.html;
     #       location = /50x.html {
     #   }
    #}
</code></pre><pre tabindex="0"><code>nano /etc/nginx/conf.d/alerta.conf
server {
        listen 80 default_server;

        location /api { try_files $uri @api; }
        location @api {
            include uwsgi_params;
            uwsgi_pass unix:/var/run/alerta/uwsgi.sock;
            proxy_set_header Host $host:$server_port;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }

        location / {
                root /usr/share/nginx/alerta;
        }
}
</code></pre><pre tabindex="0"><code>nginx -t
systemctl restart nginx
systemctl enable nginx
</code></pre><h3 id="test">test</h3>
<p>Send testalert with alerta client.</p>
<pre tabindex="0"><code>su - alerta

echo '[DEFAULT]
endpoint = http://localhost/api' &gt;  $HOME/.alerta.conf

echo 'export PATH=$PATH:/home/alerta/.local/bin' &gt;&gt; ~/.bashrc
PATH=$PATH:/home/alerta/.local/bin

alerta query
alerta send --resource net01 --event down --severity critical --environment Code --service Network --text 'net01 is down.'
alerta query
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install alerta on Centos7]]></title>
            <link href="https://devopstales.github.io/home/alerta-on-centos7/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/monitoring/alerta-on-centos7/?utm_source=atom_feed" rel="related" type="text/html" title="Install alerta on Centos7" />
                <link href="https://devopstales.github.io/home/gitlab-install/?utm_source=atom_feed" rel="related" type="text/html" title="Gitlab Install" />
                <link href="https://devopstales.github.io/home/prometheus-alertmanagger/?utm_source=atom_feed" rel="related" type="text/html" title="Install Alertmanagger" />
                <link href="https://devopstales.github.io/home/openshift-restrict-access/?utm_source=atom_feed" rel="related" type="text/html" title="Restrict access to OpenShift routes by IP address" />
                <link href="https://devopstales.github.io/home/openshift-kompose/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Run docker-compoe in Openshift" />
            
                <id>https://devopstales.github.io/home/alerta-on-centos7/</id>
            
            
            <published>2019-09-27T00:00:00+00:00</published>
            <updated>2019-09-27T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>AIn this post I will show you how to install alerta monitoring dashboard on Centos 7.</p>
<pre tabindex="0"><code>yum install epel-release nano -y
yum upgrade -y
</code></pre><h3 id="install-and-configure-postgresql">Install and configure postgresql</h3>
<p>In a previous post I wrote about how to <a href="/linux/install-postgresql/#install-postgresql-10-on-centos-7">Install PostgreSQL 10</a></p>
<pre tabindex="0"><code>sudo su - postgres
createuser alerta
createdb -O alerta alerta
psql
ALTER USER &quot;alerta&quot; WITH PASSWORD 'alerta';
\q
</code></pre><h3 id="install-python3-packages">install python3 packages</h3>
<pre tabindex="0"><code>yum install https://centos7.iuscommunity.org/ius-release.rpm -y
yum install python36u python36u-pip python36u-setuptools python36u-devel gcc git tmux
yum install uwsgi-plugin-psgi nginx -y
</code></pre><h1 id="alerta-server">alerta server</h1>
<pre tabindex="0"><code>python3.6 -m venv alerta
alerta/bin/pip install --upgrade pip wheel alerta-server alerta uwsgi
</code></pre><pre tabindex="0"><code>nano /etc/alertad.conf
DATABASE_URL = 'postgres://alerta:alerta@localhost:5432/alerta'
PLUGINS=['reject']
ALLOWED_ENVIRONMENTS=['Production', 'Development', 'Code']
</code></pre><p>We use uwsgi to create a unix socket wgere the alerta webgui can connect.</p>
<pre tabindex="0"><code>sudo mkdir -p /var/log/uwsgi
sudo chown -R nginx:nginx /var/log/uwsgi
mkdir /var/run/alerta
chown -R nginx.nginx /var/run/alerta/

nano /var/www/wsgi.py
from alerta import app
</code></pre><pre tabindex="0"><code>nano /etc/uwsgi.ini
[uwsgi]
chdir = /var/www
mount = /api=wsgi.py
callable = app
manage-script-name = true
env = BASE_URL=/api

master = true
processes = 5
#logger = syslog:alertad
logto = /var/log/uwsgi/%n.log

socket = /var/run/alerta/uwsgi.sock
chmod-socket = 664
uid = nginx
gid = nginx
vacuum = true

die-on-term = true
</code></pre><pre tabindex="0"><code>nano /etc/systemd/system/alerta-app.service
[Unit]
Description=uWSGI service
After=syslog.target

[Service]
ExecStart=/opt/alerta/bin/uwsgi --ini /etc/uwsgi.ini
RuntimeDirectory=uwsgi
Type=notify
StandardError=syslog
NotifyAccess=all

[Install]
WantedBy=multi-user.target
</code></pre><pre tabindex="0"><code>systemctl enable alerta-app
systemctl start alerta-app
systemctl status alerta-app
</code></pre><h1 id="alerta-webgui">alerta webgui</h1>
<pre tabindex="0"><code>wget https://github.com/alerta/alerta-webui/releases/latest/download/alerta-webui.tar.gz
tar zxvf alerta-webui.tar.gz
mv dist/ /usr/share/nginx/alerta
echo '{&quot;endpoint&quot;: &quot;/api&quot;}' &gt; /usr/share/nginx/dist/config.json
</code></pre><pre tabindex="0"><code>nano /etc/nginx/nginx.conf
    #server {
    #    listen       80 default_server;
    #    listen       [::]:80 default_server;
    #    server_name  _;
    #    root         /usr/share/nginx/html;

    #    # Load configuration files for the default server block.
    #    include /etc/nginx/default.d/*.conf;

     #   location / {
     #   }

     #   error_page 404 /404.html;
     #       location = /40x.html {
     #   }

     #   error_page 500 502 503 504 /50x.html;
     #       location = /50x.html {
     #   }
    #}
</code></pre><pre tabindex="0"><code>nano /etc/nginx/conf.d/alerta.conf
server {
        listen 80 default_server;

        location /api { try_files $uri @api; }
        location @api {
            include uwsgi_params;
            uwsgi_pass unix:/var/run/alerta/uwsgi.sock;
            proxy_set_header Host $host:$server_port;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }

        location / {
                root /usr/share/nginx/alerta;
        }
}
</code></pre><pre tabindex="0"><code>nginx -t
systemctl restart nginx
systemctl enable nginx
</code></pre><h3 id="test">test</h3>
<p>Send testalert with alerta client.</p>
<pre tabindex="0"><code>nano $HOME/.alerta.conf
[DEFAULT]
endpoint = http://localhost/api

/opt/alerta/bin/alerta query
/opt/alerta/bin/alerta send --resource net01 --event down --severity critical --environment Code --service Network --text 'net01 is down.'
/opt/alerta/bin/alerta query
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Restrict access to OpenShift routes by IP address]]></title>
            <link href="https://devopstales.github.io/home/openshift-restrict-access/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure Openshift vSphere Cloud Provider" />
                <link href="https://devopstales.github.io/home/openshift-kompose/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Run docker-compoe in Openshift" />
                <link href="https://devopstales.github.io/home/ansible-operator-overview/?utm_source=atom_feed" rel="related" type="text/html" title="Ansible Operator Overview" />
                <link href="https://devopstales.github.io/home/openshift-add-node/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Add Nodes to a Cluster" />
                <link href="https://devopstales.github.io/home/openshift-letsencrypt/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Letsencrypt certificates" />
            
                <id>https://devopstales.github.io/home/openshift-restrict-access/</id>
            
            
            <published>2019-09-20T00:00:00+00:00</published>
            <updated>2019-09-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how can restrict access to the routes by source IP address.</p>
<H3>Parst of the Openshift series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
     <li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
     <li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
     <li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
     <li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
     <li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
     <li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
     <li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
     <li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
     <li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
     <li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
     <li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
     <li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
     <li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
     <li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
     <li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
     <li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
     <li>Part16a: <a href="../../kubernetes/openshift-elasticsearch-error/">Opeshift elasticsearch search-guard error</a></li>
     <li>Part16b: <a href="../../kubernetes/openshift-log4shell/">Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)</a></li>
</ul>


<h3 id="restricting-access-to-a-route">Restricting access to a route</h3>
<p>After creating and exposing a route, you can add an annotation to the route specifying the IP address(es) that you would like to whitelist. Whitelisting a IP address automatically blacklists everything else.</p>
<pre tabindex="0"><code>oc annotate route test-route haproxy.router.openshift.io/ip_whitelist=192.168.0.0/24
</code></pre><p>To allow several IP addresses through to the route, separate each IP with a space:</p>
<pre tabindex="0"><code>oc annotate route test-route haproxy.router.openshift.io/ip_whitelist=192.168.1.10 180.5.61.153 192.168.1.0/24 192.168.0.0/24
</code></pre><p>To delete the IPs from the annotation, you can run the command:</p>
<pre tabindex="0"><code>oc annotate route test-route haproxy.router.openshift.io/ip_whitelist-
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift: Run docker-compoe in Openshift]]></title>
            <link href="https://devopstales.github.io/home/openshift-kompose/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-add-node/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Add Nodes to a Cluster" />
                <link href="https://devopstales.github.io/home/openshift-letsencrypt/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Letsencrypt certificates" />
                <link href="https://devopstales.github.io/home/openshift-hostalreadyclaimed/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Error: HostAlreadyClaimed" />
                <link href="https://devopstales.github.io/home/openshift-gitlabrunner/?utm_source=atom_feed" rel="related" type="text/html" title="Install Gitlab runner on Openshift" />
                <link href="https://devopstales.github.io/home/openshift-extregistry/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: External registry" />
            
                <id>https://devopstales.github.io/home/openshift-kompose/</id>
            
            
            <published>2019-09-08T00:00:00+00:00</published>
            <updated>2019-09-08T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Kompose is a open source tool that uses &ldquo;docker-compose&rdquo; file to deploy on kubernetes. Openshift is also Kubernetes based and Kompose is support Openshift too.</p>
<H3>Parst of the Openshift series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
     <li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
     <li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
     <li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
     <li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
     <li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
     <li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
     <li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
     <li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
     <li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
     <li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
     <li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
     <li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
     <li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
     <li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
     <li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
     <li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
     <li>Part16a: <a href="../../kubernetes/openshift-elasticsearch-error/">Opeshift elasticsearch search-guard error</a></li>
     <li>Part16b: <a href="../../kubernetes/openshift-log4shell/">Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)</a></li>
</ul>


<p>Let’s say we have project with multiple microsevices that needs to deploy on Openshift and they have docker-compose.yml and Dockerfile. How do we deploy on Openshift?<!-- raw HTML omitted --></p>
<p>Use Kompose to convert the docker-compose file to openshift compatible kubernetes config.</p>
<pre tabindex="0"><code>kompose convert -f docker-compose.yaml --provider=openshift
</code></pre><p>This command creates a separet yaml file for all kubernetes building block like  &ldquo;-imagestream.yaml&rdquo;, &ldquo;-service.yaml&rdquo;, &ldquo;-deploymentconfig.yaml&rdquo; for each microservice. We can use these config files to deploy on Openshift easily.</p>
<pre tabindex="0"><code>kompose up --provider=openshift -f docker-compose.yml --build build-config --namespace=devopstales
</code></pre><p>If you have &ldquo;build&rdquo; option in docker-compose file, kompose automatically detects remote git url to deploy automatically.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to backup Graylog's logs in elasticsearch]]></title>
            <link href="https://devopstales.github.io/home/elasticsearch-backup/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/elasticsearch-backup/?utm_source=atom_feed" rel="related" type="text/html" title="How to backup Graylog&#39;s logs in elasticsearch" />
                <link href="https://devopstales.github.io/home/k8s-metallb/?utm_source=atom_feed" rel="related" type="text/html" title="Install MetalLB load balancer for Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-tillerless-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Tillerless helm2 install" />
                <link href="https://devopstales.github.io/home/k8s-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/clonedeploy/?utm_source=atom_feed" rel="related" type="text/html" title="Install clonedeploy pxeboot server" />
            
                <id>https://devopstales.github.io/home/elasticsearch-backup/</id>
            
            
            <published>2019-09-07T00:00:00+00:00</published>
            <updated>2019-09-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Graylog store the log data in elasticsearch so I will show you how to create and restore snapshot with elasticsearch.</p>
<h3 id="requirement">Requirement</h3>
<p>First you will need to add the repo.path location to your elasticsearch.yml. This is the local path of the folder where the snapshot files will store.</p>
<pre tabindex="0"><code>mkdir -p /mnt/elasticsearch-backup
chown -R elasticsearch. /mnt/elasticsearch-backup

cat &gt;&gt; /etc/elasticsearch/elasticsearch.yml &lt;&lt; EOF
path.repo: [&quot;/mnt/elasticsearch-backup&quot;]
EOF

systemctl restart elasticsearch
</code></pre><h3 id="elasticsearch">Elasticsearch</h3>
<p>Elasticsearch needs to know the backup path by registering a backup repository:</p>
<pre tabindex="0"><code>curl -XPUT 'http://localhost:9200/_snapshot/my_backup' -d {
  &quot;type&quot;: &quot;fs&quot;,
  &quot;settings&quot;: {
     &quot;location&quot;: &quot;/mnt/elasticsearch-backup&quot;,
     &quot;compress&quot;: true
  }
}'
</code></pre><h3 id="create-backup">Create Backup</h3>
<pre tabindex="0"><code>curl -XPUT &quot;localhost:9200/_snapshot/my_backup/snapshot_1?wait_for_completion=true&quot;

# list snapshots:
curl -XGET 'localhost:9200/_snapshot/my_backup/_all?pretty'
</code></pre><h3 id="restore-backup">Restore backup</h3>
<pre tabindex="0"><code>curl -XPOST &quot;localhost:9200/_snapshot/my_backup/snapshot_1/_restore?wait_for_completion=true&quot;
</code></pre><h3 id="delete-snapshot">Delete snapshot</h3>
<pre tabindex="0"><code>curl -XDELETE 'localhost:9200/_snapshot/my_backup/snapshot_1'
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install MetalLB load balancer for Kubernetes]]></title>
            <link href="https://devopstales.github.io/home/k8s-metallb/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-nginx-ingress/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
                <link href="https://devopstales.github.io/kubernetes/k8s-metallb/?utm_source=atom_feed" rel="related" type="text/html" title="Install MetalLB load balancer for Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-tillerless-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Tillerless helm2 install" />
                <link href="https://devopstales.github.io/home/k8s-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install kubernetes with kubeadm" />
            
                <id>https://devopstales.github.io/home/k8s-metallb/</id>
            
            
            <published>2019-08-08T00:00:00+00:00</published>
            <updated>2019-08-08T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this tutorial I will show you how to install  Metal LB load balancer running on Kubernetes (k8s).</p>
<H3>Parst of the Kubernetes series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
     <li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
     <li>Part1c: <a href="../../kubernetes/k8s-install-containerd/">Install K8S with containerd and kubeadm</a></li>
     <li>Part1d: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
     <li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a></li>
<!--      <li>Part3b: k8s-multiple-nginx-ingress -->
<!--      <li>Part3c: k8s-contour-ingress -->
     <li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a></li>
<!-- local folder (+ autoprovisioning ) -->
     <li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
     <li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
     <li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
     <li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
     <li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
     <li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a></li>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... -->
     <li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
     <li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
     <li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
     <li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
     <li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
     <li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
     <li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a></li>
<!-- istio-install.md -->
     <li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a></li>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog -->
</ul>


<h3 id="enviroment">Enviroment</h3>
<pre tabindex="0"><code>kubectl get node
NAME     STATUS   ROLES    EXTERNAL-IP
host-1   Ready    master   203.0.113.1
host-2   Ready    node     203.0.113.2
host-3   Ready    node     203.0.113.3
host-4   Ready    node     203.0.113.4
</code></pre><p>MetalLB provides a network load-balancer implementation for Kubernetes clusters that do not run on a supported cloud provider, effectively allowing the usage of LoadBalancer Services within ber-metal Installation. Kubernetes does not offer an implementation of network load-balancers (Services of type LoadBalancer) for bare metal clusters. The implementations of Network LB that Kubernetes does ship with are all glue code that calls out to various IaaS platforms (GCP, AWS, Azure…). If you’re not running on a supported IaaS platform (GCP, AWS, Azure…), LoadBalancers will remain in the “pending” state indefinitely when created.</p>
<p><img src="/img/include/metallb.jpg" alt="Example image"  class="zoomable" /></p>
<p>First we need to apply the MetalLB manifest.</p>
<pre tabindex="0"><code>kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.8.1/manifests/metallb.yaml
</code></pre><p>Create a metallb-configmap.yaml file and modify your IP range accordingly.</p>
<pre tabindex="0"><code>cat &lt; EOF &gt; metallb-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: my-ip-space
      protocol: layer2
      addresses:
      - 203.0.113.2-203.0.113.4
EOF

kubectl apply -f metallb-config.yaml
kubectl get pods -n metallb-system
</code></pre><p>Exposing a service through the load balancer</p>
<pre tabindex="0"><code>cat &lt;EOF&gt;&gt; nginx.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  type: LoadBalancer
  selector:
    app: nginx
  ports:
  - port: 80
    name: http
EOF

kubectl apply -f nginx-deployment.yaml
kubectl get svc
NAME           TYPE           CLUSTER-IP       EXTERNAL-IP    PORT(S)        AGE
nginx          LoadBalancer   10.109.51.83     203.0.113.2    80:30452/TCP   5m
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install s3cmd with CEHP Radosgateway]]></title>
            <link href="https://devopstales.github.io/home/s3cmd-with-radosgw/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/s3cmd-with-radosgw/</id>
            
            
            <published>2019-08-04T00:00:00+00:00</published>
            <updated>2019-08-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>s3cmd is a cli utility for s3.</p>
<pre tabindex="0"><code>root@pve1:~# apt-get install s3cmd
root@pve1:~# s3cmd --configure
Access Key: xxxxxxxxxxxxxxxxxxxxxx
Secret Key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

root@pve1:~#  s3cmd mb s3://devopstales
Bucket 's3://devopstales/' created
</code></pre><h3 id="create-user">Create User</h3>
<pre tabindex="0"><code>adosgw-admin user create --uid={username} --display-name=&quot;{display-name}&quot; \[--email={email}\]
radosgw-admin user create --uid=devopstales --display-name=&quot;devopstales&quot; --email=devopstales@devopstales.intra
</code></pre><h3 id="user-info">User Info</h3>
<pre tabindex="0"><code>radosgw-admin user info --uid=devopstales
</code></pre><h3 id="modify-user">Modify User</h3>
<pre tabindex="0"><code>radosgw-admin user modify --uid=devopstales --display-name=&quot;John Doe&quot;
</code></pre><h3 id="disable-and-enable-user">Disable and Enable User</h3>
<pre tabindex="0"><code>radosgw-admin user suspend --uid=devopstales
radosgw-admin user enable --uid=devopstales
</code></pre><h3 id="remove-user">Remove User</h3>
<pre tabindex="0"><code>radosgw-admin user rm --uid=devopstales
</code></pre><h1 id="add-quota-for-user">Add quota for User</h1>
<pre tabindex="0"><code>radosgw-admin quota set --uid=&quot;devopstales@devopstales.intra&quot; --quota-scope=bucket --max-size=30G
radosgw-admin quota enable --quota-scope=bucket --uid=&quot;devopstales@devopstales.intra&quot;
</code></pre><h3 id="set-bucket-policy">Set Bucket Policy</h3>
<p>Create a Bucket Policy fot <a href="mailto:devopstales@devopstales.intra">devopstales@devopstales.intra</a> to allow privileges for devopstales Bucket.</p>
<pre tabindex="0"><code>cat &lt;EOF&gt; bucket-policy.json
{
  &quot;Version&quot;: &quot;2012-10-17&quot;,
  &quot;Statement&quot;: [{
    &quot;Effect&quot;: &quot;Allow&quot;,
    &quot;Principal&quot;: {&quot;AWS&quot;: [&quot;arn:aws:iam:::user/devopstales2@devopstales.intra&quot;]},
    &quot;Action&quot;: [&quot;s3:ListBucket&quot;, &quot;s3:GetObject&quot;, &quot;s3:PutObject&quot;, &quot;s3:DeleteObject&quot;],
    &quot;Resource&quot;: [
      &quot;arn:aws:s3:::devopstales&quot;,
      &quot;arn:aws:s3:::devopstales/*&quot;
    ]
  }]
}
EOF

s3cmd setpolicy ./bucket-policy.json s3://devopstales
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Tillerless helm2 install]]></title>
            <link href="https://devopstales.github.io/home/k8s-tillerless-helm/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-copy-secret/?utm_source=atom_feed" rel="related" type="text/html" title="Copying Kubernetes Secrets Between Namespaces" />
                <link href="https://devopstales.github.io/home/k8s-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/k8s-nginx-ingress/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
                <link href="https://devopstales.github.io/home/k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install kubernetes with kubeadm" />
                <link href="https://devopstales.github.io/home/openshift-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Helm" />
            
                <id>https://devopstales.github.io/home/k8s-tillerless-helm/</id>
            
            
            <published>2019-07-23T00:00:00+00:00</published>
            <updated>2019-07-23T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>It looks like it is not so hard to have Tillerless Helm. So let me go to more details.</p>
<H3>Parst of the Kubernetes series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
     <li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
     <li>Part1c: <a href="../../kubernetes/k8s-install-containerd/">Install K8S with containerd and kubeadm</a></li>
     <li>Part1d: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
     <li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a></li>
<!--      <li>Part3b: k8s-multiple-nginx-ingress -->
<!--      <li>Part3c: k8s-contour-ingress -->
     <li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a></li>
<!-- local folder (+ autoprovisioning ) -->
     <li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
     <li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
     <li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
     <li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
     <li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
     <li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a></li>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... -->
     <li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
     <li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
     <li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
     <li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
     <li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
     <li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
     <li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a></li>
<!-- istio-install.md -->
     <li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a></li>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog -->
</ul>


<p>Since Helm v2, helm got a server part called The Tiller Server which is interacts with the helm client, and the Kubernetes API server. By default helm init installs a Tiller deployment to Kubernetes clusters and communicates via gRPC.</p>
<p><img src="/img/include/tiller1.png" alt="Example image"  class="zoomable" /></p>
<p>The community voted that Helm v3 should be Tillerless. If we can run tiller localli we can achieve the same goal.</p>
<p><img src="/img/include/tiller2.png" alt="Example image"  class="zoomable" /></p>
<p>There is a helm plugin for this same purpose.</p>
<pre tabindex="0"><code>$ helm plugin install https://github.com/rimusz/helm-tiller
Installed plugin: tiller
</code></pre><h3 id="use-this-plugin-locally">Use this plugin locally</h3>
<pre tabindex="0"><code>helm tiller start
</code></pre><p>It will start the tiller locally and kube-system namespace will be used to store helm releases but you can change the name of the namespace if you want:</p>
<pre tabindex="0"><code>helm tiller start my-team-namespace

# stop tiller
helm tiller stop
</code></pre><h3 id="how-to-use-this-plugin-in-cicd-pipelines">How to use this plugin in CI/CD pipelines</h3>
<pre tabindex="0"><code>helm tiller start-ci
export HELM_HOST=localhost:44134
</code></pre><p>Then your helm will know where to connect to Tiller and you do not need to make any changes in your CI pipelines.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes Ceph RBD for dynamic provisioning]]></title>
            <link href="https://devopstales.github.io/home/k8s-ceph/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/install-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ceph cluster" />
                <link href="https://devopstales.github.io/home/ceph-block-device/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph Block Device" />
                <link href="https://devopstales.github.io/home/ceph-cephfs/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph CephFS" />
                <link href="https://devopstales.github.io/kubernetes/k8s-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/k8s-nginx-ingress/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
            
                <id>https://devopstales.github.io/home/k8s-ceph/</id>
            
            
            <published>2019-07-18T00:00:00+00:00</published>
            <updated>2019-07-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how can you use CEPH RBD for persistent storagi on Kubernetes.</p>
<H3>Parst of the Kubernetes series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
     <li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
     <li>Part1c: <a href="../../kubernetes/k8s-install-containerd/">Install K8S with containerd and kubeadm</a></li>
     <li>Part1d: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
     <li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a></li>
<!--      <li>Part3b: k8s-multiple-nginx-ingress -->
<!--      <li>Part3c: k8s-contour-ingress -->
     <li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a></li>
<!-- local folder (+ autoprovisioning ) -->
     <li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
     <li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
     <li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
     <li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
     <li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
     <li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a></li>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... -->
     <li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
     <li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
     <li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
     <li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
     <li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
     <li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
     <li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a></li>
<!-- istio-install.md -->
     <li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a></li>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog -->
</ul>


<h3 id="environment">Environment</h3>
<pre tabindex="0"><code># openshift cluster
192.168.1.41  kubernetes01 # master node
192.168.1.42  kubernetes02 # frontend node
192.168.1.43  kubernetes03 # worker node
192.168.1.44  kubernetes04 # worker node
192.168.1.45  kubernetes05 # worker node

# ceph cluster
192.168.1.31    ceph01
192.168.1.32    ceph02
192.168.1.33    ceph03
</code></pre><h3 id="prerequirement">Prerequirement</h3>
<p>RBD volume provisioner needs admin key from Ceph to provision storage. To get the admin key from Ceph cluster use this command:</p>
<pre tabindex="0"><code>sudo ceph --cluster ceph auth get-key client.admin

nano ceph-admin-secret.yaml
apiVersion: v1
data:
  key: QVFBOFF2SlZheUJQRVJBQWgvS2cwT1laQUhPQno3akZwekxxdGc9PQ==
kind: Secret
metadata:
  name: ceph-admin-secret
  namespace: kube-system
type: kubernetes.io/rbd

</code></pre><p>I will also create a separate Ceph pool for</p>
<pre tabindex="0"><code>sudo ceph --cluster ceph osd pool create k8s 1024 1024
sudo ceph --cluster ceph auth get-or-create client.k8s mon 'allow r' osd 'allow rwx pool=k8s'
sudo ceph --cluster ceph auth get-key client.k8s

nano ceph-secret-k8s.yaml
apiVersion: v1
data:
  key: QVFBOFF2SlZheUJQRVJBQWgvS2ctS2htOFNSZnRvclJPRk1jdXc9PQ==
kind: Secret
metadata:
  name: ceph-secret-k8s
  namespace: kube-system
type: kubernetes.io/rbd

</code></pre><pre tabindex="0"><code># on all openshift node
wget http://download.proxmox.com/debian/proxmox-ve-release-5.x.gpg \
-O /etc/apt/trusted.gpg.d/proxmox-ve-release-5.x.gpg
chmod +r /etc/apt/trusted.gpg.d/proxmox-ve-release-5.x.gpg

echo deb http://download.proxmox.com/debian/ceph-luminous $(lsb_release -sc) main \
&gt; /etc/apt/sources.list.d/ceph.list

apt-get update
apt-get install ceph-common -y
</code></pre><pre tabindex="0"><code>cat &lt;&lt;EOF &gt; rbd-provisioner.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: rbd-provisioner
rules:
  - apiGroups: [&quot;&quot;]
    resources: [&quot;persistentvolumes&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;persistentvolumeclaims&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]
  - apiGroups: [&quot;storage.k8s.io&quot;]
    resources: [&quot;storageclasses&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;events&quot;]
    verbs: [&quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;services&quot;]
#    resourceNames: [&quot;kube-dns&quot;]
    verbs: [&quot;list&quot;, &quot;get&quot;]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: rbd-provisioner
subjects:
  - kind: ServiceAccount
    name: rbd-provisioner
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: rbd-provisioner
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: rbd-provisioner
rules:
- apiGroups: [&quot;&quot;]
  resources: [&quot;secrets&quot;]
  verbs: [&quot;get&quot;]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: rbd-provisioner
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: rbd-provisioner
subjects:
- kind: ServiceAccount
  name: rbd-provisioner
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: rbd-provisioner
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: rbd-provisioner
spec:
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: rbd-provisioner
    spec:
      containers:
      - name: rbd-provisioner
        image: &quot;quay.io/external_storage/rbd-provisioner:v1.0.0-k8s1.10&quot;
        env:
        - name: PROVISIONER_NAME
          value: ceph.com/rbd
      serviceAccount: rbd-provisioner
EOF

kubectl create -n kube-system -f rbd-provisioner.yaml
kubectl get pods -l app=rbd-provisioner -n kube-system
</code></pre><p>Please check that <code>quay.io/external_storage/rbd-provisioner:latest</code> image has the same Ceph version as your Ceph cluster.</p>
<pre tabindex="0"><code>docker pull quay.io/external_storage/rbd-provisioner:latest
docker history quay.io/external_storage/rbd-provisioner:latest | grep CEPH_VERSION

# pfroxmox ceph use luminous so I'will use v1.0.0-k8s1.10
docker history quay.io/external_storage/rbd-provisioner:v1.0.0-k8s1.10 | grep CEPH_VERSION
&lt;missing&gt;           13 months ago       /bin/sh -c #(nop)  ENV CEPH_VERSION=luminous    0B
</code></pre><pre tabindex="0"><code># on one openshift master node
nano  k8s-storage.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: &quot;true&quot;
  name: k8s
parameters:
  adminId: admin
  adminSecretName: ceph-admin-secret
  adminSecretNamespace: kube-system
  imageFeatures: layering
  imageFormat: &quot;2&quot;
  monitors: 192.168.1.31.xip.io:6789, 192.168.1.32.xip.io:6789, 192.168.1.33.xip.io:6789
  pool: k8s
  userId: k8s
  userSecretName: ceph-secret-k8s
provisioner: ceph.com/rbd
reclaimPolicy: Delete
volumeBindingMode: Immediate
allowVolumeExpansion: true


kubectl apply -f ceph-admin-secret.yaml
kubectl apply -f ceph-secret-k8s.yaml
kubectl apply -f k8s-storage.yaml
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install clonedeploy pxeboot server]]></title>
            <link href="https://devopstales.github.io/home/clonedeploy/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/clonedeploy/?utm_source=atom_feed" rel="related" type="text/html" title="Install clonedeploy pxeboot server" />
                <link href="https://devopstales.github.io/home/k8s-nginx-ingress/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
                <link href="https://devopstales.github.io/home/k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install kubernetes with kubeadm" />
                <link href="https://devopstales.github.io/home/ansible-operator-overview/?utm_source=atom_feed" rel="related" type="text/html" title="Ansible Operator Overview" />
                <link href="https://devopstales.github.io/home/graylog3-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Analyzing PFsense logs in Graylog3" />
            
                <id>https://devopstales.github.io/home/clonedeploy/</id>
            
            
            <published>2019-07-17T00:00:00+00:00</published>
            <updated>2019-07-17T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I’ll show you how to create network booting (PXE) with clusterdeploy.</p>
<h3 id="install-web-application">Install Web Application</h3>
<pre tabindex="0"><code>yum -y install yum-utils
yum -y install epel-release
rpm --import &quot;http://keyserver.ubuntu.com/pks/lookup?op=get&amp;search=0x3FA7E0328081BFF6A14DA29AA6A19B38D3D831EF&quot;
yum-config-manager --add-repo http://download.mono-project.com/repo/centos7/

yum -y install mono-devel apache2-mod_mono httpd udpcast lz4 mkisofs wget
</code></pre><pre tabindex="0"><code>wget &quot;https://sourceforge.net/projects/clonedeploy/files/CloneDeploy 1.4.0/clonedeploy-1.4.0.tar.gz&quot;
tar xvzf clonedeploy-1.4.0.tar.gz
cd clonedeploy
cp clonedeploy.conf /etc/httpd/conf.d/
mkdir /var/www/html/clonedeploy
cp -r frontend /var/www/html/clonedeploy
cp -r api /var/www/html/clonedeploy
cp -r tftpboot /

ln -s ../../images /tftpboot/proxy/bios/images
ln -s ../../images /tftpboot/proxy/efi32/images
ln -s ../../images /tftpboot/proxy/efi64/images
ln -s ../../kernels /tftpboot/proxy/bios/kernels
ln -s ../../kernels /tftpboot/proxy/efi32/kernels
ln -s ../../kernels /tftpboot/proxy/efi64/kernels

mkdir -p /cd_dp/images
mkdir /cd_dp/resources
mkdir /var/www/.mono
mkdir /usr/share/httpd/.mono
mkdir /etc/mono/registry
chown -R apache:apache /tftpboot /cd_dp /var/www/html/clonedeploy /var/www/.mono /usr/share/httpd/.mono /etc/mono/registry
chmod 1777 /tmp

sysctl fs.inotify.max_user_instances=1024
echo fs.inotify.max_user_instances=1024 &gt;&gt; /etc/sysctl.conf
chkconfig httpd on
</code></pre><h3 id="install-database">Install Database</h3>
<pre tabindex="0"><code>echo &quot;[mariadb]
name = MariaDB
baseurl = http://yum.mariadb.org/10.3/centos7-amd64
gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDB
gpgcheck=1&quot; &gt;&gt; /etc/yum.repos.d/MariaDB.repo

yum -y install mariadb-server
service mariadb start
chkconfig mariadb on

mysql

create database clonedeploy;
CREATE USER 'cduser'@'localhost' IDENTIFIED BY 'Password1';
GRANT ALL PRIVILEGES ON clonedeploy.* TO 'cduser'@'localhost';
quit

mysql clonedeploy &lt; cd.sql -v
</code></pre><p>Open <code>/var/www/html/clonedeploy/api/Web.config</code> with a text editor and change the following values:<!-- raw HTML omitted -->
<code>xx_marker1_xx</code> to your cduser database password you created earlier<!-- raw HTML omitted -->
On that same line change <code>Uid=root to Uid=cduser</code><!-- raw HTML omitted -->
<code>xx_marker2_xx</code> to some random characters(alphanumeric only), probably should be a minimum of 8<!-- raw HTML omitted --></p>
<pre tabindex="0"><code>service httpd restart
</code></pre><h3 id="install-samba-server">Install Samba Server</h3>
<pre tabindex="0"><code>yum -y install samba
groupadd cdsharewriters
useradd cd_share_ro
useradd cd_share_rw -G cdsharewriters
usermod -a -G cdsharewriters apache

smbpasswd -a cd_share_ro
smbpasswd -a cd_share_rw

echo &quot;[cd_share]
path = /cd_dp
valid users = @cdsharewriters, cd_share_ro
create mask = 02775
directory mask = 02775
guest ok = no
writable = yes
browsable = yes
read list = @cdsharewriters, cd_share_ro
write list = @cdsharewriters
force create mode = 02775
force directory mode = 02775
force group = +cdsharewriters&quot; &gt;&gt; /etc/samba/smb.conf

chown -R apache:cdsharewriters /cd_dp
chmod -R 2775 /cd_dp
service smb restart
chkconfig smb on
</code></pre><h3 id="install-tftp-server">Install TFTP Server</h3>
<pre tabindex="0"><code>yum -y install tftp-server
sed -i 's/\/var\/lib\/tftpboot/\/tftpboot -m \/tftpboot\/remap/g' /usr/lib/systemd/system/tftp.service
systemctl daemon-reload
service tftp restart
chkconfig tftp on
</code></pre><h3 id="create-firewall-exceptions">Create Firewall Exceptions</h3>
<pre tabindex="0"><code>firewall-cmd --permanent --add-service=http
firewall-cmd --permanent --add-service=https
firewall-cmd --permanent --add-service=samba
firewall-cmd --permanent --add-service=tftp
firewall-cmd --permanent --add-service=dhcp
firewall-cmd --permanent --add-service=proxy-dhcp
firewall-cmd --permanent --add-port=9000-10002/udp
service firewalld reload
</code></pre><h3 id="post-install-setup">Post Install Setup</h3>
<pre tabindex="0"><code># Open the CloneDeploy Web Interface
http://server-ip/clonedeploy

# éogin with
clonedeploy / password

# Upon login you will be greeted with the Initial Setup Page
# Fill out the fields and click Finalize Setup
</code></pre><pre tabindex="0"><code>yum -y install syslinux vsftpd
cp -v /usr/share/syslinux/pxelinux.0 /tftpboot/
cp -v /usr/share/syslinux/vesamenu.c32 /tftpboot/

mkdir /var/ftp/pub/rhel7
mkdir -p /tftpboot/networkboot/rhel7/

cd /opt
wget  http://files.clonedeploy.org/CloneDeploy-Services.dll
cp CloneDeploy-Services.dll /var/www/html/clonedeploy/api/bin/

wget http://ftp.bme.hu/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1810.iso
wget http://ftp.bme.hu/centos/7/isos/x86_64/sha256sum.txt

sha256sum CentOS-7-x86_64-Minimal-1810.iso
cat sha256sum.txt

mount -o loop /opt/CentOS-7-x86_64-Minimal-1810.iso  /mnt
cp -raf /mnt/* /var/ftp/pub/rhel7
cp /var/ftp/pub/rhel7/images/pxeboot/{initrd.img,vmlinuz} /tftpboot/networkboot/rhel7/

systemctl enable vsftpd.service &amp;&amp; systemctl start vsftpd.service

LABEL CentOS 7 X64
MENU LABEL CentOS 7 X64
kernel /networkboot/rhel7/vmlinuz
append initrd=/networkboot/rhel7/initrd.img inst.repo=ftp://192.168.100.100/pub/rhel7 devfs=nomount
</code></pre><hr>
<p><a href="http://clonedeploy.org/docs/create-and-deploy-your-first-image/0">http://clonedeploy.org/docs/create-and-deploy-your-first-image/0</a></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install pxeboot server]]></title>
            <link href="https://devopstales.github.io/home/pxe1/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/pxe1/</id>
            
            
            <published>2019-07-16T00:00:00+00:00</published>
            <updated>2019-07-16T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Have you ever needed to troubleshoot or diagnose a problematic computer and you forgot where the utility CD is? We’ll show you how to utilize network booting (PXE) to make that problem a thing of the past.</p>
<h3 id="dhcp">dhcp</h3>
<pre tabindex="0"><code>yum install -y dhcp nano

# ha több interfész van
echo &quot;DHCPDARGS=enp0s8&quot; &gt;&gt; /etc/sysconfig/dhcpd

cat &gt; /etc/dhcp/dhcpd.conf &lt;&lt; EOF
#DHCP configuration for PXE boot server
ddns-update-style interim;
ignore client-updates;
authoritative;
allow booting;
allow bootp;
allow unknown-clients;

# A slightly different configuration for an internal subnet.
subnet 192.168.100.0
netmask 255.255.255.0
{
range 192.168.100.101 192.168.100.200;
option domain-name-servers 192.168.100.100;
option routers 192.168.100.100;
default-lease-time 600;
max-lease-time 7200;

# PXE SERVER IP
next-server 192.168.100.100; #  DHCP server ip
filename &quot;pxelinux.0&quot;;
}
EOF

systemctl start dhcpd.service
systemctl enable dhcpd.service
firewall-cmd --permanent --add-service={dhcp,proxy-dhcp}
firewall-cmd --reload
</code></pre><pre tabindex="0"><code>cd /opt
wget http://ftp.bme.hu/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1810.iso
wget http://ftp.bme.hu/centos/7/isos/x86_64/sha256sum.txt

sha256sum CentOS-7-x86_64-Minimal-1810.iso
cat sha256sum.txt

mount -o loop /opt/CentOS-7-x86_64-Minimal-1810.iso  /mnt
</code></pre><h3 id="ftp">ftp</h3>
<pre tabindex="0"><code>yum install -y tftp-server vsftpd syslinux syslinux-tftpboot

cp -v /usr/share/syslinux/pxelinux.0 /var/lib/tftpboot/
cp -v /usr/share/syslinux/menu.c32 /var/lib/tftpboot/
cp -v /usr/share/syslinux/mboot.c32 /var/lib/tftpboot/
cp -v /usr/share/syslinux/chain.c32 /var/lib/tftpboot/

mkdir /var/lib/tftpboot/pxelinux.cfg
mkdir -p /var/lib/tftpboot/networkboot/rhel7
mkdir /var/ftp/pub/rhel7

cp -raf /mnt/* /var/ftp/pub/rhel7
cp /var/ftp/pub/rhel7/images/pxeboot/{initrd.img,vmlinuz} /var/lib/tftpboot/networkboot/rhel7/
umount /mnt
</code></pre><pre tabindex="0"><code>cat &gt; /var/lib/tftpboot/pxelinux.cfg/default &lt;&lt; EOF
default menu.c32
prompt 0
timeout 30
menu title mydomail.local PXE Menu

label 1
menu label ^1) Boot from local drive
localboot 0x00

label 2
menu label ^2) CentOS 7 X64
kernel /networkboot/rhel7/vmlinuz
append initrd=/networkboot/rhel7/initrd.img ks=ftp://192.168.100.100/pub/rhel7/ks.cfg inst.repo=ftp://192.168.100.100/pub/rhel7 devfs=nomount

label 3
menu label ^3) CentOS 7 X64
kernel /networkboot/rhel7/vmlinuz
append initrd=/networkboot/rhel7/initrd.img inst.repo=ftp://192.168.100.100/pub/rhel7 devfs=nomount

#label 4
#menu label ^4) Install CentOS 7 x64 with Local Repo using VNC
#kernel /networkboot/rhel7/vmlinuz
#append  initrd=/networkboot/rhel7/initrd.img inst.repo=ftp://192.168.100.100/pub/rhel7 devfs=nomount inst.vnc inst.vncpassword=Aa123456
EOF
</code></pre><pre tabindex="0"><code>echo '# Use network installation
url --url=&quot;ftp://192.168.100.100/pub/rhel7/&quot;
install

firstboot --disable
selinux --disabled
firewall --disabled
services --enabled=NetworkManager,sshd,chronyd
eula --agreed
ignoredisk --only-use=sda
# Keyboard layouts
keyboard --vckeymap=hu --xlayouts='hu'
# System language
lang en_US.UTF-8

#Text mode installation
text

# Network information
network  --bootproto=dhcp --device=eth0 --hostname=test.mydomain --noipv6 --activate
# Root password
authconfig --enableshadow --enablemd5
# Gen hash: openssl passwd -1 &quot;Password1&quot;
rootpw --iscrypted $1$Skulk114$/QcbiPYHtUnB/rJaAehAH0
# System timezone
timezone Europe/Budapest --ntpservers=0.hu.pool.ntp.org,.hu.pool.ntp.org,2.hu.pool.ntp.org

# System bootloader configuration
bootloader --append=&quot;net.ifnames=0 biosdevname=0&quot; --location=mbr
zerombr

# Disk partitioning information
clearpart --all --drives=sda
part /boot --fstype=ext4 --size=512
part pv.01 --size=1 --grow

volgroup vg00 pv.01
logvol swap --name=lv_swap --vgname=vg00 --size=1024
logvol / --name=lv_01 --vgname=vg00 --size=1 --grow

reboot --eject

%packages
@core
chrony
openssh-clients
net-tools
%end' &gt; /var/ftp/pub/rhel7/ks.cfg
</code></pre><pre tabindex="0"><code>systemctl start tftp.service
systemctl enable tftp.service
systemctl enable vsftpd.service
systemctl start vsftpd.service

firewall-cmd --permanent --add-service=tftp
firewall-cmd --permanent --add-service=ftp
firewall-cmd --reload
</code></pre><p>CLIENT NEEDS AT LEASET 2 GB RAM</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes nginx ingress with helm]]></title>
            <link href="https://devopstales.github.io/home/k8s-nginx-ingress/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install kubernetes with kubeadm" />
                <link href="https://devopstales.github.io/home/k8s-copy-secret/?utm_source=atom_feed" rel="related" type="text/html" title="Copying Kubernetes Secrets Between Namespaces" />
                <link href="https://devopstales.github.io/home/k8s-helm-rbac/?utm_source=atom_feed" rel="related" type="text/html" title="RBAC permissions for Helm" />
                <link href="https://devopstales.github.io/home/kubectl-multi-cluster-config/?utm_source=atom_feed" rel="related" type="text/html" title="Configure kubectl for multiple clusters" />
                <link href="https://devopstales.github.io/kubernetes/k8s-nginx-ingress/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
            
                <id>https://devopstales.github.io/home/k8s-nginx-ingress/</id>
            
            
            <published>2019-07-14T00:00:00+00:00</published>
            <updated>2019-07-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how can you use install IngressControllert on Kubernetes with helm.</p>
<H3>Parst of the Kubernetes series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
     <li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
     <li>Part1c: <a href="../../kubernetes/k8s-install-containerd/">Install K8S with containerd and kubeadm</a></li>
     <li>Part1d: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
     <li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a></li>
<!--      <li>Part3b: k8s-multiple-nginx-ingress -->
<!--      <li>Part3c: k8s-contour-ingress -->
     <li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a></li>
<!-- local folder (+ autoprovisioning ) -->
     <li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
     <li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
     <li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
     <li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
     <li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
     <li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a></li>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... -->
     <li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
     <li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
     <li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
     <li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
     <li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
     <li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
     <li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a></li>
<!-- istio-install.md -->
     <li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a></li>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog -->
</ul>


<h3 id="environment">Environment</h3>
<pre tabindex="0"><code># openshift cluster
192.168.1.41  kubernetes01 # master node
192.168.1.42  kubernetes02 # frontend node
192.168.1.43  kubernetes03 # worker node
192.168.1.44  kubernetes04 # worker node
192.168.1.45  kubernetes05 # worker node
</code></pre><h3 id="helm-with-cluster-admin-permissions">Helm with cluster-admin permissions</h3>
<pre tabindex="0"><code>at &lt;&lt;EOF&gt; helm-cluster-admin.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller-admin
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tiller-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: tiller-admin
  namespace: kube-system
EOF
</code></pre><h3 id="init-helm">Init Helm</h3>
<pre tabindex="0"><code>kubectl create -f helm-cluster-admin.yaml

helm init --service-account helm
kubectl get po --all-namespaces | grep tiller
</code></pre><h3 id="tag-node-for-ingress">Tag node for ingress</h3>
<pre tabindex="0"><code>kubectl get nodes --show-labels
kubectl label nodes kubernetes02 node-role.kubernetes.io/frontend= --overwrite=true

helm install stable/nginx-ingress \
    --name nginx-ingress \
    --namespace=nginx-ingress \
    --set rbac.create=true \
    --set controller.kind=DaemonSet \
    --set controller.hostNetwork=true \
    --set controller.daemonset.useHostPort=true \
    --set controller.nodeSelector.&quot;node-role\.kubernetes\.io/frontend&quot;= \
    --set controller.stats.enabled=true \
    --set controller.metrics.enabled=true

kubectl --namespace nginx-ingress get services -o wide -w nginx-ingress-controller
kubectl create secret tls default-ingress-tls --key /path/to/private.pem --cert /path/to/cert.pem --namespace nginx-ingress
</code></pre><pre tabindex="0"><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta1/aio/deploy/recommended.yaml

kubectl create secret tls default-ingress-tls --key /path/to/private.pem --cert /path/to/cert.pem --namespace kubernetes-dashboard

cat &lt;&lt;EOF&gt; dashboard_ingress.yml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kubernetes-dashboard-ingress
  namespace: kubernetes-dashboard
  annotations:
    kubernetes.io/tls-acme: 'true'
    ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-passthrough: &quot;true&quot;
    nginx.ingress.kubernetes.io/secure-backends: &quot;true&quot;
    ingress.kubernetes.io/ssl-redirect: &quot;true&quot;
    ingress.kubernetes.io/force-ssl-redirect: &quot;true&quot;
spec:
  tls:
  - hosts:
    - dashboard.devopstales.intra
    secretName: default-ingress-tls
  rules:
  - host: dashboard.devopstales.intra
    http:
     paths:
     - backend:
         serviceName: kubernetes-dashboard
         servicePort: 443
EOF

kubectl apply -f dashboard_ingress.yml
</code></pre><pre tabindex="0"><code>kubectl create serviceaccount dashboard-admin-sa
kubectl create clusterrolebinding dashboard-admin-sa --clusterrole=cluster-admin --serviceaccount=default:dashboard-admin-sa

kubectl get secrets
NAME                  TYPE                                  DATA   AGE
dashboard-admin-sa-token-XXXXX   kubernetes.io/service-account-token   3      22h

kubectl describe secret dashboard-admin-sa-token-XXXXX
Name:         dashboard-admin-sa-token-bq9cr
...
token:      XXXXXXXXXXXXXXXXXXXXXXXXXX

# use this token to login
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install kubernetes with kubeadm]]></title>
            <link href="https://devopstales.github.io/home/k8s-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-copy-secret/?utm_source=atom_feed" rel="related" type="text/html" title="Copying Kubernetes Secrets Between Namespaces" />
                <link href="https://devopstales.github.io/home/k8s-helm-rbac/?utm_source=atom_feed" rel="related" type="text/html" title="RBAC permissions for Helm" />
                <link href="https://devopstales.github.io/home/kubectl-multi-cluster-config/?utm_source=atom_feed" rel="related" type="text/html" title="Configure kubectl for multiple clusters" />
                <link href="https://devopstales.github.io/kubernetes/k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install kubernetes with kubeadm" />
                <link href="https://devopstales.github.io/home/ansible-operator-overview/?utm_source=atom_feed" rel="related" type="text/html" title="Ansible Operator Overview" />
            
                <id>https://devopstales.github.io/home/k8s-install/</id>
            
            
            <published>2019-07-12T00:00:00+00:00</published>
            <updated>2019-07-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Kubeadm is a tool that helps you bootstrap a simple Kubernetes cluster and simplifies the deployment process.</p>
<H3>Parst of the Kubernetes series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
     <li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
     <li>Part1c: <a href="../../kubernetes/k8s-install-containerd/">Install K8S with containerd and kubeadm</a></li>
     <li>Part1d: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
     <li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a></li>
<!--      <li>Part3b: k8s-multiple-nginx-ingress -->
<!--      <li>Part3c: k8s-contour-ingress -->
     <li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a></li>
<!-- local folder (+ autoprovisioning ) -->
     <li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
     <li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
     <li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
     <li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
     <li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
     <li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a></li>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... -->
     <li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
     <li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
     <li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
     <li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
     <li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
     <li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
     <li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a></li>
<!-- istio-install.md -->
     <li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a></li>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog -->
</ul>


<pre tabindex="0"><code>192.168.1.41  kubernetes01 # master node
192.168.1.42  kubernetes02 # frontend node
192.168.1.43  kubernetes03 # worker node
192.168.1.44  kubernetes04 # worker node
192.168.1.45  kubernetes05 # worker node

# hardware requirement
4 CPU
16G RAM
</code></pre><h3 id="install-docker">Install Docker</h3>
<pre tabindex="0"><code>apt-get update
apt-get install apt-transport-https ca-certificates curl gnupg2 software-properties-common
curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add -
add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/debian $(lsb_release -cs) stable&quot;
apt-get update
apt-get install docker-ce docker-ce-cli containerd.io
systemct start docker
systemct enable docker
</code></pre><h3 id="configuuration">Configuuration</h3>
<pre tabindex="0"><code>cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
EOF

echo '1' &gt; /proc/sys/net/ipv4/ip_forward
echo '1' &gt; /proc/sys/net/bridge/bridge-nf-call-iptables
</code></pre><h3 id="disable-swap">Disable swap</h3>
<pre tabindex="0"><code>free -h
swapoff -a
swapoff -a
sed -i.bak -r 's/(.+ swap .+)/#\1/' /etc/fstab
free -h
</code></pre><h3 id="install-kubeadm">Install kubeadm</h3>
<pre tabindex="0"><code>apt-get install ebtables ethtool apt-transport-https

curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF

apt-get update &amp;&amp; apt-get install -y kubelet kubeadm kubectl
apt-mark hold kubelet kubeadm kubect
systemctl enable kubelet &amp;&amp; systemctl start kubelet
kubeadm config images pul
</code></pre><h3 id="init-master">Init master</h3>
<pre tabindex="0"><code>kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=192.168.1.41


mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config


kubectl apply -f https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml
</code></pre><h3 id="join-workers-to-cluster">Join workers to cluster</h3>
<pre tabindex="0"><code>kubeadm join 192.168.1.41:6443 --token XXXXXXXX \
    --discovery-token-ca-cert-hash sha256:XXXXXXXX
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Ansible Operator Overview]]></title>
            <link href="https://devopstales.github.io/home/ansible-operator-overview/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
                <link href="https://devopstales.github.io/kubernetes/ansible-operator-overview/?utm_source=atom_feed" rel="related" type="text/html" title="Ansible Operator Overview" />
                <link href="https://devopstales.github.io/home/openshift-add-node/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Add Nodes to a Cluster" />
                <link href="https://devopstales.github.io/home/openshift-letsencrypt/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Letsencrypt certificates" />
                <link href="https://devopstales.github.io/home/openshift-hostalreadyclaimed/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Error: HostAlreadyClaimed" />
            
                <id>https://devopstales.github.io/home/ansible-operator-overview/</id>
            
            
            <published>2019-07-10T00:00:00+00:00</published>
            <updated>2019-07-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Operators make it easy to manage complex stateful applications on top of Kubernetes. In this pos I will show you how to read an ansible based Openshift operator.</p>
<H3>Parst of the Openshift series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
     <li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
     <li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
     <li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
     <li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
     <li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
     <li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
     <li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
     <li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
     <li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
     <li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
     <li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
     <li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
     <li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
     <li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
     <li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
     <li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
     <li>Part16a: <a href="../../kubernetes/openshift-elasticsearch-error/">Opeshift elasticsearch search-guard error</a></li>
     <li>Part16b: <a href="../../kubernetes/openshift-log4shell/">Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)</a></li>
</ul>


<h3 id="why-an-operator">Why an Operator?</h3>
<p>Operators make it easy to manage complex stateful applications on top of Kubernetes. However writing an operator today can be difficult because of challenges such as using low level APIs, writing boilerplate, and a lack of modularity which leads to duplication.</p>
<h3 id="what-is-an-ansible-operator">What is an Ansible Operator?</h3>
<p>Ansible Operator is one of the available types of Operators that Operator SDK is able to generate. Operator SDK can create an operator using with Golang, Helm, or Ansible.
It is a collection of building blocks from Operator SDK that enables Ansible to handle the reconciliation logic for an Operator.</p>
<h3 id="how-ansible-operator-works">How Ansible Operator works?</h3>
<p>We want to trigger this Ansible logic when a Custom Resource changes. The Ansible Operator uses a Watches file, written in YAML, which holds the mapping between Custom Resources and Ansible Roles/Playbooks. The Operator expects this mapping file in a predefined location: <code>/opt/ansible/watches.yaml</code></p>
<p>Each mapping within the Watches file has mandatory fields:</p>
<ul>
<li>group: Group of the Custom Resource that you will be watching.</li>
<li>version: Version of the Custom Resource that you will be watching.</li>
<li>kind: Kind of the Custom Resource that you will be watching.</li>
<li>role (default): Path to the Role that should be run by the Operator for a particular Group-Version-Kind (GVK). This field is mutually exclusive with the &ldquo;playbook&rdquo; field.</li>
<li>playbook (optional): Path to the Playbook that should be run by the Operator.</li>
</ul>
<h3 id="initialize-new-operator-template">Initialize new operator template</h3>
<pre tabindex="0"><code># istall sdk client
brew install operator-sdk

# generate base temlate
operator-sdk new memcached-operator --type=ansible --api-version=cache.example.com/v1alpha1 --kind=Memcached --skip-git-init
cd memcached-operator
</code></pre><p>The sdk cli generated the base structure for all the componets. The main parts are the <code>watches.yaml</code> the Dockerfile in <code>build/Dockerfile</code> and the ansible role in <code>roles/</code><!-- raw HTML omitted --> folder. Fore this tutorial I will use this role <a href="https://github.com/dymurray/memcached-operator-role">https://github.com/dymurray/memcached-operator-role</a>.<!-- raw HTML omitted -->
In the role we must use k8s ansible module to deploy kubernetes compnets to the cluster.<!-- raw HTML omitted --></p>
<pre tabindex="0"><code>nano roles/memcached/tasks/main.yml
---
- name: start memcached
  k8s:
    definition:
      kind: Deployment
      apiVersion: apps/v1
      metadata:
        name: '{{ meta.name }}-memcached'
        namespace: '{{ meta.namespace }}'
      spec:
        replicas: &quot;{{size}}&quot;
        selector:
          matchLabels:
            app: memcached
        template:
          metadata:
            labels:
              app: memcached
          spec:
            containers:
            - name: memcached
              command:
              - memcached
              - -m=64
              - -o
              - modern
              - -v
              image: &quot;memcached:1.4.36-alpine&quot;
              ports:
                - containerPort: 11211
</code></pre><p>You can use variables in ansible Jinja temlate from the CR spec or from kubernetes enviroment like namespace: <code>{{ meta.namespace }}</code>.<!-- raw HTML omitted -->
Add default value to the <code>{{size}}</code> variable:</p>
<pre tabindex="0"><code>nano roles/memcached/defaults/main.yml
---
size: 1
</code></pre><h4 id="variable-sharing-example">Variable sharing Example</h4>
<pre tabindex="0"><code>apiVersion: &quot;foo.example.com/v1alpha1&quot;
kind: &quot;Foo&quot;
metadata:
  name: &quot;example&quot;
annotations:
  ansible.operator-sdk/reconcile-period: &quot;30s&quot;
  name: &quot;example&quot;
spec:
  message: &quot;Hello world 2&quot;
  newParameter: &quot;newParam&quot;
# associates GVK with Role
role: /opt/ansible/roles/Foo
</code></pre><pre tabindex="0"><code>- debug:
    msg: &quot;message value from CR spec: {{ message }}&quot;

- debug:
    msg: &quot;newParameter value from CR spec: {{ new_parameter }}&quot;

- debug:
    msg: &quot;name: {{ meta.name }}, namespace: {{ meta.namespace }}&quot;
</code></pre><p>The Openshidt SDK created a simple Dockerfile in <code>build/Dockerfile</code> to run the newly created ansible role. We nead to build a docker image from this Dockerfile and use this image in our memcached-operator deployment.</p>
<pre tabindex="0"><code># buid image
operator-sdk build memcached-operator:v0.0.1

# Edit deploy/operator.yaml to use the newly created memcached-operator:v0.0.1 docker image
sed -i 's|{{ REPLACE_IMAGE }}|memcached-operator:v0.0.1|g' deploy/operator.yaml

# If we did not want to download the image (besause we build it on the worker or it is representid on all of my workes) we can disable image pulling.
sed -i &quot;s|{{ pull_policy\|default('Always') }}|Never|g&quot; deploy/operator.yaml
</code></pre><h3 id="creating-the-operator-from-deploy-manifests">Creating the Operator from deploy manifests</h3>
<pre tabindex="0"><code>oc create -f deploy/crds/cache_v1alpha1_memcached_crd.yaml

oc new-project tutorial
oc create -f deploy/service_account.yaml
oc create -f deploy/role.yaml
oc create -f deploy/role_binding.yaml
oc create -f deploy/operator.yaml

oc get deployment
</code></pre><p>Now that we have deployed our Operator, let&rsquo;s create a CR and deploy an instance of memcached.
There is a sample CR in the scaffolding created as part of the Operator SDK. Inspect <code>deploy/crds/cache_v1alpha1_memcached_cr.yaml</code>, and then use it to create a Memcached custom resource.</p>
<pre tabindex="0"><code>oc create -f deploy/crds/cache_v1alpha1_memcached_cr.yaml



$ oc get deployment
NAME                 DESIRED CURRENT UP-TO-DATE AVAILABLE AGE
memcached-operator   1       1       1          1         2m
example-memcached    3       3       3          3         1m

#Check the pods to confirm 3 replicas were created:

$ oc get pods
NAME                                READY STATUS   RESTARTS AGE
example-memcached-6cc844747c-2hbln  1/1   Running  0        1m
example-memcached-6cc844747c-54q26  1/1   Running  0        1m
example-memcached-6cc844747c-7jfhc  1/1   Running  0        1m
memcached-operator-68b5b558c5-dxjwh 1/1   Running  0        2m
</code></pre><h3 id="removing-memcached-from-the-cluster">Removing Memcached from the cluster</h3>
<pre tabindex="0"><code># First, delete the 'memcached' CR, which will remove the 4 Memcached pods and the associated deployment.
oc delete -f deploy/crds/cache_v1alpha1_memcached_cr.yaml

# Then, delete the memcached-operator deployment.
oc delete -f deploy/operator.yaml
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Analyzing PFsense logs in Graylog3]]></title>
            <link href="https://devopstales.github.io/home/graylog3-pfsense/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/graylog3-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Graylog3" />
                <link href="https://devopstales.github.io/linux/graylog3-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Analyzing PFsense logs in Graylog3" />
                <link href="https://devopstales.github.io/linux/graylog3-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Graylog3" />
                <link href="https://devopstales.github.io/home/grafana-loki/?utm_source=atom_feed" rel="related" type="text/html" title="Grafana Loki" />
                <link href="https://devopstales.github.io/home/grafana-sso/?utm_source=atom_feed" rel="related" type="text/html" title="SSO login to Grafana" />
            
                <id>https://devopstales.github.io/home/graylog3-pfsense/</id>
            
            
            <published>2019-07-04T00:00:00+00:00</published>
            <updated>2019-07-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>We will parse the log records generated by the PfSense Firewall. We already have our graylog server running and we will start preparing the terrain to capture those logs records.</p>
<p>Many thanks to opc40772 developed the original contantpack for pfsense log agregation what I updated for the new Graylog3 and Elasticsearch 6.</p>
<h3 id="celebro-localinstall">Celebro localinstall</h3>
<pre tabindex="0"><code># celebro van to use port 9000 so change graylog3 bindport
nano /etc/graylog/server/server.conf
http_bind_address = 127.0.0.1:9400
nano /etc/nginx/conf.d/graylog.conf

systemctl restart graylog-server.service
systemctl restart nginx

wget https://github.com/lmenezes/cerebro/releases/download/v0.9.3/cerebro-0.9.3-1.noarch.rpm
yum localinstall cerebro-0.9.3-1.noarch.rpm

sudo sed -i 's|# JAVA_OPTS=&quot;-Dpidfile.path=/var/run/cerebro/play.pid&quot;|JAVA_OPTS=&quot;-Dpidfile.path=/var/run/cerebro/play.pid -Dhttp.address=0.0.0.0&quot;|' /etc/default/cerebro

chown cerebro:cerebro -R /usr/share/cerebro

systemctl start cerebro
</code></pre><h3 id="create-indices">Create indices</h3>
<p>We now create the Pfsense indice on Graylog at <code>System / Indexes</code> <!-- raw HTML omitted -->
<img src="/img/include/graylog_pfsense1.png" alt="image"  class="zoomable" /> <!-- raw HTML omitted --></p>
<h3 id="import-index-template-for-elasticsearch-6x">Import index template for elasticsearch 6.x</h3>
<pre tabindex="0"><code>systemctl stop graylog-server.service

git clone https://github.com/devopstales/pfsense-graylog.git
cd pfsense-graylog/service-names-port-numbers/
cp service-names-port-numbers.csv /etc/graylog/server/
</code></pre><p>Go to <code>celebro &gt; more &gt; index templates</code>
Create new with name: <code>pfsense-custom</code> and copy the template from file <code>pfsense_custom_template_es6.json</code></p>
<p>In Cerebro we stand on top of the pfsense index and unfold the options and select delete index.</p>
<h3 id="geoip-database">Geoip database</h3>
<pre tabindex="0"><code>wget -t0 -c http://geolite.maxmind.com/download/geoip/database/GeoLite2-City.tar.gz
tar -xvf GeoLite2-City.tar.gz
cp GeoLite2-City_*/GeoLite2-City.mmdb /etc/graylog/server

systemctl start graylog-server.service
</code></pre><p>Enable geoip database at <code>System &gt; Configurations &gt; Plugins &gt; Geo-Location Processor &gt; update</code>
Chane the order of the <code>Message Processors Configuration</code></p>
<ul>
<li>AWS Instance Name Lookup</li>
<li>Message Filter Chain</li>
<li>Pipeline Processor</li>
<li>GeoIP Resolver</li>
</ul>
<p>Enable geoip database</p>
<h3 id="import-contantpack">Import contantpack</h3>
<p>import
chaneg date timezone in Pipeline rule
Go tu Stream menu and edit stream <!-- raw HTML omitted -->
<img src="/img/include/graylog_pfsense2.png" alt="image"  class="zoomable" /> <!-- raw HTML omitted --></p>
<p><code>System &gt; Pipelines</code>
Manage rules and then Edit rule (Change the timezone)</p>
<pre tabindex="0"><code>rule &quot;timestamp_pfsense_for_grafana&quot;
 when
 has_field(&quot;timestamp&quot;)
then
// the following date format assumes there's no time zone in the string
 let source_timestamp = parse_date(substring(to_string(now(&quot;Europe/Budapest&quot;)),0,23), &quot;yyyy-MM-dd'T'HH:mm:ss.SSS&quot;);
 let dest_timestamp = format_date(source_timestamp,&quot;yyyy-MM-dd HH:mm:ss&quot;);
 set_field(&quot;real_timestamp&quot;, dest_timestamp);
end
</code></pre><h3 id="confifure-pfsense">Confifure pfsense</h3>
<p><code>Status &gt; System Logs &gt; Settings</code> <!-- raw HTML omitted -->
<img src="/img/include/graylog_pfsense3.png" alt="image"  class="zoomable" /> <!-- raw HTML omitted --></p>
<h3 id="confifure-opnsense">Confifure Opnsense</h3>
<p>Access the Opnsense GUI
<code>System</code> menu, access the <code>Settings</code> sub-menu and select the  <code>Logging / Targets</code> option.
<img src="/img/include/graylog_pfsense13.png" alt="image"  class="zoomable" /> <!-- raw HTML omitted --></p>
<p>Add a new logging target and perform the following configuration: <!-- raw HTML omitted --></p>
<p><img src="/img/include/graylog_pfsense14.png" alt="image"  class="zoomable" /> <!-- raw HTML omitted --></p>
<h3 id="install-grafana-dashboard">Install grafana Dashboard</h3>
<pre tabindex="0"><code># install nececery plugins
grafana-cli plugins install grafana-piechart-panel
grafana-cli plugins install grafana-worldmap-panel
grafana-cli plugins install savantly-heatmap-panel
systemctl restart grafana-server
</code></pre><p>Create new datasource: <!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<p><img src="/img/include/graylog_pfsense4.png" alt="image"  class="zoomable" /> <!-- raw HTML omitted --></p>
<p>Import dashboadr from store: <!-- raw HTML omitted -->
id: 5420</p>
<h2 id="imageimgincludegraylog_pfsense12png-br"><img src="/img/include/graylog_pfsense12.png" alt="image"  class="zoomable" /> <!-- raw HTML omitted --></h2>
<h5 id="contantpack">Contantpack:</h5>
<p><a href="https://github.com/devopstales/pfsense-graylog">https://github.com/devopstales/pfsense-graylog</a></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[SSO login to Grafana]]></title>
            <link href="https://devopstales.github.io/home/grafana-sso/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-sso-2/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift SSO with Gitlab" />
                <link href="https://devopstales.github.io/home/nextcloud-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Nextcloud SSO" />
                <link href="https://devopstales.github.io/home/nexus-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Sonatype Nexus SSO" />
                <link href="https://devopstales.github.io/home/hashicorp-sso/?utm_source=atom_feed" rel="related" type="text/html" title="SSO for hashicorp vault" />
                <link href="https://devopstales.github.io/home/rundeck-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Rundeck SSO" />
            
                <id>https://devopstales.github.io/home/grafana-sso/</id>
            
            
            <published>2019-06-28T00:00:00+00:00</published>
            <updated>2019-06-28T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Configurate Gitab to use Keycloak as SSO Identity Proider.</p>
<h3 id="configurate-keycloak">Configurate Keycloak</h3>
<p>Login to Keycloak and create client for Grafana:
<img src="/img/include/gitlab-keycloak1.png" alt="Example image"  class="zoomable" /></p>
<h3 id="configurate-gitlab">Configurate Gitlab</h3>
<pre tabindex="0"><code>nano /etc/grafana/grafana.ini
#################################### Generic OAuth ##########################
[auth.generic_oauth]
enabled = true
name = SSO
allow_sign_up = true
client_id = gitlab
client_secret = 47fd3013-4333-4825-bbfa-b7688548d9cf
# for old version
# scopes = user:email,read:org
scopes = openid email profile
auth_url = https://sso.devopstales.intra/auth/realms/gitlab/protocol/openid-connect/auth
token_url = https://sso.devopstales.intra/auth/realms/gitlab/protocol/openid-connect/token
api_url = https://sso.devopstales.intra/auth/realms/gitlab/protocol/openid-connect/userinfo
;team_ids =
;allowed_organizations =

</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift: Add Nodes to a Cluster]]></title>
            <link href="https://devopstales.github.io/home/openshift-add-node/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-letsencrypt/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Letsencrypt certificates" />
                <link href="https://devopstales.github.io/home/openshift-hostalreadyclaimed/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Error: HostAlreadyClaimed" />
                <link href="https://devopstales.github.io/home/openshift-gitlabrunner/?utm_source=atom_feed" rel="related" type="text/html" title="Install Gitlab runner on Openshift" />
                <link href="https://devopstales.github.io/home/openshift-extregistry/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: External registry" />
                <link href="https://devopstales.github.io/home/openshift-cert/?utm_source=atom_feed" rel="related" type="text/html" title="Change Certificates in Openshift" />
            
                <id>https://devopstales.github.io/home/openshift-add-node/</id>
            
            
            <published>2019-06-27T00:00:00+00:00</published>
            <updated>2019-06-27T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Add Nodes to an existing Cluster.</p>
<H3>Parst of the Openshift series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
     <li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
     <li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
     <li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
     <li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
     <li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
     <li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
     <li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
     <li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
     <li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
     <li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
     <li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
     <li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
     <li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
     <li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
     <li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
     <li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
     <li>Part16a: <a href="../../kubernetes/openshift-elasticsearch-error/">Opeshift elasticsearch search-guard error</a></li>
     <li>Part16b: <a href="../../kubernetes/openshift-log4shell/">Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)</a></li>
</ul>


<p>In the last post I used the basic htpasswd authentication method for the installatipn.<!-- raw HTML omitted -->
But I can use Ansible-openshift to configure an LDAP backed at the install for the authentication.</p>
<h3 id="environment">Environment</h3>
<pre tabindex="0"><code>192.168.1.40    deployer
192.168.1.41    openshift01 # master node
192.168.1.42    openshift02 # infra node
192.168.1.43    openshift03 # worker node
192.168.1.44    openshift04 # new-worker node
192.168.1.45    openshift00 # new-master node
</code></pre><h3 id="heading"></h3>
<pre tabindex="0"><code>useradd origin
passwd origin
echo -e 'Defaults:origin !requiretty\norigin ALL = (root) NOPASSWD:ALL' | tee /etc/sudoers.d/openshift
chmod 440 /etc/sudoers.d/openshift

# if Firewalld is running, allow SSH

firewall-cmd --add-service=ssh --permanent
firewall-cmd --reload

yum -y install centos-release-openshift-origin36 docker
vgcreate vg_origin01 /dev/sdb1

Volume group &quot;vg_origin01&quot; successfully created
echo VG=vg_origin01 &gt;&gt; /etc/sysconfig/docker-storage-setup
systemctl start docker
systemctl enable docker
</code></pre><h3 id="configurate-installer">Configurate Installer</h3>
<pre tabindex="0"><code>nano /etc/ansible/hosts
# add into OSEv3 section

[OSEv3:children]
masters
nodes
new_nodes
new_masters
new_etcd

[new_nodes]
openshift04.devopstales.intra openshift_node_group_name='node-config-compute'
openshift00.devopstales.intra openshift_node_group_name='node-config-master'

[new_masters]
openshift00.devopstales.intra openshift_node_group_name='node-config-master'

[new_etcd]
openshift00.devopstales.intra containerized=true
</code></pre><h3 id="run-the-installer">Run the Installer</h3>
<p>Run Ansible Playbook for scaleout the Cluster.</p>
<pre tabindex="0"><code># deployer
cd /usr/share/ansible/openshift-ansible/

ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-node/scaleup.yml
ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-master/scaleup.yml
ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/openshift-etcd/scaleup.yml
</code></pre><h3 id="configurate-installer-1">Configurate Installer</h3>
<p>After finishing to add new Nodes, Open [/etc/ansible/hosts] again and move new definitions to existing [nodes] section like follows.</p>
<pre tabindex="0"><code>nano /etc/ansible/hosts
# add into OSEv3 section

[OSEv3:children]
masters
nodes
new_nodes
new_masters

[nodes]
openshift00.devopstales.intra openshift_node_group_name='node-config-master'
openshift01.devopstales.intra openshift_node_group_name='node-config-master'
openshift02.devopstales.intra openshift_node_group_name='node-config-infra'
openshift03.devopstales.intra openshift_node_group_name='node-config-compute'
openshift04.devopstales.intra openshift_node_group_name='node-config-compute

[new_nodes]

[masters]
openshift00.devopstales.intra openshift_node_group_name='node-config-master'
openshift01.devopstales.intra openshift_node_group_name='node-config-master'

[new_masters]

[etcd]
openshift01.devopstales.intra containerized=true
openshift00.devopstales.intra containerized=true

[new_etcd]
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Graylog3]]></title>
            <link href="https://devopstales.github.io/home/graylog3-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/graylog3-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Graylog3" />
                <link href="https://devopstales.github.io/home/grafana-loki/?utm_source=atom_feed" rel="related" type="text/html" title="Grafana Loki" />
                <link href="https://devopstales.github.io/home/gitlab-install/?utm_source=atom_feed" rel="related" type="text/html" title="Gitlab Install" />
                <link href="https://devopstales.github.io/home/openshift-sso-2/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift SSO with Gitlab" />
                <link href="https://devopstales.github.io/home/install-prometheus-for-gitlab/?utm_source=atom_feed" rel="related" type="text/html" title="Install Prometheus for Gitlab" />
            
                <id>https://devopstales.github.io/home/graylog3-install/</id>
            
            
            <published>2019-06-24T00:00:00+00:00</published>
            <updated>2019-06-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Graylog is defined in terms of log management platform for collecting, indexing, and analyzing both structured and unstructured data from almost any source.</p>
<h3 id="install-requirement">Install requirement</h3>
<pre tabindex="0"><code>yum install epel-release -y
yum install java-1.8.0-openjdk-headless.x86_64 pwgen nano -y
java -version
</code></pre><h3 id="set-timezone">Set Timezone</h3>
<pre tabindex="0"><code>rm -f /etc/localtime
ln -s /usr/share/zoneinfo/CET /etc/localtime

yum install -y ntp
ntpd
</code></pre><h3 id="elasticsearch">Elasticsearch</h3>
<pre tabindex="0"><code>rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch

echo '[elasticsearch-6.x]
name=Elasticsearch repository for 6.x packages
baseurl=https://artifacts.elastic.co/packages/6.x/yum
gpgcheck=1
gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
enabled=1
autorefresh=1
type=rpm-md
' | tee /etc/yum.repos.d/elasticsearch.repo

sudo yum -y install elasticsearch

sudo -E sed -i -e 's/#cluster.name: my-application/cluster.name: graylog/' \
/etc/elasticsearch/elasticsearch.yml

systemctl restart elasticsearch
systemctl enable elasticsearch

curl -XGET 'http://localhost:9200/_cluster/health?pretty=true'
</code></pre><h3 id="mongodb">Mongodb</h3>
<pre tabindex="0"><code>echo '[mongodb-org-4.0]
name=MongoDB Repository
baseurl=https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/4.0/x86_64/
gpgcheck=1
enabled=1
gpgkey=https://www.mongodb.org/static/pgp/server-4.0.asc' | tee /etc/yum.repos.d/mongodb-org.repo

yum -y install mongodb-org

systemctl restart mongod
systemctl enable  mongod
</code></pre><h3 id="graylog3">Graylog3</h3>
<pre tabindex="0"><code>rpm -Uvh https://packages.graylog2.org/repo/packages/graylog-3.3-repository_latest.rpm
yum -y install graylog-server

SECRET=$(pwgen -s 96 1)
sudo -E sed -i -e 's/password_secret =.*/password_secret = '$SECRET'/' /etc/graylog/server/server.conf
PASSWORD=$(echo -n Password1 | sha256sum | awk '{print $1}')
sudo -E sed -i -e 's/root_password_sha2 =.*/root_password_sha2 = '$PASSWORD'/' /etc/graylog/server/server.conf

# Set to your timezone
sudo -E sed -i -e 's/#root_timezone = UTC/root_timezone = CET/' /etc/graylog/server/server.conf

# Set to your email
sudo -E sed -i -e 's/#root_email = &quot;&quot;/root_email = &quot;admin@devopstales.intra&quot;/' /etc/graylog/server/server.conf
sudo -E sed -i -e 's/elasticsearch_shards = 4/elasticsearch_shards = 1/' /etc/graylog/server/server.conf
sudo -E sed -i -e 's/#http_bind_address = 127.0.0.1:9000/http_bind_address = 127.0.0.1:9400/' /etc/graylog/server/server.conf

# got ta https://dev.maxmind.com/geoip/geoip2/geolite2/ and download
# or use an old one
wget -t0 -c https://github.com/DocSpring/geolite2-city-mirror/raw/master/GeoLite2-City.tar.gz
tar -xvf GeoLite2-City.tar.gz
cp GeoLite2-City_*/GeoLite2-City.mmdb /etc/graylog/server

systemctl daemon-reload
systemctl restart graylog-server
systemctl enable graylog-server

tailf /var/log/graylog-server/server.log

If everything goes well, you should see below message in the logfile:
2019-06-20T13:37:04.059Z INFO  [ServerBootstrap] Graylog server up and running.
</code></pre><h3 id="install-grafana">Install Grafana</h3>
<pre tabindex="0"><code>echo '[grafana]
name=grafana
baseurl=https://packages.grafana.com/oss/rpm
repo_gpgcheck=1
enabled=1
gpgcheck=1
gpgkey=https://packages.grafana.com/gpg.key
sslverify=1
sslcacert=/etc/pki/tls/certs/ca-bundle.crt
' &gt; /etc/yum.repos.d/grafana.repo


sudo yum install -y grafana
grafana-cli plugins install grafana-piechart-panel

sudo -E sed -i -e 's/;http_addr =/http_addr = 127.0.0.1/' /etc/grafana/grafana.ini

systemctl start grafana-server
systemctl status grafana-server
systemctl enable grafana-server
</code></pre><h3 id="nginx-proxy">Nginx Proxy</h3>
<pre tabindex="0"><code>yum install nginx -y

echo 'server {
    listen 80;
    server_name graylog.devopstales.intra;

    location / {
      proxy_set_header Host $http_host;
      proxy_set_header X-Forwarded-Host $host;
      proxy_set_header X-Forwarded-Server $host;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      proxy_set_header X-Graylog-Server-URL http://$server_name/;
      proxy_pass       http://127.0.0.1:9400;
    }
}' &gt; /etc/nginx/conf.d/graylog.conf

echo 'server {
    listen 80;
    server_name grafana.devopstales.intra;

    location / {
      proxy_set_header Host $http_host;
      proxy_set_header X-Forwarded-Host $host;
      proxy_set_header X-Forwarded-Server $host;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      proxy_pass       http://127.0.0.1:3000;
    }
}' &gt; /etc/nginx/conf.d/grafana.conf

nginx -t
systemctl restart nginx
systemctl enable nginx
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Grafana Loki]]></title>
            <link href="https://devopstales.github.io/home/grafana-loki/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/monitoring/grafana-loki/?utm_source=atom_feed" rel="related" type="text/html" title="Grafana Loki" />
                <link href="https://devopstales.github.io/home/naemon-influxdb/?utm_source=atom_feed" rel="related" type="text/html" title="Install Nemon with Influxdb storage" />
                <link href="https://devopstales.github.io/home/openshift-sso-2/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift SSO with Gitlab" />
                <link href="https://devopstales.github.io/home/install-prometheus-for-gitlab/?utm_source=atom_feed" rel="related" type="text/html" title="Install Prometheus for Gitlab" />
                <link href="https://devopstales.github.io/home/install-mattermost-for-gitlab/?utm_source=atom_feed" rel="related" type="text/html" title="Install mattermost for Gitlab" />
            
                <id>https://devopstales.github.io/home/grafana-loki/</id>
            
            
            <published>2019-06-22T00:00:00+00:00</published>
            <updated>2019-06-22T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Loki is a Prometheus-inspired logging service for cloud native infrastructure.
It’s similar to well-known ELK stack but more simple use and is intended to be used mostly Kubernetes.</p>
<h3 id="loki-components">Loki components</h3>
<p>Loki-stack consists of three main components:</p>
<ul>
<li>promtail – agent to collect logs on a host and push them to a Loki instance</li>
<li>loki – TSDB (Time-series database) logs aggregation and processing server</li>
<li>Grafana – for querying and displaying logs</li>
</ul>
<h3 id="deployment">Deployment</h3>
<pre tabindex="0"><code>yum install nginx -y
systemct start nginx

mkdir /opt/loki
cd /opt/loki/
</code></pre><pre tabindex="0"><code>nano loki-promtail-conf.yml
server:

  http_listen_port: 9080
  grpc_listen_port: 0

positions:

  filename: /tmp/positions.yaml

client:

  url: http://loki:3100/api/prom/push

scrape_configs:

  - job_name: system
    entry_parser: raw
    static_configs:
    - targets:
        - localhost
      labels:
        job: varlogs
        __path__: /var/log/*log

  - job_name: nginx
    entry_parser: raw
    static_configs:
    - targets:
        - localhost
      labels:
        job: nginx
        __path__: /var/log/nginx/*log
</code></pre><pre tabindex="0"><code>nano docker-compose.yml
version: &quot;3&quot;

networks:
  loki:

services:
  loki:
    image: grafana/loki:master
    ports:
      - &quot;3100:3100&quot;
    command: -config.file=/etc/loki/local-config.yaml
    networks:
      - loki

  promtail:
    image: grafana/promtail:master
    volumes:
       - /opt/loki/loki-promtail-conf.yml:/etc/promtail/docker-config.yaml
      - /var/log:/var/log
    command: -config.file=/etc/promtail/docker-config.yaml
    networks:
      - loki

  grafana:
    image: grafana/grafana:master
    ports:
      - &quot;3000:3000&quot;
    networks:
      - loki
</code></pre><pre tabindex="0"><code>docker-compose up -d

# add datasource
key=”{job=\”nginx\”}” appeared – all good.
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift SSO with Gitlab]]></title>
            <link href="https://devopstales.github.io/home/openshift-sso-2/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift SSO authentication" />
                <link href="https://devopstales.github.io/home/nextcloud-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Nextcloud SSO" />
                <link href="https://devopstales.github.io/home/nexus-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Sonatype Nexus SSO" />
                <link href="https://devopstales.github.io/home/hashicorp-sso/?utm_source=atom_feed" rel="related" type="text/html" title="SSO for hashicorp vault" />
                <link href="https://devopstales.github.io/home/rundeck-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Rundeck SSO" />
            
                <id>https://devopstales.github.io/home/openshift-sso-2/</id>
            
            
            <published>2019-06-17T00:00:00+00:00</published>
            <updated>2019-06-17T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Configure Openshift Cluster to use Gitlab as a user backend for login with oauth2 and SSO.</p>
<H3>Parst of the Openshift series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
     <li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
     <li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
     <li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
     <li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
     <li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
     <li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
     <li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
     <li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
     <li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
     <li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
     <li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
     <li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
     <li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
     <li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
     <li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
     <li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
     <li>Part16a: <a href="../../kubernetes/openshift-elasticsearch-error/">Opeshift elasticsearch search-guard error</a></li>
     <li>Part16b: <a href="../../kubernetes/openshift-log4shell/">Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)</a></li>
</ul>


<p>With Ansible-openshift you can not change the authetication method after Install !! If you installed the cluster with htpasswd, then change to LDAP the playbook trys to add a second authentication methot for the config. It is forbidden to add a second type of identity provider in the version 3.11 of Ansible-openshift. To solve this problem we must change the configuration manually.</p>
<h3 id="environment">Environment</h3>
<pre tabindex="0"><code>192.168.1.40    deployer
192.168.1.41    openshift01 # master node
192.168.1.42    openshift02 # infra node
192.168.1.43    openshift03 # worker node
</code></pre><h3 id="configuration-gitlab">Configuration Gitlab</h3>
<p>Login to Gitlab and create client for the app:
<img src="/img/include/openshift_gitlab_sso.png" alt="Example image"  class="zoomable" /></p>
<h3 id="configurate-the-cluster">Configurate The cluster</h3>
<pre tabindex="0"><code># on all openshift hosts
nano /etc/origin/master/master-config.yaml
...
  identityProviders:
  - name: gitlabsso
    challenge: true
    login: true
    mappingMethod: claim
    provider:
      apiVersion: v1
      kind: GitLabIdentityProvider
      legacy: true
      clientID: 7305abce637a123654a2c9dd4f8caec1156a1bc41cd80be4db0f14253fe24e58
      clientSecret: 2d5aebe7831c99383d876cc235febb401906263de748a29b03b058f62f15c2f7
      url: https://gitlab.devopstales.intra/
  - challenge: true
</code></pre><h3 id="reconfigurate-the-cluster">Reconfigurate the cluster</h3>
<pre tabindex="0"><code># on all openshift hosts
master-restart api
master-restart controllers
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install CEHP Radosgateway on Proxmox]]></title>
            <link href="https://devopstales.github.io/home/proxmox-ceph-radosgw/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/proxmox-ceph-radosgw/</id>
            
            
            <published>2019-06-14T00:00:00+00:00</published>
            <updated>2019-06-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>RADOS Gateway  is an object storage interface in Ceph. It provides interfaces compatible with OpenStack Swift and Amazon S3.</p>
<p>First create a keyring than generated the keys and added them to the keyring:</p>
<pre tabindex="0"><code>root@pve1:~# ceph-authtool --create-keyring /etc/ceph/ceph.client.radosgw.keyring

root@pve1:~# ceph-authtool /etc/ceph/ceph.client.radosgw.keyring -n client.radosgw.pve1 --gen-key
root@pve1:~# ceph-authtool /etc/ceph/ceph.client.radosgw.keyring -n client.radosgw.pve2 --gen-key
root@pve1:~# ceph-authtool /etc/ceph/ceph.client.radosgw.keyring -n client.radosgw.pve3 --gen-key
</code></pre><p>And then I added the proper capabilities and add the keys to the cluster:</p>
<pre tabindex="0"><code>root@pve1:~# ceph-authtool -n client.radosgw.pve1 --cap osd 'allow rwx' --cap mon 'allow rwx' /etc/ceph/ceph.client.radosgw.keyring
root@pve1:~# ceph-authtool -n client.radosgw.pve2 --cap osd 'allow rwx' --cap mon 'allow rwx' /etc/ceph/ceph.client.radosgw.keyring
root@pve1:~# ceph-authtool -n client.radosgw.pve3 --cap osd 'allow rwx' --cap mon 'allow rwx' /etc/ceph/ceph.client.radosgw.keyring

root@pve1:~# ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.radosgw.pve1 -i /etc/ceph/ceph.client.radosgw.keyring
root@pve1:~# ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.radosgw.pve2 -i /etc/ceph/ceph.client.radosgw.keyring
root@pve1:~# ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.radosgw.pve3 -i /etc/ceph/ceph.client.radosgw.keyring
</code></pre><p>Copy the rings to the proxmox ClusterFS</p>
<pre tabindex="0"><code>root@pve1:~# cp /etc/ceph/ceph.client.radosgw.keyring /etc/pve/priv
</code></pre><p>Add the following lines to <code>/etc/ceph/ceph.conf</code>:</p>
<pre tabindex="0"><code>[client.radosgw.pve1]
        host = pve1
        keyring = /etc/pve/priv/ceph.client.radosgw.keyring
        log file = /var/log/ceph/client.radosgw.$host.log
        rgw_dns_name = s3.devopstales.intra

[client.radosgw.pve2]
        host = pve2
        keyring = /etc/pve/priv/ceph.client.radosgw.keyring
        log file = /var/log/ceph/client.radosgw.$host.log
        rgw_dns_name = s3.devopstales.intra

[client.radosgw.pve3]
        host = pve3
        keyring = /etc/pve/priv/ceph.client.radosgw.keyring
        log file = /var/log/ceph/client.rados.$host.log
        rgw_dns_name = s3.devopstales.intra
</code></pre><p>Install the pcakages and start the service. If all goes well, RADOSGW will create some default pools for you.</p>
<pre tabindex="0"><code>root@pve1:~# apt install radosgw
root@pve1:~# service radosgw start

root@pve1:~# tail -f /var/log/ceph/client.rados.pve1.log
</code></pre><pre tabindex="0"><code>root@pve1:~# ceph osd pool application enable .rgw.root rgw
root@pve1:~# ceph osd pool application enable default.rgw.control rgw
root@pve1:~# ceph osd pool application enable default.rgw.data.root rgw
root@pve1:~# ceph osd pool application enable default.rgw.gc rgw
root@pve1:~# ceph osd pool application enable default.rgw.log rgw
root@pve1:~# ceph osd pool application enable default.rgw.users.uid rgw
root@pve1:~# ceph osd pool application enable default.rgw.users.email rgw
root@pve1:~# ceph osd pool application enable default.rgw.users.keys rgw
root@pve1:~# ceph osd pool application enable default.rgw.buckets.index rgw
root@pve1:~# ceph osd pool application enable default.rgw.buckets.data rgw
root@pve1:~# ceph osd pool application enable default.rgw.lc rgw
</code></pre><pre tabindex="0"><code>root@pve1:~#  ssh pve2 'apt install radosgw &amp;&amp; service radosgw start'
root@pve1:~#  ssh pve3 'apt install radosgw &amp;&amp; service radosgw start'

root@pve1:~#  ceph osd pool ls
</code></pre><pre tabindex="0"><code>root@pve1:~# radosgw-admin user create --uid=devopstales --display-name=&quot;devopstales&quot; --email=devopstales@devopstales.intra
root@pve1:~# radosgw-admin user info devopstales

root@pve1:~# ceph osd pool application enable default.rgw.buckets.index rgw
root@pve1:~# ceph osd pool application enable default.rgw.buckets.data rgw
</code></pre><pre tabindex="0"><code>root@pve1:~# apt-get install s3cmd
root@pve1:~# s3cmd --configure
Access Key: xxxxxxxxxxxxxxxxxxxxxx
Secret Key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

root@pve1:~#  s3cmd mb s3://devopstales
Bucket 's3://devopstales/' created
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Prometheus for Gitlab]]></title>
            <link href="https://devopstales.github.io/home/install-prometheus-for-gitlab/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/prometheus-alertmanagger/?utm_source=atom_feed" rel="related" type="text/html" title="Install Alertmanagger" />
                <link href="https://devopstales.github.io/monitoring/install-prometheus-for-gitlab/?utm_source=atom_feed" rel="related" type="text/html" title="Install Prometheus for Gitlab" />
                <link href="https://devopstales.github.io/home/pfsense-telegraf/?utm_source=atom_feed" rel="related" type="text/html" title="Install telegraf on pfsense" />
                <link href="https://devopstales.github.io/home/naemon-influxdb/?utm_source=atom_feed" rel="related" type="text/html" title="Install Nemon with Influxdb storage" />
                <link href="https://devopstales.github.io/home/install-mattermost-for-gitlab/?utm_source=atom_feed" rel="related" type="text/html" title="Install mattermost for Gitlab" />
            
                <id>https://devopstales.github.io/home/install-prometheus-for-gitlab/</id>
            
            
            <published>2019-06-10T00:00:00+00:00</published>
            <updated>2019-06-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Prometheus is an open-source monitoring system with a built-in noSQL time-series database. It offers a multi-dimensional data model, a flexible query language, and diverse visualization possibilities. Prometheus collects metrics from http nedpoint. Most service dind’t have this endpoint so you need optional programs that generate additional metrics cald exporters.</p>
<h3 id="install-prometheus-from-package">Install prometheus from package</h3>
<pre tabindex="0"><code>curl -s https://packagecloud.io/install/repositories/prometheus-rpm/release/script.rpm.sh | sudo bash

yum install prometheus2 alertmanager -y
</code></pre><h3 id="configurate-prometheus">Configurate Prometheus</h3>
<pre tabindex="0"><code>nano /etc/prometheus/promethsu.yml
---
global:
  scrape_interval: 15s
  scrape_timeout: 15s
  evaluation_interval: 15s

alerting:
  alertmanagers:
  - static_configs:
    - targets:
       - localhost:9095

rule_files:
  - &quot;/etc/prometheus/alert.rules&quot;

scrape_configs:
- job_name: prometheus
  static_configs:
  - targets:
    - localhost:9090
- job_name: redis
  static_configs:
  - targets:
    - localhost:9121
- job_name: postgres
  static_configs:
  - targets:
    - localhost:9187
- job_name: node
  static_configs:
  - targets:
    - localhost:9100
- job_name: gitlab-workhorse
  static_configs:
  - targets:
    - localhost:9229
- job_name: gitlab-unicorn
  metrics_path: &quot;/-/metrics&quot;
  static_configs:
  - targets:
    - 127.0.0.1:8080
- job_name: gitlab-sidekiq
  static_configs:
  - targets:
    - 127.0.0.1:8082
- job_name: gitlab_monitor_database
  metrics_path: &quot;/database&quot;
  static_configs:
  - targets:
    - localhost:9168
- job_name: gitlab_monitor_sidekiq
  metrics_path: &quot;/sidekiq&quot;
  static_configs:
  - targets:
    - localhost:9168
- job_name: gitlab_monitor_process
  metrics_path: &quot;/process&quot;
  static_configs:
  - targets:
    - localhost:9168
- job_name: gitaly
  static_configs:
  - targets:
    - localhost:9236
- job_name: nginx
  static_configs:
  - targets:
    - localhost:9913
- job_name: 'playframework-app'
  scrape_interval: 5s
  metrics_path: '/metrics'
  static_configs:
  - targets: ['localhost:9000']
</code></pre><pre tabindex="0"><code>nano /etc/prometheus/alert.rules
groups:
- name: host
  rules:
  - alert: low_connected_users
    expr: play_current_users &lt; 2
    for: 30s
    labels:
      severity: slack
    annotations:
      summary: &quot;Instance {{ $labels.instance }} under lower load&quot;
      description: &quot;{{ $labels.instance }} of job {{ $labels.job }} is under lower load.&quot;
</code></pre><p><img src="/img/include/mattermost_gitlab4.png" alt="Example image"  class="zoomable" /><!-- raw HTML omitted --><!-- raw HTML omitted -->
<img src="/img/include/mattermost_gitlab5.png" alt="Example image"  class="zoomable" /><!-- raw HTML omitted --><!-- raw HTML omitted -->
<img src="/img/include/mattermost_gitlab6.png" alt="Example image"  class="zoomable" /><!-- raw HTML omitted --><!-- raw HTML omitted -->
<img src="/img/include/mattermost_gitlab7.png" alt="Example image"  class="zoomable" /><!-- raw HTML omitted --><!-- raw HTML omitted -->
<img src="/img/include/mattermost_gitlab8.png" alt="Example image"  class="zoomable" /><!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<pre tabindex="0"><code>nano /etc/prometheus/alertmanager.yml
global:

templates:
- '/etc/prometheus/template/*.tmpl'

route:
 group_by: [alertname, job]
 # If an alert isn't caught by a route, send it to slack.
 receiver: slack_general
 routes:
  - match:
      severity: slack
    receiver: slack_general

receivers:
- name: slack_general
  slack_configs:
  - api_url: http://mattermost.devopstales.intra/hooks/9g4qwgpkzi898jzzeszzzzutmc
    channel: 'monitoring'
    username: &quot;prometheus&quot; #name ins mattermost
    text: &quot;&quot;
    send_resolved: true
</code></pre><pre tabindex="0"><code>mkdir /etc/prometheus/template/
nano /etc/prometheus/template/alertmessage.tmpl
{{ define &quot;__slack_text&quot; }}
{{ range .Alerts }}{{ .Annotations.description}}{{ end }}
{{ end }}

{{ define &quot;__slack_title&quot; }}
{{ range .Alerts }} :scream: {{ .Annotations.summary}} :scream: {{ end }}
{{ end }}

{{ define &quot;slack.default.text&quot; }}{{ template &quot;__slack_text&quot; . }}{{ end }}
{{ define &quot;slack.default.title&quot; }}{{ template &quot;__slack_title&quot; . }}{{ end }}
</code></pre><h3 id="configurate-gitlab">Configurate Gitlab</h3>
<p><img src="/img/include/mattermost_gitlab1.png" alt="Example image"  class="zoomable" /><!-- raw HTML omitted --><!-- raw HTML omitted -->
<img src="/img/include/mattermost_gitlab2.png" alt="Example image"  class="zoomable" /><!-- raw HTML omitted --><!-- raw HTML omitted -->
<img src="/img/include/mattermost_gitlab3.png" alt="Example image"  class="zoomable" /><!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<pre tabindex="0"><code>nano /etc/gitlab/gitlab.rb
alertmanager['enable'] = false
prometheus['enable'] = false
node_exporter['enable'] = true
redis_exporter['enable'] = true
postgres_exporter['enable'] = true
gitlab_monitor['enable'] = true

gitlab-ctl reconfigure

systemctl start alertmanager.service
systemctl status alertmanager.service
systemctl start prometheus.service
systemctl status prometheus.service
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install mattermost for Gitlab]]></title>
            <link href="https://devopstales.github.io/home/install-mattermost-for-gitlab/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/gitlab-install/?utm_source=atom_feed" rel="related" type="text/html" title="Gitlab Install" />
                <link href="https://devopstales.github.io/home/openshift-letsencrypt/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Letsencrypt certificates" />
                <link href="https://devopstales.github.io/home/nextcloud-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Nextcloud SSO" />
                <link href="https://devopstales.github.io/home/nexus-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Sonatype Nexus SSO" />
                <link href="https://devopstales.github.io/home/hashicorp-sso/?utm_source=atom_feed" rel="related" type="text/html" title="SSO for hashicorp vault" />
            
                <id>https://devopstales.github.io/home/install-mattermost-for-gitlab/</id>
            
            
            <published>2019-06-09T00:00:00+00:00</published>
            <updated>2019-06-09T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Mattermost is an open source on premise alternative of Slack.</p>
<h3 id="install-mattermost-from-package">Install mattermost from package</h3>
<pre tabindex="0"><code>sudo yum -y install https://harbottle.gitlab.io/harbottle-main/7/x86_64/harbottle-main-release.rpm

yum install mattermost-server -y
</code></pre><h3 id="configurate-mattermost">Configurate mattermost</h3>
<pre tabindex="0"><code>nano /etc/mattermost/config.json
# postgresql
&quot;SiteURL&quot;: &quot;http://mattermost.devopstales.intra&quot;
&quot;DriverName&quot;: &quot;postgres&quot;,
&quot;DataSource&quot;: &quot;postgres://mmuser:Password1@localhost:5432/mattermost?sslmode=disable&amp;connect_timeout=10&quot;

# gitlab austh
&quot;AllowedUntrustedInternalConnections&quot;: &quot;gitlab.devopstales.intra&quot;
...
    &quot;GitLabSettings&quot;: {
        &quot;Enable&quot;: true,
        &quot;Secret&quot;: &quot;&lt;secret&gt;&quot;,
        &quot;Id&quot;: &quot;&lt;id&gt;&quot;,
        &quot;Scope&quot;: &quot;&quot;,
        &quot;AuthEndpoint&quot;: &quot;http://gitlab.devopstales.intra/oauth/authorize&quot;,
        &quot;TokenEndpoint&quot;: &quot;http://gitlab.devopstales.intra/oauth/token&quot;,
        &quot;UserApiEndpoint&quot;: &quot;http://gitlab.devopstales.intra/api/v4/user&quot;
    },
</code></pre><h3 id="edit-systemd-serice">Edit systemd serice</h3>
<pre tabindex="0"><code>nano /usr/lib/systemd/system/mattermost.service
[Unit]
Description=Mattermost
After=syslog.target network.target
After=postgresql.service
Requires=postgresql-9.6.service

[Service]
Type=notify
NotifyAccess=main
WorkingDirectory=/usr/share/mattermost
User=mattermost
Group=mattermost
ExecStart=/usr/share/mattermost/bin/mattermost
TimeoutStartSec=3600
LimitNOFILE=49152

[Install]
WantedBy=multi-user.target

systemctl daemon-reload
systemctl start mattermost.service
</code></pre><h3 id="configurate-nginx-proxy">Configurate nginx proxy</h3>
<pre tabindex="0"><code>nano /etc/nginx/conf.d/mattermost.conf
upstream backend {
   server localhost:8065;
   keepalive 32;
}

proxy_cache_path /var/cache/nginx levels=1:2 keys_zone=mattermost_cache:10m max_size=3g inactive=120m use_temp_path=off;

server {
   listen 80;
   server_name    mattermost.devopstales.intra;

   location ~ /api/v[0-9]+/(users/)?websocket$ {
       proxy_set_header Upgrade $http_upgrade;
       proxy_set_header Connection &quot;upgrade&quot;;
       client_max_body_size 50M;
       proxy_set_header Host $http_host;
       proxy_set_header X-Real-IP $remote_addr;
       proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
       proxy_set_header X-Forwarded-Proto $scheme;
       proxy_set_header X-Frame-Options SAMEORIGIN;
       proxy_buffers 256 16k;
       proxy_buffer_size 16k;
       client_body_timeout 60;
       send_timeout 300;
       lingering_timeout 5;
       proxy_connect_timeout 90;
       proxy_send_timeout 300;
       proxy_read_timeout 90s;
       proxy_pass http://backend;
   }

   location / {
       client_max_body_size 50M;
       proxy_set_header Connection &quot;&quot;;
       proxy_set_header Host $http_host;
       proxy_set_header X-Real-IP $remote_addr;
       proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
       proxy_set_header X-Forwarded-Proto $scheme;
       proxy_set_header X-Frame-Options SAMEORIGIN;
       proxy_buffers 256 16k;
       proxy_buffer_size 16k;
       proxy_read_timeout 600s;
       proxy_cache mattermost_cache;
       proxy_cache_revalidate on;
       proxy_cache_min_uses 2;
       proxy_cache_use_stale timeout;
       proxy_cache_lock on;
       proxy_http_version 1.1;
       proxy_pass http://backend;
   }
}
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Gitlab Install]]></title>
            <link href="https://devopstales.github.io/home/gitlab-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/gitlab-install/?utm_source=atom_feed" rel="related" type="text/html" title="Gitlab Install" />
                <link href="https://devopstales.github.io/home/gitlab-keycloak/?utm_source=atom_feed" rel="related" type="text/html" title="SSO login to Gitlab" />
                <link href="https://devopstales.github.io/home/openshift-letsencrypt/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Letsencrypt certificates" />
                <link href="https://devopstales.github.io/home/nextcloud-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Nextcloud SSO" />
                <link href="https://devopstales.github.io/home/nexus-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Sonatype Nexus SSO" />
            
                <id>https://devopstales.github.io/home/gitlab-install/</id>
            
            
            <published>2019-06-05T00:00:00+00:00</published>
            <updated>2019-06-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Install Gitab with custom postgresql and nginx proxy.</p>
<h3 id="install-ntpd">Install NTPD</h3>
<pre tabindex="0"><code>yum install -y epel-release yum-utils
yum-config-manager --enable epel

sudo chkconfig ntpd on
sudo ntpdate 0.hu.pool.ntp.org
sudo service ntpd start
</code></pre><h3 id="install-postgresql">Install postgresql</h3>
<p>In a previous post I wrote about how to <a href="/linux/install-postgresql/#install-postgresql-10-on-centos-7">Install PostgreSQL 10</a></p>
<pre tabindex="0"><code>su - postgres
createdb -U postgres gitlab
createdb -U postgres gitlab_ci
createdb -U postgres mattermost

createuser gituser
createuser ciuser
createuser mmuser

psql -U postgres gitlab
ALTER USER &quot;gituser&quot; WITH PASSWORD 'Password1';
GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO gituser;
ALTER ROLE gituser CREATEROLE SUPERUSER;
\q

psql -U postgres gitlab_ci
ALTER USER &quot;ciuser&quot; WITH PASSWORD 'Password1';
GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO ciuser;
\q

psql -U postgres mattermost
ALTER USER &quot;mmuser&quot; WITH PASSWORD 'Password1';
GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO mmuser;
\q
</code></pre><h3 id="install-nginx">install nginx</h3>
<pre tabindex="0"><code>yum install -y nginx
mkdir /var/log/gitlab/nginx/

echo 'upstream gitlab-workhorse {
        server unix:/var/opt/gitlab/gitlab-workhorse/socket;
}

server {
        listen *:80;
#  listen *:443 ssl;
        server_name gitlab.devopstales.intra;
        server_tokens off;
        root /opt/gitlab/embedded/service/gitlab-rails/public;

        client_max_body_size 256m;

        real_ip_header X-Forwarded-For;

        access_log /var/log/gitlab/nginx/gitlab_access.log;
        error_log  /var/log/gitlab/nginx/gitlab_error.log;

 # ssl_certificate   /etc/nginx/ssl.d/gitlab.pem;
 # ssl_certificate_key   /etc/nginx/ssl.d/gitlab.key;


        location / {
                proxy_read_timeout	300;
                proxy_connect_timeout   300;
                proxy_redirect          off;

                proxy_buffering off;

                proxy_set_header    Host                $http_host;
                proxy_set_header    X-Real-IP           $remote_addr;
                proxy_set_header    X-Forwarded-For     $proxy_add_x_forwarded_for;
                proxy_set_header    X-Forwarded-Proto   $scheme;

                proxy_pass http://gitlab-workhorse;

    proxy_request_buffering off;
    proxy_http_version 1.1;
        }

        location ~ ^/(assets)/ {
                root /opt/gitlab/embedded/service/gitlab-rails/public;
                gzip_static on; # to serve pre-gzipped version
                        expires max;
               	add_header Cache-Control public;
        }

        error_page 502 /502.html;
}' &gt; /etc/nginx/conf.d/gitlab.conf

nano /etc/nginx/nginx.conf
log_format gitlab_access '$remote_addr - $remote_user [$time_local] &quot;$request&quot; $status $body_bytes_sent &quot;$http_referer&quot; &quot;$http_user_agent&quot;';
log_format gitlab_ci_access '$remote_addr - $remote_user [$time_local] &quot;$request&quot; $status $body_bytes_sent &quot;$http_referer&quot; &quot;$http_user_agent&quot;';
log_format gitlab_mattermost_access '$remote_addr - $remote_user [$time_local] &quot;$request&quot; $status $body_bytes_sent &quot;$http_referer&quot; &quot;$http_user_agent&quot;';

nginx -t
nginx -s reload

sudo usermod -aG gitlab-www nginx
</code></pre><h3 id="install-gitlab">install gitlab</h3>
<pre tabindex="0"><code>curl -sS https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh | sudo bash
sudo yum install gitlab-ce -y

cd /opt/gitlab/embedded/bin
mv psql psql_moved
mv pg_dump pg_dump_moved

which pg_dump psql

ln -s /usr/bin/pg_dump /usr/bin/psql /opt/gitlab/embedded/bin/
</code></pre><pre tabindex="0"><code>nano /etc/gitlab/gitlab.rb
external_url 'http://gitlab.devopstales.intra'
gitlab_rails['time_zone'] = 'Europe/Budapest'

gitlab_rails['db_adapter'] = &quot;postgresql&quot;
gitlab_rails['db_encoding'] = &quot;unicode&quot;
gitlab_rails['db_database'] = &quot;gitlab&quot;
gitlab_rails['db_pool'] = 10
gitlab_rails['db_username'] = &quot;gituser&quot;
gitlab_rails['db_password'] = &quot;Password1&quot;
gitlab_rails['db_host'] = '127.0.0.1'
gitlab_rails['db_port'] = 5432

gitlab_rails['redis_socket'] = &quot;/var/opt/gitlab/redis/redis.socket&quot;

unicorn['socket'] = '/var/opt/gitlab/gitlab-rails/sockets/gitlab.socket'
unicorn['log_directory'] = &quot;/var/log/gitlab/unicorn&quot;
unicorn['port'] = 8081

user['username'] = &quot;git&quot;
user['group'] = &quot;git&quot;

postgresql['enable'] = false

nginx['enable'] = false
web_server['external_users'] = ['nginx']
</code></pre><pre tabindex="0"><code>gitlab-ctl reconfigure
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Migrate BIND to Windows DNS]]></title>
            <link href="https://devopstales.github.io/home/migrate-bind-to-windows-dns/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/migrate-bind-to-windows-dns/</id>
            
            
            <published>2019-06-04T00:00:00+00:00</published>
            <updated>2019-06-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Migrate dns Zones from bind to Windows DNS Server</p>
<h3 id="linux">Linux</h3>
<pre tabindex="0"><code>nano /etc/bind/zones.active
allow-transfer { 192.168.0.8; };
</code></pre><h3 id="windows">Windows</h3>
<pre tabindex="0"><code>Add-DnsServerSecondaryZone -Name &quot;devopstales.intra&quot; -ZoneFile &quot;devopstales.intra.dns&quot; -MasterServers 192.168.0.60
ConvertTo-DnsServerPrimaryZone -Name &quot;devopstales.intra&quot; -PassThru -Verbose -ZoneFile &quot;devopstales.intra.dns&quot; -Force
Set-DnsServerPrimaryZone -Name &quot;devopstales.intra&quot; –Notify Notifyservers –notifyservers &quot;192.168.0.5&quot; -SecondaryServers &quot;192.168.0.5&quot; –SecureSecondaries TransferToSecureServers
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Override a single external hostname with internal DNS-entry]]></title>
            <link href="https://devopstales.github.io/home/override-a-single-external-hostname-with-internal-dns-entry/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/override-a-single-external-hostname-with-internal-dns-entry/</id>
            
            
            <published>2019-06-04T00:00:00+00:00</published>
            <updated>2019-06-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Override a single external hostname with internal DNS-entry</p>
<h3 id="problem">Problem:</h3>
<p>Company.com has an exernal dns-record for service.company.com which should be resolved to an inernal IP by internal clients.</p>
<p>Let’s say that service.company.com resolves to 1.1.1.1 by the external DNS but when computers are connecting to this URL from inside the company network the internal DNS servers at ad.company.com needs to resolve service.company.com to 172.16.51.25.</p>
<p>Adding an entry to the hosts-file on each client computer to override service.company.com will not work when clients connect on exteral networks like from home or a coffeeshop.</p>
<h3 id="solution">Solution:</h3>
<p>The solution is to add a new Forward Lookup Zone named service.company.com and add a new Host-record, enter the internal IP-address but leave the Name blank.</p>
<p>On a DNS server running Windows Server 2012 this is of course achieved by using PowerShell!</p>
<p>First off, create a new DNS Forward Lookup Zone using PowerShell:</p>
<pre tabindex="0"><code>Add-DnsServerPrimaryZone -Name service.company.com -ReplicationScope Forest

#Then add a host record to the zone:

Add-DnsServerResourceRecordA -IPv4Address 172.16.51.25 -ZoneName service.company.com -Name service.company.com
</code></pre><p>By specifying service.company.com as both ZoneName and Name a record with the name “(same as parent folder)” will be created.</p>
<p>This will only override DNS queries for the FQDN service.company.com and will not affect other records in company.com</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Sonarqube Install]]></title>
            <link href="https://devopstales.github.io/home/sonarkube-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/sonarkube-install/</id>
            
            
            <published>2019-05-30T00:00:00+00:00</published>
            <updated>2019-05-30T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Sonarqube Repository OSS is an artifact repository with universal support for popular formats.</p>
<pre tabindex="0"><code>yum install epel-release -y
yum update -y
</code></pre><h3 id="install-postgresql">Install Postgresql</h3>
<p>In a previous post I wrote about how to <a href="/linux/install-postgresql/#install-postgresql-9-6-on-centos-7">Install PostgreSQL 10</a></p>
<pre tabindex="0"><code>su - postgres
createuser -S sonar
createdb -O sonar sonar
psql
ALTER USER &quot;sonar&quot; WITH PASSWORD 'Password1';
\q
exit
</code></pre><h3 id="install-sonarqube">Install Sonarqube</h3>
<pre tabindex="0"><code># https://binaries.sonarsource.com/Distribution/sonarqube/
cd /opt
wget https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-7.6.zip
unzip sonarqube-7.6.zip
ln -s /opt/sonarqube-7.6 /opt/sonarqube

nano /opt/sonarqube/conf/sonar.properties
sonar.jdbc.username=sonar
sonar.jdbc.password=Password1
sonar.jdbc.url=jdbc:postgresql://localhost/sonar

/opt/sonarqube/bin/linux-x86-64/sonar.sh
RUN_AS_USER=sonar

adduser -s /bin/false sonar
chown -R sonar:sonar /opt/sonarqube/

sysctl -w vm.max_map_count=262144
sysctl -w fs.file-max=65536
ulimit -n 65536
ulimit -u 4096
</code></pre><h3 id="create-sistemd-serice-for-sonarqube">Create sistemd serice for Sonarqube</h3>
<pre tabindex="0"><code>echo '[Unit]
Description=Sonar
After=network.target network-online.target
Wants=network-online.target

[Service]
ExecStart=/opt/sonarqube/bin/linux-x86-64/sonar.sh start
ExecStop=/opt/sonarqube/bin/linux-x86-64/sonar.sh stop
ExecReload=/opt/sonarqube/bin/linux-x86-64/sonar.sh restart
PIDFile=/opt/sonarqube/bin/linux-x86-64/./SonarQube.pid
Type=forking
User=sonar
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target' &gt; /etc/systemd/system/sonar.service
</code></pre><h3 id="start-sonarqube">Start Sonarqube</h3>
<pre tabindex="0"><code>sudo systemctl daemon-reload
sudo systemctl enable sonar.service
sudo systemctl start sonar.service

tailf /opt/sonarqube/logs/es.log
### To check, point your browser to http://localhost:9000. Default username is admin with password admin.
</code></pre><h3 id="apache-proxy">Apache proxy</h3>
<pre tabindex="0"><code>echo 'ProxyRequests Off
ProxyPreserveHost On
&lt;VirtualHost *:80&gt;
  ServerName sonar.devopstales.intra
  ServerAdmin admin@somecompany.com
  ProxyPass / http://localhost:9000/
  ProxyPassReverse / http://localhost:9000/
  ErrorLog /var/log/sonar-error.log
  CustomLog /var/log/sonar-access.log common
&lt;/VirtualHost&gt;' &gt; /etc/httpd/conf.d/sonar.conf

systemctl start httpd
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift Letsencrypt certificates]]></title>
            <link href="https://devopstales.github.io/home/openshift-letsencrypt/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/openshift-letsencrypt/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Letsencrypt certificates" />
                <link href="https://devopstales.github.io/home/openshift-hostalreadyclaimed/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Error: HostAlreadyClaimed" />
                <link href="https://devopstales.github.io/home/openshift-gitlabrunner/?utm_source=atom_feed" rel="related" type="text/html" title="Install Gitlab runner on Openshift" />
                <link href="https://devopstales.github.io/home/openshift-extregistry/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: External registry" />
                <link href="https://devopstales.github.io/home/openshift-cert/?utm_source=atom_feed" rel="related" type="text/html" title="Change Certificates in Openshift" />
            
                <id>https://devopstales.github.io/home/openshift-letsencrypt/</id>
            
            
            <published>2019-05-28T00:00:00+00:00</published>
            <updated>2019-05-28T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Thanks to Tomáš Nožička developed openshift-acme as an ACME Controller for OpenShift and Kubernetes clusters. <!-- raw HTML omitted -->
It automatically provision certficates</p>
<H3>Parst of the Openshift series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
     <li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
     <li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
     <li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
     <li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
     <li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
     <li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
     <li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
     <li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
     <li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
     <li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
     <li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
     <li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
     <li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
     <li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
     <li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
     <li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
     <li>Part16a: <a href="../../kubernetes/openshift-elasticsearch-error/">Opeshift elasticsearch search-guard error</a></li>
     <li>Part16b: <a href="../../kubernetes/openshift-log4shell/">Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)</a></li>
</ul>


<h3 id="environment">Environment</h3>
<pre tabindex="0"><code>192.168.1.40    deployer
192.168.1.41    openshift01 # master node
192.168.1.42    openshift02 # infra node
192.168.1.43    openshift03 # worker node
</code></pre><h3 id="deploy-route">Deploy route</h3>
<pre tabindex="0"><code>oc project default

oc label node openshift02.devopstales.intra &quot;router=letsencrypt&quot;
oc get node --show-labels

oc adm policy add-scc-to-user hostnetwork -z router
oc adm router router-letsencrypt --replicas=0 --ports=&quot;8080:8080,8443:8443&quot; --stats-port=1937 --selector=&quot;router=letsencrypt&quot; --labels=&quot;router=letsencrypt&quot;

oc set env dc/router-letsencrypt \
NAMESPACE_LABELS=&quot;router=letsencrypt&quot; \
ROUTER_ALLOW_WILDCARD_ROUTES=true \
ROUTER_SERVICE_HTTP_PORT=8080 \
ROUTER_SERVICE_HTTPS_PORT=8443 \
ROUTER_TCP_BALANCE_SCHEME=roundrobin

oc set env dc/router NAMESPACE_LABELS=&quot;router != letsencrypt&quot;

oc scale dc/router-letsencrypt --replicas=3
</code></pre><h3 id="deploy-letsencrypt">Deploy letsencrypt</h3>
<pre tabindex="0"><code>GIT_REPO=https://raw.githubusercontent.com/devopstales/openshift-examples
GIT_PATH=/master/letsencrypt
oc new-project letsencrypt
oc create -f$GIT_REPO/$GIT_PATH/{clusterrole,serviceaccount,imagestream,deployment}.yaml
oc adm policy add-cluster-role-to-user openshift-acme -z openshift-acme
</code></pre><h1 id="demo">Demo</h1>
<pre tabindex="0"><code>oc new-project test
oc label namespace test router=letsencrypt
oc new-app centos/ruby-25-centos7~https://github.com/sclorg/ruby-ex.git
oc expose svc/ruby-ex

oc patch route ruby-ex \
    -p '{&quot;metadata&quot;:{&quot;annotations&quot;:{  &quot;kubernetes.io/tls-acme&quot; : &quot;true&quot;   }}}'
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configure OpenVPN HA opnsense cluster]]></title>
            <link href="https://devopstales.github.io/home/opnsense-openvpn/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/opnsense-openvpn/</id>
            
            
            <published>2019-05-25T00:00:00+00:00</published>
            <updated>2019-05-25T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this LAB I&rsquo;ll be creating OpenVPN SSL Peer to Peer connection.</p>
<h3 id="the-architecture">The Architecture</h3>
<pre tabindex="0"><code> ------ WAN ------
 |               |
PF1 -- sync -- PF2
 |               |
 ----- LAN -------  

WAN: 192.168.0.0/24 (Bridgelt)
LAN: 192.168.20.0/24
SYNC: 192.168.30.0/24
</code></pre><pre tabindex="0"><code>opn01:
WAN 192.168.0.28
LAN: 192.168.20.28
SYNC:192.168.30.28

opn02:
WAN 192.168.0.29
LAN: 192.168.20.29
SYNC:192.168.30.29
</code></pre><h3 id="configurate-the-opevpn-service">Configurate the OpeVPN service</h3>
<p>Got to <code>VPN &gt; OpenVPN &gt; Wizards</code>
<img src="/img/include/opnsense_ovpn1.png" alt="Example image"  class="zoomable" /></p>
<p>If you ulodad your certificate seledt that in the drop doew menu or select Add new Certificate to generate a new one.
<img src="/img/include/opnsense_ovpn2.png" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/opnsense_ovpn3.png" alt="Example image"  class="zoomable" /></p>
<p>Edit the Adwanced Configuration: <!-- raw HTML omitted -->
<img src="/img/include/opnsense_ovpn4.png" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/opnsense_ovpn5.png" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/opnsense_ovpn6.png" alt="Example image"  class="zoomable" /></p>
<h3 id="configurate-nat-rules-to-ha">Configurate NAT Rules to HA</h3>
<p>Go to <code>Firewall &gt; NAT &gt; Outbound</code> and clone the manul LAN Rule
<img src="/img/include/opnsense_ovpn8.png" alt="Example image"  class="zoomable" /></p>
<h3 id="enable-connection-from-openvpn-to-master-and-slave">Enable Connection from OpenVPN to master and slave</h3>
<p>In default there in no rout to the salve nod. <!-- raw HTML omitted -->
Go to <code>Firewll &gt; Aliases &gt; Add</code> and create alias for CARP members: <!-- raw HTML omitted -->
<img src="/img/include/opnsense_ovpn7.png" alt="Example image"  class="zoomable" /></p>
<p>Then go back to <code>Firewall &gt; NAT &gt; Outbound &gt; Settings</code> and create a new rule:
<img src="/img/include/opnsense_ovpn9.png" alt="Example image"  class="zoomable" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configure opnsense nextcloud backup]]></title>
            <link href="https://devopstales.github.io/home/opnsense-nextcloud/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/opnsense-nextcloud/</id>
            
            
            <published>2019-05-25T00:00:00+00:00</published>
            <updated>2019-05-25T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this LAB I&rsquo;ll be configurate the opnsense cloud backup solutuon for nextcloud.</p>
<h3 id="configurate-the-nextcloud">Configurate the nextcloud</h3>
<ul>
<li>
<p>login to your Nextcloud instance with the admin account</p>
</li>
<li>
<p>go to users
<img src="/img/include/nextcloud_sso1.png" alt="Example image"  class="zoomable" /></p>
</li>
<li>
<p>create a new user for opnsense
<img src="/img/include/opnsense_nextcloud2.png" alt="Example image"  class="zoomable" /></p>
</li>
<li>
<p>login with tehe new user and go to <code>profiele &gt; settings &gt; seurity</code></p>
</li>
<li>
<p>create token for user
<img src="/img/include/opnsense_nextcloud3.png" alt="Example image"  class="zoomable" /></p>
</li>
</ul>
<h3 id="configurate-opnsense-backup">Configurate opnsense backup</h3>
<ul>
<li>login to opnsense</li>
<li>go to <code>system &gt; config &gt; bckups</code></li>
<li>enable the nextclod config
<img src="/img/include/opnsense_nextcloud4.png" alt="Example image"  class="zoomable" /></li>
<li>click the <code>setup/test nextcloud</code> button</li>
</ul>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Nextcloud SSO]]></title>
            <link href="https://devopstales.github.io/home/nextcloud-sso/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/nexus-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Sonatype Nexus SSO" />
                <link href="https://devopstales.github.io/home/hashicorp-sso/?utm_source=atom_feed" rel="related" type="text/html" title="SSO for hashicorp vault" />
                <link href="https://devopstales.github.io/home/rundeck-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Rundeck SSO" />
                <link href="https://devopstales.github.io/home/mod-auth-openidc/?utm_source=atom_feed" rel="related" type="text/html" title="Apaceh2 oauth plugin" />
                <link href="https://devopstales.github.io/home/openshift-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift SSO authentication" />
            
                <id>https://devopstales.github.io/home/nextcloud-sso/</id>
            
            
            <published>2019-05-25T00:00:00+00:00</published>
            <updated>2019-05-25T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Nextcloud is a suite of client-server software for creating and using file hosting services. Nextcloud application functionally is similar to Dropbox.</p>
<h2 id="configuring-keycloak-and-nextcloud">Configuring Keycloak and Nextcloud</h2>
<h3 id="keycloak-side">Keycloak side</h3>
<ul>
<li>login to keycloak using the admin account</li>
<li>Under <code>Clients</code>, create a new client with <code>Client ID</code> &ldquo;nextcloud&rdquo; and <code>Root URL</code> &ldquo;cloud.devopstales.intra&rdquo;</li>
<li>On next screen, under the <code>Settings</code> tab, change <code>Access Type</code> from <code>public</code> to <code>confidential</code>, then Save</li>
<li>Go the the <code>Credentials</code> tab, note the <code>Secret</code></li>
<li>OPTIONAL: If there is no registered user yet you can create a test user: go to <code>Users</code>, click the <code>Add User</code> button, fill the <code>Username</code> with &ldquo;test&rdquo; and save. Then go to the <code>Credentials</code> tab, put the new password, toggle the <code>Temporary</code> option to <code>OFF</code>, press <code>Reset Password</code> and confirm</li>
</ul>
<p>Keycloak is now ready to be used for Nextcloud.</p>
<h3 id="nextcloud-side">NextCloud side</h3>
<ul>
<li>login to your Nextcloud instance with the admin account</li>
<li>Click on the user profile, then <code>Apps</code>
<img src="/img/include/nextcloud_sso1.png" alt="Example image"  class="zoomable" /><!-- raw HTML omitted --><!-- raw HTML omitted --></li>
<li>Go to <code>Social &amp; communication</code> and install the <code>Social Login</code> app</li>
<li>Go to <code>Settings</code> (in your user profile) the <code>Social Login</code><!-- raw HTML omitted --><!-- raw HTML omitted -->
<img src="/img/include/nextcloud_sso2.png" alt="Example image"  class="zoomable" /><!-- raw HTML omitted --><!-- raw HTML omitted --></li>
<li>Add a new <code>Custom OpenID Connect</code> by clicking on the <code>+</code> to its side</li>
<li>Fill the following:
<ul>
<li><code>Title</code> -&gt; &ldquo;keycloak&rdquo;</li>
<li><code>Authorize url</code> -&gt; <code>https://keycloak.devopstales.intra:8443/auth/realms/mydomain/protocol/openid-connect/auth</code></li>
<li><code>Token url</code> -&gt; <code>https:/keycloak.devopstales.intra:8443/auth/realms/mydomain/protocol/openid-connect/token</code></li>
<li><code>Client id</code> -&gt; &ldquo;nextcloud&rdquo;</li>
<li><code>Client Secret</code> -&gt; put the secret you noted down during the Keycloak configuration</li>
<li><code>Scope</code> -&gt; &ldquo;openid&rdquo;</li>
</ul>
</li>
<li>Press <code>Save</code></li>
</ul>
<p>Your Nextcloud instance is now configured. Log out and log back in using the <code>Alternative Logins -&gt; keycloak</code> method on the login page. It should redirect you to a keycloak auth form where you can log in with a registered keycloak user, then back to Nextcloud where you are now logged.
<img src="/img/include/nextcloud_sso3.png" alt="Example image"  class="zoomable" /><!-- raw HTML omitted --><!-- raw HTML omitted --></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configurate HA opnsense cluster]]></title>
            <link href="https://devopstales.github.io/home/opnsense-ha/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/opnsense-ha/</id>
            
            
            <published>2019-05-24T00:00:00+00:00</published>
            <updated>2019-05-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will configure 2 opnsense server to a HA cluster.</p>
<h3 id="the-architecture">The Architecture</h3>
<pre tabindex="0"><code> ------ WAN ------
 |               |
PF1 -- sync -- PF2
 |               |
 ----- LAN -------  

WAN: 192.168.0.0/24 (Bridgelt)
LAN: 192.168.20.0/24
SYNC: 192.168.30.0/24
</code></pre><pre tabindex="0"><code>opn01:
WAN 192.168.0.28
LAN: 192.168.20.28
SYNC:192.168.30.28

opn02:
WAN 192.168.0.29
LAN: 192.168.20.29
SYNC:192.168.30.29
</code></pre><h3 id="firewall-rules-for-sync">Firewall rules For sync</h3>
<p>On both firewalls add two rules to allow traffic on the SYNC interface: <!-- raw HTML omitted -->
go to <code>Firewall &gt; Rules &gt; Sync</code> and click <code>Add</code>.</p>
<p>Rule 1:
<img src="/img/include/opnsense_carp1.png" alt="Example image"  class="zoomable" /></p>
<p>Rule 2:
<img src="/img/include/opnsense_carp2.png" alt="Example image"  class="zoomable" /></p>
<p>Rule 3:
<img src="/img/include/opnsense_carp3.png" alt="Example image"  class="zoomable" /></p>
<h3 id="synchronization-settings">Synchronization Settings</h3>
<p>Go to <code>System &gt; High Availalility &gt; Settings</code>. Configure the sections like on the pictures.</p>
<p>Master:
<img src="/img/include/opnsense_carp4.png" alt="Example image"  class="zoomable" /></p>
<p>Slave:
<img src="/img/include/opnsense_carp5.png" alt="Example image"  class="zoomable" /></p>
<p>Test the synchronisation. Go to <code>System &gt; User management</code> and createa new user on the master node. <!-- raw HTML omitted -->
Then check on the slave node.</p>
<p>If it doesn&rsquo;t work, check:</p>
<ul>
<li>Are the firewall web interfaces running on the same protocols and ports?</li>
<li>Is the admin password set correctly? (User Manager &gt; Users &gt; admin.)</li>
<li>Are the firewall rules to allow synch set to use the correct interface (SYNC)?</li>
<li>If you&rsquo;re using VMs, are the firewalls on the same internal network?</li>
</ul>
<h3 id="create-virtual-ips">create virtual IPs</h3>
<p>On the master node go to<code> Firewall &gt; Virtual IPs &gt; Settings</code> and click Add. Create a new VIP adres for LAN and WAN interfaces.</p>
<p>WAN VIP on master:
<img src="/img/include/opnsense_carp6.png" alt="Example image"  class="zoomable" /></p>
<p>LAN VIP on master:
<img src="/img/include/opnsense_carp7.png" alt="Example image"  class="zoomable" /></p>
<h3 id="change-outbound-nat">Change outbound NAT</h3>
<p>Change the configuration of the outbound NAT to use the shared public IP (the WAN VIP) <!-- raw HTML omitted -->
Go to <code>Firewall &gt; NAT &gt; Outbound</code> and set the mode to Hybrid Outbound NAT rule generation. <!-- raw HTML omitted --> <!-- raw HTML omitted -->
Create a new Outbound rule like this: <!-- raw HTML omitted -->
<img src="/img/include/opnsense_carp8.png" alt="Example image"  class="zoomable" /></p>
<p>The translatino / target must be the WANIP IP. <!-- raw HTML omitted -->
It should end up looking like this: <!-- raw HTML omitted --></p>
<p><img src="/img/include/opnsense_carp8.png" alt="Example image"  class="zoomable" /></p>
<p>If you’ll be using your opnsense firewall as a DNS resolver you must change the settings of the DNS service (<code>Services &gt; DNS Resolver &gt; General Settings</code>) to lissen on the LAN VIP address. Then chnage the address of the DNS server in the DHCP configuration to us the LAN VIP adress.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Sonatype Nexus SSO]]></title>
            <link href="https://devopstales.github.io/home/nexus-sso/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/hashicorp-sso/?utm_source=atom_feed" rel="related" type="text/html" title="SSO for hashicorp vault" />
                <link href="https://devopstales.github.io/home/rundeck-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Rundeck SSO" />
                <link href="https://devopstales.github.io/home/mod-auth-openidc/?utm_source=atom_feed" rel="related" type="text/html" title="Apaceh2 oauth plugin" />
                <link href="https://devopstales.github.io/home/openshift-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift SSO authentication" />
                <link href="https://devopstales.github.io/home/openproject-sso/?utm_source=atom_feed" rel="related" type="text/html" title="OpenProject SSO" />
            
                <id>https://devopstales.github.io/home/nexus-sso/</id>
            
            
            <published>2019-05-23T00:00:00+00:00</published>
            <updated>2019-05-23T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Nexus Repository OSS is an artifact repository with universal support for popular formats.</p>
<h3 id="install-nexus">Install Nexus</h3>
<pre tabindex="0"><code>cd /opt
wget https://download.sonatype.com/nexus/3/latest-unix.tar.gz
tar xvf latest-unix.tar.gz -C /opt
ln -s /opt/nexus-3.16.1-02/ /opt/nexus

adduser -s /bin/false nexus
chown -R nexus:nexus /opt/nexus
chown -R nexus:nexus /opt/sonatype-work/

echo 'run_as_user=&quot;nexus&quot;' &gt; /opt/nexus/bin/nexus.rc

nano /opt/nexus/bin/nexus
INSTALL4J_JAVA_HOME_OVERRIDE=/usr/lib/jvm/jre-1.8.0
</code></pre><h3 id="create-sistemd-serice-for-nexus">Create sistemd serice for Nexus</h3>
<pre tabindex="0"><code>echo '[Unit]
Description=nexus service
After=network.target

[Service]
Type=forking
LimitNOFILE=65536
ExecStart=/opt/nexus/bin/nexus start
ExecStop=/opt/nexus/bin/nexus stop
User=nexus
Restart=on-abort

[Install]
WantedBy=multi-user.target' &gt; /etc/systemd/system/nexus.service
</code></pre><h3 id="start-nexus">Start Nexus</h3>
<pre tabindex="0"><code>sudo systemctl daemon-reload
sudo systemctl enable nexus.service
sudo systemctl start nexus.service

tailf /opt/sonatype-work/nexus3/log/nexus.log
### To check, point your browser to http://localhost:8081. Default username is admin with password admin123.
</code></pre><h3 id="install-keycloak-authentication-plugin">Install Keycloak authentication plugin</h3>
<pre tabindex="0"><code>NEXUS_PLUGINS=/opt/nexus/system
KEYCLOAK_PLUGIN_VERSION=0.3.3-SNAPSHOT
cd /opt
mkdir -p ${NEXUS_PLUGINS}/org/github/flytreeleft/nexus3-keycloak-plugin/${KEYCLOAK_PLUGIN_VERSION}/
cd ${NEXUS_PLUGINS}/org/github/flytreeleft/nexus3-keycloak-plugin/${KEYCLOAK_PLUGIN_VERSION}/
wget https://github.com/flytreeleft/nexus3-keycloak-plugin/releases/download/${KEYCLOAK_PLUGIN_VERSION}/nexus3-keycloak-plugin-${KEYCLOAK_PLUGIN_VERSION}.jar
chmod 644 ${NEXUS_PLUGINS}/org/github/flytreeleft/nexus3-keycloak-plugin/${KEYCLOAK_PLUGIN_VERSION}/nexus3-keycloak-plugin-${KEYCLOAK_PLUGIN_VERSION}.jar
echo &quot;mvn\\:org.github.flytreeleft/nexus3-keycloak-plugin/${KEYCLOAK_PLUGIN_VERSION} = 200&quot; &gt;&gt; /opt/nexus/etc/karaf/startup.properties
</code></pre><p>Login to your Keycloak, and navigate relm &gt; client
<img src="/img/include/nexus_sso1.png" alt="Example image"  class="zoomable" /><!-- raw HTML omitted --><!-- raw HTML omitted -->
Configurate Service Account Roles
<img src="/img/include/nexus_sso2.png" alt="Example image"  class="zoomable" /><!-- raw HTML omitted --><!-- raw HTML omitted -->
Configurate User Roles
<img src="/img/include/nexus_sso4.png" alt="Example image"  class="zoomable" /><!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<pre tabindex="0"><code>nano /opt/nexus/etc/keycloak.json
{
  &quot;realm&quot;: &quot;mydomain&quot;,
  &quot;auth-server-url&quot;: &quot;http://nexus.devopstales.intra:8080/auth&quot;,
  &quot;ssl-required&quot;: &quot;external&quot;,
  &quot;resource&quot;: &quot;web&quot;,
  &quot;credentials&quot;: {
    &quot;secret&quot;: &quot;41e39b6b-e23a-4fb1-be21-d30c02941ffc&quot;
  },
  &quot;confidential-port&quot;: 0
}

systemct restart nexus
</code></pre><p>After login to nexus you can navigate to the realm administration. Activate the Keycloak Authentication Realm plugin by dragging it to the right hand side.
<img src="/img/include/nexus_sso3.png" alt="Example image"  class="zoomable" /><!-- raw HTML omitted --><!-- raw HTML omitted -->
Mapp the Keycloak roles to nexus
<img src="/img/include/nexus_sso5.png" alt="Example image"  class="zoomable" /><!-- raw HTML omitted --><!-- raw HTML omitted -->
Go to server administration &gt; system &gt; capabilities &gt; add <!-- raw HTML omitted -->
type: Ruth auth <!-- raw HTML omitted -->
HTTP Header: X-Proxy-REMOTE-USER <!-- raw HTML omitted -->
<img src="/img/include/nexus_sso6.png" alt="Example image"  class="zoomable" /><!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<pre tabindex="0"><code>yum install mod_auth_openidc httpd mod_ssl -y
nano /etc/httpd/conf.d/nexus-site.conf
ProxyRequests Off
ProxyPreserveHost On

&lt;VirtualHost *:80&gt;
    ServerName nexus.devopstales.intra
     Redirect permanent / https://nexus.devopstales.intra
    ErrorLog /var/log/httpd/error.log
    CustomLog /var/log/httpd/access.log common
&lt;/VirtualHost&gt;

&lt;VirtualHost *:443&gt;
    ServerAdmin webmaster@example.com
    ServerName nexus.devopstales.intra
    ServerAlias www.nexus.devopstales.intra
    DirectoryIndex index.html index.php

    SSLEngine on
    SSLCertificateFile /etc/httpd/ssl/domain.pem
    SSLCertificateKeyFile /etc/httpd/ssl/domain.pem
    SSLCertificateChainFile /etc/httpd/ssl/domain.pem

    AllowEncodedSlashes NoDecode
    AllowEncodedSlashes On
    RequestHeader set X-Forwarded-Proto &quot;https&quot;

    # keycloak
    OIDCProviderMetadataURL https://nexus.devopstales.intra:8443/auth/realms/mydomain/.well-known/openid-configuration
    OIDCSSLValidateServer Off
    OIDCClientID web
    OIDCClientSecret 41e39b6b-e23a-4fb1-be21-d30c02941ffc
    OIDCRedirectURI https://nexus.devopstales.intra/redirect_uri
    OIDCCryptoPassphrase passphrase
    OIDCJWKSRefreshInterval 3600
    OIDCScope &quot;openid email profile&quot;
    # maps the prefered_username claim to the REMOTE_USER environment variable
    OIDCRemoteUserClaim preferred_username

    &lt;Location /&gt;
      AuthType openid-connect
      Require valid-user
      RequestHeader set &quot;X-Proxy-REMOTE-USER&quot; %{REMOTE_USER}s
      ProxyPass http://localhost:8081/ nocanon
      ProxyPassReverse http://localhost:8081/
    &lt;/Location&gt;

    ErrorLog /var/log/httpd/error.log
    CustomLog /var/log/httpd/access.log common
&lt;/VirtualHost&gt;

# secure neus server
nano /opt/nexus/etc/nexus-default.properties
application-host=127.0.0.1
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[SSO for hashicorp vault]]></title>
            <link href="https://devopstales.github.io/home/hashicorp-sso/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/rundeck-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Rundeck SSO" />
                <link href="https://devopstales.github.io/home/mod-auth-openidc/?utm_source=atom_feed" rel="related" type="text/html" title="Apaceh2 oauth plugin" />
                <link href="https://devopstales.github.io/home/openshift-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift SSO authentication" />
                <link href="https://devopstales.github.io/home/openproject-sso/?utm_source=atom_feed" rel="related" type="text/html" title="OpenProject SSO" />
                <link href="https://devopstales.github.io/home/gitlab-keycloak/?utm_source=atom_feed" rel="related" type="text/html" title="SSO login to Gitlab" />
            
                <id>https://devopstales.github.io/home/hashicorp-sso/</id>
            
            
            <published>2019-05-20T00:00:00+00:00</published>
            <updated>2019-05-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I wil shiw you hiw to configure Hashicorp vault with Keycloak for SSO.</p>
<pre tabindex="0"><code>vault auth enable oidc

vault write auth/oidc/config \
    oidc_discovery_url=&quot;https://sso.devopstales.intra/auth/realms/mydomain&quot; \
    oidc_client_id=&quot;web&quot; \
    oidc_client_secret=&quot;07d66ebd-1018-46c6-9c88-80aa3d4c2f68&quot; \
    default_role=&quot;reader&quot;
</code></pre><pre tabindex="0"><code>vault write auth/oidc/role/reader \
        bound_audiences=&quot;web&quot; \
        allowed_redirect_uris=&quot;http://192.168.0.112:8200/ui/vault/auth/oidc/oidc/callback&quot; \
        allowed_redirect_uris=&quot;http://192.168.0.112:8250/oidc/callback&quot; \
        user_claim=&quot;sub&quot; \
        policies=&quot;reader&quot;
</code></pre><pre tabindex="0"><code>nano reader.hcl
# Read permission on the k/v secrets
path &quot;/secret/*&quot; {
    capabilities = [&quot;read&quot;, &quot;list&quot;]
}

nano manager.hcl
# Manage k/v secrets
path &quot;/secret/*&quot; {
    capabilities = [&quot;create&quot;, &quot;read&quot;, &quot;update&quot;, &quot;delete&quot;, &quot;list&quot;]
}
</code></pre><pre tabindex="0"><code>vault policy write reader reader.hcl
vault policy write manager manager.hcl

vault policy list
</code></pre><p><img src="/img/include/vault-oidc.png" alt="Example image"  class="zoomable" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install hashicorp vault]]></title>
            <link href="https://devopstales.github.io/home/hashicorp-vault/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/hashicorp-vault/?utm_source=atom_feed" rel="related" type="text/html" title="Install hashicorp vault" />
                <link href="https://devopstales.github.io/home/rundeck-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Rundeck SSO" />
                <link href="https://devopstales.github.io/home/mod-auth-openidc/?utm_source=atom_feed" rel="related" type="text/html" title="Apaceh2 oauth plugin" />
                <link href="https://devopstales.github.io/home/nagios-ncpa/?utm_source=atom_feed" rel="related" type="text/html" title="Nagios Cross Platform Agent" />
                <link href="https://devopstales.github.io/home/nagios-nrpe/?utm_source=atom_feed" rel="related" type="text/html" title="Nagios Remote Plugin Executor" />
            
                <id>https://devopstales.github.io/home/hashicorp-vault/</id>
            
            
            <published>2019-05-17T00:00:00+00:00</published>
            <updated>2019-05-17T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Hashicorp vault is a highly scalable, highly available, environment agnostic way to generate, manage, and store secrets.</p>
<h3 id="dowload--vault">Dowload  Vault</h3>
<pre tabindex="0"><code># https://releases.hashicorp.com/vault/
cd /opt
VAULT_VERSION=&quot;1.1.2&quot;
curl -sO https://releases.hashicorp.com/vault/${VAULT_VERSION}/vault_${VAULT_VERSION}_linux_amd64.zip

unzip vault_${VAULT_VERSION}_linux_amd64.zip
mv vault /usr/bin/
vault --version
</code></pre><pre tabindex="0"><code>vault -autocomplete-install
complete -C /usr/bin/vault vault
mkdir /etc/vault
mkdir -p /var/lib/vault/data

sudo useradd --system --home /etc/vault --shell /bin/false vault
sudo chown -R vault:vault /etc/vault /var/lib/vault/
</code></pre><h3 id="configure-vault-systemd-service">Configure Vault systemd service</h3>
<pre tabindex="0"><code>nano /etc/systemd/system/vault.service
[Unit]
Description=vault server
Requires=network-online.target
After=network-online.target
ConditionFileNotEmpty=/etc/vault/config.hcl

[Service]
User=vault
Group=vault
Restart=on-failure
ExecStart=/usr/bin/vault server -config=/etc/vault
ExecStop=/usr/bin/vault step-down
#ExecReload=/bin/kill --signal HUP $MAINPID

[Install]
WantedBy=multi-user.target
</code></pre><h3 id="create-vault-config">Create vault config</h3>
<pre tabindex="0"><code>nano /etc/vault/config.hcl
disable_cache = true
disable_mlock = true
ui = true
listener &quot;tcp&quot; {
   address          = &quot;0.0.0.0:8200&quot;
   tls_disable      = 1
}
storage &quot;file&quot; {
   path  = &quot;/var/lib/vault/data&quot;
 }
api_addr         = &quot;http://0.0.0.0:8200&quot;
max_lease_ttl         = &quot;10h&quot;
default_lease_ttl    = &quot;10h&quot;
cluster_name         = &quot;vault&quot;
raw_storage_endpoint     = true
disable_sealwrap     = true
disable_printable_check = true
</code></pre><pre tabindex="0"><code>systemctl daemon-reload
systemctl enable --now vault
systemctl status vault
</code></pre><h3 id="configurate-client">Configurate Client</h3>
<pre tabindex="0"><code>export VAULT_ADDR=http://127.0.0.1:8200
echo &quot;export VAULT_ADDR=http://127.0.0.1:8200&quot; &gt;&gt; ~/.bashrc

sudo rm -rf  /var/lib/vault/data/*
vault operator init &gt; /etc/vault/init.file

cat /etc/vault/init.file | grep &quot;Initial Root Token:&quot;
export VAULT_TOKEN=&quot;s.RcW0LuNIyCoTLWxrDPtUDkCw&quot;

### go to gou and un seal with 3 keys

vault status
</code></pre><h3 id="configurate-user-base-authentication">Configurate user-base authentication</h3>
<pre tabindex="0"><code>vault auth enable userpass
vault write auth/userpass/users/devopstales \
    password=Password1 \
    policies=admins
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Rundeck SSO]]></title>
            <link href="https://devopstales.github.io/home/rundeck-sso/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/mod-auth-openidc/?utm_source=atom_feed" rel="related" type="text/html" title="Apaceh2 oauth plugin" />
                <link href="https://devopstales.github.io/home/openshift-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift SSO authentication" />
                <link href="https://devopstales.github.io/home/openproject-sso/?utm_source=atom_feed" rel="related" type="text/html" title="OpenProject SSO" />
                <link href="https://devopstales.github.io/home/gitlab-keycloak/?utm_source=atom_feed" rel="related" type="text/html" title="SSO login to Gitlab" />
                <link href="https://devopstales.github.io/home/keycloak1/?utm_source=atom_feed" rel="related" type="text/html" title="Install keycloak with mysql" />
            
                <id>https://devopstales.github.io/home/rundeck-sso/</id>
            
            
            <published>2019-05-14T00:00:00+00:00</published>
            <updated>2019-05-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will use Preauthenticated Mode for Rundeck with mod_auth_openidc and Keycloak</p>
<p>“Preauthenticated” means that the user name and role list are provided to Rundeck from another system, usually a reverse proxy set up “in front” of the Rundeck web application, such as Apache HTTPD.</p>
<h3 id="configurate-mapping-in-keycloak">Configurate mapping in Keycloak</h3>
<p>Login to Keycloak and create client for the app:
<img src="/img/include/gitlab-keycloak1.png" alt="Example image"  class="zoomable" /></p>
<p>At Mappers create mappers for user information:</p>
<ul>
<li>name: Audience
<ul>
<li>mapper type: Audience</li>
<li>Included Client Audience: web</li>
<li>Add to ID token: on</li>
</ul>
</li>
<li>name: groups
<ul>
<li>Mapper Type: Group Membership</li>
<li>Token Claim Name: groups</li>
<li>Full group path: off</li>
</ul>
</li>
</ul>
<h3 id="install-httpd">Install httpd</h3>
<pre tabindex="0"><code>yum install mod_auth_openidc httpd php mod_ssl -y
</code></pre><h3 id="configurate-rundeck">Configurate Rundeck</h3>
<pre tabindex="0"><code>nano /etc/rundeck/rundeck-config.properties
grails.serverURL=https://oauth.devopstales.intra
# Pre Auth mode settings
rundeck.security.authorization.preauthenticated.enabled=true
rundeck.security.authorization.preauthenticated.attributeName=REMOTE_USER_GROUPS
rundeck.security.authorization.preauthenticated.delimiter=;
# Header from which to obtain user name
rundeck.security.authorization.preauthenticated.userNameHeader=X-Forwarded-User
# Header from which to obtain list of roles
rundeck.security.authorization.preauthenticated.userRolesHeader=X-Forwarded-Roles
</code></pre><h3 id="create-vhost">Create vhost</h3>
<pre tabindex="0"><code>nano /etc/httpd/conf.d/outh-site.conf
# NameVirtualHost *:80
&lt;VirtualHost *:80&gt;
   ServerName oauth.devopstales.intra
   DocumentRoot /var/www/oauth/
   Redirect permanent / https://oauth.devopstales.intra
&lt;/VirtualHost&gt;

&lt;VirtualHost *:443&gt;
    ServerAdmin webmaster@example.com
    ServerName oauth.devopstales.intra
    ServerAlias www.oauth.devopstales.intra
    DocumentRoot /var/www/oauth/
    DirectoryIndex index.html index.php
    ErrorLog /var/log/httpd/oauth-error.log
    CustomLog /var/log/httpd/oauth-access.log combined

    SSLEngine on
    SSLCertificateFile /etc/httpd/ssl/domain.pem
    SSLCertificateKeyFile /etc/httpd/ssl/domain.pem
    SSLCertificateChainFile /etc/httpd/ssl/domain.pem

    # keycloak
    OIDCProviderMetadataURL https://sso.devopstales.intra/auth/realms/mydomain/.well-known/openid-configuration
    OIDCSSLValidateServer Off
    OIDCClientID web
    OIDCClientSecret 6b6c68c3-ad51-4124-ac37-4784ed58797e
    OIDCRedirectURI https://oauth.devopstales.intra/redirect_uri
    OIDCCryptoPassphrase passphrase
    OIDCJWKSRefreshInterval 3600
    OIDCScope &quot;openid email profile&quot;
    # maps the prefered_username claim to the REMOTE_USER environment variable
    OIDCRemoteUserClaim preferred_username

    ProxyPreserveHost On

    &lt;Location /&gt;
        AuthType openid-connect
            Require valid-user
        RequestHeader set &quot;X-Forwarded-User&quot; %{REMOTE_USER}s
        RequestHeader set &quot;X-Forwarded-Roles&quot; %{OIDC_CLAIM_groups}e
        ProxyPass  http://localhost:4440/
        ProxyPassReverse http://localhost:4440/
    &lt;/Location&gt;

&lt;/VirtualHost&gt;
</code></pre><h3 id="start-apache">Start apache</h3>
<pre tabindex="0"><code>systemct start httpd
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Apaceh2 oauth plugin]]></title>
            <link href="https://devopstales.github.io/home/mod-auth-openidc/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/sso/mod-auth-openidc/?utm_source=atom_feed" rel="related" type="text/html" title="Apaceh2 oauth plugin" />
                <link href="https://devopstales.github.io/home/openshift-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift SSO authentication" />
                <link href="https://devopstales.github.io/home/openproject-sso/?utm_source=atom_feed" rel="related" type="text/html" title="OpenProject SSO" />
                <link href="https://devopstales.github.io/home/gitlab-keycloak/?utm_source=atom_feed" rel="related" type="text/html" title="SSO login to Gitlab" />
                <link href="https://devopstales.github.io/home/keycloak1/?utm_source=atom_feed" rel="related" type="text/html" title="Install keycloak with mysql" />
            
                <id>https://devopstales.github.io/home/mod-auth-openidc/</id>
            
            
            <published>2019-05-13T00:00:00+00:00</published>
            <updated>2019-05-13T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Configure Apache plugin to use Keycloak as a user backend for login with OpenID and SSO.</p>
<p>mod_auth_openidc is an OpenID Connect Relying Party implementation for Apache HTTP Server 2.x</p>
<h3 id="install-the-plugin">Install the plugin</h3>
<pre tabindex="0"><code>yum install mod_auth_openidc httpd php mod_ssl -y

mkdir -p /var/www/html/oauth/protected
echo &quot;index&quot; &gt; /var/www/html/oauth/index.htm
</code></pre><pre tabindex="0"><code>nano /var/www/html/oauth/protected/index.php
&lt;!DOCTYPE html&gt;
&lt;html lang=&quot;en&quot;&gt;

&lt;head&gt;

   &lt;meta charset=&quot;utf-8&quot;&gt;
   &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot;&gt;
   &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot;&gt;
   &lt;meta name=&quot;description&quot; content=&quot;&quot;&gt;
   &lt;meta name=&quot;author&quot; content=&quot;&quot;&gt;

   &lt;title&gt;OpenID Connect: Received Claims&lt;/title&gt;

&lt;/head&gt;

&lt;body&gt;
         &lt;h3&gt;
            Claims sent back from OpenID Connect via the Apache module
         &lt;/h3&gt;
         &lt;br/&gt;


   &lt;!-- OpenAthens attribtues --&gt;
      &lt;?php session_start(); ?&gt;

         &lt;h2&gt;Claims&lt;/h2&gt;
         &lt;br/&gt;
         &lt;div class=&quot;row&quot;&gt;

               &lt;table class=&quot;table&quot; style=&quot;width:80%;&quot; border=&quot;1&quot;&gt;
                 &lt;?php foreach ($_SERVER as $key=&gt;$value): ?&gt;
                    &lt;?php if ( preg_match(&quot;/OIDC_/i&quot;, $key) ): ?&gt;
                       &lt;tr&gt;
                          &lt;td data-toggle=&quot;tooltip&quot; title=&lt;?php echo $key; ?&gt;&gt;&lt;?php echo $key; ?&gt;&lt;/td&gt;
                          &lt;td data-toggle=&quot;tooltip&quot; title=&lt;?php echo $value; ?&gt;&gt;&lt;?php echo $value; ?&gt;&lt;/td&gt;
                       &lt;/tr&gt;
                    &lt;?php endif; ?&gt;
                 &lt;?php endforeach; ?&gt;
               &lt;/table&gt;

&lt;/body&gt;&lt;/html&gt;
</code></pre><h3 id="create-vhost">Create vhost</h3>
<pre tabindex="0"><code>nano /etc/httpd/conf.d/aouth-site.conf
# NameVirtualHost *:80
&lt;VirtualHost *:80&gt;
   ServerName oauth.devopstales.intra
   DocumentRoot /var/www/oauth/
   Redirect permanent / https://oauth.devopstales.intra
&lt;/VirtualHost&gt;

&lt;VirtualHost *:443&gt;
    ServerAdmin webmaster@example.com
    ServerName oauth.devopstales.intra
    ServerAlias www.oauth.devopstales.intra
    DocumentRoot /var/www/html/oauth/
    DirectoryIndex index.html index.php
    ErrorLog /var/log/httpd/oauth-error.log
    CustomLog /var/log/httpd/oauth-access.log combined

    SSLEngine on
    SSLCertificateFile /etc/httpd/ssl/domain.pem
    SSLCertificateKeyFile /etc/httpd/ssl/domain.pem
    SSLCertificateChainFile /etc/httpd/ssl/domain.pem

    # keycloak server
    OIDCProviderMetadataURL http://sso.devopstales.intra/auth/realms/mydomain/.well-known/openid-configuration
    # for self signed certificate
    OIDCSSLValidateServer Off
    OIDCClientID web
    OIDCClientSecret 5b721a2b-681f-402d-807c-b98c80672c16
    OIDCRedirectURI http://oauth.devopstales.intra/protected/redirect_uri
    OIDCCryptoPassphrase passphrase
    OIDCJWKSRefreshInterval 3600

    &lt;Location /protected/&gt;
       AuthType openid-connect
       Require valid-user
    &lt;/Location&gt;

&lt;/VirtualHost&gt;
</code></pre><h3 id="start-apache">Start apache</h3>
<pre tabindex="0"><code>systemct start httpd
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Nagios Cross Platform Agent]]></title>
            <link href="https://devopstales.github.io/home/nagios-ncpa/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/nagios-nrpe/?utm_source=atom_feed" rel="related" type="text/html" title="Nagios Remote Plugin Executor" />
                <link href="https://devopstales.github.io/home/centreon-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Centreon on Centos 7" />
                <link href="https://devopstales.github.io/home/naemon-influxdb/?utm_source=atom_feed" rel="related" type="text/html" title="Install Nemon with Influxdb storage" />
                <link href="https://devopstales.github.io/monitoring/nagios-ncpa/?utm_source=atom_feed" rel="related" type="text/html" title="Nagios Cross Platform Agent" />
                <link href="https://devopstales.github.io/home/openshift-hostalreadyclaimed/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Error: HostAlreadyClaimed" />
            
                <id>https://devopstales.github.io/home/nagios-ncpa/</id>
            
            
            <published>2019-05-07T00:00:00+00:00</published>
            <updated>2019-05-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this article I will show you how to add Remote Linux machine and it’s services to Nagios Monitoring host using NCPA agent.</p>
<h3 id="what-is-ncpa">What is NCPA</h3>
<p>NCPA is written in Python and is able to run on almost any Operating System. IT build official binaries for Windows, Mac OS X, and various Linux flavors.</p>
<h3 id="ncpa-client">NCPA Client</h3>
<pre tabindex="0"><code>rpm -Uvh https://repo.nagios.com/nagios/7/nagios-repo-7-3.el7.noarch.rpm
yum install ncpa -y

nano /usr/local/ncpa/etc/ncpa.cfg
# [listener]
# allowed_hosts = &lt;nagios host&gt;
[api]
community_string = Password1
[nrdp]
# hostname =
# [nrdp]
# parent =
# token =
[plugin directives]
plugin_path = /usr/lib64/nagios/plugins/

systemctl enable ncpa_listener
systemctl start ncpa_listener
systemctl status ncpa_listener

# https://192.168.0.100:5693/
</code></pre><h3 id="ncpa-server">NCPA Server</h3>
<pre tabindex="0"><code>cd /tmp
wget https://assets.nagios.com/downloads/ncpa/check_ncpa.tar.gz
tar xvf check_ncpa.tar.gz
chown nagios:nagios check_ncpa.py
chmod 775 check_ncpa.py
mv check_ncpa.py /usr/lib64/nagios/plugins
</code></pre><p>Test the commands</p>
<pre tabindex="0"><code>/usr/lib64/naemon/plugins/check_ncpa.py -H localhost -t 'Password1' -P 5693 -M system/agent_version
/usr/lib64/naemon/plugins/check_ncpa.py -H localhost -t 'Password1' -P 5693 -M cpu/percent -w 20 -c 40 -q 'aggregate=avg'
/usr/lib64/naemon/plugins/check_ncpa.py -H localhost -t 'Password1' -P 5693 -M memory/virtual -w 50 -c 80 -u G
/usr/lib64/naemon/plugins/check_ncpa.py -H localhost -t 'Password1' -P 5693 -M processes -w 150 -c 200

# Run custom plugin trouth NCPA
/usr/lib64/naemon/plugins/check_ncpa.py -H localhost -t 'Password1' -P 5693 -M plugins/check_users -q args=&quot;-w 5 -c 10&quot;
</code></pre><pre tabindex="0"><code># store password in macro
nano /etc/nagios/resource.cfg
USER10 = Password1

# createcustom commands for ncpa
nano /etc/nagios/commands.cfg
define command {
    command_name    check_ncpa
    command_line    $USER1$/check_ncpa.py -H $HOSTADDRESS$ -t $USER10$ -P 5693 $ARG1$
}

define command {
    command_name    check_ncpa_cpu
    command_line    $USER1$/check_ncpa.py -H $HOSTADDRESS$ -t $USER10$ -P 5693 cpu/percent -w 20 -c 40 -q 'aggregate=avg'
}
</code></pre><pre tabindex="0"><code>nano /etc/nagios/conf.d/ncpa-test.cfg
        define host{
        use                    generic-host
        host_name              devopstales
        address                192.168.0.20
}

define service{
        use                     generic-service
        host_name               devopstales
        service_description     NCPA Version
        check_command           check_ncpa|system/agent_version
        }

define service{
        use                     generic-service
        host_name               devopstales
        service_description     CPU Load
        check_command           check_ncpa_cpu
        }
</code></pre><h3 id="restart-nagios">Restart nagios</h3>
<pre tabindex="0"><code>nagios -v /etc/nagios/nagios.cfg

Total Warnings: 0
Total Errors:   0

service nagios restart
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Nagios Remote Plugin Executor]]></title>
            <link href="https://devopstales.github.io/home/nagios-nrpe/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/centreon-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Centreon on Centos 7" />
                <link href="https://devopstales.github.io/home/naemon-influxdb/?utm_source=atom_feed" rel="related" type="text/html" title="Install Nemon with Influxdb storage" />
                <link href="https://devopstales.github.io/monitoring/nagios-nrpe/?utm_source=atom_feed" rel="related" type="text/html" title="Nagios Remote Plugin Executor" />
                <link href="https://devopstales.github.io/home/openshift-hostalreadyclaimed/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Error: HostAlreadyClaimed" />
                <link href="https://devopstales.github.io/home/k8s-copy-secret/?utm_source=atom_feed" rel="related" type="text/html" title="Copying Kubernetes Secrets Between Namespaces" />
            
                <id>https://devopstales.github.io/home/nagios-nrpe/</id>
            
            
            <published>2019-05-06T00:00:00+00:00</published>
            <updated>2019-05-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this article I will show you how to add Remote Linux machine and it’s services to Nagios Monitoring host using NRPE agent.</p>
<h3 id="what-is-nrpe">What is NRPE</h3>
<p>NRPE allows you to remotely execute Nagios plugins on other Linux/Unix machines.</p>
<p><img src="/img/include/nrpe.png" alt="Example image"  class="zoomable" /></p>
<h3 id="nrpe-client">NRPE Client</h3>
<pre tabindex="0"><code>yum install nrpe nagios-plugins-all
# OR
apt-get install nagios-nrpe-server nagios-plugins
</code></pre><h3 id="nrpe-client-config">NRPE Client Config</h3>
<pre tabindex="0"><code>nano /etc/nagios/nrpe.cfg
...
only_from = 127.0.0.1 localhost &lt;nagios_ip_address&gt;
...
command[check_users]=/usr/lib64/nagios/plugins/check_users -w 5 -c 10
command[check_load]=/usr/lib64/nagios/plugins/check_load -r -w 8.0,7.5,7.0 -c 11.0,10.0,9.0
command[check_disk]=/usr/lib64/nagios/plugins/check_disk -w 15% -c 10% /
command[check_mem]=/usr/lib64/nagios/plugins/check_mem -w 75% -c 90%
command[check_total_procs]=/usr/lib64/nagios/plugins/check_procs -w 300 -c 400
command[check_swap]=/usr/lib64/nagios/plugins/check_swap -w 10 -c 5
</code></pre><h3 id="nrpe-client-logging">NRPE Client Logging</h3>
<pre tabindex="0"><code>nano /etc/nagios/nrpe.cfg
log_facility=local1
debug=1

nano  /etc/rsyslog.conf
local1.*                                                /var/log/nrpe.log
</code></pre><h3 id="start-nrpe-client">Start NRPE Client</h3>
<pre tabindex="0"><code>systemctl start nrpe
systemctl enable nrp

ss -altn | grep 5666
LISTEN   0         5                   0.0.0.0:5666             0.0.0.0:*       
LISTEN   0         5                      [::]:5666                [::]:*

/usr/lib64/nagios/plugins/check_nrpe -H 127.0.0.1 -c check_total_procs
PROCS OK: 105 processes | procs=105;300;400;0;
</code></pre><h3 id="nrpe-server">NRPE Server</h3>
<pre tabindex="0"><code>yum install nagios-plugins-nrpe
</code></pre><pre tabindex="0"><code># createcustom commands for nrpe
nano /etc/nagios/commands.cfg
define command {
    command_name check_nrpe
    command_line $USER1$/check_nrpe -H $HOSTADDRESS$ -c $ARG1$
}
</code></pre><pre tabindex="0"><code>nano /etc/nagios/conf.d/nrpe-test.cfg
        define host{
        use                    generic-host
        host_name              devopstales
        address                192.168.0.20
}


define service{
        use                     generic-service
        host_name               tecmint
        service_description     CPU Load
        check_command           check_nrpe!check_load
        }
</code></pre><h3 id="restart-nagios">Restart nagios</h3>
<pre tabindex="0"><code>nagios -v /etc/nagios/nagios.cfg

Total Warnings: 0
Total Errors:   0

service nagios restart
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift Error: HostAlreadyClaimed]]></title>
            <link href="https://devopstales.github.io/home/openshift-hostalreadyclaimed/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-gitlabrunner/?utm_source=atom_feed" rel="related" type="text/html" title="Install Gitlab runner on Openshift" />
                <link href="https://devopstales.github.io/home/openshift-extregistry/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: External registry" />
                <link href="https://devopstales.github.io/home/openshift-cert/?utm_source=atom_feed" rel="related" type="text/html" title="Change Certificates in Openshift" />
                <link href="https://devopstales.github.io/home/openshift-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure Openshift vSphere Cloud Provider" />
                <link href="https://devopstales.github.io/home/openshift-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Helm" />
            
                <id>https://devopstales.github.io/home/openshift-hostalreadyclaimed/</id>
            
            
            <published>2019-05-05T00:00:00+00:00</published>
            <updated>2019-05-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>How to solvee Openshift Error: HostAlreadyClaimed</p>
<p>I created a new route for a service and the rout not created. Get this error:</p>
<pre tabindex="0"><code>Name:			keycloak-gatekeeper
Namespace:		phpmyadmin
Created:		17 minutes ago
Labels:			app=keycloak-gatekeeper
Annotations:		&lt;none&gt;
Requested Host:		phpmyadmin.devopstales.intra
			  rejected by router router: HostAlreadyClaimed (36 seconds ago)
			    route phpmyadmin/phpmyadmin-phpmyadmin has host phpmyadmin.devopstales.intra
Path:			&lt;none&gt;
TLS Termination:	edge
Insecure Policy:	Redirect
Endpoint Port:		http

Service:	keycloak-gatekeeper
Weight:		100 (100%)
Endpoints:	10.130.2.149:3000
</code></pre><p>So I listed all the routes but I dod not found this route. In the end the force delete solved my problem.</p>
<pre tabindex="0"><code>oc delete route --grace-period=0 --force=true --ignore-not-found=true -n phpmyadmin phpmyadmin-phpmyadmin
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Katello client]]></title>
            <link href="https://devopstales.github.io/home/katello-client/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/katello-client/</id>
            
            
            <published>2019-05-04T00:00:00+00:00</published>
            <updated>2019-05-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Katello brings the full power of content management alongside the provisioning and configuration capabilities of Foreman. Katello is the upstream community project from which the Red Hat Satellite product is derived after Red Hat Satellite Server 6.</p>
<h3 id="install-subscription-manager">Install subscription manager</h3>
<pre tabindex="0"><code>yum install subscription-manager
rpm -ivh http://192.168.0.109/pub/katello-ca-consumer-latest.noarch.rpm
subscription-manager register --org=&quot;mydomain&quot; --activationkey=&quot;el7-key&quot;

subscription-manager repos --list

cd /etc/yum.repos.d/
mv CentOS-* epel* katello-client.repo /tmp/
yum clean all
yum repolist
</code></pre><h3 id="install-katello-client">Install Katello client</h3>
<pre tabindex="0"><code>yum install katello-agent -y
systemctl start goferd
systemctl enable goferd
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Copying Kubernetes Secrets Between Namespaces]]></title>
            <link href="https://devopstales.github.io/home/k8s-copy-secret/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Helm" />
                <link href="https://devopstales.github.io/home/k8s-helm-rbac/?utm_source=atom_feed" rel="related" type="text/html" title="RBAC permissions for Helm" />
                <link href="https://devopstales.github.io/home/kubectl-multi-cluster-config/?utm_source=atom_feed" rel="related" type="text/html" title="Configure kubectl for multiple clusters" />
                <link href="https://devopstales.github.io/kubernetes/k8s-copy-secret/?utm_source=atom_feed" rel="related" type="text/html" title="Copying Kubernetes Secrets Between Namespaces" />
                <link href="https://devopstales.github.io/home/awx-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install AWX" />
            
                <id>https://devopstales.github.io/home/k8s-copy-secret/</id>
            
            
            <published>2019-05-03T00:00:00+00:00</published>
            <updated>2019-05-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>A simple way of copying common secret data between namespaces</p>
<pre tabindex="0"><code>kubectl get secret private-registry --namespace=dev1 --export -o yaml |\
   kubectl apply --namespace=dev2 -f -
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Errata]]></title>
            <link href="https://devopstales.github.io/home/katello-errata/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/katello-errata/</id>
            
            
            <published>2019-05-02T00:00:00+00:00</published>
            <updated>2019-05-02T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Katello brings the full power of content management alongside the provisioning and configuration capabilities of Foreman. Katello is the upstream community project from which the Red Hat Satellite product is derived after Red Hat Satellite Server 6.</p>
<h3 id="errata">Errata</h3>
<pre tabindex="0"><code>yum install -y git \
pulp-admin-client \
pulp-rpm-admin-extensions \
pulp-rpm-consumer-extensions \
pulp-rpm-handlers \
pulp-rpm-yumplugins \
pulp-rpm-admin-extensions \
pulp-consumer-client \
python-pulp-agent-lib \
perl-Text-Unidecode \
perl-XML-Simple \
perl-XML-Parser

cd /opt
git clone https://github.com/rdrgmnzs/pulp_centos_errata_import.git
cd ./pulp_centos_errata_import
wget -N https://cefs.steve-meier.de/errata.latest.xml.bz2
bunzip2 ./errata.latest.xml.bz2
mkdir -m0700 ~/.pulp
cat /etc/pki/katello/certs/pulp-client.crt /etc/pki/katello/private/pulp-client.key &gt; ~/.pulp/user-cert.pem
chmod 0400 ~/.pulp/user-cert.pem

# import Errata
perl ./errata_import.pl --errata=errata.latest.xml

pulp-admin repo list | less

add rarrata to rebo by repoid

perl ./errata_import.pl \
--errata=errata.latest.xml \
--include-repo=6356cfe9-766e-4f07-a85e-f287b25395e3 \
--include-repo=2-el7_content-v1_0-6356cfe9-766e-4f07-a85e-f287b25395e3 \
--include-repo=2-el7_content-Library-6356cfe9-766e-4f07-a85e-f287b25395e3 \
--include-repo=2-el7_content-stable-6356cfe9-766e-4f07-a85e-f287b25395e3 \
--include-repo=cb299646-b1a3-4225-9c78-986851a1725f \
--include-repo=2-el7_content-v1_0-cb299646-b1a3-4225-9c78-986851a1725f \
--include-repo=2-el7_content-Library-cb299646-b1a3-4225-9c78-986851a1725f \
--include-repo=2-el7_content-stable-cb299646-b1a3-4225-9c78-986851a1725f
</code></pre><h3 id="update-repo">Update repo</h3>
<pre tabindex="0"><code>hammer repository synchronize \
--skip-metadata-check true \
--name &quot;base_x86_64&quot; \
--product &quot;el7_repos&quot;

hammer repository synchronize \
--skip-metadata-check true \
--name &quot;updates_x86_64&quot; \
--product &quot;el7_repos&quot;

hammer content-view publish \
--name &quot;el7_content&quot; \
--description &quot;Publishing repositories&quot;

hammer content-view version promote \
--content-view &quot;el7_content&quot; \
--version &quot;4.0&quot; \
--to-lifecycle-environment &quot;stable&quot;
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install AWX]]></title>
            <link href="https://devopstales.github.io/home/awx-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/awx-docker/?utm_source=atom_feed" rel="related" type="text/html" title="Install AWX in docker" />
                <link href="https://devopstales.github.io/linux/awx-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install AWX" />
                <link href="https://devopstales.github.io/linux/awx-docker/?utm_source=atom_feed" rel="related" type="text/html" title="Install AWX in docker" />
                <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
                <link href="https://devopstales.github.io/home/centreon-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Centreon on Centos 7" />
            
                <id>https://devopstales.github.io/home/awx-install/</id>
            
            
            <published>2019-04-30T00:00:00+00:00</published>
            <updated>2019-04-30T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>AWX is an open source web application that provides a user interface, REST API, and task engine for Ansible.</p>
<h3 id="install-postgresql">Install Postgresql</h3>
<pre tabindex="0"><code>yum install epel-release -y
yum update -y
rpm -Uvh https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-centos10-10-2.noarch.rpm

yum -y install postgresql10-server postgresql10-contrib postgresql10
/usr/pgsql-10/bin/postgresql-10-setup initdb

sudo systemctl start postgresql-10
sudo systemctl enable postgresql-10

sudo -u postgres createuser -S awx
sudo -u postgres createdb -O awx awx
</code></pre><h3 id="install-requirements-and-awx">Install requirements and AWX</h3>
<pre tabindex="0"><code># if selinux enabled
yum -y install policycoreutils-python
setsebool -P httpd_can_network_connect 1

# if firewall enabled
firewall-cmd --permanent --add-service=http
firewall-cmd --reload


yum -y install memcached ansible epel-release nginx
systemctl enable memcached
systemctl start memcached

yum install https://github.com/rabbitmq/erlang-rpm/releases/download/v20.1.7.1/erlang-20.1.7.1-1.el7.centos.x86_64.rpm
yum -y install https://dl.bintray.com/rabbitmq/all/rabbitmq-server/3.7.5/rabbitmq-server-3.7.5-1.el7.noarch.rpm
systemctl enable rabbitmq-server
systemctl start rabbitmq-server

cp -p /etc/nginx/nginx.conf{,.org}
wget -O /etc/nginx/nginx.conf https://raw.githubusercontent.com/sunilsankar/awx-build/master/nginx.conf
systemctl start nginx
systemctl enable nginx

yum -y install centos-release-scl centos-release-scl-rh
yum -y install rh-python36 rh-python36-Django rh-python36-django-split-settings \
rh-python36-django-qsstats-magic rh-python36-ansiconv rh-python36-prometheus_client \
rh-python36-python-memcached rh-python36-asn1crypto rh-python36-asgiref \
rh-python36-hyperlink rh-python36-Automat rh-python36-asgi_amqp rh-python36-uwsgi \
rh-python36-msgpack-python rh-python36-msgpack-python-debuginfo rh-python36-jsonpickle \
rh-python36-django-radius rh-python36-python-django-radius rh-python36-python-radius \
rh-python36-future rh-python36-pyrad rh-python36-netaddr rh-python36-tacacs_plus \
rh-python36-python3-saml rh-python36-xmlsec rh-python36-lxml rh-python36-defusedxml \
rh-python36-boto

wget -O /etc/yum.repos.d/ansible-awx.repo https://copr.fedorainfracloud.org/coprs/mrmeee/ansible-awx/repo/epel-7/mrmeee-ansible-awx-epel-7.repo
yum install -y ansible-awx


sudo -u awx scl enable rh-python36 rh-postgresql10 &quot;awx-manage migrate&quot;
echo &quot;from django.contrib.auth.models import User; User.objects.create_superuser('admin', 'root@localhost', 'Password1')&quot; \
| sudo -u awx scl enable rh-python36 rh-postgresql10 &quot;awx-manage shell&quot;

sudo -u awx scl enable rh-python36 rh-postgresql10 &quot;awx-manage create_preload_data&quot;
sudo -u awx scl enable rh-python36 rh-postgresql10 &quot;awx-manage provision_instance --hostname=$(hostname)&quot;
sudo -u awx scl enable rh-python36 rh-postgresql10 &quot;awx-manage register_queue --queuename=tower --hostnames=$(hostname)&quot;
</code></pre><h3 id="configure-database">Configure database</h3>
<pre tabindex="0"><code>systemctl start awx-cbreceiver
systemctl start awx-dispatcher
systemctl start awx-channels-worker
systemctl start awx-daphne
systemctl start awx-web

systemctl status awx-cbreceiver
systemctl status awx-dispatcher
systemctl status awx-channels-worker
systemctl status awx-daphne
systemctl status awx-web

systemctl enable awx-cbreceiver
systemctl enable awx-dispatcher
systemctl enable awx-channels-worker
systemctl enable awx-daphne
systemctl enable awx-web
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install AWX in docker]]></title>
            <link href="https://devopstales.github.io/home/awx-docker/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/awx-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install AWX" />
                <link href="https://devopstales.github.io/linux/awx-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install AWX" />
                <link href="https://devopstales.github.io/linux/awx-docker/?utm_source=atom_feed" rel="related" type="text/html" title="Install AWX in docker" />
                <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
                <link href="https://devopstales.github.io/home/centreon-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Centreon on Centos 7" />
            
                <id>https://devopstales.github.io/home/awx-docker/</id>
            
            
            <published>2019-04-30T00:00:00+00:00</published>
            <updated>2019-04-30T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>AWX is an open source web application that provides a user interface, REST API, and task engine for Ansible.</p>
<h3 id="install-docker-and-docker-compose">Install Docker and docker-compose</h3>
<pre tabindex="0"><code>sudo yum install -y yum-utils \
device-mapper-persistent-data \
lvm2

sudo yum-config-manager \
--add-repo \
https://download.docker.com/linux/centos/docker-ce.repo

sudo yum -y install docker-ce docker-ce-cli containerd.io

service docker start
systemctl enable docker

yum install epel-release
yum install python-pip -y
pip install docker-compose
</code></pre><pre tabindex="0"><code>yum install git ansible -y

cd /opt
git clone https://github.com/ansible/awx.git
cd awx/installer/

nano inventory
postgres_data_dir=/opt/pgdocker
docker_compose_dir=/opt/awxcompose
pg_username=awx
pg_password=Password1
rabbitmq_password=Password1
admin_user=admin
admin_password=Password1
project_data_dir=/var/lib/awx/projects

ansible-playbook -i inventory install.yml

docker logs awx_task -f
</code></pre><p>dockerhub_base=ansible
dockerhub_version=latest</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configurate ipmitool]]></title>
            <link href="https://devopstales.github.io/home/ipmitool-config/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/ipmitool-config/</id>
            
            
            <published>2019-04-29T00:00:00+00:00</published>
            <updated>2019-04-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>The Intelligent Platform Management Interface (IPMI) is a set of computer interface specifications for an autonomous computer subsystem that provides management and monitoring capabilities independently of the host system&rsquo;s CPU, firmware (BIOS or UEFI) and operating system.</p>
<h3 id="ipmitool-on-pfsense">Ipmitool on pfsense</h3>
<pre tabindex="0"><code>[2.3.3-RELEASE][root@fw.makz.me]/root: ipmitool
Could not open device at /dev/ipmi0 or /dev/ipmi/0 or /dev/ipmidev/0: No such file or directory

# solution
kldload ipmi
nano /boot/loader.conf
#Load ipmi.ko into the kernel
ipmi_load=&quot;YES&quot;
</code></pre><h3 id="ipmitool-on-linux">Ipmitool on Linux</h3>
<pre tabindex="0"><code>modprobe ipmi_msghandler
modprobe ipmi_devintf
modprobe ipmi_si

nano /etc/modules
# OR
nano /etc/modprobe.d/*.conf
ipmi_msghandler
ipmi_devintf
ipmi_si
</code></pre><h3 id="ipmitool-configuration">Ipmitool Configuration</h3>
<pre tabindex="0"><code>ipmitool lan set 1 ipsrc static
ipmitool lan set 1 ipaddr 192.168.0.211
ipmitool lan set 1 netmask 255.255.255.0
ipmitool lan set 1 defgw ipaddr 192.168.0.1
ipmitool lan set 1 arp respond on
ipmitool lan set 1 access on

ipmitool lan print 1
</code></pre><h3 id="monitor-ipmi-with-telegraf">Monitor ipmi with telegraf</h3>
<pre tabindex="0"><code>nano /etc/telegraf/telegraf.conf
[[inputs.ipmi_sensor]]
        path = &quot;/usr/bin/ipmitool&quot;
        servers = [&quot;username:password@lan(192.168.0.211)&quot;]
        interval = &quot;30s&quot;
        timeout = &quot;20s&quot;
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configure spacewalk 2.9]]></title>
            <link href="https://devopstales.github.io/home/spacewalk-software-channels/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/spacewalk-software-channels/</id>
            
            
            <published>2019-04-29T00:00:00+00:00</published>
            <updated>2019-04-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how to create software channels on spacewalk server.</p>
<p>Login to the spacewalk web interface</p>
<h3 id="get-gpg-key">Get GPG key</h3>
<p>First, we need have an extracted GPG key information, To get the key information download it and extract it using GPG command.</p>
<pre tabindex="0"><code>cd /etc/pki/rpm-gpg
wget http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-7
gpg --with-fingerprint RPM-GPG-KEY-CentOS-7
wget https://copr-be.cloud.fedoraproject.org/results/@spacewalkproject/spacewalk-2.9/pubkey.gpg -o RPM-GPG-KEY-spacewalk-2.9
gpg --with-fingerprint RPM-GPG-KEY-spacewalk-2.9

cp /etc/pki/rpm-gpg/RPM-GPG-KEY-* /var/www/html/pub/
</code></pre><h3 id="create-software-chanel">Create software chanel</h3>
<p>Channels (top) &gt; Manage Software Channels(Left side pane) &gt; Create Channel(Right side top corner).<!-- raw HTML omitted --></p>
<p><img src="/img/include/spacewalk_1.png" alt="Example image"  class="zoomable" /></p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Channel Name</td>
<td>centos 7 base - x86_64</td>
</tr>
<tr>
<td>Channel Label</td>
<td>centos7-base-x86_64</td>
</tr>
<tr>
<td>Parent Channel</td>
<td>none</td>
</tr>
<tr>
<td>Architecture</td>
<td>x86_64</td>
</tr>
<tr>
<td>Channel Summary</td>
<td>centos7-base-x86_64</td>
</tr>
<tr>
<td>GPG Key URL</td>
<td>file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7</td>
</tr>
<tr>
<td>GPG Key ID</td>
<td>F4A80EB5</td>
</tr>
<tr>
<td>GPG Fingerprint</td>
<td>6341 AB27 53D7 8A78 A7C2 7BB1 24C6 A8A7 F4A8 0EB5</td>
</tr>
</tbody>
</table>
<p><img src="/img/include/spacewalk_2.png" alt="Example image"  class="zoomable" /></p>
<h3 id="create-repositories">Create repositories</h3>
<p>Channels (Top) &gt; Manage Software Channels (Left side pane) &gt; Manage Repositories (Left side pane) &gt; Create Repository(Right side top corner).</p>
<table>
<thead>
<tr>
<th>Repository Label:</th>
<th>centos7_base_x86_64_repo</th>
</tr>
</thead>
<tbody>
<tr>
<td>Repository URL:</td>
<td><a href="http://mirror.centos.org/centos/7/os/x86_64/">http://mirror.centos.org/centos/7/os/x86_64/</a></td>
</tr>
<tr>
<td>Repository Type:</td>
<td>yum</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Repository Label:</th>
<th>centos7_update_x86_64_repo</th>
</tr>
</thead>
<tbody>
<tr>
<td>Repository URL:</td>
<td><a href="http://mirror.centos.org/centos/7/updates/x86_64/">http://mirror.centos.org/centos/7/updates/x86_64/</a></td>
</tr>
<tr>
<td>Repository Type:</td>
<td>yum</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Repository Label:</th>
<th>centos7_extra_x86_64_repo</th>
</tr>
</thead>
<tbody>
<tr>
<td>Repository URL:</td>
<td><a href="http://mirror.centos.org/centos/7/extras/x86_64/">http://mirror.centos.org/centos/7/extras/x86_64/</a></td>
</tr>
<tr>
<td>Repository Type:</td>
<td>yum</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Repository Label:</th>
<th>centos7_opnstk_x86_64_repo</th>
</tr>
</thead>
<tbody>
<tr>
<td>Repository URL:</td>
<td><a href="http://mirror.centos.org/centos/7/cloud/x86_64/">http://mirror.centos.org/centos/7/cloud/x86_64/</a></td>
</tr>
<tr>
<td>Repository Type:</td>
<td>yum</td>
</tr>
</tbody>
</table>
<h3 id="adding-repository-to-channel">Adding Repository to Channel</h3>
<p>Channels (Top) &gt; Manage Software Channels (Left side pane) &gt; Centos 7 Base x86_64  &gt; Repositories (Tab)  &gt; centos7_base_x86_64_repo (Check box)  &gt; Update Repositories (Bottom right corner). <!-- raw HTML omitted --></p>
<p><img src="/img/include/spacewalk_3.png" alt="Example image"  class="zoomable" /></p>
<h3 id="creating-activation-key">Creating activation Key</h3>
<p>System  &gt; ( Top menu) Activation Keys (Left Side pane)  &gt; Create Key (Right side top corner) &gt; Fill description <!-- raw HTML omitted --></p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Description:</td>
<td>CentOS Linux 7 x86_64</td>
</tr>
<tr>
<td>Key:</td>
<td>centoslinux7-x86_64</td>
</tr>
<tr>
<td>Usage:</td>
<td></td>
</tr>
<tr>
<td>Base channels:</td>
<td>Centos 7 Base - x86_64</td>
</tr>
<tr>
<td>Add-On Entitlements:</td>
<td>Choose all available feature you about to use.</td>
</tr>
</tbody>
</table>
<p><img src="/img/include/spacewalk_4.png" alt="Example image"  class="zoomable" />
<img src="/img/include/spacewalk_5.png" alt="Example image"  class="zoomable" /></p>
<h3 id="start-syncing-repositories">Start Syncing repositories</h3>
<pre tabindex="0"><code>spacewalk-repo-sync --channel centos-7-base-x86_64 --type yum
tail -f /var/log/rhn/reposync/centos-7-base-x86_64.log

df -hP /var/satellite
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Google Authenticator on pfSense]]></title>
            <link href="https://devopstales.github.io/home/pfsense-2fa/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/pfsense-2fa/</id>
            
            
            <published>2019-04-29T00:00:00+00:00</published>
            <updated>2019-04-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>This article explains how to set up OpenVPN with Google Authenticator on pfSense.</p>
<h3 id="set-up-the-freeradius">Set up the FreeRADIUS</h3>
<ul>
<li>Go to  <code>System &gt; Package Manager &gt; Available Packages</code> and install <code>FreeRADIUS</code> package.</li>
<li><code>Services &gt; FreeRADIUS &gt; Interfaces &gt; Add</code></li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Interface IP Address</td>
<td>127.0.0.1</td>
</tr>
<tr>
<td>Port</td>
<td>1812</td>
</tr>
<tr>
<td>Interface Type</td>
<td>Authentication</td>
</tr>
<tr>
<td>IP Version</td>
<td>IPv4</td>
</tr>
<tr>
<td>Description</td>
<td>Authentication</td>
</tr>
</tbody>
</table>
<p><img src="/img/include/pfsense_2fa_1.png" alt="Example image"  class="zoomable" /></p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Interface IP Address</td>
<td>127.0.0.1</td>
</tr>
<tr>
<td>Port</td>
<td>1813</td>
</tr>
<tr>
<td>Interface Type</td>
<td>Authentication</td>
</tr>
<tr>
<td>IP Version</td>
<td>IPv4</td>
</tr>
<tr>
<td>Description</td>
<td>Accounting</td>
</tr>
</tbody>
</table>
<p><img src="/img/include/pfsense_2fa_2.png" alt="Example image"  class="zoomable" /></p>
<h3 id="add-a-nas-client">Add a NAS client</h3>
<ul>
<li><code>Services &gt; FreeRADIUS &gt; NAS/Clients &gt; Add</code></li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Client IP Address</td>
<td>127.0.0.1</td>
</tr>
<tr>
<td>Client IP Version</td>
<td>IPv4</td>
</tr>
<tr>
<td>Client Shortname</td>
<td>pfsenselocal</td>
</tr>
<tr>
<td>Client Shared Secret</td>
<td>Password1</td>
</tr>
<tr>
<td>Client Protocol</td>
<td>UDP</td>
</tr>
<tr>
<td>Client Type</td>
<td>other</td>
</tr>
<tr>
<td>Require Message Authenticator</td>
<td>No</td>
</tr>
<tr>
<td>Max Connections</td>
<td>16</td>
</tr>
<tr>
<td>Description</td>
<td>pfsenselocal</td>
</tr>
</tbody>
</table>
<p><img src="/img/include/pfsense_2fa_3.png" alt="Example image"  class="zoomable" /></p>
<h3 id="add-an-authentication-server-ro-pfsense">Add an authentication server ro pfSense</h3>
<ul>
<li><code>System &gt; User Manager &gt; Authentication Servers &gt; Add</code></li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Descriptive Name</td>
<td>localfreeradius</td>
</tr>
<tr>
<td>Type</td>
<td>RADIUS</td>
</tr>
<tr>
<td>Protocol</td>
<td>PAP</td>
</tr>
<tr>
<td>Hostname or IP address</td>
<td>127.0.0.1</td>
</tr>
<tr>
<td>Shared Secret</td>
<td>Password1</td>
</tr>
<tr>
<td>Services offered</td>
<td>Authentication and Accounting</td>
</tr>
<tr>
<td>Authentiocation port</td>
<td>1812</td>
</tr>
<tr>
<td>Accounting port</td>
<td>1813</td>
</tr>
<tr>
<td>Authentication Timeout</td>
<td>5</td>
</tr>
<tr>
<td>RADIUS NAS IP Attribute</td>
<td>LAN</td>
</tr>
</tbody>
</table>
<p><img src="/img/include/pfsense_2fa_4.png" alt="Example image"  class="zoomable" /></p>
<h3 id="configurate-otp-for-users">Configurate OTP for Users</h3>
<ul>
<li><code>Services &gt; FreeRADIUS &gt; Users &gt; Add</code></li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Username</td>
<td>tester</td>
</tr>
<tr>
<td>Password</td>
<td></td>
</tr>
<tr>
<td>Password Encryption</td>
<td>Cleartext-Password</td>
</tr>
<tr>
<td>One-Time Password</td>
<td>Enable One-Time Password (OTP) for this user</td>
</tr>
<tr>
<td>OTP Auth Method</td>
<td>Google-Authenticator</td>
</tr>
<tr>
<td>Init-Secret</td>
<td>click Generator OTP Secret</td>
</tr>
<tr>
<td>PIN</td>
<td>enter 4-8 numbers and remember them.</td>
</tr>
<tr>
<td>QR Code</td>
<td>click Generate QR Code.</td>
</tr>
</tbody>
</table>
<p>At this point open Google Authenticator on your phone and scan the QRCODE.</p>
<p><img src="/img/include/pfsense_2fa_5.png" alt="Example image"  class="zoomable" /></p>
<p>You can use One-Time Password (OTP) only for local FreeRadius users. FreeRadius users from diferent backenl like mysql or ldap did not work.</p>
<h3 id="configurate-openvpn">Configurate openvpn</h3>
<ul>
<li>Go to <code>VPN &gt; OpenVPN &gt; Servers &gt; Edit</code></li>
<li>Select localfreeradius for Backend for authentication</li>
</ul>
<p><img src="/img/include/pfsense_2fa_6.png" alt="Example image"  class="zoomable" /></p>
<ul>
<li>In the OpenVPN Server configuration, under <code>Advanced Configuration &gt; Custom options</code></li>
<li>add: <code>reneg-sec 0</code></li>
</ul>
<p>If you connect your OpenVPN client you must enter your username and the PIN + the Google Authenticator one-time code as your password. <!-- raw HTML omitted --> If PIN is 1234 and the Google Authenticator code is 445 745 then the password is: 1234445745</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Katello]]></title>
            <link href="https://devopstales.github.io/home/katello-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/katello-install/</id>
            
            
            <published>2019-04-29T00:00:00+00:00</published>
            <updated>2019-04-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Katello brings the full power of content management alongside the provisioning and configuration capabilities of Foreman. Katello is the upstream community project from which the Red Hat Satellite product is derived after Red Hat Satellite Server 6.</p>
<h3 id="base-komponents">Base komponents</h3>
<ul>
<li>Foreman: provisioning on new clients.</li>
<li>Pulp: patch and content (package repository) management.</li>
<li>Candlepin: subscription and entitlement management.</li>
<li>Puppet: configuration management (actual running of modules assigned in Foreman).</li>
<li>Katello: unified workflow and WebUI for content (Pulp) and subscriptions (Candlepin).</li>
</ul>
<h3 id="hardware-requirements">Hardware Requirements</h3>
<ul>
<li>Two Logical CPUs</li>
<li>8 GB of memory (12 GB highly recommended)</li>
<li>The filesystem holding /var/lib/pulp needs to be large</li>
</ul>
<h3 id="required-repositories">Required Repositories</h3>
<pre tabindex="0"><code># hostnevet beállítani !!!

yum -y localinstall https://fedorapeople.org/groups/katello/releases/yum/3.11/katello/el7/x86_64/katello-repos-latest.rpm
yum -y localinstall https://yum.theforeman.org/releases/1.21/el7/x86_64/foreman-release.rpm
yum -y localinstall https://yum.puppetlabs.com/puppetlabs-release-pc1-el-7.noarch.rpm
yum -y localinstall https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
</code></pre><h3 id="installation">Installation</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">yum -y install foreman-release-scl python-django
yum -y update
yum -y install katello


foreman-installer <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--scenario <span style="color:#e6db74">&#34;katello&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--foreman-initial-organization <span style="color:#e6db74">&#34;mydomain&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--foreman-initial-location <span style="color:#e6db74">&#34;office&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--enable-foreman-plugin-ansible <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--enable-foreman-proxy-plugin-ansible <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--enable-foreman-plugin-remote-execution <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--enable-foreman-proxy-plugin-remote-execution-ssh

<span style="color:#75715e"># reset/gen Password</span>
foreman-rake permissions:reset
</code></pre></div><h3 id="configure-hammer-cli">Configure hammer-cli</h3>
<pre tabindex="0"><code>nano ~/.hammer/cli.modules.d/foreman.yml
:foreman:
 :host: 'https://katello.devopstales.intra/'
 :username: 'admin'
 :password: '**********'

hammer defaults add --param-name organization --param-value &quot;mydomain&quot;
hammer defaults add --param-name location --param-value &quot;office&quot;
hammer defaults list
</code></pre><h3 id="configure-gpg-keys">Configure gpg keys</h3>
<pre tabindex="0"><code>hammer product create \
--name &quot;el7_repos&quot; \
--description &quot;Various repositories to use with CentOS 7&quot;

mkdir /etc/pki/rpm-gpg/import/
cd /etc/pki/rpm-gpg/import/
wget https://repo.mysql.com/RPM-GPG-KEY-mysql
wget http://mirror.centos.org/centos/7/os/x86_64/RPM-GPG-KEY-CentOS-7
wget https://archive.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-7Server
wget https://rpms.remirepo.net/RPM-GPG-KEY-remi
wget https://packages.cisofy.com/keys/cisofy-software-rpms-public.key

hammer gpg create \
--key &quot;RPM-GPG-KEY-CentOS-7&quot; \
--name &quot;RPM-GPG-KEY-CentOS-7&quot;

hammer gpg create \
--key &quot;RPM-GPG-KEY-mysql&quot; \
--name &quot;RPM-GPG-KEY-mysql&quot;

hammer gpg create \
--key &quot;RPM-GPG-KEY-EPEL-7Server&quot; \
--name &quot;RPM-GPG-KEY-EPEL-7Server&quot;

hammer gpg create \
--key &quot;RPM-GPG-KEY-remi&quot; \
--name &quot;RPM-GPG-KEY-remi&quot;

hammer gpg create \
--key &quot;cisofy-software-rpms-public.key&quot; \
--name &quot;RPM-GPG-KEY-cisofy&quot;
</code></pre><h3 id="create-yum-repositories">Create yum repositories</h3>
<pre tabindex="0"><code>hammer gpg list

hammer repository create \
--product &quot;el7_repos&quot; \
--name &quot;base_x86_64&quot; \
--label &quot;base_x86_64&quot; \
--content-type &quot;yum&quot; \
--download-policy &quot;on_demand&quot; \
--gpg-key &quot;RPM-GPG-KEY-CentOS-7&quot; \
--url &quot;http://mirror.centos.org/centos/7/os/x86_64/&quot; \
--mirror-on-sync &quot;no&quot;

hammer repository create \
--product &quot;el7_repos&quot; \
--name &quot;extras_x86_64&quot; \
--label &quot;extras_x86_64&quot; \
--content-type &quot;yum&quot; \
--download-policy &quot;on_demand&quot; \
--gpg-key &quot;RPM-GPG-KEY-CentOS-7&quot; \
--url &quot;http://mirror.centos.org/centos/7/extras/x86_64/&quot; \
--mirror-on-sync &quot;no&quot;

hammer repository create \
--product &quot;el7_repos&quot; \
--name &quot;updates_x86_64&quot; \
--label &quot;updates_x86_64&quot; \
--content-type &quot;yum&quot; \
--download-policy &quot;on_demand&quot; \
--gpg-key &quot;RPM-GPG-KEY-CentOS-7&quot; \
--url &quot;http://mirror.centos.org/centos/7/updates/x86_64/&quot; \
--mirror-on-sync &quot;no&quot;

hammer repository create \
--product &quot;el7_repos&quot; \
--name &quot;epel_x86_64&quot; \
--label &quot;epel_x86_64&quot; \
--content-type &quot;yum&quot; \
--download-policy &quot;on_demand&quot; \
--gpg-key &quot;RPM-GPG-KEY-EPEL-7Server&quot; \
--url &quot;https://dl.fedoraproject.org/pub/epel/7Server/x86_64/&quot;

hammer repository create \
--product &quot;el7_repos&quot; \
--name &quot;lynis&quot; \
--label &quot;lynis&quot; \
--content-type &quot;yum&quot; \
--download-policy &quot;on_demand&quot; \
--gpg-key &quot;RPM-GPG-KEY-cisofy&quot; \
--url &quot;https://packages.cisofy.com/community/lynis/rpm/&quot;

hammer repository create \
--product &quot;el7_repos&quot; \
--name &quot;mysql_57_x86_64&quot; \
--label &quot;mysql_57_x86_64&quot; \
--content-type &quot;yum&quot; \
--download-policy &quot;on_demand&quot; \
--gpg-key &quot;RPM-GPG-KEY-mysql&quot; \
--url &quot;https://repo.mysql.com/yum/mysql-5.7-community/el/7/x86_64/&quot;

hammer repository create \
--product &quot;el7_repos&quot; \
--name &quot;katello_agent_x86_64&quot; \
--label &quot;katello_agent_x86_64&quot; \
--content-type &quot;yum&quot; \
--download-policy &quot;on_demand&quot; \
--url &quot;https://fedorapeople.org/groups/katello/releases/yum/latest/client/el7/x86_64/&quot;

hammer repository create \
--product &quot;el7_repos&quot; \
--name &quot;remi_php_56_x86_64&quot; \
--label &quot;remi_php_56_x86_64&quot; \
--content-type &quot;yum&quot; \
--download-policy &quot;on_demand&quot; \
--gpg-key &quot;RPM-GPG-KEY-remi&quot; \
--url &quot;https://mirrors.ukfast.co.uk/sites/remi/enterprise/7/php56/x86_64/&quot;

hammer repository create \
--product &quot;el7_repos&quot; \
--name &quot;remi_php_72_x86_64&quot; \
--label &quot;remi_php_72_x86_64&quot; \
--content-type &quot;yum&quot; \
--download-policy &quot;on_demand&quot; \
--gpg-key &quot;RPM-GPG-KEY-remi&quot; \
--url &quot;https://mirrors.ukfast.co.uk/sites/remi/enterprise/7/php72/x86_64/&quot;

hammer repository create \
--product &quot;el7_repos&quot; \
--name &quot;remi_safe_x86_64&quot; \
--label &quot;remi_safe_x86_64&quot; \
--content-type &quot;yum&quot; \
--download-policy &quot;on_demand&quot; \
--gpg-key &quot;RPM-GPG-KEY-remi&quot; \
--url &quot;https://mirrors.ukfast.co.uk/sites/remi/enterprise/7/safe/x86_64/&quot;
</code></pre><h3 id="sync-repos">Sync repos</h3>
<pre tabindex="0"><code>hammer repository list

for i in $(seq 1 12); do \
hammer repository synchronize \
--product &quot;el7_repos&quot; \
--id &quot;$i&quot;; \
done

# Create a Content View
hammer content-view create \
--name &quot;el7_content&quot; \
--description &quot;Content view for CentOS 7&quot;

hammer product list

# Add Repositories to Content View
for i in $(seq 1 12); do \
hammer content-view add-repository \
--name &quot;el7_content&quot; \
--product &quot;el7_repos&quot; \
--repository-id &quot;$i&quot;; \
done

# Create a Lifecycle Environment
hammer lifecycle-environment create \
--name &quot;stable&quot; \
--label &quot;stable&quot; \
--prior &quot;Library&quot;

hammer lifecycle-environment list

# Publish a Content View
hammer content-view publish \
--name &quot;el7_content&quot; \
--description &quot;Publishing repositories&quot;

hammer content-view version list

# Promote Version to Lifecycle Environment
hammer content-view version promote \
--content-view &quot;el7_content&quot; \
--version &quot;1.0&quot; \
--to-lifecycle-environment &quot;stable&quot;

hammer content-view version list

# Create an Activation Key
hammer activation-key create \
--name &quot;el7-key&quot; \
--description &quot;Key to use with CentOS7&quot; \
--lifecycle-environment &quot;stable&quot; \
--content-view &quot;el7_content&quot; \
--unlimited-hosts

hammer activation-key list

# Add Subscription to Activation Key
hammer subscription list

hammer activation-key add-subscription \
--name &quot;el7-key&quot; \
--quantity &quot;1&quot; \
--subscription-id &quot;1&quot;

# Backup Katello Configuration
foreman-maintain backup snapshot -y /mnt/backup/
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install privacyIDEA]]></title>
            <link href="https://devopstales.github.io/home/privacyidea-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/privacyidea-install/</id>
            
            
            <published>2019-04-29T00:00:00+00:00</published>
            <updated>2019-04-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>privacyIDEA is a Two Factor Authentication System which is multi-tenency- and multi-instance-capable. It is opensource, written in Python and hosted at GitHub.</p>
<h3 id="configure-privacyidea-repo">Configure privacyidea repo</h3>
<pre tabindex="0"><code># base os: Debian 9

apt update
apt install dirmngr -y

nano /etc/apt/sources.list.d/privacyidea.list
deb http://lancelot.netknights.it/community/xenial/stable xenial main

wget https://lancelot.netknights.it/NetKnights-Release.asc
apt-key add NetKnights-Release.asc
</code></pre><h3 id="install-privacyidea">Install privacyidea</h3>
<pre tabindex="0"><code>apt update
apt install privacyidea-apache2

ln -s /etc/apache2/mods-available/rewrite.load /etc/apache2/mods-enabled/rewrite.load

nano nano sites-enabled/privacyidea.conf
# enable 80 to 443 redirection
systemctl restart apache2
</code></pre><h3 id="create-admin-user">Create admin user</h3>
<pre tabindex="0"><code>pi-manage admin add admin -e admin@localhost
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install spacewalk 2.9]]></title>
            <link href="https://devopstales.github.io/home/spacewalk-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/spacewalk-install/</id>
            
            
            <published>2019-04-29T00:00:00+00:00</published>
            <updated>2019-04-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Spacewalk is an open source Linux systems management solution. Spacewalk is the upstream community project from which the Red Hat Satellite product is derived before Red Hat Satellite Server 6.</p>
<h3 id="create-answerfile">Create answerfile</h3>
<pre tabindex="0"><code>cat &gt; /root/spacewalk_answers.txt &lt;&lt; EOF
admin-email = operation@devopstales.intra
ssl-set-cnames = spacewalk
ssl-set-org = Spacewalk Org
ssl-set-org-unit = spacewalk
ssl-set-city = Budapest
ssl-set-state = non
ssl-set-country = HU
ssl-password = Password1
ssl-set-email = operation@devopstales.intra
ssl-config-sslvhost = Y
enable-tftp=Y
EOF
</code></pre><h3 id="install-requirements">Install requirements</h3>
<pre tabindex="0"><code>yum install ntp -y
service ntpd restart


rpm -Uvh https://copr-be.cloud.fedoraproject.org/results/@spacewalkproject/spacewalk-2.9/epel-7-x86_64/00830557-spacewalk-repo/spacewalk-repo-2.9-4.el7.noarch.rpm

rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm

yum clean all &amp;&amp; yum repolist
</code></pre><h3 id="install-spacewalk">Install Spacewalk</h3>
<pre tabindex="0"><code>yum install -y spacewalk-setup-postgresql
yum install -y spacewalk-postgresql
spacewalk-setup --answer-file=/root/spacewalk_answers.txt
yum install spacecmd -y
</code></pre><p>Go to the WebUI and configure the admin user.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Update ILO firmware]]></title>
            <link href="https://devopstales.github.io/home/update-ilo/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/update-ilo/</id>
            
            
            <published>2019-04-29T00:00:00+00:00</published>
            <updated>2019-04-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Integrated Lights-Out, or iLO, is a proprietary embedded server management technology by Hewlett-Packard which provides out-of-band management facilities.</p>
<h3 id="update-from-linux-cli">Update from Linux CLI</h3>
<p>Download iLO firmware say CP032487.scexe</p>
<pre tabindex="0"><code>cd /tmp
chnmod 755 CP032487.scexe
./CP032487.scexe
curl http:///xmldata?item=All
</code></pre><h3 id="update-from-ilo-cli">Update from ILO CLI</h3>
<pre tabindex="0"><code>./CP032487.scexe --unpack=firmware
cd firmware
cp firmware/ilo2_NNN.bin $DocumentRoot (usually /var/www/html/)

# Login on iLO over ssh and upload firmware
ssh -l Administrator

iLO&gt; show
iLO&gt; cd /map1
iLO&gt; oemhp_ping
iLO&gt; load -source http:///firmware/ilo2_NNN.bin

curl http:///xmldata?item=All
</code></pre><h3 id="update-from-ilo-webui">Update from iLO WebUI</h3>
<ul>
<li>access iLO Web UI, go to Administration tab and upload bin file to upgrade iLO firmware</li>
</ul>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Proxmox Mail Gateway]]></title>
            <link href="https://devopstales.github.io/home/proxmox-mail-gateway/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/proxmox-mail-gateway/</id>
            
            
            <published>2019-04-28T00:00:00+00:00</published>
            <updated>2019-04-28T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Proxmox Mail Gateway is a full featured, open-source mail proxy and protects your mail server from spam, viruses, trojans and phishing emails.</p>
<h3 id="configurate-cluster">Configurate Cluster</h3>
<pre tabindex="0"><code>pmg1.devopstales.intra 192.168.0.27
pmg2.devopstales.intra 192.168.0.28
</code></pre><p>After the base installation login the web interfate:</p>
<ul>
<li>192.168.0.27:8006</li>
<li>192.168.0.28:8006</li>
</ul>
<p>At Configuration &gt; Cluster create a new cluster
<img src="/img/include/pmg_1.png" alt="Example image"  class="zoomable" />
<img src="/img/include/pmg_2.png" alt="Example image"  class="zoomable" /></p>
<p>Copy the cluster info:
<img src="/img/include/pmg_3.png" alt="Example image"  class="zoomable" /></p>
<p>On the other host (pmg2) go to the same menu and click Join
<img src="/img/include/pmg_4.png" alt="Example image"  class="zoomable" />
Add the datat copyd from the master node (pmg1)
<img src="/img/include/pmg_5.png" alt="Example image"  class="zoomable" /></p>
<h3 id="basic-configuration">Basic Configuration</h3>
<p>On the master node&rsquo;s (pmg1) weg interface go to Configuration &gt; Mail Proxy
<img src="/img/include/pmg_6.png" alt="Example image"  class="zoomable" />
Edit the Default Relay and add your interbal mailservers ip:
<img src="/img/include/pmg_7.png" alt="Example image"  class="zoomable" />
On the pmg2 check the config is replicated</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Centreon on Centos 7]]></title>
            <link href="https://devopstales.github.io/home/centreon-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/naemon-influxdb/?utm_source=atom_feed" rel="related" type="text/html" title="Install Nemon with Influxdb storage" />
                <link href="https://devopstales.github.io/monitoring/centreon-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Centreon on Centos 7" />
                <link href="https://devopstales.github.io/home/pfsense-usg/?utm_source=atom_feed" rel="related" type="text/html" title="Pfsese USG S2S VPN" />
                <link href="https://devopstales.github.io/home/openshift-gitlabrunner/?utm_source=atom_feed" rel="related" type="text/html" title="Install Gitlab runner on Openshift" />
                <link href="https://devopstales.github.io/home/openshift-extregistry/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: External registry" />
            
                <id>https://devopstales.github.io/home/centreon-install/</id>
            
            
            <published>2019-04-27T00:00:00+00:00</published>
            <updated>2019-04-27T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Centreon is a free Open Source monitoring software which allows an administrator to easily configure alerts based on thresholds, generate email alerts, add systems to be monitored quickly without the need of configuring complicated configuration files.</p>
<h3 id="install-requisites">Install requisites</h3>
<pre tabindex="0"><code>selinuxenabled &amp;&amp; echo enabled || echo disabled

nano /etc/selinux/config
SELINUX=disabled

yum install -y epel-release mariadb-server
mkdir /etc/systemd/system/mariadb.service.d/
echo -ne &quot;[Service]\nLimitNOFILE=32000\n&quot; | tee /etc/systemd/system/mariadb.service.d/limits.conf

systemctl daemon-reload
systemctl enable mariadb
systemctl start mariadb
systemctl status mariadb

yum install centos-release-scl
</code></pre><h3 id="install-centreon">Install Centreon</h3>
<pre tabindex="0"><code>cd /opt
wget http://yum.centreon.com/standard/19.04/el7/stable/noarch/RPMS/centreon-release-19.04-1.el7.centos.noarch.rpm
yum install --nogpgcheck centreon-release-19.04-1.el7.centos.noarch.rpm
yum install -y centreon-base-config-centreon-engine centreon

echo &quot;date.timezone = Europe/Budapest&quot; &gt; /etc/opt/rh/rh-php71/php.d/php-timezone.ini
systemctl restart rh-php71-php-fpm

systemctl enable httpd24-httpd
systemctl enable snmpd
systemctl enable snmptrapd
systemctl enable rh-php71-php-fpm
systemctl enable centcore
systemctl enable centreontrapd
systemctl enable cbd
systemctl enable centengine
systemctl enable centreon

systemctl start rh-php71-php-fpm
systemctl start httpd24-httpd
systemctl start mysqld
systemctl start cbd
systemctl start snmpd
systemctl start snmptrapd
</code></pre><h3 id="configurate-centreon">Configurate centreon</h3>
<p><img src="/img/include/centreon_1.png" alt="Example image"  class="zoomable" />
<img src="/img/include/centreon_2.png" alt="Example image"  class="zoomable" />
<img src="/img/include/centreon_3.png" alt="Example image"  class="zoomable" />
<img src="/img/include/centreon_4.png" alt="Example image"  class="zoomable" />
<img src="/img/include/centreon_5.png" alt="Example image"  class="zoomable" />
<img src="/img/include/centreon_6.png" alt="Example image"  class="zoomable" />
<img src="/img/include/centreon_7.png" alt="Example image"  class="zoomable" />
<img src="/img/include/centreon_8.png" alt="Example image"  class="zoomable" />
<img src="/img/include/centreon_9.png" alt="Example image"  class="zoomable" /></p>
<pre tabindex="0"><code>systemctl start cbd
systemctl start centcore
systemctl start centreontrapd
yum install centreon-widget* -y
</code></pre><p><img src="/img/include/centreon_10.png" alt="Example image"  class="zoomable" />
Select Central and Click Export Configuration. <!-- raw HTML omitted -->
Then the poller will ativated.
<img src="/img/include/centreon_11.png" alt="Example image"  class="zoomable" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Pfsese USG S2S VPN]]></title>
            <link href="https://devopstales.github.io/home/pfsense-usg/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/pfsense-usg/?utm_source=atom_feed" rel="related" type="text/html" title="Pfsese USG S2S VPN" />
                <link href="https://devopstales.github.io/home/pfsense-telegraf/?utm_source=atom_feed" rel="related" type="text/html" title="Install telegraf on pfsense" />
                <link href="https://devopstales.github.io/home/centreon-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Centreon on Centos 7" />
                <link href="https://devopstales.github.io/home/openshift-gitlabrunner/?utm_source=atom_feed" rel="related" type="text/html" title="Install Gitlab runner on Openshift" />
                <link href="https://devopstales.github.io/home/openshift-extregistry/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: External registry" />
            
                <id>https://devopstales.github.io/home/pfsense-usg/</id>
            
            
            <published>2019-04-27T00:00:00+00:00</published>
            <updated>2019-04-27T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I will show you how to create a site-to-site VPN for pfSense and unifi usg.</p>
<h3 id="creating-a-new-ipsec-vpn-on-pfsense">Creating a new IPsec VPN on pfsense</h3>
<p>At <code>VPN &gt; IPsec &gt; Add</code><!-- raw HTML omitted --></p>
<p><img src="/img/include/usg-pfsense-1.png" alt="Example image"  class="zoomable" />
<img src="/img/include/usg-pfsense-2.png" alt="Example image"  class="zoomable" />
<img src="/img/include/usg-pfsense-3.png" alt="Example image"  class="zoomable" /></p>
<p>At <code>Firewall &gt; Roles &gt; IPsec &gt; Add</code><!-- raw HTML omitted -->
<img src="/img/include/usg-pfsense-4.png" alt="Example image"  class="zoomable" /></p>
<h3 id="configure-usg">Configure USG</h3>
<p><img src="/img/include/usg-pfsense-5.png" alt="Example image"  class="zoomable" />
<img src="/img/include/usg-pfsense-6.png" alt="Example image"  class="zoomable" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Gitlab runner on Openshift]]></title>
            <link href="https://devopstales.github.io/home/openshift-gitlabrunner/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/openshift-gitlabrunner/?utm_source=atom_feed" rel="related" type="text/html" title="Install Gitlab runner on Openshift" />
                <link href="https://devopstales.github.io/home/openshift-extregistry/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: External registry" />
                <link href="https://devopstales.github.io/home/openshift-cert/?utm_source=atom_feed" rel="related" type="text/html" title="Change Certificates in Openshift" />
                <link href="https://devopstales.github.io/home/openshift-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure Openshift vSphere Cloud Provider" />
                <link href="https://devopstales.github.io/home/openshift-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Helm" />
            
                <id>https://devopstales.github.io/home/openshift-gitlabrunner/</id>
            
            
            <published>2019-04-20T00:00:00+00:00</published>
            <updated>2019-04-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will configure a gtlab rubber for Openshift.</p>
<h3 id="creating-a-service-account">Creating a Service Account</h3>
<pre tabindex="0"><code>oc new-project gitlab-rubber
oc create sa gitlab-ci
oc policy add-role-to-user edit system:serviceaccount:gitlab-rubber:gitlab-ci

oc get sa
NAME         SECRETS   AGE
builder      2         2d
default      2         2d
deployer     2         2d
gitlab-ci    2         2d

oc describe sa gitlab-ci
Name:           gitlab-ci
Namespace:      constellation
Labels:         &lt;none&gt;
Annotations:    &lt;none&gt;

Image pull secrets:     gitlab-ci-dockercfg-q5mj9

Mountable secrets:      gitlab-ci-token-gvvkv
                        gitlab-ci-dockercfg-q5mj9

Tokens:                 gitlab-ci-token-gvvkv
                        gitlab-ci-token-tfsf7

oc describe secret gitlab-ci-token-gvvkv
...
token:          eyJ...&lt;very-long-token&gt;...-cw

oc login --token=eyJ...&lt;very-long-token&gt;...-cw
</code></pre><h3 id="edit-gitlab-ci-config">Edit Gitlab-ci config</h3>
<pre tabindex="0"><code>nano  .gitlab-ci.yml
image: ebits/openshift-client

stages:
  - deployToOpenShift

variables:
  OPENSHIFT_SERVER: https://master.openshift.devopstales.intra:443
  OPENSHIFT_DOMAIN: openshift.devopstales.intra
  # Configure this variable in Secure Variables:
  OPENSHIFT_TOKEN: eyJ...&lt;very-long-token&gt;...-cw

.deploy: &amp;deploy
  before_script:
    - oc login &quot;$OPENSHIFT_SERVER&quot; --token=&quot;$OPENSHIFT_TOKEN&quot; --insecure-skip-tls-verify
  # login with the service account
    - oc project &quot;slides-openshift&quot;
  # enter into our slides project on OpenShift
  script:
    - &quot;oc get services $APP 2&gt; /dev/null || oc new-app . --name=$APP&quot;
  # create a new application from the image in the OpenShift registry
    - &quot;oc start-build $APP --from-dir=. --follow || sleep 3s&quot;
  # start a new build
    - &quot;oc get routes $APP 2&gt; /dev/null || oc expose service $APP --hostname=$APP_HOST&quot;
  # expose our application

develop:
  &lt;&lt;: *deploy
  stage: deployToOpenShift
  tags:
rss_ignore: true
    - docker
  variables:
    APP: slides-openshift
    APP_HOST: demo-slides.$OPENSHIFT_DOMAIN
  environment:
    name: develop
    url: http://demo-slides.$OPENSHIFT_DOMAIN
  except:
    - master
</code></pre><h3 id="create-a-kubernetes-runner-in-openshift-from-template">Create a kubernetes runner in Openshift from template:</h3>
<pre tabindex="0"><code>wget https://raw.githubusercontent.com/devopstales/openshift-examples/master/template/gitlab-runner-template.yml
oc deploy gitlab-runner-template.yml
</code></pre><p>Deploy the template from the gui:</p>
<pre tabindex="0"><code>oc adm policy add-scc-to-user privileged system:serviceaccount:gitlab-rubber:&lt;application-name&gt;-user
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift: External registry]]></title>
            <link href="https://devopstales.github.io/home/openshift-extregistry/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-cert/?utm_source=atom_feed" rel="related" type="text/html" title="Change Certificates in Openshift" />
                <link href="https://devopstales.github.io/home/openshift-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure Openshift vSphere Cloud Provider" />
                <link href="https://devopstales.github.io/home/openshift-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Helm" />
                <link href="https://devopstales.github.io/home/openshift-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/openshift-ldap/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift LDAP authentication" />
            
                <id>https://devopstales.github.io/home/openshift-extregistry/</id>
            
            
            <published>2019-04-19T00:00:00+00:00</published>
            <updated>2019-04-19T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I this post I will demonstrate howyou can use an external registry in Openshift.</p>
<H3>Parst of the Openshift series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
     <li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
     <li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
     <li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
     <li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
     <li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
     <li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
     <li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
     <li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
     <li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
     <li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
     <li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
     <li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
     <li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
     <li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
     <li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
     <li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
     <li>Part16a: <a href="../../kubernetes/openshift-elasticsearch-error/">Opeshift elasticsearch search-guard error</a></li>
     <li>Part16b: <a href="../../kubernetes/openshift-log4shell/">Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)</a></li>
</ul>


<h3 id="configuring-openshift">Configuring Openshift</h3>
<p>From your client machine, create a Kubernetes secret object for Harbor.:</p>
<pre tabindex="0"><code>oc new-proyect registrytest

oc create secret docker-registry harbor \
--docker-server=https://harbor.devopstales.intra \
--docker-username=admin \
--docker-email=admin@devopstales.intra \
--docker-password='[your_admin_harbor_password]'
</code></pre><p>If you want you can add this secret to the deafult template of the project creation.</p>
<h3 id="deploy-the-private-image-on-the-openshift-cluster">Deploy the private image on the Openshift cluster</h3>
<pre tabindex="0"><code>nano registrytest-deployment.yaml
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  labels:
    run: nginx
  name: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      run: nginx
  template:
    metadata:
      labels:
        run: nginx
    spec:
      containers:
      - image: harbor.devopstales.intra/test/nginx:V2
        name: nginx
      imagePullSecrets:
      - name: harbor

oc apply -f kuard-deployment.yaml
oc get pods
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install vMWare Harbor]]></title>
            <link href="https://devopstales.github.io/home/harbor-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/harbor-install/</id>
            
            
            <published>2019-04-18T00:00:00+00:00</published>
            <updated>2019-04-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Vmware harbor ia an open source trusted cloud native registry project that stores, signs, and scans content.</p>
<p>Why harbor? Opeshift and Gitlab has its own docker regytry but nether can intgrate with clair Vulnerability scanner.</p>
<h3 id="install-docker-and-docker-compose">Install Docker and Docker-Compose</h3>
<pre tabindex="0"><code>yum install epel-release wget -y
yum install -y yum-utils
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum install -y docker-ce

sudo yum install -y python-pip
pip install docker-compose

sudo systemctl start docker
sudo systemctl enable docker
</code></pre><h3 id="generate-your-own-ssl-certificate">Generate your own SSL certificate</h3>
<pre tabindex="0"><code>nano certgen.sh
#!/bin/sh

export PASSPHRASE=$(head -c 500 /dev/urandom | tr -dc a-z0-9A-Z | head -c 128; echo)
DOMAIN=devopstales.intra

subj=&quot;
C=HU
ST=Pest
O=My Company
localityName=Budapest
commonName=*.$DOMAIN
organizationalUnitName=OU
emailAddress=root@$DOMAIN
&quot;

openssl genrsa -des3 -out domain.key -passout env:PASSPHRASE 2048

openssl req \
    -new \
    -batch \
    -subj &quot;$(echo -n &quot;$subj&quot; | tr &quot;\n&quot; &quot;/&quot;)&quot; \
    -key domain.key \
    -out domain.csr \
    -passin env:PASSPHRASE

cp domain.key domain.key.org

openssl rsa -in domain.key.org -out domain.key -passin env:PASSPHRASE

openssl x509 -req -days 3650 -in domain.csr -signkey domain.key -out domain.crt
cat domain.crt domain.key &gt; domain.pem
</code></pre><pre tabindex="0"><code>chmod +x certgen.sh
./certgen.sh

mkdir -p /etc/docker/certs.d/harbor.devopstales.intra
cp domain.crt domain.key /etc/docker/certs.d/harbor.devopstales.intra/
cp domain.crt /etc/docker/certs.d/harbor.devopstales.intra/domain.cert
sudo systemctl restart docker
</code></pre><h3 id="install-notary">Install notary</h3>
<pre tabindex="0"><code>curl -L https://github.com/theupdateframework/notary/releases/download/v0.6.1/notary-$(uname -s)-amd64 -o /usr/local/bin/notary
chmod +x /usr/local/bin/notary

mkdir -p ~/.docker/tls/harbor.devopstales.intra:4443/
cp ~/domain.crt ~/.docker/tls/harbor.devopstales.intra:4443/
cp ~/domain.key ~/.docker/tls/harbor.devopstales.intra:4443/
cp ~/domain.crt ~/.docker/tls/harbor.devopstales.intra:4443/domain.cert
</code></pre><h3 id="install-harbor">Install Harbor</h3>
<pre tabindex="0"><code># https://github.com/vmware/harbor/releases/
wget https://storage.googleapis.com/harbor-releases/release-1.7.0/harbor-online-installer-v1.7.5.tgz
tar -xzf harbor-online-installer-v1.7.5.tgz

cd harbor
nano harbor.cfg
hostname = harbor.devopstales.intra
ui_url_protocol = https
ssl_cert = /root/domain.crt
ssl_cert_key = /root/domain.key

./prepare
./install.sh --with-notary --with-clair

docker login harbor.devopstales.intra
</code></pre><p>Access the Harbor UI with the username &ldquo;admin&rdquo; and password &ldquo;Harbor12345&rdquo;
<img src="/img/include/harbor_1.png" alt="Example image"  class="zoomable" /></p>
<p>Create a nwe project.
<img src="/img/include/harbor_2.png" alt="Example image"  class="zoomable" /></p>
<p>Configure automatic Vulnerability scan for project.
<img src="/img/include/harbor_3.png" alt="Example image"  class="zoomable" /></p>
<pre tabindex="0"><code>docker pull nginx
docker tag nginx:latest harbor.devopstales.intra/test/nginx:V1
docker push harbor.devopstales.intra/test/nginx:V1
</code></pre><p><img src="/img/include/harbor_4.png" alt="Example image"  class="zoomable" /></p>
<pre tabindex="0"><code>docker tag nginx:latest harbor.devopstales.intra/test/nginx:V2
export DOCKER_CONTENT_TRUST_SERVER=https://harbor.devopstales.intra:4443
export DOCKER_CONTENT_TRUST=1
docker push harbor.devopstales.intra/test/nginx:V2
</code></pre><p><img src="/img/include/harbor_5.png" alt="Example image"  class="zoomable" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Change Certificates in Openshift]]></title>
            <link href="https://devopstales.github.io/home/openshift-cert/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure Openshift vSphere Cloud Provider" />
                <link href="https://devopstales.github.io/home/openshift-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Helm" />
                <link href="https://devopstales.github.io/home/openshift-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/openshift-ldap/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift LDAP authentication" />
                <link href="https://devopstales.github.io/kubernetes/openshift-cert/?utm_source=atom_feed" rel="related" type="text/html" title="Change Certificates in Openshift" />
            
                <id>https://devopstales.github.io/home/openshift-cert/</id>
            
            
            <published>2019-04-17T00:00:00+00:00</published>
            <updated>2019-04-17T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how can you chnage certificate in Openshift.</p>
<H3>Parst of the Openshift series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
     <li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
     <li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
     <li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
     <li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
     <li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
     <li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
     <li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
     <li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
     <li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
     <li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
     <li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
     <li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
     <li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
     <li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
     <li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
     <li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
     <li>Part16a: <a href="../../kubernetes/openshift-elasticsearch-error/">Opeshift elasticsearch search-guard error</a></li>
     <li>Part16b: <a href="../../kubernetes/openshift-log4shell/">Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)</a></li>
</ul>


<h3 id="configure-certs">Configure certs:</h3>
<p>If you want to configure your Openshift cluster to use your own certificate you can do that wit this configuration.<!-- raw HTML omitted -->
In my case the certificate files is MyCert.crt MyCert.key and the root CA is ccca.pem.</p>
<pre tabindex="0"><code>nano /ec/ansible/hosts
openshift_master_overwrite_named_certificates=true
openshift_hosted_router_certificate={&quot;certfile&quot;: &quot;/root/cert/MyCert.crt&quot;, &quot;keyfile&quot;: &quot;/root/cert/MyCert.key&quot;, &quot;cafile&quot;: &quot;/root/cert/ccca.pem&quot;}
openshift_master_named_certificates=[{&quot;names&quot;: [&quot;master.openshit.devopstales.intra&quot;],&quot;certfile&quot;: &quot;/root/cert/MyCert.crt&quot;, &quot;keyfile&quot;: &quot;/root/cert/MyCert.key&quot;, &quot;cafile&quot;: &quot;/root/cert/ccca.pem&quot;}]

openshift_redeploy_openshift_ca=true
openshift_certificate_expiry_fail_on_warn=false

# registry
openshift_hosted_registry_routecertificates={&quot;certfile&quot;: &quot;/root/cert/MyCert.crt&quot;, &quot;keyfile&quot;: &quot;/root/cert/MyCert.key&quot;, &quot;cafile&quot;: &quot;/root/cert/ccca.pem&quot;}
openshift_hosted_registry_routetermination=reencrypt
</code></pre><h3 id="run-the-installer">Run the Installer</h3>
<p>If your certificate is renewd you can cahge the certificate in the cluster with this playbooks.</p>
<pre tabindex="0"><code>oc get csr | grep Pending | awk '{print $1}' | xargs oc adm certificate approve

ansible-playbook -i hosts /usr/share/ansible/openshift-ansible/playbooks/redeploy-certificates.yml

ansible-playbook -i hosts /usr/share/ansible/openshift-ansible/playbooks/openshift-master/redeploy-openshift-ca.yml
ansible-playbook -i hosts /usr/share/ansible/openshift-ansible/playbooks/openshift-etcd/redeploy-ca.yml
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configure Openshift vSphere Cloud Provider]]></title>
            <link href="https://devopstales.github.io/home/openshift-vmware/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Helm" />
                <link href="https://devopstales.github.io/home/openshift-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/openshift-ldap/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift LDAP authentication" />
                <link href="https://devopstales.github.io/kubernetes/openshift-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure Openshift vSphere Cloud Provider" />
                <link href="https://devopstales.github.io/kubernetes/openshift-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Helm" />
            
                <id>https://devopstales.github.io/home/openshift-vmware/</id>
            
            
            <published>2019-04-16T00:00:00+00:00</published>
            <updated>2019-04-16T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how can you use vmware for persistent storagi on Openshift.</p>
<H3>Parst of the Openshift series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
     <li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
     <li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
     <li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
     <li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
     <li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
     <li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
     <li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
     <li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
     <li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
     <li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
     <li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
     <li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
     <li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
     <li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
     <li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
     <li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
     <li>Part16a: <a href="../../kubernetes/openshift-elasticsearch-error/">Opeshift elasticsearch search-guard error</a></li>
     <li>Part16b: <a href="../../kubernetes/openshift-log4shell/">Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)</a></li>
</ul>


<h3 id="vsphere-configuration">vSphere Configuration</h3>
<ul>
<li>Create a folder for all the VMs in vCenter</li>
<li>In the navigator, select the data center</li>
<li>Right-click and select the menu option to create the folder.</li>
<li>Select All vCenter Actions &gt; New VM and Template Folder.</li>
<li>Move Openshift vms to this folder</li>
<li>The name of the virtual machine must match the name of the nodes for the OpenShift cluster.</li>
</ul>
<p><img src="/img/include/k8s-vmware.png" alt="Example image"  class="zoomable" /></p>
<h3 id="set-up-the-govc-environment">Set up the GOVC environment:</h3>
<pre tabindex="0"><code># on deployer
curl -LO https://github.com/vmware/govmomi/releases/download/v0.20.0/govc_linux_amd64.gz
gunzip govc_linux_amd64.gz
chmod +x govc_linux_amd64
cp govc_linux_amd64 /usr/bin/govc
echo &quot;export GOVC_URL='vCenter IP OR FQDN'&quot; &gt;&gt; /etc/profile
echo &quot;export GOVC_USERNAME='vCenter User'&quot; &gt;&gt; /etc/profile
echo &quot;export GOVC_PASSWORD='vCenter Password'&quot; &gt;&gt; /etc/profile
echo &quot;export GOVC_INSECURE=1&quot; &gt;&gt; /etc/profile
source /etc/profile
</code></pre><p>Add <code>disk.enableUUID=1</code> for all VM:</p>
<pre tabindex="0"><code>govc vm.info &lt;vm&gt;
govc ls /Datacenter/kubernetes/&lt;vm-folder-name&gt;
# example:
govc ls /Datacenter/kubernetes/okd-01

govc vm.change -e=&quot;disk.enableUUID=1&quot; -vm='VM Path'
# example:
govc vm.change -e=&quot;disk.enableUUID=1&quot; -vm='/datacenter/kubernetes/okd-01/okd-m01'
</code></pre><p>VM Hardware should be at version 15 or higher. Upgrade if needed:</p>
<pre tabindex="0"><code>govc vm.option.info '/datacenter/kubernetes/okd-01/okd-m01' | grep HwVersion

govc vm.upgrade -version=15 -vm '/datacenter/kubernetes/okd-01/okd-m01'
</code></pre><h3 id="create-the-required-roles">Create the required Roles</h3>
<ul>
<li>Navigate in the vSphere Client - Menu &gt; Administration &gt; Roles</li>
<li>Add a new Role and select the permissions required. Repeat for each role.</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:center">Roles</th>
<th style="text-align:center">Privileges</th>
<th style="text-align:center">Entities</th>
<th style="text-align:center">Propagate to Children</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">vcp-manage-okd-node-vms</td>
<td style="text-align:center">Resource.AssignVMToPoolVirtualMachine.Config.AddExistingDisk, VirtualMachine.Config.AddNewDisk, VirtualMachine.Config.AddRemoveDevice, VirtualMachine.Config.RemoveDisk, VirtualMachine.Config.SettingsVirtualMachine.Inventory.Create, VirtualMachine.Inventory.Delete</td>
<td style="text-align:center">Cluster, Hosts, VM Folder</td>
<td style="text-align:center">Yes</td>
</tr>
<tr>
<td style="text-align:center">vcp-manage-okd-volumes</td>
<td style="text-align:center">Datastore.AllocateSpace, Datastore.FileManagement (Low level file operations)</td>
<td style="text-align:center">Datastore</td>
<td style="text-align:center">No</td>
</tr>
<tr>
<td style="text-align:center">vcp-view-okd-spbm-profile</td>
<td style="text-align:center">StorageProfile.View (Profile-driven storage view)</td>
<td style="text-align:center">vCenter</td>
<td style="text-align:center">No</td>
</tr>
<tr>
<td style="text-align:center">Read-only (pre-existing default role)</td>
<td style="text-align:center">System.Anonymous, System.Read, System.View</td>
<td style="text-align:center">Datacenter, Datastore Cluster, Datastore Storage Folder</td>
<td style="text-align:center">No</td>
</tr>
</tbody>
</table>
<h3 id="create-a-service-account">Create a service account</h3>
<ul>
<li>Create a vsphere user, or add a domain user, to provide access and assign the new roles to.</li>
</ul>
<h3 id="configure-ansible-installer">Configure ansible installer</h3>
<pre tabindex="0"><code>nano /etc/hosts
openshift_master_dynamic_provisioning_enabled=true
openshift_cloudprovider_kind=vsphere
openshift_cloudprovider_vsphere_username=&lt;vCenter User&gt;
openshift_cloudprovider_vsphere_password=&lt;vCenter Password&gt;
openshift_cloudprovider_vsphere_host=&lt;vCenter IP OR FQDN&gt;
openshift_cloudprovider_vsphere_datacenter=&lt;Datacenter&gt;
openshift_cloudprovider_vsphere_datastore=&lt;Datastore&gt;
openshift_cloudprovider_vsphere_folder=&lt;vm-folder-name&gt;
</code></pre><h3 id="add-providerid">Add providerID</h3>
<pre tabindex="0"><code>nano openshift-vmware-pacher.sh
DATACENTER='&lt;Datacenter&gt;'
FOLDER='&lt;vm-folder-name&gt;'
for vm in $(govc ls /$DATACENTER/vm/$FOLDER ); do
  MACHINE_INFO=$(govc vm.info -json -dc=$DATACENTER -vm.ipath=&quot;$vm&quot; -e=true)
  # My VMs are created on vmware with upper case names, so I need to edit the names with awk
  VM_NAME=$(jq -r ' .VirtualMachines[] | .Name' &lt;&lt;&lt; $MACHINE_INFO | awk '{print tolower($0)}')
  # UUIDs come in lowercase, upper case then
  VM_UUID=$( jq -r ' .VirtualMachines[] | .Config.Uuid' &lt;&lt;&lt; $MACHINE_INFO | awk '{print toupper($0)}')
  echo &quot;Patching $VM_NAME with UUID:$VM_UUID&quot;
  # This is done using dry-run to avoid possible mistakes, remove when you are confident you got everything right.
  kubectl patch node $VM_NAME -p &quot;{\&quot;spec\&quot;:{\&quot;providerID\&quot;:\&quot;vsphere://$VM_UUID\&quot;}}&quot;
done

chmod +x openshift-vmware-pacher.sh
./openshift-vmware-pacher.sh
</code></pre><h3 id="run-the-installer">Run the Installer</h3>
<pre tabindex="0"><code># deployer
cd /usr/share/ansible/openshift-ansible/
sudo ansible-playbook -i inventory/hosts.localhost playbooks/prerequisites.yml
sudo ansible-playbook -i inventory/hosts.localhost playbooks/deploy_cluster.yml

# If installastion failed or went wrong, the following uninstallation script can be run, and running installation can be tried again:
sudo ansible-playbook -i inventory/hosts.localhost playbooks/adhoc/uninstall.yml
</code></pre><h3 id="create-vsphere-storage-class">Create vSphere storage-class</h3>
<pre tabindex="0"><code>nano vmware-sc.yml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: &quot;true&quot;
  name: &quot;vsphere-standard&quot;
provisioner: kubernetes.io/vsphere-volume
parameters:
    diskformat: zeroedthick
    datastore: &quot;NFS&quot;
reclaimPolicy: Delete

oc aplay -f vmware-sc.yml
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift Helm]]></title>
            <link href="https://devopstales.github.io/home/openshift-helm/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/openshift-ldap/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift LDAP authentication" />
                <link href="https://devopstales.github.io/kubernetes/openshift-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Helm" />
                <link href="https://devopstales.github.io/kubernetes/openshift-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/kubernetes/openshift-ldap/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift LDAP authentication" />
            
                <id>https://devopstales.github.io/home/openshift-helm/</id>
            
            
            <published>2019-04-15T00:00:00+00:00</published>
            <updated>2019-04-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I this post I will demonstrate the basic configuration of Helm on Openshift.</p>
<H3>Parst of the Openshift series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
     <li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
     <li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
     <li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
     <li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
     <li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
     <li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
     <li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
     <li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
     <li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
     <li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
     <li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
     <li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
     <li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
     <li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
     <li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
     <li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
     <li>Part16a: <a href="../../kubernetes/openshift-elasticsearch-error/">Opeshift elasticsearch search-guard error</a></li>
     <li>Part16b: <a href="../../kubernetes/openshift-log4shell/">Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)</a></li>
</ul>


<h3 id="helm">Helm</h3>
<p>Helm is a package manager and teplating engine for Kubernetes. It based on tree main components:</p>
<ul>
<li>the helm cli client</li>
<li>the helm server called tiller</li>
<li>the template pcakage called halm chart</li>
</ul>
<h3 id="install-helm-cli">Install helm cli</h3>
<pre tabindex="0"><code># https://github.com/helm/helm/releases
curl -s https://storage.googleapis.com/kubernetes-helm/helm-v2.9.1-linux-amd64.tar.gz | tar xz
cd linux-amd64
cp helm /usr/bin
</code></pre><h3 id="helm-with-cluster-admin-permissions">Helm with cluster-admin permissions</h3>
<pre tabindex="0"><code>nano helm-cluster-admin.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller-admin
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tiller-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: tiller-admin
  namespace: kube-system
</code></pre><h3 id="init-helm">Init helm</h3>
<pre tabindex="0"><code>oc login master.openshift.devopstales.intra:443
kubectl apply -f helm-cluster-admin.yaml
helm init --service-account tiller-admin
</code></pre><h3 id="test-hem">Test hem</h3>
<pre tabindex="0"><code>oc new-project myapp
helm install stable/ghost -n blog

oc get pods -n myapp
export APP_HOST=$(kubectl get svc --namespace myapp blog-ghost --template &quot;{{ range (index .status.loadBalancer.ingress 0) }}{{ . }}{{ end }}&quot;)
export APP_PASSWORD=$(kubectl get secret --namespace myapp blog-ghost -o jsonpath=&quot;{.data.ghost-password}&quot; | base64 --decode)
export APP_DATABASE_PASSWORD=$(kubectl get secret --namespace myapp blog-mariadb -o jsonpath=&quot;{.data.mariadb-password}&quot; | base64 --decode)
helm upgrade blog stable/ghost --set service.type=LoadBalancer,ghostHost=$APP_HOST,ghostPassword=$APP_PASSWORD,mariadb.db.password=$APP_DATABASE_PASSWORD

oc get pods -n myapp
echo Password: $(kubectl get secret --namespace myapp blog-ghost -o jsonpath=&quot;{.data.ghost-password}&quot; | base64 --decode)
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Pfsese https]]></title>
            <link href="https://devopstales.github.io/home/pfsense-cert/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/pfsense-cert/</id>
            
            
            <published>2019-04-15T00:00:00+00:00</published>
            <updated>2019-04-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I will show you how to Enable SSL for pfSense.</p>
<h3 id="creating-a-new-certificate">Creating a new Certificate</h3>
<p>At <code>System &gt; Certificate Manager &gt; Certificates &gt; Add</code><!-- raw HTML omitted -->
Make sure you choose &ldquo;Import an existing Certificate&rdquo; under Method and enter Descriptive name so you know what the certificate is.
<img src="/img/include/pfsense_cert_1.png" alt="Example image"  class="zoomable" /></p>
<p>At System &gt; Advanced &gt; Admin Access<!-- raw HTML omitted -->
Make sure HTTPS is selected as Protocol and now change the SSL Certificate to the one you have created. Scroll down and click on Save. Now, when you restart your Web Browser, you should see a Secure Connection to pfSense when accessing it.
<img src="/img/include/pfsense_cert_2.png" alt="Example image"  class="zoomable" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[RBAC permissions for Helm]]></title>
            <link href="https://devopstales.github.io/home/k8s-helm-rbac/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/kubectl-multi-cluster-config/?utm_source=atom_feed" rel="related" type="text/html" title="Configure kubectl for multiple clusters" />
                <link href="https://devopstales.github.io/home/openshift-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift SSO authentication" />
                <link href="https://devopstales.github.io/home/openshift-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/openshift-ldap/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift LDAP authentication" />
                <link href="https://devopstales.github.io/home/openproject-sso/?utm_source=atom_feed" rel="related" type="text/html" title="OpenProject SSO" />
            
                <id>https://devopstales.github.io/home/k8s-helm-rbac/</id>
            
            
            <published>2019-04-14T00:00:00+00:00</published>
            <updated>2019-04-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I this post I will demonstrate the basic mechanism of helm and Role-based access control (RBAC).</p>
<H3>Parst of the Kubernetes series</H3>
<ul>
     <li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
     <li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
     <li>Part1c: <a href="../../kubernetes/k8s-install-containerd/">Install K8S with containerd and kubeadm</a></li>
     <li>Part1d: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
     <li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
     <li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a></li>
<!--      <li>Part3b: k8s-multiple-nginx-ingress -->
<!--      <li>Part3c: k8s-contour-ingress -->
     <li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a></li>
<!-- local folder (+ autoprovisioning ) -->
     <li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
     <li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
     <li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
     <li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
     <li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
     <li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a></li>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... -->
     <li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
     <li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
     <li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
     <li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
     <li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
     <li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
     <li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a></li>
<!-- istio-install.md -->
     <li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a></li>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog -->
</ul>


<p>I whant to use helm on Openshift but firt I startid with the basics of helm and Role-based access control (RBAC) on a simple Kubernestes cluster. Most people seem to be running Helm with their own credentials or a dedicated service account with cluster-admin permissions. This isn’t very good from a security perspective, especially so if it’s being run within CI/CD.</p>
<h3 id="helm">Helm</h3>
<p>Helm is a package manager and teplating engine for Kubernetes. It based on tree main components:</p>
<ul>
<li>the helm cli client</li>
<li>the helm server called tiller</li>
<li>the template pcakage called halm chart</li>
</ul>
<h3 id="helm-with-cluster-admin-permissions">Helm with cluster-admin permissions</h3>
<pre tabindex="0"><code>nano helm-cluster-admin.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller-admin
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tiller-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: tiller-admin
  namespace: kube-system
</code></pre><h3 id="helm-with-namespace-permissions">Helm with namespace permissions</h3>
<p>We are granting permissions on only the API groups and resources that Tiller needs to deploy and manage releases in its namespace.</p>
<pre tabindex="0"><code>nano helm-dev-namespace.yaml
kind: Namespace
apiVersion: v1
metadata:
  name: dev
---
kind: ServiceAccount
apiVersion: v1
metadata:
  name: tiller
  namespace: dev
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: tiller-manager
  namespace: dev
rules:
- apiGroups: [&quot;&quot;, &quot;batch&quot;, &quot;extensions&quot;, &quot;apps&quot;]
  resources: [&quot;*&quot;]
  verbs: [&quot;*&quot;]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: tiller-binding
  namespace: dev
subjects:
- kind: ServiceAccount
  name: tiller
  namespace: dev
roleRef:
  kind: Role
  name: tiller-manager
  apiGroup: rbac.authorization.k8s.io
</code></pre><pre tabindex="0"><code>nano helm-prod-namespace.yaml
kind: Namespace
apiVersion: v1
metadata:
  name: prod
---
kind: ServiceAccount
apiVersion: v1
metadata:
  name: tiller
  namespace: prod
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: tiller-manager
  namespace: prod
rules:
- apiGroups: [&quot;&quot;, &quot;batch&quot;, &quot;extensions&quot;, &quot;apps&quot;]
  resources: [&quot;*&quot;]
  verbs: [&quot;*&quot;]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: tiller-binding
  namespace: prod
subjects:
- kind: ServiceAccount
  name: tiller
  namespace: prod
roleRef:
  kind: Role
  name: tiller-manager
  apiGroup: rbac.authorization.k8s.io
</code></pre><pre tabindex="0"><code>kubectl create -f helm-dev-namespace.yaml
kubectl create -f helm-prod-namespace.yaml

kubectl -n dev get sa
kubectl -n prod get sa

helm init --service-account tiller --tiller-namespace dev
helm init --service-account tiller --tiller-namespace prod
</code></pre><h3 id="helm-with-minimal-cluster-permissions">Helm with minimal cluster permissions</h3>
<pre tabindex="0"><code>nano helm-cluster-role.yml
kind: Namespace
apiVersion: v1
metadata:
  name: helm
---
kind: ServiceAccount
apiVersion: v1
metadata:
  name: helm
  namespace: helm
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: helm-clusterrole
rules:
  - apiGroups: [&quot;&quot;]
    resources: [&quot;pods/portforward&quot;]
    verbs: [&quot;create&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;pods&quot;]
    verbs: [&quot;list&quot;, &quot;get&quot;]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: helm-clusterrolebinding
roleRef:
  kind: ClusterRole
  apiGroup: rbac.authorization.k8s.io
  name: helm-clusterrole
subjects:
  - kind: ServiceAccount
    name: helm
    namespace: helm
</code></pre><h3 id="generate-a-kubeconfig-file-from-the-helm-service-account">Generate a Kubeconfig file from the Helm Service Account</h3>
<p>Credit to Ami Mahloof for this script.</p>
<pre tabindex="0"><code>NAMESPACE=helm
# Find the secret associated with the Service Account
SECRET=$(kubectl -n $NAMESPACE get sa helm -o jsonpath='{.secrets[].name}')
# Get the token from the secret
TOKEN=$(kubectl get secrets -n $NAMESPACE $SECRET -o jsonpath='{.data.token}' | base64 -D)
# Get the CA from the secret
kubectl get secrets -n $NAMESPACE $SECRET -o jsonpath='{.data.ca\.crt}' | base64 -D &gt; ca.crt

CONTEXT=$(kubectl config current-context)
CLUSTER_NAME=$(kubectl config get-contexts $CONTEXT --no-headers=true | awk '{print $3}')
SERVER=$(kubectl config view -o jsonpath=&quot;{.clusters[?(@.name == \&quot;${CLUSTER_NAME}\&quot;)].cluster.server}&quot;)
KUBECONFIG_FILE=config
USER=helm
CA=ca.crt

# Set up config
kubectl config set-cluster $CLUSTER_NAME \
--kubeconfig=$KUBECONFIG_FILE \
--server=$SERVER \
--certificate-authority=$CA \
--embed-certs=true

kubectl config set-credentials $USER \
--kubeconfig=$KUBECONFIG_FILE \
--token=$TOKEN

kubectl config set-context $USER \
--kubeconfig=$KUBECONFIG_FILE \
--cluster=$CLUSTER_NAME \
--user=$USER

kubectl config use-context $USER \
--kubeconfig=$KUBECONFIG_FILE
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift SSO authentication]]></title>
            <link href="https://devopstales.github.io/home/openshift-sso/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openproject-sso/?utm_source=atom_feed" rel="related" type="text/html" title="OpenProject SSO" />
                <link href="https://devopstales.github.io/home/gitlab-keycloak/?utm_source=atom_feed" rel="related" type="text/html" title="SSO login to Gitlab" />
                <link href="https://devopstales.github.io/home/keycloak1/?utm_source=atom_feed" rel="related" type="text/html" title="Install keycloak with mysql" />
                <link href="https://devopstales.github.io/home/keycloak2/?utm_source=atom_feed" rel="related" type="text/html" title="Install keycloak with postgresql" />
                <link href="https://devopstales.github.io/sso/openshift-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift SSO authentication" />
            
                <id>https://devopstales.github.io/home/openshift-sso/</id>
            
            
            <published>2019-04-13T00:00:00+00:00</published>
            <updated>2019-04-13T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Configure Openshift Cluster to use Keycloak as a user backend for login with oauth2 and SSO.</p>
<H3>Parst of the Openshift series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
     <li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
     <li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
     <li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
     <li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
     <li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
     <li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
     <li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
     <li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
     <li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
     <li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
     <li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
     <li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
     <li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
     <li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
     <li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
     <li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
     <li>Part16a: <a href="../../kubernetes/openshift-elasticsearch-error/">Opeshift elasticsearch search-guard error</a></li>
     <li>Part16b: <a href="../../kubernetes/openshift-log4shell/">Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)</a></li>
</ul>


<p>With Ansible-openshift you can not change the authetication method after Install !! If you installed the cluster with htpasswd, then change to LDAP the playbook trys to add a second authentication methot for the config. It is forbidden to add a second type of identity provider in the version 3.11 of Ansible-openshift. To solv this problem we must change the configuration manually.</p>
<h3 id="environment">Environment</h3>
<pre tabindex="0"><code>192.168.1.40    deployer
192.168.1.41    openshift01 # master node
192.168.1.42    openshift02 # infra node
192.168.1.43    openshift03 # worker node
</code></pre><h3 id="configuration-on-keycloak">Configuration on Keycloak</h3>
<pre tabindex="0"><code>create new client on keycloak in relm mydomain
Client ID: openshift
Clyent Protocol: openid-connect
Access type: confidential
Valid Redirect URIs: https://master.openshift.mydomain.itra/*
delete other urls

# On Credentials tap copy the secret to clientSecrethez in config.
</code></pre><h3 id="configurate-the-cluster">Configurate The cluster</h3>
<pre tabindex="0"><code># on all openshift hosts
nano /etc/origin/master/master-config.yaml
...
  identityProviders:
  - name: keycloak
    challenge: false
    login: true
    provider:
      apiVersion: v1
      kind: OpenIDIdentityProvider
      clientID: openshift
      clientSecret: ef03ffe6-854a-48b4-a26d-190c2861e3c8
      claims:
        id:
        - sub
        preferredUsername:
        - preferred_username
        name:
        - name
        email:
        - email
      urls:
        authorize: https://sso.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/auth
        token: https://sso.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/token
        logoutURL: &quot;https://sso.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/logout?redirect_uri=https://master.openshift.mydomain.itra/console&quot;
  - challenge: true
</code></pre><h3 id="reconfigurate-the-cluster">Reconfigurate the cluster</h3>
<pre tabindex="0"><code># on all openshift hosts
master-restart api
master-restart controllers
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift Ceph RBD for dynamic provisioning]]></title>
            <link href="https://devopstales.github.io/home/openshift-ceph/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-ldap/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift LDAP authentication" />
                <link href="https://devopstales.github.io/kubernetes/openshift-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/kubernetes/openshift-ldap/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift LDAP authentication" />
                <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
                <link href="https://devopstales.github.io/kubernetes/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
            
                <id>https://devopstales.github.io/home/openshift-ceph/</id>
            
            
            <published>2019-04-12T00:00:00+00:00</published>
            <updated>2019-04-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how can you use CEPH RBD for persistent storagi on Openshift.</p>
<H3>Parst of the Openshift series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
     <li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
     <li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
     <li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
     <li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
     <li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
     <li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
     <li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
     <li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
     <li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
     <li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
     <li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
     <li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
     <li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
     <li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
     <li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
     <li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
     <li>Part16a: <a href="../../kubernetes/openshift-elasticsearch-error/">Opeshift elasticsearch search-guard error</a></li>
     <li>Part16b: <a href="../../kubernetes/openshift-log4shell/">Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)</a></li>
</ul>


<h3 id="environment">Environment</h3>
<pre tabindex="0"><code># openshift cluster
192.168.1.41    openshift01 # master node
192.168.1.42    openshift02 # infra node
192.168.1.43    openshift03 # worker node

# ceph cluster
192.168.1.31    ceph01
192.168.1.32    ceph02
192.168.1.33    ceph03
</code></pre><h3 id="prerequirement">Prerequirement</h3>
<p>RBD volume provisioner needs admin key from Ceph to provision storage. To get the admin key from Ceph cluster use this command:</p>
<pre tabindex="0"><code>sudo ceph --cluster ceph auth get-key client.admin | base64
QVFBOFF2SlZheUJQRVJBQWgvS2cwT1laQUhPQno3akZwekxxdGc9PQ==

nano ceph-admin-secret.yaml
apiVersion: v1
data:
  key: QVFBOFF2SlZheUJQRVJBQWgvS2cwT1laQUhPQno3akZwekxxdGc9PQ==
kind: Secret
metadata:
  name: ceph-admin-secret
  namespace: kube-system
type: kubernetes.io/rbd

</code></pre><p>I will also create a separate Ceph pool for</p>
<pre tabindex="0"><code>sudo ceph --cluster ceph osd pool create k8s 1024 1024
sudo ceph --cluster ceph auth get-or-create client.k8s mon 'allow r' osd 'allow rwx pool=k8s'
sudo ceph --cluster ceph auth get-key client.k8s | base64
QVFBOFF2SlZheUJQRVJBQWgvS2ctS2htOFNSZnRvclJPRk1jdXc9PQ==

nano ceph-secret-k8s.yaml
apiVersion: v1
data:
  key: QVFBOFF2SlZheUJQRVJBQWgvS2ctS2htOFNSZnRvclJPRk1jdXc9PQ==
kind: Secret
metadata:
  name: ceph-secret-k8s
  namespace: kube-system
type: kubernetes.io/rbd

</code></pre><pre tabindex="0"><code># on all openshift node
yum install -y ceph-common

# on one openshift master node
nano  k8s-storage.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: &quot;true&quot;
  name: k8s
parameters:
  adminId: admin
  adminSecretName: ceph-admin-secret
  adminSecretNamespace: kube-system
  imageFeatures: layering
  imageFormat: &quot;2&quot;
  monitors: 192.168.1.31:6789, 192.168.1.32:6789, 192.168.1.33:6789
  pool: k8s
  userId: k8s
  userSecretName: ceph-secret-k8s
provisioner: kubernetes.io/rbd
reclaimPolicy: Delete
volumeBindingMode: Immediate


oc create -f ceph-admin-secret.yaml
oc create -f ceph-secret-k8s.yaml
oc create -f k8s-storage.yaml
</code></pre><h3 id="add-secrets-to-existng-namespaces">Add secrets to existng namespaces</h3>
<pre tabindex="0"><code># on one openshift master node
oc project default
oc apply -f ceph-secret-k8s.yaml

oc project management-infra
oc apply -f ceph-secret-k8s.yaml

oc project openshift-infra
oc apply -f ceph-secret-k8s.yaml

oc project openshift-logging
oc apply -f ceph-secret-k8s.yaml

oc project openshift-metrics-server
oc apply -f ceph-secret-k8s.yaml

oc project openshift-monitoring
oc apply -f ceph-secret-k8s.yaml
</code></pre><h3 id="add-secret-to-template">Add secret to template</h3>
<p>If we add the secret to the template iw will be present in all of the newly created namespaces.</p>
<pre tabindex="0"><code># on one openshift master node
su - origin
oc adm create-bootstrap-project-template -o yaml &gt; template.yaml
# add secrets to  the yml without namespace
nano template.yaml
...
- apiVersion: v1
  data:
    key: QVFBOFF2SlZheUJQRVJBQWgvS2ctS2htOFNSZnRvclJPRk1jdXc9PQ==
  kind: Secret
  metadata:
    name: ceph-secret-k8s
  type: kubernetes.io/rbd
...
oc create -f template.yaml -n default

# on all the openshift master nodes
nano /etc/origin/master/master-config.yaml
...
projectConfig:
  projectRequestTemplate: &quot;default/project-request&quot;
...

master-restart api
master-restart controllers
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift LDAP authentication]]></title>
            <link href="https://devopstales.github.io/home/openshift-ldap/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/kubernetes/openshift-ldap/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift LDAP authentication" />
                <link href="https://devopstales.github.io/kubernetes/openshift-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
                <link href="https://devopstales.github.io/kubernetes/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
            
                <id>https://devopstales.github.io/home/openshift-ldap/</id>
            
            
            <published>2019-04-12T00:00:00+00:00</published>
            <updated>2019-04-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Configure Openshift Cluster to use LDAP as a user backend for login with Ansible-openshift</p>
<H3>Parst of the Openshift series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
     <li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
     <li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
     <li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
     <li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
     <li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
     <li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
     <li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
     <li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
     <li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
     <li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
     <li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
     <li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
     <li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
     <li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
     <li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
     <li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
     <li>Part16a: <a href="../../kubernetes/openshift-elasticsearch-error/">Opeshift elasticsearch search-guard error</a></li>
     <li>Part16b: <a href="../../kubernetes/openshift-log4shell/">Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)</a></li>
</ul>


<p>In the last post I used the basic htpasswd authentication method for the installatipn.<!-- raw HTML omitted -->
But I can use Ansible-openshift to configure an LDAP backed at the install for the authentication.</p>
<h3 id="environment">Environment</h3>
<pre tabindex="0"><code>192.168.1.40    deployer
192.168.1.41    openshift01 # master node
192.168.1.42    openshift02 # infra node
192.168.1.43    openshift03 # worker node
</code></pre><p>With Ansible-openshift you can not change the authetication method after Install !! If you installed the cluster with htpasswd, then change to LDAP the playbook trys to add a second authentication methot for the config. It is forbidden to add a second type of identity provider in the version 3.11 of Ansible-openshift so choose wisely.</p>
<h3 id="configurate-installer">Configurate Installer</h3>
<pre tabindex="0"><code># deployer
nano /etc/ansible/ansible.cfg
# use HTPasswd for authentication
#openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'}]

# LDAP
openshift_master_identity_providers=[{'name': 'email_jira_ldap', 'challenge': 'true', 'login': 'true', 'kind': 'LDAPPasswordIdentityProvider', 'attributes': {'id': ['mail'], 'email': ['mail'], 'name': ['displayName'], 'preferredUsername': ['mail']}, 'bindDN': 'CN=ldapbrowser,DC=mydomain,DC=myintra', 'bindPassword': '*******', 'insecure': 'true', 'url': 'ldap://ldap01.mydomain.myintra/dc=mydomain,dc=myintra?mail?sub?(objectClass=*)'}]
</code></pre><h3 id="run-the-installer">Run the Installer</h3>
<pre tabindex="0"><code># deployer
cd /usr/share/ansible/openshift-ansible/
sudo ansible-playbook -i inventory/hosts.localhost playbooks/prerequisites.yml
sudo ansible-playbook -i inventory/hosts.localhost playbooks/deploy_cluster.yml
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[WAN failower on pfsense]]></title>
            <link href="https://devopstales.github.io/home/pfsense-wlan/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/pfsense-wlan/</id>
            
            
            <published>2019-04-12T00:00:00+00:00</published>
            <updated>2019-04-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this pos I&rsquo;ll create a WAN failower configuration.</p>
<h3 id="the-architecture">The Architecture</h3>
<pre tabindex="0"><code>
------- WAN1 ------
| ----- WAN2 ---- |
| |             | |
PF1 -- sync -- PF2
 |               |
 ----- LAN -------  


WAN1: 192.168.0.0/24 (Bridgelt)
LAN: 10.0.1.0/24
SYNC: 10.0.2.0/24
WAN2: 10.0.4.0/24
</code></pre><h3 id="configurate-wip-for-wan2">Configurate WIP for WAN2</h3>
<p>At <code>Firewall &gt; Virtual IPs &gt; Add</code>
<img src="/img/include/pfsenseWAN_1.jpg" alt="Example image"  class="zoomable" /></p>
<h3 id="add-gateway-for-wan-interfaces">Add Gateway for WAN interfaces</h3>
<p>At <code>System &gt; Routing &gt; Add</code>
<img src="/img/include/pfsenseWAN_2.jpg" alt="Example image"  class="zoomable" /></p>
<h3 id="configuring-monitor-ip">Configuring Monitor IP</h3>
<p>At<code> System &gt; Routing &gt; Edit gateways</code> and add google dns ad monitoring ip
<img src="/img/include/pfsenseWAN_3.jpg" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/pfsenseWAN_4.jpg" alt="Example image"  class="zoomable" /></p>
<h3 id="configuring-gateway-group">Configuring Gateway Group</h3>
<p>At<code> System &gt; Routing &gt; Gateway Groups</code> Create 3 Groups
<img src="/img/include/pfsenseWAN_5.jpg" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/pfsenseWAN_6.jpg" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/pfsenseWAN_7.jpg" alt="Example image"  class="zoomable" /></p>
<h3 id="configuring-firewall-rules">Configuring Firewall Rules</h3>
<p>Got to <code>Firewall &gt; Rules &gt; LAN</code> and edit the IPv4 rule. Chane the Gateway</p>
<p><img src="/img/include/pfsenseWAN_8.jpg" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/pfsenseWAN_9.jpg" alt="Example image"  class="zoomable" /></p>
<p>Clone the changed roles to two other rules and change the Gateway to the other Gateway Groups.
<img src="/img/include/pfsenseWAN_10.jpg" alt="Example image"  class="zoomable" /></p>
<h3 id="configurate-nat">Configurate NAT</h3>
<p>Go to<code> Firewall &gt; NAT &gt; Outbound</code> <!-- raw HTML omitted -->
Clone WAN1 rules and edit them to WLAN2
<img src="/img/include/pfsenseWAN_11.jpg" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/pfsenseWAN_12.jpg" alt="Example image"  class="zoomable" /></p>
<h2 id="pfsense-email-notification-when-wan-connection-goes-down">pfSense email notification when WAN connection goes down</h2>
<p>Go to <code>System &gt; Advanced &gt; Notifications</code></p>
<h3 id="example-with-google-gmail-smtp">Example with Google Gmail SMTP</h3>
<p><img src="/img/include/pfsenseWAN_13.jpg" alt="Example image"  class="zoomable" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configure OpenVPN HA pfsense cluster]]></title>
            <link href="https://devopstales.github.io/home/pfsense-openvpn/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/pfsense-openvpn/</id>
            
            
            <published>2019-04-11T00:00:00+00:00</published>
            <updated>2019-04-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this LAB I&rsquo;ll be creating OpenVPN SSL Peer to Peer connection.</p>
<h3 id="generating-ca-certificate">Generating CA Certificate</h3>
<p>At <code>System &gt; Cert.Manager &gt; CAs &gt; Add</code>
<img src="/img/include/OpenVPN_1.jpg" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/OpenVPN_2.jpg" alt="Example image"  class="zoomable" /></p>
<h3 id="generate-server-certificate">Generate Server Certificate</h3>
<p>At <code>System &gt; Cert.Manager &gt; Certificates &gt; Add</code>
<img src="/img/include/OpenVPN_3.jpg" alt="Example image"  class="zoomable" /></p>
<h3 id="generate-user-certificate">Generate User Certificate</h3>
<p>For this demo I will&rsquo;create one certificate for all users, but in live you should create a separate certificate for all users.</p>
<p>At <code>System &gt; Cert.Manager &gt; Certificates &gt; Add</code>
<img src="/img/include/OpenVPN_4.jpg" alt="Example image"  class="zoomable" /></p>
<p>At <code>SystemUser &gt; ManagerUsers</code> add the User certificate for the users.
<img src="/img/include/OpenVPN_5.jpg" alt="Example image"  class="zoomable" /></p>
<h3 id="intall-openvpn-package-exporter">Intall Openvpn package exporter</h3>
<p>Got to<code> System &gt; Package Manager &gt; Available Packages</code> and install <code>openvpn-client-export</code> plugin.</p>
<h3 id="configurate-the-opevpn-service">Configurate the OpeVPN service</h3>
<p>Got to <code>VPN &gt; OpenVPN &gt; Wizards</code>
<img src="/img/include/OpenVPN_6.jpg" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/OpenVPN_7.jpg" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/OpenVPN_8.jpg" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/OpenVPN_9.jpg" alt="Example image"  class="zoomable" /></p>
<p>Edit the Adwanced Configuration:
<img src="/img/include/OpenVPN_18.jpg" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/OpenVPN_10.jpg" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/OpenVPN_11.jpg" alt="Example image"  class="zoomable" /></p>
<h3 id="configurate-nat-rules-to-ha">Configurate NAT Rules to HA</h3>
<p>Go to <code>Firewall &gt; NAT &gt; Outbound</code> and clone the LAN Rules?
<img src="/img/include/OpenVPN_12.jpg" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/OpenVPN_13.jpg" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/OpenVPN_14.jpg" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/OpenVPN_15.jpg" alt="Example image"  class="zoomable" /></p>
<h3 id="enable-connection-from-openvpn-to-master-and-slave">Enable Connection from OpenVPN to master and slave</h3>
<p>In default there in no rout to the salve nod. Go to <code>Firewll &gt; Aliases &gt; Add</code> and create alias for CARP members:
<img src="/img/include/OpenVPN_16.png" alt="Example image"  class="zoomable" /></p>
<p>Then go back to <code>Firewall &gt; NAT &gt; Outbound</code> and create a new rule:
<img src="/img/include/OpenVPN_17.png" alt="Example image"  class="zoomable" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configurate HA pfsense cluster]]></title>
            <link href="https://devopstales.github.io/home/pfsense-ha/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/pfsense-ha/</id>
            
            
            <published>2019-04-10T00:00:00+00:00</published>
            <updated>2019-04-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will configure 2 pfsense server to a HA cluster.</p>
<h3 id="the-architecture">The Architecture</h3>
<pre tabindex="0"><code> ------ WAN ------
 |               |
PF1 -- sync -- PF2
 |               |
 ----- LAN -------  

WAN: 192.168.0.0/24 (Bridge)
LAN: 10.0.1.0/24
SYNC: 10.0.2.0/24
</code></pre><pre tabindex="0"><code>pf1:
WAN 192.168.0.21
LAN: 10.0.1.21
SYNC:10.0.2.21

pf2:
WAN 192.168.0.22
LAN: 10.0.1.22
SYNC:10.0.2.22
</code></pre><p><img src="/img/include/carp_1.png" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/carp_2.png" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/carp_3.png" alt="Example image"  class="zoomable" /></p>
<h3 id="firewall-rules-for-sync">Firewall rules For sync</h3>
<p>On both firewalls add two rules to allow traffic on the SYNC interface: <!-- raw HTML omitted -->
go to <code>Firewall &gt; Rules &gt; Sync</code> and click <code>Add</code>.</p>
<p>Rule 1:
<img src="/img/include/carp_4.png" alt="Example image"  class="zoomable" /></p>
<p>Rule 2:
<img src="/img/include/carp_5.png" alt="Example image"  class="zoomable" /></p>
<p>Rule 3:
<img src="/img/include/carp_6.png" alt="Example image"  class="zoomable" /></p>
<h3 id="synchronization-settings">Synchronization Settings</h3>
<p>Go to <code>System &gt; High Availability Sync</code> and configure the sections like on the pictures.</p>
<p>Master:
<img src="/img/include/carp_7.png" alt="Example image"  class="zoomable" /></p>
<p>Slave:
<img src="/img/include/carp_8.png" alt="Example image"  class="zoomable" /></p>
<p>Test the synchronisation. Go to <code>System &gt; User management</code> and createa new user on the master node. <!-- raw HTML omitted -->
Then check on the slave node.</p>
<p>If it doesn&rsquo;t work, check:</p>
<ul>
<li>Are the firewall web interfaces running on the same protocols and ports?</li>
<li>Is the admin password set correctly? (<code>User Manager &gt; Users &gt; admin</code>.)</li>
<li>Are the firewall rules to allow synch set to use the correct interface (SYNC)?</li>
<li>If you&rsquo;re using VMs, are the firewalls on the same internal network?</li>
</ul>
<h3 id="create-virtual-ips">create virtual IPs</h3>
<p>On the master node go to <code>Firewall &gt; Virtual IPs</code> and click <code>Add</code>. Create a new VIP adres for LAN and WAN interfaces.</p>
<p>WAN VIP on master:
<img src="/img/include/carp_9.png" alt="Example image"  class="zoomable" /></p>
<p>WAN VIP on salave:
<img src="/img/include/carp_10.png" alt="Example image"  class="zoomable" /></p>
<p>LAN VIP on master:
<img src="/img/include/carp_11.png" alt="Example image"  class="zoomable" /></p>
<p>LAN VIP on slave:
<img src="/img/include/carp_12.png" alt="Example image"  class="zoomable" /></p>
<h3 id="change-outbound-nat">Change outbound NAT</h3>
<p>Change the configuration of the outbound NAT to use the shared public IP (the WAN VIP) <!-- raw HTML omitted -->
Go to <code>Firewall &gt; NAT &gt; Outbound</code> and set the mode to <code>Hybrid Outbound NAT</code> rule generation.</p>
<p><img src="/img/include/carp_13.png" alt="Example image"  class="zoomable" /></p>
<p><img src="/img/include/carp_14.png" alt="Example image"  class="zoomable" /></p>
<p>Find your LAN IP ranges (there should be two) and click the edit icon and change the Translation Address to the WAN VIP address.</p>
<p><img src="/img/include/carp_15.png" alt="Example image"  class="zoomable" /></p>
<p>Do the same for the other LAN network mapping. It should end up looking like this:</p>
<p><img src="/img/include/carp_16.png" alt="Example image"  class="zoomable" /></p>
<p>If you’ll be using your pfSense firewall as a DNS resolver you must change the settings of the DNS service (<code>Services &gt; DNS Resolver &gt; General Settings</code>) to lissen on the LAN VIP address. Then chnage the address of the DNS server in the DHCP configuration to us the LAN VIP adress.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Rundeck]]></title>
            <link href="https://devopstales.github.io/home/rundeck/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/rundeck/</id>
            
            
            <published>2019-04-09T00:00:00+00:00</published>
            <updated>2019-04-09T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Rundeck is open source software that powers self-service operations.</p>
<h3 id="install-mysql">Install MySQL</h3>
<pre tabindex="0"><code>echo &quot;[mariadb]
name = MariaDB
baseurl = http://yum.mariadb.org/10.0/centos7-amd64
gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDB
gpgcheck=1&quot; &gt; /etc/yum.repos.d/MariaDB.repo

yum -y install httpd MariaDB-Galera-server.x86_64

systemctl enable httpd
systemctl enable mysql
systemctl start httpd
systemctl start mysql

mysql_secure_installation
mysql -u root -p
create database rundeck;
grant all on rundeck.* to 'rundeck'@'localhost' identified by 'Password1';
quit
</code></pre><h3 id="install-rundeck">Install rundeck</h3>
<pre tabindex="0"><code>yum install java-1.8.0 httpd -y
rpm -Uvh http://repo.rundeck.org/latest.rpm
yum install rundeck
</code></pre><h3 id="configure-rundeck">Configure rundeck</h3>
<pre tabindex="0"><code>nano /etc/rundeck/rundeck-config.properties
# change hostname here
# grails.serverURL=http://localhost:4440
grails.serverURL=http://rundeck.devopstales.intra
#dataSource.url = jdbc:h2:file:/var/lib/rundeck/data/rundeckdb
dataSource.url = jdbc:mysql://localhost/rundeck?autoReconnect=true&amp;useSSL=false
dataSource.username=rundeck
dataSource.password=Password1
dataSource.driverClassName=com.mysql.jdbc.Driver
# mail server
grails.mail.host=localhost
grails.mail.port=25

service rundeckd start
</code></pre><h3 id="configure-httpd">Configure httpd</h3>
<pre tabindex="0"><code>nano /etc/httpd/conf.d/rundeck_proxy.conf
&lt;virtualhost *:80&gt;
        ServerName rundeck.devopstales.intra
        ServerAlias www.rundeck.devopstales.intra
        ServerAdmin admin@rundeck.devopstales.intra

        ProxyRequests Off
        ProxyPass / http://localhost:4440/
        ProxyPassReverse / http://localhost:4440/
&lt;/virtualhost&gt;

service httpd restart
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Nextcloud]]></title>
            <link href="https://devopstales.github.io/home/nextcloud/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/nextcloud/</id>
            
            
            <published>2019-04-08T00:00:00+00:00</published>
            <updated>2019-04-08T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Nextcloud is a suite of client-server software for creating and using file hosting services. Nextcloud application functionally is similar to Dropbox.</p>
<h3 id="install-postgresql">Install Postgresql</h3>
<pre tabindex="0"><code>wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
sudo sh -c 'echo &quot;deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -sc)-pgdg main&quot; &gt; /etc/apt/sources.list.d/PostgreSQL.list'

apt update
apt-get install postgresql-10
apt-get install pgadmin4

systemctl enable postgresql.service
</code></pre><h3 id="configure-database">Configure database</h3>
<pre tabindex="0"><code>createuser cloud
psql
ALTER USER cloud WITH ENCRYPTED password 'Password1';
CREATE DATABASE cloud WITH ENCODING='UTF8' OWNER=cloud;
\q
</code></pre><h3 id="install-requirements">Install requirements</h3>
<pre tabindex="0"><code>apt-get install -y libapache2-mod-php php7.0 php7.0-xml php7.0-curl php7.0-gd php7.0 php7.0-cgi php7.0-cli php7.0-zip php7.0-mbstring wget unzip php7.0-pgsql
</code></pre><h3 id="configurate-php">Configurate php</h3>
<pre tabindex="0"><code>nano /etc/php/7.0/apache2/php.ini
file_uploads = On
allow_url_fopen = On
short_open_tag = On
memory_limit = 256M
upload_max_filesize = 100M
max_execution_time = 360
date.timezone = Europe/Budapest
</code></pre><h3 id="install-them">Install Them</h3>
<pre tabindex="0"><code>cd /usr/share/redmine/public/themes
# https://github.com/akabekobeko/redmine-theme-minimalflat2/releases
wget https://github.com/akabekobeko/redmine-theme-minimalflat2/releases/download/v1.5.0/minimalflat2-1.5.0.zip
unzip minimalflat2-1.5.0.zip
</code></pre><h3 id="configurate-apache">Configurate Apache</h3>
<pre tabindex="0"><code>mkdir /var/www/nextcloud
chown www-data:www-data /var/www/nextcloud
chmod 750 /var/www/nextcloud

mkdir -p /var/nextcloud/data
chown www-data:www-data /var/nextcloud/data
chmod 750 /var/nextcloud/data

cd  /var/www/nextcloud
wget https://download.nextcloud.com/server/installer/setup-nextcloud.php
chown www-data:www-data setup-nextcloud.php
</code></pre><h3 id="create-vhostfile">Create vhostfile</h3>
<pre tabindex="0"><code>echo '&lt;VirtualHost *:80&gt;
ServerAdmin admin@example.com
DocumentRoot &quot;/var/www/nextcloud&quot;
ServerName cloud.devopstales.intra
&lt;Directory &quot;/var/www/nextcloud/&quot;&gt;
Options MultiViews FollowSymlinks

AllowOverride All
Order allow,deny
Allow from all
&lt;/Directory&gt;
TransferLog /var/log/apache2/nextcloud_access.log
ErrorLog /var/log/apache2/nextcloud_error.log
&lt;/VirtualHost&gt;' &gt; /etc/apache2/sites-available/nextcloud.conf

a2dissite 000-default
a2ensite nextcloud
a2enmod rewrite
a2enmod headers
a2enmod env
a2enmod dir
a2enmod mime
service apache2 reload
</code></pre><h3 id="install-nextcloud">Install nextcloud</h3>
<p>Go to <a href="http://cloud.devopstales.intra/setup-nextcloud.php">http://cloud.devopstales.intra/setup-nextcloud.php</a> and add the db configuration to install the aplication.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Redmine]]></title>
            <link href="https://devopstales.github.io/home/redmine/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/redmine/</id>
            
            
            <published>2019-04-06T00:00:00+00:00</published>
            <updated>2019-04-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Redmine is a free and open source, web-based project management and issue tracking tool. I will install it on Ubuntu becous on CetOS there in no pre build package for redmine.</p>
<h3 id="install-and-configure-postgresql">Install and configure Postgresql</h3>
<p>In a previous post I wrote about how to <a href="/linux/install-postgresql/#install-postgresql-10-on-debian">Install PostgreSQL 10</a></p>
<pre tabindex="0"><code>su - postgres
createuser redmine
psql
ALTER USER redmine WITH ENCRYPTED password 'Password1';
CREATE DATABASE redmine WITH ENCODING='UTF8' OWNER=redmine;
\q
</code></pre><h3 id="install-redmine">Install Redmine</h3>
<pre tabindex="0"><code>apt install git redmine redmine-pgsql
chmod 777 /usr/share/redmine/instances/default/tmp/cache/
cd /usr/share/redmine
ruby bin/rails server webrick –e production
</code></pre><pre tabindex="0"><code>echo '[Unit]
Description=Redmine server
After=syslog.target
After=network.target

[Service]
Type=simple
User=redmine
Group=redmine
WorkingDirectory=/usr/share/redmine
ExecStart=/usr/bin/ruby /usr/share/redmine/bin/rails server webrick –e production

# Give a reasonable amount of time for the server to start up/shut down
TimeoutSec=300

[Install]
WantedBy=multi-user.target' &gt; /etc/systemd/system/redmine.service
</code></pre><pre tabindex="0"><code>apt install apache2 libapache2-mod-passenger
cp /usr/share/doc/redmine/examples/apache2-passenger-host.conf /etc/apache2/sites-available/redmine.conf
nano /etc/apache2/sites-available/redmine.conf

a2enmod passenger
a2enmod proxy
a2enmod rewrite
a2ensite redmine.conf
a2dissite 000-default
service apache2 reload
</code></pre><h3 id="install-them">Install Them</h3>
<pre tabindex="0"><code>cd /usr/share/redmine/public/themes
# https://github.com/akabekobeko/redmine-theme-minimalflat2/releases
wget https://github.com/akabekobeko/redmine-theme-minimalflat2/releases/download/v1.5.0/minimalflat2-1.5.0.zip
unzip minimalflat2-1.5.0.zip
</code></pre><h3 id="install-plugin">Install plugin</h3>
<pre tabindex="0"><code>ln -s /usr/share/redmine/bin /usr/share/rubygems-integration/all/specifications/

mkdir /usr/share/redmine/plugins
cd /usr/share/redmine/plugins

git clone https://github.com/applewu/redmine_omniauth_gitlab
cd /usr/share/redmine
bundle install --without development test
bundle exec rake redmine:plugins NAME=redmine_omniauth_gitlab RAILS_ENV=production
ll /usr/share/redmine/public/plugin_assets/
service apache2 restart
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[OpenProject SSO]]></title>
            <link href="https://devopstales.github.io/home/openproject-sso/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/gitlab-keycloak/?utm_source=atom_feed" rel="related" type="text/html" title="SSO login to Gitlab" />
                <link href="https://devopstales.github.io/home/keycloak1/?utm_source=atom_feed" rel="related" type="text/html" title="Install keycloak with mysql" />
                <link href="https://devopstales.github.io/home/keycloak2/?utm_source=atom_feed" rel="related" type="text/html" title="Install keycloak with postgresql" />
                <link href="https://devopstales.github.io/sso/openproject-sso/?utm_source=atom_feed" rel="related" type="text/html" title="OpenProject SSO" />
                <link href="https://devopstales.github.io/sso/gitlab-keycloak/?utm_source=atom_feed" rel="related" type="text/html" title="SSO login to Gitlab" />
            
                <id>https://devopstales.github.io/home/openproject-sso/</id>
            
            
            <published>2019-04-06T00:00:00+00:00</published>
            <updated>2019-04-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Configurate openproject to use Keycloak as sso Identity Provider.</p>
<h3 id="configure-openproject">Configure OpenProject</h3>
<p>Login to openproject with admin and change the config of Self-registrtion to automatic account activation:
Administraion &gt; System Settings &gt; Authentication &gt; Self-registration</p>
<pre tabindex="0"><code>nano /opt/openproject/config/configuration.yml
default:
  omniauth_direct_login_provider: openid
  openid_connect:
    openid:
      host: &quot;sso.devopstales.intra&quot;
      identifier: &quot;project&quot;
      secret: &quot;57583084-b54b-4b32-935b-73776f27b89f&quot;
      icon: &quot;openid_connect/auth_provider-google.png&quot;
      display_name: &quot;SSO&quot;
      authorization_endpoint: &quot;http://sso.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/auth&quot;
      token_endpoint: 'http://sso.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/token'
      userinfo_endpoint: 'http://sso.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/userinfo'
      end_session_endpoint: 'http://sso.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/logout'
      check_session_iframe: 'http://sso.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/login-status-iframe.html'
      sso: true
      issuer: 'http://project.devopstales.intra/login'
      discovery: false

service openproject restart
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[SSO login to Gitlab]]></title>
            <link href="https://devopstales.github.io/home/gitlab-keycloak/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openproject-sso/?utm_source=atom_feed" rel="related" type="text/html" title="OpenProject SSO" />
                <link href="https://devopstales.github.io/home/keycloak1/?utm_source=atom_feed" rel="related" type="text/html" title="Install keycloak with mysql" />
                <link href="https://devopstales.github.io/home/keycloak2/?utm_source=atom_feed" rel="related" type="text/html" title="Install keycloak with postgresql" />
                <link href="https://devopstales.github.io/sso/gitlab-keycloak/?utm_source=atom_feed" rel="related" type="text/html" title="SSO login to Gitlab" />
                <link href="https://devopstales.github.io/sso/openproject-sso/?utm_source=atom_feed" rel="related" type="text/html" title="OpenProject SSO" />
            
                <id>https://devopstales.github.io/home/gitlab-keycloak/</id>
            
            
            <published>2019-04-06T00:00:00+00:00</published>
            <updated>2019-04-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Configurate Gitab to use Keycloak as SSO Identity Proider.</p>
<h3 id="configurate-keycloak">Configurate Keycloak</h3>
<p>Login to Keycloak and create client for Gitlab:
<img src="/img/include/gitlab-keycloak1.png" alt="Example image"  class="zoomable" /></p>
<p>At Mappers create mappers for all user information to GitLab:</p>
<ul>
<li>Name: name
<ul>
<li>Mapper Type: User Property</li>
<li>Property: Username</li>
</ul>
</li>
<li>Name: email
<ul>
<li>Mapper Type: User Property</li>
<li>Property: Email</li>
</ul>
</li>
<li>Name: first_name
<ul>
<li>Mapper Type: User Property</li>
<li>Property: FirstName</li>
</ul>
</li>
<li>Name: last_name
<ul>
<li>Mapper Type: User Property</li>
<li>Property: LastName</li>
</ul>
</li>
</ul>
<h3 id="configurate-gitlab">Configurate Gitlab</h3>
<pre tabindex="0"><code>nano /etc/gitlab/gitlab.rb
gitlab_rails['omniauth_enabled'] = true
gitlab_rails['omniauth_block_auto_created_users'] = false
gitlab_rails['omniauth_allow_single_sign_on'] = ['oauth2_generic']
# gitlab_rails['omniauth_auto_sign_in_with_provider'] = 'oauth2_generic'

gitlab_rails['omniauth_providers'] = [
{
        'name' =&gt; 'oauth2_generic',
        'app_id' =&gt; 'gitlab',
        'app_secret' =&gt; 'KEYCLOAK SECRET GOES HERE',
        'args' =&gt; {
        client_options: {
                'site' =&gt; 'http://sso.devopstales.intra', # including port if necessary
                'user_info_url' =&gt; '/auth/realms/devopstales/protocol/openid-connect/userinfo',
                'authorize_url' =&gt; '/auth/realms/devopstales/protocol/openid-connect/auth',
                'token_url' =&gt; '/auth/realms/devopstales/protocol/openid-connect/token',
        },
        user_response_structure: {
        #root_path: ['user'], # i.e. if attributes are returned in JsonAPI format (in a 'user' node nested under a 'data' node)
        attributes: { email:'email', first_name:'given_name', last_name:'family_name', name:'name', nickname:'preferred_username' }, # if the nickname attribute of a user is called 'username'
        id_path: 'preferred_username'
        },
        }
}
]

gitlab-ctl reconfigure
</code></pre><h3 id="gitlab-mattermost-config">Gitlab Mattermost config</h3>
<pre tabindex="0"><code># on gitlab gui:
login: admin area / Applications / new
Redirect URI use:
http://mattermost.devopstales.intra/login/gitlab/complete
http://mattermost.devopstales.intra/signup/gitlab/complete

# configfile
nano /etc/gitlab/gitlab.rb

mattermost_external_url 'http://mattermost.devopstales.intra'
mattermost['enable'] = true
mattermost['service_address'] = &quot;127.0.0.1&quot;
mattermost['service_port'] = &quot;8065&quot;
mattermost['sql_driver_name'] = 'postgres'
mattermost['sql_data_source'] = &quot;postgres://mmuser:Password1@127.0.0.1:5432/mattermost?sslmode=disable&amp;connect_timeout=10&quot;
mattermost['log_file_directory'] = '/var/log/gitlab/mattermost/'
mattermost_nginx['enable'] = false

mattermost['gitlab_enable'] = true
mattermost['gitlab_id'] = &quot;&lt;ID&gt;&quot; # oauth id drom gitlab gui
mattermost['gitlab_secret'] = &quot;&lt;token&gt;&quot; # oauth token drom gitlab gui
mattermost['gitlab_scope'] = &quot;&quot;
mattermost['gitlab_auth_endpoint'] = &quot;http://gitlab.devopstales.intra/oauth/authorize&quot;
mattermost['gitlab_token_endpoint'] = &quot;http://gitlab.devopstales.intra/oauth/token&quot;
mattermost['gitlab_user_api_endpoint'] = &quot;http://gitlab.devopstales.intra/api/v4/user&quot;

gitlab-ctl reconfigure
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install keycloak with mysql]]></title>
            <link href="https://devopstales.github.io/home/keycloak1/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/keycloak2/?utm_source=atom_feed" rel="related" type="text/html" title="Install keycloak with postgresql" />
                <link href="https://devopstales.github.io/sso/keycloak1/?utm_source=atom_feed" rel="related" type="text/html" title="Install keycloak with mysql" />
                <link href="https://devopstales.github.io/sso/keycloak2/?utm_source=atom_feed" rel="related" type="text/html" title="Install keycloak with postgresql" />
                <link href="https://devopstales.github.io/home/install-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ceph cluster" />
                <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
            
                <id>https://devopstales.github.io/home/keycloak1/</id>
            
            
            <published>2019-04-05T00:00:00+00:00</published>
            <updated>2019-04-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Keycloak is an open source identity and access management solution.</p>
<h3 id="install-dependencies">Install dependencies</h3>
<pre tabindex="0"><code>yum install -y epel-release
yum install -y java-1.8.0-openjdk-headless tmux nano mariadb-server unzip nginx

cd /opt/
wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.47.zip
unzip mysql-connector-java-5.1.47.zip
</code></pre><h3 id="configure-database">Configure database</h3>
<pre tabindex="0"><code>service mariadb start

mysql -uroot
CREATE DATABASE keycloak CHARACTER SET utf8 COLLATE utf8_unicode_ci;
GRANT ALL PRIVILEGES ON keycloak.* TO 'keycloak'@'%' identified by 'Password1';
GRANT ALL PRIVILEGES ON keycloak.* TO 'keycloak'@'localhost' identified by 'Password1';
FLUSH privileges;
exit;
</code></pre><h3 id="install-keycloak">Install keycloak</h3>
<pre tabindex="0"><code>groupadd -r keycloak
useradd -m -d /var/lib/keycloak -s /sbin/nologin -r -g keycloak keycloak

mkdir -p /opt/keycloak/
cd /opt/keycloak/

# https://www.keycloak.org/downloads.html
wget https://downloads.jboss.org/keycloak/4.8.2.Final/keycloak-4.8.2.Final.tar.gz

tar -xzf keycloak-4.8.2.Final.tar.gz
ln -s /opt/keycloak/keycloak-4.8.2.Final /opt/keycloak/current
chown keycloak: -R /opt/keycloak
sudo -u keycloak chmod 700 /opt/keycloak/current/standalone

mkdir /var/log/keycloak
chown keycloak: -R /var/log/keycloak

chown keycloak: -R /opt/keycloak
sudo -u keycloak chmod 700 /opt/keycloak/current/standalone
</code></pre><pre tabindex="0"><code>echo '[Unit]
Description=Keycloak
After=network.target syslog.target

[Service]
Type=idle
User=keycloak
Group=keycloak
ExecStart=/opt/keycloak/current/bin/standalone.sh -b 0.0.0.0
TimeoutStartSec=600
TimeoutStopSec=600

StandardOutput=syslog
StandardError=syslog
SyslogIdentifier=keycloak

[Install]
WantedBy=multi-user.target
' &gt; /etc/systemd/system/keycloak.service
</code></pre><pre tabindex="0"><code>echo 'if $programname == &quot;keycloak&quot; then /var/log/keycloak/jboss.log
&amp; stop
'&gt;/etc/rsyslog.d/keycloak.conf

systemctl daemon-reload
service rsyslog restart
systemctl start keycloak.service
</code></pre><h3 id="configure-wildfly">Configure wildfly</h3>
<pre tabindex="0"><code>cd /opt/keycloak/current/

./bin/jboss-cli.sh -c 'module add --name=org.mysql  --dependencies=javax.api,javax.transaction.api --resources=/opt/mysql-connector-java-5.1.47/mysql-connector-java-5.1.47.jar'

./bin/jboss-cli.sh 'embed-server,/subsystem=datasources/jdbc-driver=org.mysql:add(driver-name=org.mysql,driver-module-name=org.mysql,driver-class-name=com.mysql.jdbc.Driver)'

./bin/jboss-cli.sh 'embed-server,/subsystem=datasources/data-source=KeycloakDS:remove'

./bin/jboss-cli.sh 'embed-server,/subsystem=datasources/data-source=KeycloakDS:add(driver-name=org.mysql,enabled=true,use-java-context=true,connection-url=&quot;jdbc:mysql://localhost:3306/keycloak?useSSL=false&amp;amp;useLegacyDatetimeCode=false&amp;amp;serverTimezone=Europe/Budapest&amp;amp;characterEncoding=UTF-8&quot;,jndi-name=&quot;java:/jboss/datasources/KeycloakDS&quot;,user-name=keycloak,password=&quot;Password1&quot;,valid-connection-checker-class-name=org.jboss.jca.adapters.jdbc.extensions.mysql.MySQLValidConnectionChecker,validate-on-match=true,exception-sorter-class-name=org.jboss.jca.adapters.jdbc.extensions.mysql.MySQLValidConnectionChecker)'

./bin/add-user-keycloak.sh -u admin -p Password1 -r master

# for nginx proxy
./bin/jboss-cli.sh 'embed-server,/subsystem=undertow/server=default-server/http-listener=default:write-attribute(name=proxy-address-forwarding,value=true)'

./bin/jboss-cli.sh 'embed-server,/socket-binding-group=standard-sockets/socket-binding=proxy-https:add(port=443)'

./bin/jboss-cli.sh 'embed-server,/subsystem=undertow/server=default-server/http-listener=default:write-attribute(name=redirect-socket,value=proxy-https)'

# disabla color in log
./bin/jboss-cli.sh -c '/subsystem=logging/console-handler=CONSOLE:write-attribute(name=named-formatter, value=PATTERN)'
</code></pre><h3 id="configurate-proxy">Configurate proxy</h3>
<pre tabindex="0"><code>systemctl restart keycloak.service

echo 'upstream keycloak {
    # Use IP Hash for session persistence
    ip_hash;

    # List of Keycloak servers
    server 127.0.0.1:8080;
}


server {
    listen 80;
    server_name sso.devopstales.intra;

    # Redirect all HTTP to HTTPS
    location / {
      return 301 https://$server_name$request_uri;
    }
}

server {
    listen 443 ssl http2;
    server_name sso.devopstales.intra;

    ssl_certificate /etc/nginx/ssl/domain.pem;
    ssl_certificate_key /etc/nginx/ssl/domain.pem;
    ssl_session_cache shared:SSL:1m;
    ssl_prefer_server_ciphers on;

    location / {
      proxy_set_header Host $host;
      proxy_set_header X-Real-IP $remote_addr;
      proxy_set_header X-Forwarded-For  $proxy_add_x_forwarded_for;
      proxy_set_header X-Forwarded-Proto  $scheme;
      proxy_pass http://keycloak;
    }
}
' &gt; /etc/nginx/conf.d/keycloak.conf

mkdir /etc/nginx/ssl

systemctl restart nginx

# go to sso.devopstales.intra
# login admin / Password1
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install keycloak with postgresql]]></title>
            <link href="https://devopstales.github.io/home/keycloak2/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/keycloak1/?utm_source=atom_feed" rel="related" type="text/html" title="Install keycloak with mysql" />
                <link href="https://devopstales.github.io/sso/keycloak1/?utm_source=atom_feed" rel="related" type="text/html" title="Install keycloak with mysql" />
                <link href="https://devopstales.github.io/sso/keycloak2/?utm_source=atom_feed" rel="related" type="text/html" title="Install keycloak with postgresql" />
                <link href="https://devopstales.github.io/home/install-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ceph cluster" />
                <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
            
                <id>https://devopstales.github.io/home/keycloak2/</id>
            
            
            <published>2019-04-05T00:00:00+00:00</published>
            <updated>2019-04-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Keycloak is an open source identity and access management solution.</p>
<h3 id="install-dependencies">Install dependencies</h3>
<pre tabindex="0"><code>yum install -y epel-release
yum install -y java-1.8.0-openjdk-headless tmux nano mariadb-server unzip httpd

cd /opt
# https://jdbc.postgresql.org/download.html
wget https://jdbc.postgresql.org/download/postgresql-42.2.5.jar
</code></pre><h3 id="install-and-configure-database">Install and configure database</h3>
<p>In a previous post I wrote about how to <a href="/linux/install-postgresql/#install-postgresql-10-on-centos-7">Install PostgreSQL 10</a></p>
<pre tabindex="0"><code>nano /var/lib/pgsql/10/data/pg_hba.conf
# TYPE  DATABASE        USER            ADDRESS                 METHOD
local   all             keycloak                                md5

su - postgres
createuser keycloak
psql
ALTER USER keycloak WITH ENCRYPTED password 'Password1';
CREATE DATABASE keycloak WITH ENCODING='UTF8' OWNER=keycloak;
\q
</code></pre><h3 id="install-keycloak">Install keycloak</h3>
<pre tabindex="0"><code>groupadd -r keycloak
useradd -m -d /var/lib/keycloak -s /sbin/nologin -r -g keycloak keycloak

mkdir -p /opt/keycloak/
cd /opt/keycloak/

# https://www.keycloak.org/downloads.html
wget https://downloads.jboss.org/keycloak/4.8.2.Final/keycloak-4.8.2.Final.tar.gz

tar -xzf keycloak-4.8.2.Final.tar.gz
ln -s /opt/keycloak/keycloak-4.8.2.Final /opt/keycloak/current
chown keycloak: -R /opt/keycloak
sudo -u keycloak chmod 700 /opt/keycloak/current/standalone

mkdir /var/log/keycloak
chown keycloak: -R /var/log/keycloak

chown keycloak: -R /opt/keycloak
sudo -u keycloak chmod 700 /opt/keycloak/current/standalone
</code></pre><pre tabindex="0"><code>echo '[Unit]
Description=Keycloak
After=network.target syslog.target

[Service]
Type=idle
User=keycloak
Group=keycloak
ExecStart=/opt/keycloak/current/bin/standalone.sh -b 0.0.0.0
TimeoutStartSec=600
TimeoutStopSec=600

StandardOutput=syslog
StandardError=syslog
SyslogIdentifier=keycloak

[Install]
WantedBy=multi-user.target
' &gt; /etc/systemd/system/keycloak.service
</code></pre><pre tabindex="0"><code>echo 'if $programname == &quot;keycloak&quot; then /var/log/keycloak/jboss.log
&amp; stop
'&gt;/etc/rsyslog.d/keycloak.conf

systemctl daemon-reload
service rsyslog restart
systemctl start keycloak.service
</code></pre><h3 id="configure-wildfly">Configure wildfly</h3>
<pre tabindex="0"><code>cd /opt/keycloak/current/modules
mkdir -p org/postgresql/main
cp /opt/postgresql-42.2.5.jar .

echo '&lt;?xml version=&quot;1.0&quot; ?&gt;
&lt;module xmlns=&quot;urn:jboss:module:1.3&quot; name=&quot;org.postgresql&quot;&gt;

    &lt;resources&gt;
        &lt;resource-root path=&quot;postgresql-42.2.5.jar&quot;/&gt;
	&lt;/resources&gt;

	&lt;dependencies&gt;
		&lt;module name=&quot;javax.api&quot;/&gt;
		&lt;module name=&quot;javax.transaction.api&quot;/&gt;
	&lt;/dependencies&gt;
&lt;/module&gt;' &gt; org/postgresql/main/module.xml
</code></pre><pre tabindex="0"><code>cd /opt/keycloak/current/standalone/configuration/
nano standalone.xml
...
        &lt;datasources&gt;
				&lt;datasource jndi-name=&quot;java:jboss/datasources/KeycloakDS&quot; pool-name=&quot;KeycloakDS&quot; enabled=&quot;true&quot; use-java-context=&quot;true&quot;&gt;
					&lt;connection-url&gt;jdbc:postgresql://localhost:5432/keycloak&lt;/connection-url&gt;
					&lt;driver&gt;postgresql&lt;/driver&gt;
					&lt;pool&gt;
						&lt;max-pool-size&gt;20&lt;/max-pool-size&gt;
					&lt;/pool&gt;
					&lt;security&gt;
						&lt;user-name&gt;keycloak&lt;/user-name&gt;
						&lt;password&gt;Password1&lt;/password&gt;
					&lt;/security&gt;
				&lt;/datasource&gt;
...
        &lt;drivers&gt;
					&lt;driver name=&quot;postgresql&quot; module=&quot;org.postgresql&quot;&gt;
						&lt;xa-datasource-class&gt;org.postgresql.xa.PGXADataSource&lt;/xa-datasource-class&gt;
				&lt;/driver&gt;
...
&lt;default-bindings context-service=&quot;java:jboss/ee/concurrency/context/default&quot; datasource=&quot;java:jboss/datasources/KeycloakDS&quot;
</code></pre><pre tabindex="0"><code>cd /opt/keycloak/current
./bin/add-user-keycloak.sh -u admin -p Password1 -r master
systemctl restart keycloak.service
</code></pre><h3 id="configurate-proxy">Configurate proxy</h3>
<pre tabindex="0"><code>echo '&lt;VirtualHost *:80&gt;
    ServerName sso.devopstales.intra

    ProxyPreserveHost On
#    SSLProxyEngine On
#    SSLProxyCheckPeerCN on
#    SSLProxyCheckPeerExpire on
    RequestHeader set X-Forwarded-Proto &quot;https&quot;
    RequestHeader set X-Forwarded-Port &quot;80&quot; #443
    ProxyPass / http://127.0.0.1:8080/
    ProxyPassReverse / http://127.0.0.1:8080/
&lt;/VirtualHost&gt;' &gt; /etc/apache2/sites-available/keycloak.conf

sudo a2enmod headers
a2enmod proxy
a2enmod rewrite
a2ensite keycloak.confcd
service httpd restart

# go to sso.devopstales.intra
# login admin / Password1
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Openproject]]></title>
            <link href="https://devopstales.github.io/home/openproject/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/openproject/</id>
            
            
            <published>2019-04-05T00:00:00+00:00</published>
            <updated>2019-04-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Openproject is a free and open source, web-based project management and issue tracking tool.</p>
<h3 id="install-openproject">Install OpenProject</h3>
<pre tabindex="0"><code>sudo wget -O /etc/yum.repos.d/openproject-ce.repo https://dl.packager.io/srv/opf/openproject-ce/stable/8/installer/el/7.repo

yum install openproject memcached -y
service memcached start
</code></pre><h3 id="install-postgresql">Install Postgresql</h3>
<p>In a previous post I wrote about how to <a href="/linux/install-postgresql/#install-postgresql-9-6-on-centos-7">Install PostgreSQL 10</a></p>
<pre tabindex="0"><code>su - postgres
createuser project
psql
ALTER USER project WITH ENCRYPTED password 'Password1';
CREATE DATABASE project WITH ENCODING='UTF8' OWNER=project;
\q
</code></pre><h3 id="configure-openproject">Configure OpenProject</h3>
<pre tabindex="0"><code>openproject configure
</code></pre><h3 id="reset-password">Reset Password</h3>
<pre tabindex="0"><code>openproject run console

admin = User.find_by(login: 'admin')
admin.password = 'Password11' # minimum 10 characters
admin.password_confirmation = 'Password11'

admin.save! # Watch the output for errors
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Ceph cluster]]></title>
            <link href="https://devopstales.github.io/home/install-ceph/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/ceph-block-device/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph Block Device" />
                <link href="https://devopstales.github.io/home/ceph-cephfs/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph CephFS" />
                <link href="https://devopstales.github.io/linux/install-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ceph cluster" />
                <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
                <link href="https://devopstales.github.io/linux/ceph-block-device/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph Block Device" />
            
                <id>https://devopstales.github.io/home/install-ceph/</id>
            
            
            <published>2019-03-12T00:00:00+00:00</published>
            <updated>2019-03-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Ceph is free and open source distributed objectstorage solution. With Ceph we can easily provide and manage block storage, object storage and file storage.</p>
<h3 id="base-components">Base Components</h3>
<ul>
<li>Monitors (ceph-mon) : As the name suggests a ceph monitor nodes keep an eye on cluster state, OSD Map and Crush map</li>
<li>OSD ( Ceph-osd): These are the nodes which are part of cluster and provides data store, data replication and recovery functionalities. OSD also provides information to monitor nodes.</li>
<li>MDS (Ceph-mds) : It is a ceph meta-data server and stores the meta data of ceph file systems like block storage.</li>
<li>Ceph Deployment Node : It is used to deploy the Ceph cluster, it is also called as Ceph-admin or Ceph-utility node.</li>
</ul>
<h3 id="environment">Environment</h3>
<pre tabindex="0"><code>192.168.1.31    ceph01
192.168.1.32    ceph02
192.168.1.33    ceph03
</code></pre><h3 id="install-requirements">Install Requirements</h3>
<pre tabindex="0"><code># all hosts
yum install ntp ntpdate ntp-doc epel-release -y
ntpdate europe.pool.ntp.org
systemctl start ntpd
systemctl enable ntpd

useradd cephuser &amp;&amp; echo &quot;Password1&quot; | passwd --stdin cephuser
echo &quot;cephuser ALL = (root) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/cephuser
chmod 0440 /etc/sudoers.d/cephuser

sed -i --follow-symlinks 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux
systemctl mask firewalld

reboot
</code></pre><pre tabindex="0"><code># cep01
ssh-keygen
ssh-copy-it ceph02
ssh-copy-it ceph03

nano ~/.ssh/config
Host ceph01
   Hostname ceph01
   User cephuser
Host ceph02
   Hostname ceph02
   User cephuser
Host ceph03
   Hostname ceph03
   User cephuser

chmod 644 ~/.ssh/config
</code></pre><h3 id="install-ceph-deployer">Install ceph-deployer</h3>
<pre tabindex="0"><code># cep01
sudo rpm -Uvh https://download.ceph.com/rpm-mimic/el7/noarch/ceph-release-1-1.el7.noarch.rpm

yum update
yum install -y ceph-deploy

mkdir /home/ceph/cluster1
cd ~/cluster1
</code></pre><h3 id="delete-ceph-config-if-exists">Delete ceph config if exists</h3>
<pre tabindex="0"><code># cep01
ceph-deploy purge ceph01 ceph02 ceph03
# unmount if not working
ceph-deploy purgedata ceph01 ceph02 ceph03
ceph-deploy forgetkeys
</code></pre><h3 id="install-ceph">Install ceph</h3>
<pre tabindex="0"><code>ceph-deploy install ceph01 ceph02 ceph03
ceph-deploy --cluster &lt;cluster-name&gt; new ceph01 ceph02 ceph03

# edit before pupulate config
nano &lt;cluster-name&gt;.conf
osd_max_object_name_len = 256
osd_max_object_namespace_len = 64

ceph-deploy --cluster &lt;cluster-name&gt; mon create ceph01 ceph02 ceph03
ceph-deploy --cluster &lt;cluster-name&gt; gatherkeys ceph01 ceph02 ceph03
</code></pre><pre tabindex="0"><code>ceph-deploy disk list ceph01
ceph-deploy disk list ceph02
ceph-deploy disk list ceph03


ceph-deploy disk zap ceph01:sdb
ceph-deploy disk zap ceph02:sdb
ceph-deploy disk zap ceph03:sdb

ceph-deploy osd create ceph01:sdb
ceph-deploy osd create ceph02:sdb
ceph-deploy osd create ceph03:sdb

ceph-deploy osd create ceph01:sdc
ceph-deploy osd create ceph02:sdc
ceph-deploy osd create ceph03:sdc
</code></pre><h3 id="test-cluster">Test cluster</h3>
<pre tabindex="0"><code>sudo ceph health
sudo ceph -s
sudo ceph osd tree

# install adminkey-ring
ceph-deploy admin ceph01 ceph02 ceph03
ssh ceph node01 sudo ceph osd lspools
ssh ceph node01 sudo ceph osd create mycorp 128
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Openshift]]></title>
            <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
                <link href="https://devopstales.github.io/home/install-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ceph cluster" />
                <link href="https://devopstales.github.io/home/ceph-block-device/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph Block Device" />
                <link href="https://devopstales.github.io/home/ceph-cephfs/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph CephFS" />
                <link href="https://devopstales.github.io/home/kubectl-multi-cluster-config/?utm_source=atom_feed" rel="related" type="text/html" title="Configure kubectl for multiple clusters" />
            
                <id>https://devopstales.github.io/home/ansible-openshift-install/</id>
            
            
            <published>2019-03-12T00:00:00+00:00</published>
            <updated>2019-03-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Ansible-openshift is a pre made ansible playbook for Openshift installation. In this Post I will show you how to use to install a new Openshift cluster.</p>
<H3>Parst of the Openshift series</H3>
<ul>
     <li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
     <li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
     <li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
     <li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
     <li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
     <li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
     <li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
     <li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
     <li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
     <li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
     <li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
     <li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
     <li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
     <li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
     <li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
     <li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
     <li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
     <li>Part16a: <a href="../../kubernetes/openshift-elasticsearch-error/">Opeshift elasticsearch search-guard error</a></li>
     <li>Part16b: <a href="../../kubernetes/openshift-log4shell/">Openshift: Log4Shell - Remote Code Execution (CVE-2021-44228) (CVE-2021-4104)</a></li>
</ul>


<h3 id="environment">Environment</h3>
<pre tabindex="0"><code>192.168.1.40    deployer
192.168.1.41    openshift01 # master node
192.168.1.42    openshift02 # infra node
192.168.1.43    openshift03 # worker node

# hardware requirement
4 CPU
16G RAM
</code></pre><h3 id="dns-config">DNS config</h3>
<pre tabindex="0"><code>master.openshift     300 IN  A 192.168.1.41
openshift            300 IN  A 192.168.1.42
*.openshift            300 IN  A 192.168.1.42
</code></pre><h3 id="prerequirement">Prerequirement</h3>
<pre tabindex="0"><code># deployer
yum install epel-release centos-release-openshift-origin311
yum --disablerepo=* --enablerepo=centos-ansible26 install ansible
yum install openshift-ansible nano

echo &quot;exclude=ansible&quot; &gt;&gt; /etc/yum.conf

nano ~/.ssh/config
Host openshift01
    Hostname openshift01.devopstales.intra
    User origin

Host openshift02
    Hostname openshift02.devopstales.intra
    User origin

Host openshift03
    Hostname openshift03.devopstales.intra
    User origin
</code></pre><pre tabindex="0"><code># on all openshift hosts
hostnamectl set-hostname openshift01
yum -y update
yum -y install centos-release-openshift-origin311 epel-release docker git pyOpenSSL

useradd origin
passwd origin
echo -e 'Defaults:origin !requiretty\norigin ALL = (root) NOPASSWD:ALL' | tee /etc/sudoers.d/origin
chmod 440 /etc/sudoers.d/origin
reboot

# Disable swap permanently
nano /etc/fstab
#/dev/mapper/centos_openshift01-swap swap                    swap    defaults        0 0

sudo swapoff -a

sudo lvremove -Ay /dev/centos/swap
sudo lvextend -l +100%FREE centos/root
sudo xfs_growfs /

sudo nano /etc/default/grub
GRUB_TIMEOUT=5
GRUB_DEFAULT=saved
GRUB_DISABLE_SUBMENU=true
GRUB_TERMINAL_OUTPUT=&quot;console&quot;
# GRUB_CMDLINE_LINUX=&quot;rd.lvm.lv=centos/root rd.lvm.lv=centos/swap crashkernel=auto rhgb quiet&quot;
GRUB_CMDLINE_LINUX=&quot;rd.lvm.lv=centos/root crashkernel=auto rhgb quiet&quot;
GRUB_DISABLE_RECOVERY=&quot;true&quot;

dracut --regenerate-all -f
grub2-mkconfig -o /boot/grub2/grub.cfg
</code></pre><h3 id="configurate-installer">Configurate Installer</h3>
<pre tabindex="0"><code># deployer

nano /etc/ansible/hosts
[OSEv3:children]
masters
nodes
etcd

[OSEv3:vars]
# admin user created in previous section
ansible_ssh_user=origin
ansible_become=true
openshift_deployment_type=origin
os_firewall_use_firewalld=True
openshift_clock_enabled=true

# use HTPasswd for authentication
openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'}]

# define default sub-domain for Master node
openshift_master_default_subdomain=openshift.devopstales.intra
osm_default_subdomain=openshift.devopstales.intra

# allow unencrypted connection within cluster
openshift_docker_insecure_registries=172.30.0.0/16

openshift_master_cluster_hostname=master.openshift.devopstales.intra
openshift_master_cluster_public_hostname=master.openshift.devopstales.intra
openshift_public_hostname=master.openshift.devopstales.intra

openshift_master_api_port=443
openshift_master_console_port=443

[masters]
openshift01 containerized=true openshift_public_hostname=master.openshift.devopstales.intra

[etcd]
openshift01 containerized=true

[nodes]
# defined values for [openshift_node_group_name] in the file below
# [/usr/share/ansible/openshift-ansible/roles/openshift_facts/defaults/main.yml]
openshift01 openshift_node_group_name='node-config-master'
openshift02 openshift_node_group_name='node-config-infra'
openshift03 openshift_node_group_name='node-config-compute'
</code></pre><h3 id="run-the-installer">Run the Installer</h3>
<pre tabindex="0"><code># deployer
cd /usr/share/ansible/openshift-ansible/
sudo ansible-playbook playbooks/prerequisites.yml
sudo ansible-playbook playbooks/deploy_cluster.yml

# If installastion failed or went wrong, the following uninstallation script can be run, and running installation can be tried again:
sudo ansible-playbook playbooks/adhoc/uninstall.yml
</code></pre><h3 id="user-management">User management</h3>
<pre tabindex="0"><code># on openshift master

cd /etc/origin/master/
# add user
htpasswd [/path/to/users.htpasswd] [user_name]
htpasswd htpasswd devopstales

# delete user
htpasswd -D [htpasswd/file/path/]  [user-name] [password]
htpasswd -D htpasswd devopstales Password1

# it will remove only the username from the htpasswd file by default it won’t remove user identity
oc delete  identity htpasswd_auth:user
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Use Ceph Block Device]]></title>
            <link href="https://devopstales.github.io/home/ceph-block-device/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/install-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ceph cluster" />
                <link href="https://devopstales.github.io/home/ceph-cephfs/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph CephFS" />
                <link href="https://devopstales.github.io/linux/ceph-block-device/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph Block Device" />
                <link href="https://devopstales.github.io/linux/install-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ceph cluster" />
                <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
            
                <id>https://devopstales.github.io/home/ceph-block-device/</id>
            
            
            <published>2019-03-12T00:00:00+00:00</published>
            <updated>2019-03-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Ceph block devices are thin-provisioned, resizable and store data striped over multiple OSDs in a Ceph cluster. Ceph block devices leverage RADOS capabilities such as snapshotting, replication and consistency. Ceph’s RADOS Block Devices (RBD) interact with OSDs using kernel modules or the librbd library.</p>
<h3 id="environment">Environment</h3>
<pre tabindex="0"><code>192.168.1.31    ceph01
192.168.1.32    ceph02
192.168.1.33    ceph03
</code></pre><h3 id="install-client-cli">Install Client cli</h3>
<pre tabindex="0"><code>ceph-deploy install client
ceph-deploy admin client

sudo chmod 644 /etc/ceph/ceph.client.admin.keyring
</code></pre><h3 id="basic-usage">Basic Usage</h3>
<pre tabindex="0"><code># create pool
ceph osd pool create stack 64 64

# createdisk to pool
rbd create disk01 --size 10G --image-feature layering --pool stack --allow-shrink

# show list
rbd ls -l

# show info
rbd --image disk01 -p stack info

# resize
rbd resize --image disk01 -p stack --size 6G

# remove
rbd rm disk01 -p stack

# map the image to device
sudo rbd map disk01

# show mapping
rbd showmapped

# format with XFS
sudo mkfs.xfs /dev/rbd0

# mount device
sudo mount /dev/rbd0 /mnt
df -hT
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Use Ceph CephFS]]></title>
            <link href="https://devopstales.github.io/home/ceph-cephfs/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/install-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ceph cluster" />
                <link href="https://devopstales.github.io/home/ceph-block-device/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph Block Device" />
                <link href="https://devopstales.github.io/linux/install-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ceph cluster" />
                <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
                <link href="https://devopstales.github.io/linux/ceph-block-device/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph Block Device" />
            
                <id>https://devopstales.github.io/home/ceph-cephfs/</id>
            
            
            <published>2019-03-12T00:00:00+00:00</published>
            <updated>2019-03-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>The Ceph Filesystem (CephFS) is a POSIX-compliant filesystem that uses a Ceph Storage Cluster to store its data. The Ceph filesystem uses the same Ceph Storage Cluster system as Ceph Block Devices, Ceph Object Storage with its S3 and Swift APIs, or native bindings (librados).</p>
<h3 id="environment">Environment</h3>
<pre tabindex="0"><code>192.168.1.31    ceph01
192.168.1.32    ceph02
192.168.1.33    ceph03
</code></pre><h3 id="prerequirements">Prerequirements</h3>
<pre tabindex="0"><code># Create MDS (MetaData Server)
ceph-deploy --overwrite-conf mds create ceph02

# create pools
sudo ceph osd pool create cephfs_data 128
sudo ceph osd pool create cephfs_metadata 128

# enable pools
sudo ceph fs new cephfs cephfs_metadata cephfs_data

# show pools
sudo ceph osd lspools
sudo ceph fs ls
sudo ceph mds stat
</code></pre><h3 id="basic-usage">Basic Usage</h3>
<pre tabindex="0"><code># Mount CephFS on a Client.

yum -y install ceph-fuse
ssh ceph@ceph01 &quot;sudo ceph-authtool -p /etc/ceph/ceph.client.admin.keyring&quot; &gt; admin.key
chmod 600 admin.key

mount -t ceph ceph01:6789:/ /mnt -o name=admin,secretfile=admin.key
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configure kubectl for multiple clusters]]></title>
            <link href="https://devopstales.github.io/home/kubectl-multi-cluster-config/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/proxmox-backup-pve-zsync/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with pve-zsync" />
                <link href="https://devopstales.github.io/home/proxmox-backup-sanoid/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with sanoid" />
                <link href="https://devopstales.github.io/home/proxmox-backup-znapzend/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with znapzend" />
                <link href="https://devopstales.github.io/home/proxmox-node-remove/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox node removal" />
                <link href="https://devopstales.github.io/home/pfsense-telegraf/?utm_source=atom_feed" rel="related" type="text/html" title="Install telegraf on pfsense" />
            
                <id>https://devopstales.github.io/home/kubectl-multi-cluster-config/</id>
            
            
            <published>2019-03-11T00:00:00+00:00</published>
            <updated>2019-03-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I use a multiple Kubernetes clusters on a daily basis, so keeping my configs organized is important to don’t confuse myself.</p>
<p>kubectl looks at an environment variable called KUBECONFIG to hold a colon-separated list of paths to configuration files, so I can use multiple cluster config files.</p>
<h3 id="download-kubernetes-config">Download kubernetes config</h3>
<pre tabindex="0"><code>scp root@DEV_SERVER:/etc/kubernetes/admin.conf ~/.kube/dev-config
scp root@TST_SERVER:/etc/kubernetes/admin.conf ~/.kube/tst-config
scp root@UAT_SERVER:/etc/kubernetes/admin.conf ~/.kube/uat-config
scp root@PROD_SERVER:/etc/kubernetes/admin.conf ~/.kube/prod-config
</code></pre><h3 id="edit-config-files">Edit config files</h3>
<pre tabindex="0"><code>nano ~/.kube/dev-config
...
- cluster:
    server: https://1.1.1.1:6443
  name: dev-config
...
contexts:
- context:
    cluster: dev-config
    user: dev-admin
  name: dev-config
...
users:
- name: dev-admin
...
</code></pre><h3 id="use-config-files-in-kubeconfig-variable">Use config files in KUBECONFIG variable</h3>
<pre tabindex="0"><code>nano ~/.bashrc
export KUBECONFIG=$HOME/.kube/dev-config:$HOME/.kube/tst-config:$HOME/.kube/uat-config:$HOME/.kube/prod-config

echo $KUBECONFIG

/home/ME/.kube/dev-config:/home/ME/.kube/tst-config:/home/ME/.kube/uat-config:/home/ME/.kube/prod-config

source ~/.bashrc
</code></pre><h3 id="use-clusters-with-kubectl">Use clusters with kubectl</h3>
<pre tabindex="0"><code># get the current context
kubectl config current-context

# use a different context
kubectl config use-context work-dev
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Proxmox backup with pve-zsync]]></title>
            <link href="https://devopstales.github.io/home/proxmox-backup-pve-zsync/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/proxmox-backup-sanoid/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with sanoid" />
                <link href="https://devopstales.github.io/home/proxmox-backup-znapzend/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with znapzend" />
                <link href="https://devopstales.github.io/home/proxmox-node-remove/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox node removal" />
                <link href="https://devopstales.github.io/home/kubectl-multi-cluster-config/?utm_source=atom_feed" rel="related" type="text/html" title="Configure kubectl for multiple clusters" />
                <link href="https://devopstales.github.io/linux/proxmox-backup-pve-zsync/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with pve-zsync" />
            
                <id>https://devopstales.github.io/home/proxmox-backup-pve-zsync/</id>
            
            
            <published>2019-03-11T00:00:00+00:00</published>
            <updated>2019-03-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Backup your machines on Pfsense with pve-zsync tool.</p>
<h3 id="the-servers">The servers</h3>
<pre tabindex="0"><code>proxmox-node1 192.168.10.20
omv-nas1 192.168.10.50
</code></pre><h3 id="install-pve-zsync-on-servers">Install pve-zsync on servers</h3>
<pre tabindex="0"><code>apt-get install pve-zsync
</code></pre><h3 id="configure-pve-zsync">Configure pve-zsync</h3>
<pre tabindex="0"><code>pve-zsync create --source 107 --dest 192.168.10.50:tank --verbose --maxsnap 14 --name Backup_ZFS_srv_107

# test the config
cat /etc/cron.d/pve-zsync
* 8 * * * root pve-zsync sync --source 107 --dest 192.168.10.50:tank --name Backup_ZFS_srv_107 --maxsnap 14 --method ssh

# send diff
pve-zsync sync --source 107 --dest 192.168.10.50:tank --verbose --maxsnap 14 --name Backup_ZFS_srv_107

# the tool send the vm config to the /var/lib/pve-zsync/
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Proxmox backup with sanoid]]></title>
            <link href="https://devopstales.github.io/home/proxmox-backup-sanoid/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/proxmox-backup-pve-zsync/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with pve-zsync" />
                <link href="https://devopstales.github.io/home/proxmox-backup-znapzend/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with znapzend" />
                <link href="https://devopstales.github.io/home/proxmox-node-remove/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox node removal" />
                <link href="https://devopstales.github.io/home/kubectl-multi-cluster-config/?utm_source=atom_feed" rel="related" type="text/html" title="Configure kubectl for multiple clusters" />
                <link href="https://devopstales.github.io/linux/proxmox-backup-pve-zsync/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with pve-zsync" />
            
                <id>https://devopstales.github.io/home/proxmox-backup-sanoid/</id>
            
            
            <published>2019-03-11T00:00:00+00:00</published>
            <updated>2019-03-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Backup your machines on Pfsense with sanoid tool.</p>
<h3 id="the-servers">The servers</h3>
<pre tabindex="0"><code>proxmox-node1 192.168.10.20
omv-nas1 192.168.10.50
</code></pre><h3 id="build-sanoid-on-servers">Build sanoid on servers</h3>
<pre tabindex="0"><code>apt-get install libcapture-tiny-perl libconfig-inifiles-perl git

cd /opt
git clone https://github.com/jimsalterjrs/sanoid

ln /opt/sanoid/sanoid /usr/sbin/
</code></pre><p>Or you can build deb package:</p>
<h3 id="build-and-install-sanoid-deb-package">Build and install sanoid deb package</h3>
<pre tabindex="0"><code>sudo apt-get install devscripts debhelper dh-systemd
git clone https://github.com/jimsalterjrs/sanoid.git
cd sanoid
debuild -us -uc

cd ..
sudo apt-get install libconfig-inifiles-perl
sudo dpkg -i sanoid_2.0.1_all.deb
</code></pre><h3 id="configure-sanoid">Configure sanoid</h3>
<pre tabindex="0"><code>mkdir -p /etc/sanoid
cp /opt/sanoid/sanoid.conf /etc/sanoid/sanoid.conf
cp /opt/sanoid/sanoid.defaults.conf /etc/sanoid/sanoid.defaults.conf

nano /etc/crontab
* 2 * * * root /usr/sbin/sanoid --cron
* 3 * * * root /usr/sbin/syncoid --recursive tank root@192.168.10.50:tank

</code></pre><pre tabindex="0"><code>    ####################
    # sanoid.conf file #
    ####################
    [local-zfs]
            use_template = production
    #############################
    # templates below this line #
    #############################
    [template_production]
            # store hourly snapshots 36h
            # hourly = 36
            # store 14 days of daily snaps
            daily = 14
            # store back 6 months of monthly
            # monthly = 6
            # store back 3 yearly (remove manually if to large)
            # yearly = 3
            # create new snapshots
            autosnap = yes
            # clean old snapshot
            autoprune = yes
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Proxmox backup with znapzend]]></title>
            <link href="https://devopstales.github.io/home/proxmox-backup-znapzend/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/proxmox-backup-znapzend/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with znapzend" />
                <link href="https://devopstales.github.io/home/proxmox-backup-pve-zsync/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with pve-zsync" />
                <link href="https://devopstales.github.io/home/proxmox-backup-sanoid/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with sanoid" />
                <link href="https://devopstales.github.io/home/proxmox-node-remove/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox node removal" />
                <link href="https://devopstales.github.io/home/kubectl-multi-cluster-config/?utm_source=atom_feed" rel="related" type="text/html" title="Configure kubectl for multiple clusters" />
            
                <id>https://devopstales.github.io/home/proxmox-backup-znapzend/</id>
            
            
            <published>2019-03-11T00:00:00+00:00</published>
            <updated>2019-03-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Backup your machines on Pfsense with znapzend tool.</p>
<h3 id="the-servers">The servers</h3>
<pre tabindex="0"><code>proxmox-node1 192.168.10.20
omv-nas1 192.168.10.50
</code></pre><h3 id="create-sub-volume">Create sub volume</h3>
<p>Note that while recursive configurations are well supported to set up backup and retention policies for a whole dataset subtree under the dataset to which you have applied explicit configuration, at this time pruning of such trees (&ldquo;I want every dataset under var except var/tmp&rdquo;) is not supported.</p>
<pre tabindex="0"><code>zfs create local-zfs/vm-data
pvesm add zfspool local-zfs --pool local-zfs/vm-data
</code></pre><h3 id="build-znapzend-on-servers">Build znapzend on servers</h3>
<pre tabindex="0"><code>apt-get install perl unzip git mbuffer build-essential git

cd /root
git clone https://github.com/oetiker/znapzend
cd /root/znapzend
./configure --prefix=/opt/znapzend

make
make install

ln -s /opt/znapzend/bin/znapzend /usr/local/bin/znapzend
ln -s /opt/znapzend/bin/znapzendzetup /usr/local/bin/znapzendzetup
ln -s /opt/znapzend/bin/znapzendztatz /usr/local/bin/znapzendztatz

znapzend --version
</code></pre><p>Or you can download a the deb package from here:
<a href="https://github.com/devopstales/znapzend-debian/releases">https://github.com/devopstales/znapzend-debian/releases</a></p>
<h3 id="install-znapzend-on-servers">Install znapzend on servers</h3>
<pre tabindex="0"><code>dpkg -i znapzend_0.19.1_amd64_stretch.deb
</code></pre><h3 id="configure-znapzend">Configure znapzend</h3>
<pre tabindex="0"><code>znapzendzetup create --recursive\
--mbuffer=/usr/bin/mbuffer \
--mbuffersize=1G \
SRC '2d=&gt;1d' local-zfs/vmdata \
DST:a '14d=&gt;1d' root@192.168.10.50:tank

# test
znapzend --debug --noaction --runonce=local-zfs
znapzendzetup list
</code></pre><h3 id="create-znapzend-service">Create znapzend service</h3>
<pre tabindex="0"><code>nano /etc/default/znapzend
ZNAPZENDOPTIONS=&quot;--logto=/var/log/znapzend.log&quot;

systemctl enable znapzend.service
systemctl restart znapzend.service
systemctl status znapzend.service
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Proxmox node removal]]></title>
            <link href="https://devopstales.github.io/home/proxmox-node-remove/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/proxmox-node-remove/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox node removal" />
                <link href="https://devopstales.github.io/home/pfsense-telegraf/?utm_source=atom_feed" rel="related" type="text/html" title="Install telegraf on pfsense" />
                <link href="https://devopstales.github.io/home/prometheus-influxdb/?utm_source=atom_feed" rel="related" type="text/html" title="Install Prometheus with Influxdb storage" />
                <link href="https://devopstales.github.io/home/install-postgresql/?utm_source=atom_feed" rel="related" type="text/html" title="Install PostgreSQL" />
                <link href="https://devopstales.github.io/home/naemon-influxdb/?utm_source=atom_feed" rel="related" type="text/html" title="Install Nemon with Influxdb storage" />
            
                <id>https://devopstales.github.io/home/proxmox-node-remove/</id>
            
            
            <published>2019-03-07T00:00:00+00:00</published>
            <updated>2019-03-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>The correct way to remove nod from proxmox cluster.</p>
<h3 id="display-all-active-nodes">Display all active nodes</h3>
<pre tabindex="0"><code>root@proxmox-node2:~# pvecm nodes
Membership information
----------------------
Nodeid Votes Name
1 1 proxmox-node1 (local)
2 1 proxmox-node2
3 1 proxmox-node3
4 1 proxmox-node4
</code></pre><h3 id="shutdown-node-and-remove">Shutdown node and remove</h3>
<pre tabindex="0"><code>root@proxmox-node2:~# pvecm delnode proxmox-node3

root@proxmox-node2:~# ls -l /etc/pve/nodes/
proxmox-node1 proxmox-node2 proxmox-node3 proxmox-node4

root@proxmox-node2:~# rm -rf /etc/pve/nodes/proxmox-node3
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install telegraf on pfsense]]></title>
            <link href="https://devopstales.github.io/home/pfsense-telegraf/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/monitoring/pfsense-telegraf/?utm_source=atom_feed" rel="related" type="text/html" title="Install telegraf on pfsense" />
                <link href="https://devopstales.github.io/home/prometheus-alertmanagger/?utm_source=atom_feed" rel="related" type="text/html" title="Install Alertmanagger" />
                <link href="https://devopstales.github.io/home/prometheus-influxdb/?utm_source=atom_feed" rel="related" type="text/html" title="Install Prometheus with Influxdb storage" />
                <link href="https://devopstales.github.io/home/install-postgresql/?utm_source=atom_feed" rel="related" type="text/html" title="Install PostgreSQL" />
                <link href="https://devopstales.github.io/home/naemon-influxdb/?utm_source=atom_feed" rel="related" type="text/html" title="Install Nemon with Influxdb storage" />
            
                <id>https://devopstales.github.io/home/pfsense-telegraf/</id>
            
            
            <published>2019-03-06T00:00:00+00:00</published>
            <updated>2019-03-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Install and configure telegraf on pfsense to provides system information to prometheus.</p>
<h3 id="install-telegraf">Install telegraf</h3>
<ul>
<li>At the System / Package menu install the telegraf service to the pfsense.</li>
<li>ssh to the pfsense server and open a shell</li>
</ul>
<p><img src="/img/include/pfsense-telegraf.png" alt="Example image"  class="zoomable" /></p>
<h3 id="install-nano">Install nano</h3>
<pre tabindex="0"><code>pkg
pkg update
pkg install nano
</code></pre><h3 id="configure-telegraf">Configure telegraf</h3>
<pre tabindex="0"><code>cd /usr/local/etc

nano telegraf.conf
[[outputs.prometheus_client]]
 listen = &quot;:9273&quot;

echo &quot;telegraf_enable=&quot;YES&quot;&quot; &gt;&gt; /etc/rc.conf

cd /usr/local/etc/rc.d
./telegraf restart
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Prometheus with Influxdb storage]]></title>
            <link href="https://devopstales.github.io/home/prometheus-influxdb/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/prometheus-node-exporter/?utm_source=atom_feed" rel="related" type="text/html" title="Install Node-exporter" />
                <link href="https://devopstales.github.io/home/prometheus-install/?utm_source=atom_feed" rel="related" type="text/html" title="Prometheus Install" />
                <link href="https://devopstales.github.io/monitoring/prometheus-influxdb/?utm_source=atom_feed" rel="related" type="text/html" title="Install Prometheus with Influxdb storage" />
                <link href="https://devopstales.github.io/home/naemon-influxdb/?utm_source=atom_feed" rel="related" type="text/html" title="Install Nemon with Influxdb storage" />
                <link href="https://devopstales.github.io/home/install-postgresql/?utm_source=atom_feed" rel="related" type="text/html" title="Install PostgreSQL" />
            
                <id>https://devopstales.github.io/home/prometheus-influxdb/</id>
            
            
            <published>2019-02-02T00:00:00+00:00</published>
            <updated>2019-02-02T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Example article description</blockquote><p>Use Influxdb to as storage for Prometheus.</p>
<h3 id="install-infludxb">Install Infludxb</h3>
<pre tabindex="0"><code>cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/influxdb.repo
[influxdb]
name = InfluxDB Repository - RHEL \$releasever
baseurl = https://repos.influxdata.com/rhel/\$releasever/\$basearch/stable
enabled = 1
gpgcheck = 1
gpgkey = https://repos.influxdata.com/influxdb.key
EOF

yum install influxdb -y
</code></pre><h3 id="configure-influxdb">Configure Influxdb</h3>
<pre tabindex="0"><code>nano /etc/influxdb/influxdb.conf
[http]
   enabled = true
   bind-address = &quot;localhost:8086&quot;
   auth-enabled = false
</code></pre><h3 id="configure-prometheus">Configure Prometheus</h3>
<pre tabindex="0"><code>nano /etc/prometheus/prometheus.yml
remote_write:
  - url: &quot;http://localhost:8086/api/v1/prom/write?db=prometheus&quot;

remote_read:
  - url: &quot;http://localhost:8086/api/v1/prom/read?db=prometheus&quot;

# with authentication
#remote_write:
#  - url: &quot;http://localhost:8086/api/v1/prom/write?db=prometheus&amp;u=username&amp;p=password&quot;

#remote_read:
#  - url: &quot;http://localhost:8086/api/v1/prom/read?db=prometheus&amp;u=username&amp;p=password&quot;
</code></pre><pre tabindex="0"><code>systemctl start influxdb
systemctl enable influxdb

echo 'CREATE DATABASE &quot;prometheus&quot;' | influx

systemctl start prometheus
systemctl status prometheus

influx
USE prometheus
select * from /.*/ limit 1
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install PostgreSQL]]></title>
            <link href="https://devopstales.github.io/home/install-postgresql/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/install-postgresql/?utm_source=atom_feed" rel="related" type="text/html" title="Install PostgreSQL" />
                <link href="https://devopstales.github.io/home/naemon-influxdb/?utm_source=atom_feed" rel="related" type="text/html" title="Install Nemon with Influxdb storage" />
                <link href="https://devopstales.github.io/home/prometheus-alertmanagger/?utm_source=atom_feed" rel="related" type="text/html" title="Install Alertmanagger" />
                <link href="https://devopstales.github.io/home/prometheus-node-exporter/?utm_source=atom_feed" rel="related" type="text/html" title="Install Node-exporter" />
                <link href="https://devopstales.github.io/home/prometheus-install/?utm_source=atom_feed" rel="related" type="text/html" title="Prometheus Install" />
            
                <id>https://devopstales.github.io/home/install-postgresql/</id>
            
            
            <published>2019-01-10T00:00:00+00:00</published>
            <updated>2019-01-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this pos I will show you how to install Postgresql on difrent Linux distributions.</p>
<p>PostgreSQL, also known as Postgres, is a free and open-source relational database management system (RDBMS).</p>
<h3 id="install-postgresql-96-on-centos-7">Install PostgreSQL 9.6 on CentOS 7</h3>
<pre tabindex="0"><code>yum install epel-release -y
yum update -y
rpm -Uvh https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-centos10-10-2.noarch.rpm

yum install -y postgresql96-server postgresql96 postgresql96-contrib unzip
/usr/pgsql-9.6/bin/postgresql96-setup initdb

nano /var/lib/pgsql/9.6/data/pg_hba.conf
local   all             all                                      trust

sudo systemctl start postgresql-9.6
sudo systemctl enable postgresql-9.6
</code></pre><h3 id="install-postgresql-10-on-centos-7">Install PostgreSQL 10 on CentOS 7</h3>
<pre tabindex="0"><code>yum install epel-release -y
yum update -y
rpm -Uvh https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-centos10-10-2.noarch.rpm

yum -y install postgresql10-server postgresql10-contrib postgresql10
/usr/pgsql-10/bin/postgresql-10-setup initdb

nano /var/lib/pgsql/10/data/pg_hba.conf
host    all             all             127.0.0.1/32            md5
host    all             all             ::1/128                 md5
local   all             postgres                                peer

sudo systemctl start postgresql-10
sudo systemctl enable postgresql-10
</code></pre><h3 id="install-postgresql-10-on-centos-8">Install PostgreSQL 10 on CentOS 8</h3>
<pre tabindex="0"><code>yum install postgresql postgresql-server postgresql-contrib -y

postgresql-setup --initdb

nano /var/lib/pgsql/data/pg_hba.conf
host    all             all             127.0.0.1/32            md5
host    all             all             ::1/128                 md5
local   all             postgres                                peer

systemctl start postgresql
systemctl enable postgresql
</code></pre><h3 id="install-postgresql-10-on-debian">Install PostgreSQL 10 on Debian</h3>
<pre tabindex="0"><code>wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
sudo sh -c 'echo &quot;deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -sc)-pgdg main&quot; &gt; /etc/apt/sources.list.d/PostgreSQL.list'

apt update
apt-get install postgresql-10
apt-get install pgadmin4

systemctl enable postgresql.service
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Nemon with Influxdb storage]]></title>
            <link href="https://devopstales.github.io/home/naemon-influxdb/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/monitoring/naemon-influxdb/?utm_source=atom_feed" rel="related" type="text/html" title="Install Nemon with Influxdb storage" />
                <link href="https://devopstales.github.io/home/prometheus-alertmanagger/?utm_source=atom_feed" rel="related" type="text/html" title="Install Alertmanagger" />
                <link href="https://devopstales.github.io/home/prometheus-node-exporter/?utm_source=atom_feed" rel="related" type="text/html" title="Install Node-exporter" />
                <link href="https://devopstales.github.io/home/prometheus-install/?utm_source=atom_feed" rel="related" type="text/html" title="Prometheus Install" />
                <link href="https://devopstales.github.io/monitoring/prometheus-alertmanagger/?utm_source=atom_feed" rel="related" type="text/html" title="Install Alertmanagger" />
            
                <id>https://devopstales.github.io/home/naemon-influxdb/</id>
            
            
            <published>2019-01-01T00:00:00+00:00</published>
            <updated>2019-01-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I prefer to use Naemon (a fork of nagos) with Influxdb as a storage for graphical data.</p>
<h3 id="install-naemon">Install Naemon</h3>
<pre tabindex="0"><code>yum install epel-release nano -y
yum install httpd php php-gd -y

rpm -Uvh &quot;https://labs.consol.de/repo/stable/rhel7/x86_64/labs-consol-stable.rhel7.noarch.rpm&quot;

yum install naemon* -y
yum install nagios-plugins nagios-plugins-all nagios-plugins-nrpe nrpe -y

nano /etc/php.ini
date.timezone = Europe/Budapest


systemctl enable httpd
systemctl enable naemon
systemctl start httpd
systemctl start naemon

htpasswd /etc/thruk/htpasswd thrukadmin
# http://SERVER-IP/naemon
</code></pre><h3 id="install-infludxb">Install Infludxb</h3>
<pre tabindex="0"><code>cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/influxdb.repo
[influxdb]
name = InfluxDB Repository - RHEL \$releasever
baseurl = https://repos.influxdata.com/rhel/\$releasever/\$basearch/stable
enabled = 1
gpgcheck = 1
gpgkey = https://repos.influxdata.com/influxdb.key
EOF

yum install influxdb -y

nano /etc/influxdb/influxdb.conf
[http]
   enabled = true
   bind-address = &quot;localhost:8086&quot;
   auth-enabled = false

systemctl start influxdb
systemctl enable influxdb
</code></pre><h3 id="configurate-naemon">Configurate Naemon</h3>
<pre tabindex="0"><code>sed -i &quot;s@^process_performance_data=0@#process_performance_data=0@&quot; /etc/naemon/naemon.cfg
# config nagios
nano /etc/naemon/module-conf.d/nagios_nagflux.cfg
process_performance_data=1

host_perfdata_file=/var/naemon/host-perfdata
host_perfdata_file_template=DATATYPE::HOSTPERFDATA\tTIMET::$TIMET$\tHOSTNAME::$HOSTNAME$\tHOSTPERFDATA::$HOSTPERFDATA$\tHOSTCHECKCOMMAND::$HOSTCHECKCOMMAND$
host_perfdata_file_mode=a
host_perfdata_file_processing_interval=15
host_perfdata_file_processing_command=process-host-perfdata-file-nagflux

service_perfdata_file=/var/naemon/service-perfdata
service_perfdata_file_template=DATATYPE::SERVICEPERFDATA\tTIMET::$TIMET$\tHOSTNAME::$HOSTNAME$\tSERVICEDESC::$SERVICEDESC$\tSERVICEPERFDATA::$SERVICEPERFDATA$\tSERVICECHECKCOMMAND::$SERVICECHECKCOMMAND$
service_perfdata_file_mode=a
service_perfdata_file_processing_interval=15
service_perfdata_file_processing_command=process-service-perfdata-file-nagflux

chown naemon:naemon /etc/naemon/module-conf.d/nagios_nagflux.cfg

nano /etc/naemon/conf.d/histou.cfg
define command {
    command_name    process-host-perfdata-file-nagflux
    command_line    /bin/mv /var/naemon/host-perfdata /var/nagflux/perfdata/$TIMET$.perfdata.host
    }

define command {
    command_name    process-service-perfdata-file-nagflux
    command_line    /bin/mv /var/naemon/service-perfdata /var/nagflux/perfdata/$TIMET$.perfdata.service
    }

define host {
   name       host-grafana
   action_url http://192.168.10.112/grafana/dashboard/script/histou.js?host=$HOSTNAME$&amp;theme=light&amp;annotations=true
   notes_url   http://192.168.10.112/dokuwiki/doku.php?id=inventory:$HOSTNAME$
   register   0
}

define service {
   name       service-grafana
   action_url http://192.168.10.112/grafana/dashboard/script/histou.js?host=$HOSTNAME$&amp;service=$SERVICEDESC$&amp;theme=light&amp;annotations=true
   register   0
}

mkdir /var/naemon/
chown -R naemon:naemon /var/naemon/

cd /etc/thruk/ssi/
cp extinfo-header.ssi.example extinfo-header.ssi
cp status-header.ssi.example status-header.ssi

systemctl restart naemon

ll /var/naemon/
ll /var/nagflux/perfdata/
</code></pre><h3 id="install-nagflux">Install nagflux</h3>
<pre tabindex="0"><code>cd /usr/bin/
wget https://github.com/Griesbacher/nagflux/releases/download/v0.4.1/nagflux
chmod +x nagflux

mkdir -p /var/nagflux/perfdata
mkdir -p /var/nagflux/spool
chown -R naemon:apache /var/nagflux

mkdir /etc/nagflux
cat &lt;&lt;EOF | sudo tee /etc/nagflux/config.gcfg
[main]
NagiosSpoolfileFolder = &quot;/var/nagflux/perfdata&quot;
NagiosSpoolfileWorker = 1
InfluxWorker = 2
MaxInfluxWorker = 5
DumpFile = &quot;/var/log/nagflux/nagflux.dump&quot;
NagfluxSpoolfileFolder = &quot;/var/nagflux/spool&quot;
FieldSeparator = &quot;&amp;&quot;
BufferSize = 1000
FileBufferSize = 65536
DefaultTarget = &quot;Influxdb&quot;

[Log]
LogFile = &quot;/var/log/nagflux/nagflux.log&quot;
MinSeverity = &quot;INFO&quot;

[InfluxDBGlobal]
CreateDatabaseIfNotExists = true
NastyString = &quot;&quot;
NastyStringToReplace = &quot;&quot;
HostcheckAlias = &quot;hostcheck&quot;

[InfluxDB &quot;nagflux&quot;]
Enabled = true
Version = 1.0
Address = &quot;http://localhost:8086&quot;
Arguments = &quot;precision=ms&amp;db=nagflux&amp;u=admin&amp;p=Password1&quot;
StopPullingDataIfDown = true

[Livestatus]
#tcp or file
Type = &quot;file&quot;
#tcp: 127.0.0.1:6557 or file /var/run/live
Address = &quot;/var/cache/naemon/live&quot;
MinutesToWait = 3
Version = &quot;&quot;
EOF

mkdir /var/log/nagflux
mkdir /var/nagflux

cat &lt;&lt;EOF | sudo tee /etc/systemd/system/nagflux.service
[Unit]
Description=A connector which transforms performancedata from Nagios/Icinga(2)/Naemon to InfluxDB/Elasticsearch
Documentation=https://github.com/Griesbacher/nagflux
After=network-online.target

[Service]
User=root
Group=root
ExecStart=/usr/bin/nagflux -configPath /etc/nagflux/config.gcfg
Restart=on-failure

[Install]
WantedBy=multi-user.target
Alias=nagflux.service
EOF

systemctl daemon-reload
systemctl start nagflux
systemctl enable nagflux

tailf /var/log/nagflux/nagflux.log
</code></pre><h3 id="install-grafana">Install grafana</h3>
<pre tabindex="0"><code>curl -s https://packagecloud.io/install/repositories/grafana/stable/script.rpm.sh | sudo bash
yum install grafana -y

cp /etc/grafana/grafana.ini /etc/grafana/grafana.ini.bak
echo &quot;&quot; &gt; /etc/grafana/grafana.ini
nano /etc/grafana/grafana.ini
[paths]
logs = /var/log/grafana

[log]
mode = file
[log.file]
level =  Info
daily_rotate = true

[server]
http_port = 3000
http_addr = 0.0.0.0
domain = localhost
root_url = %(protocol)s://%(domain)s/grafana/
enable_gzip = false

[snapshots]
external_enabled = false

[security]
disable_gravatar = true
# same username and password for thruk
admin_user = thrukadmin
admin_password = Password1

[users]
allow_sign_up = false
default_theme = light

[auth.basic]
enabled = false

[auth.proxy]
enabled = true
auto_sign_up = true

[alerting]
enabled = true
execute_alerts = true

nano /etc/httpd/conf.d/grafana.conf
&lt;IfModule !mod_proxy.c&gt;
    LoadModule proxy_module /usr/lib64/httpd/modules/mod_proxy.so
&lt;/IfModule&gt;
&lt;IfModule !mod_proxy_http.c&gt;
    LoadModule proxy_http_module /usr/lib64/httpd/modules/mod_proxy_http.so
&lt;/IfModule&gt;

&lt;Location /grafana&gt;
    ProxyPass http://127.0.0.1:3000 retry=0 disablereuse=On
    ProxyPassReverse http://127.0.0.1:3000/grafana
    RewriteEngine On
    RewriteRule .* - [E=PROXY_USER:%{LA-U:REMOTE_USER},NS]
    SetEnvIf Request_Protocol ^HTTPS.* IS_HTTPS=1
    SetEnvIf Authorization &quot;^.+$&quot; IS_BASIC_AUTH=1
    # without thruk cookie auth, use the proxy user from the rewrite rule above
    RequestHeader set X-WEBAUTH-USER &quot;%{PROXY_USER}s&quot;  env=IS_HTTPS
    RequestHeader set X-WEBAUTH-USER &quot;%{PROXY_USER}e&quot;  env=!IS_HTTPS
    # when thruk cookie auth is used, fallback to remote user directly
    RequestHeader set X-WEBAUTH-USER &quot;%{REMOTE_USER}e&quot; env=!IS_BASIC_AUTH
    RequestHeader unset Authorization
&lt;/Location&gt;

echo &quot;
apiVersion: 1

deleteDatasources:
  - name: nagflux

datasources:
- name: nagflux
  type: influxdb
  url: http://localhost:8086
  access: proxy
  database: nagflux
  isDefault: true
  version: 1
  editable: true
&quot; &gt; /etc/grafana/provisioning/datasources/nagflux.yaml

systemctl start grafana-server
systemctl enable grafana-server
systemctl restart httpd


# http://SERVER-IP:3000
# admin/admin

# datasource:
nagflux
influxdb
http://localhost:8086
</code></pre><h3 id="install-histou">Install histou</h3>
<pre tabindex="0"><code>cd /tmp
wget -O histou.tar.gz https://github.com/Griesbacher/histou/archive/v0.4.3.tar.gz
mkdir -p /var/www/html/histou
cd /var/www/html/histou
tar xzf /tmp/histou.tar.gz --strip-components 1
cp histou.ini.example histou.ini
cp histou.js /usr/share/grafana/public/dashboards/

nano /usr/share/grafana/public/dashboards/histou.js
var url = 'http://192.168.10.112/histou/';

systemctl restart httpd
systemctl restart grafana-server

# http://192.168.10.112/histou/?host=localhost&amp;service=PING
# http://192.168.10.112:3000/dashboard/script/histou.js?host=localhost&amp;service=PING

# nagios config

sed -i '/name.*generic-host/a\        use                             host-grafana' /etc/naemon/conf.d/templates/hosts.cfg
sed -i '/name.*generic-service/a\        use                             service-grafana' /etc/naemon/conf.d/templates/services.cfg

systemctl restart naemon
</code></pre><h3 id="inpluxdb-commands">Inpluxdb commands</h3>
<pre tabindex="0"><code>influx
create database nagflux;
CREATE USER &quot;admin&quot; WITH PASSWORD 'Password1' WITH ALL PRIVILEGES;
show DATABASES;
USE nagflux;
select * from /.*/ limit 1;
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Alertmanagger]]></title>
            <link href="https://devopstales.github.io/home/prometheus-alertmanagger/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/monitoring/prometheus-alertmanagger/?utm_source=atom_feed" rel="related" type="text/html" title="Install Alertmanagger" />
                <link href="https://devopstales.github.io/home/prometheus-node-exporter/?utm_source=atom_feed" rel="related" type="text/html" title="Install Node-exporter" />
                <link href="https://devopstales.github.io/home/prometheus-install/?utm_source=atom_feed" rel="related" type="text/html" title="Prometheus Install" />
            
                <id>https://devopstales.github.io/home/prometheus-alertmanagger/</id>
            
            
            <published>2018-10-18T00:00:00+00:00</published>
            <updated>2018-10-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Example article description</blockquote><p>The Alertmanager handles alerts sent by client applications such as the Prometheus server.</p>
<h3 id="download-alertmanager">Download Alertmanager</h3>
<pre tabindex="0"><code>wget https://github.com/prometheus/alertmanager/releases/download/v0.15.0-rc.1/alertmanager-0.15.0-rc.1.linux-amd64.tar.gz
tar -xzf alertmanager-0.15.0-rc.1.linux-amd64.tar.gz
</code></pre><h3 id="install-binaris">Install binaris</h3>
<pre tabindex="0"><code>useradd --no-create-home --shell /bin/false alertmanager

mkdir /etc/alertmanager
mkdir /etc/alertmanager/template
mkdir -p /var/lib/alertmanager/data
touch /etc/alertmanager/alertmanager.yml

chown -R alertmanager:alertmanager /etc/alertmanager
chown -R alertmanager:alertmanager /var/lib/alertmanager

cp alertmanager-*linux-amd64/alertmanager /usr/local/bin/
cp alertmanager-*linux-amd64/amtool /usr/local/bin/

chown alertmanager:alertmanager /usr/local/bin/alertmanager
chown alertmanager:alertmanager /usr/local/bin/amtool
</code></pre><h3 id="create-servis-for-alertmanager">Create servis for Alertmanager</h3>
<pre tabindex="0"><code>nano /etc/systemd/system/alertmanager.service
[Unit]
Description=Prometheus Alertmanager Service
Wants=network-online.target
After=network.target

[Service]
User=alertmanager
Group=alertmanager
Type=simple
ExecStart=/usr/local/bin/alertmanager \
    --config.file /etc/alertmanager/alertmanager.yml \
    --storage.path /var/lib/alertmanager/data
Restart=always

[Install]
WantedBy=multi-user.target
</code></pre><h3 id="configure-alertmanager">Configure Alertmanager</h3>
<pre tabindex="0"><code>nano /etc/alertmanager/alertmanager.yml
global:
  smtp_smarthost: 'localhost:25'
  smtp_from: 'alertmanager@devopstales.intra'
#  smtp_auth_username: 'alertmanager'
#  smtp_auth_password: 'password'

templates:
- '/etc/alertmanager/template/*.tmpl'

route:
  repeat_interval: 3h
  receiver: mails

receivers:
- name: 'mails'
  email_configs:
  - to: 'admin@devopstales.intra'
</code></pre><h3 id="configure-prometheus">Configure Prometheus</h3>
<pre tabindex="0"><code>nano /etc/prometheus/prometheus.yml
global:
  scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.

alerting:
  alertmanagers:
  - static_configs:
    - targets:
       - localhost:9093
</code></pre><pre tabindex="0"><code>sudo systemctl daemon-reload
sudo systemctl enable alertmanager
sudo systemctl start alertmanager
sudo systemctl ststus alertmanager
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Node-exporter]]></title>
            <link href="https://devopstales.github.io/home/prometheus-node-exporter/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/prometheus-install/?utm_source=atom_feed" rel="related" type="text/html" title="Prometheus Install" />
                <link href="https://devopstales.github.io/monitoring/prometheus-node-exporter/?utm_source=atom_feed" rel="related" type="text/html" title="Install Node-exporter" />
                <link href="https://devopstales.github.io/monitoring/prometheus-install/?utm_source=atom_feed" rel="related" type="text/html" title="Prometheus Install" />
            
                <id>https://devopstales.github.io/home/prometheus-node-exporter/</id>
            
            
            <published>2018-10-17T00:00:00+00:00</published>
            <updated>2018-10-17T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Example article description</blockquote><p>Install node-exporter to provides detailed information about the system, including CPU, disk, and memory usage.</p>
<h3 id="download-node-exporter">Download Node-exporter</h3>
<pre tabindex="0"><code>cd /tmp
wget https://github.com/prometheus/node_exporter/releases/download/v0.16.0/node_exporter-0.16.0.linux-amd64.tar.gz
tar -xzf node_exporter-0.16.0.linux-amd64.tar.gz
</code></pre><h3 id="install-binaris">Install binaris</h3>
<pre tabindex="0"><code>sudo useradd -rs /bin/false node_exporter

sudo mv node_exporter*linux-amd64/node_exporter /usr/local/bin
mkdir -p /etc/node_exporter/data

chown -R node_exporter:node_exporter /etc/node_exporter

# host role based teg
cat &lt;&lt;EOF &gt; /etc/node_exporter/data/roles.prom
machine_role{role=&quot;postfix&quot;} 1
machine_role{role=&quot;apache&quot;} 1
EOF
</code></pre><h3 id="create-servis-for-node-exporter">Create servis for Node-exporter</h3>
<pre tabindex="0"><code>cat &lt;&lt;EOF &gt; /etc/systemd/system/node_exporter.service
[Unit]
Description=Node Exporter
Wants=network-online.target
After=network-online.target

[Service]
User=node_exporter
Group=node_exporter
Type=simple
ExecStart=/usr/local/bin/node_exporter --collector.textfile.directory /etc/node_exporter/data/

[Install]
WantedBy=multi-user.target
EOF
</code></pre><pre tabindex="0"><code>systemctl daemon-reload
systemctl enable node_exporter
systemctl start node_exporter
systemctl status node_exporter
</code></pre><h3 id="configure-prometheus">Configure Prometheus</h3>
<pre tabindex="0"><code>nano /etc/prometheus/prometheus.yml
global:
  scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.

alerting:
  alertmanagers:
  - static_configs:
    - targets:
       - localhost:9093

rule_files:
  # - &quot;first_rules.yml&quot;
  # - &quot;second_rules.yml&quot;

scrape_configs:
  - job_name: 'prometheus_metrics'
    scrape_interval: 5s
    static_configs:
      - targets: ['prometheus01.devopstales.intra:9090']

  - job_name: 'node_exporter_metrics'
    scrape_interval: 5s
    static_configs:
      - targets: ['prometheus01.devopstales.intra:9100']
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Prometheus Install]]></title>
            <link href="https://devopstales.github.io/home/prometheus-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/monitoring/prometheus-install/?utm_source=atom_feed" rel="related" type="text/html" title="Prometheus Install" />
            
                <id>https://devopstales.github.io/home/prometheus-install/</id>
            
            
            <published>2018-08-21T00:00:00+00:00</published>
            <updated>2018-08-21T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Example article description</blockquote><p>Prometheus is an open-source monitoring system with a built-in noSQL time-series database. It offers a multi-dimensional data model, a flexible query language, and diverse visualization possibilities. Prometheus collects metrics from http nedpoint. Most service dind&rsquo;t have this endpoint so you need optional programs that generate additional metrics cald exporters.</p>
<p>In this tutorial, you&rsquo;ll install, configure, and secure Prometheus and Node Exporter to generate metrics about your server&rsquo;s performance.</p>
<h3 id="download-prometheus">Download Prometheus</h3>
<pre tabindex="0"><code>curl -LO &quot;https://github.com/prometheus/prometheus/releases/download/v2.2.1/prometheus-2.2.1.linux-amd64.tar.gz&quot;
tar -xzf prometheus-2.2.1.linux-amd64.tar.gz
</code></pre><h3 id="install-binaris">Install binaris</h3>
<pre tabindex="0"><code>cp prometheus-*linux-amd64/prometheus /usr/local/bin/
cp prometheus-*linux-amd64/promtool /usr/local/bin/

useradd --no-create-home --shell /bin/false prometheus

mkdir /etc/prometheus
mkdir /var/lib/prometheus

chown prometheus:prometheus /var/lib/prometheus
chown prometheus:prometheus /usr/local/bin/prometheus
chown prometheus:prometheus /usr/local/bin/promtool

cp -r prometheus-*linux-amd64/consoles /etc/prometheus
cp -r prometheus-*linux-amd64/console_libraries /etc/prometheus

chown -R prometheus:prometheus /etc/prometheus/consoles
chown -R prometheus:prometheus /etc/prometheus/console_libraries

cp prometheus-*linux-amd64/prometheus.yml /etc/prometheus/
chown -R prometheus:prometheus /etc/prometheus/prometheus.yml
</code></pre><h3 id="create-servis-for-prometheus">Create servis for prometheus</h3>
<pre tabindex="0"><code>nano /etc/systemd/system/prometheus.service
[Unit]
Description=Prometheus
Wants=network-online.target
After=network-online.target

[Service]
User=prometheus
Group=prometheus
Type=simple
ExecStart=/usr/local/bin/prometheus \
    --config.file /etc/prometheus/prometheus.yml \
    --storage.tsdb.path /var/lib/prometheus/ \
    --web.console.templates=/etc/prometheus/consoles \
    --web.console.libraries=/etc/prometheus/console_libraries

[Install]
WantedBy=multi-user.target
</code></pre><h3 id="configure-prometheus">Configure Prometheus</h3>
<pre tabindex="0"><code>nano /etc/prometheus/prometheus.yml

global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'prometheus'
    scrape_interval: 5s
    static_configs:
      - targets: ['localhost:9090']
</code></pre><pre tabindex="0"><code>systemctl daemon-reload
systemctl start prometheus
systemctl status prometheus
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
</feed>
