<?xml version="1.0" encoding="utf-8"?> 
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <generator uri="https://gohugo.io/" version="0.59.1">Hugo</generator><title type="html"><![CDATA[Homes on devopstales]]></title>
    
        <subtitle type="html"><![CDATA[Blog about dev and ops stuff]]></subtitle>
    
    
    
            <link href="https://devopstales.github.io/home/" rel="alternate" type="text/html" title="HTML" />
            <link href="https://devopstales.github.io/home/index.xml" rel="alternate" type="application/rss+xml" title="RSS" />
            <link href="https://devopstales.github.io/home/atom.xml" rel="self" type="application/atom+xml" title="Atom" />
    <updated>2021-07-31T13:09:20+00:00</updated>
    
    
    <author>
            <name>Blaiserman</name>
            </author>
    
        <id>https://devopstales.github.io/home/</id>
    
        
        <entry>
            <title type="html"><![CDATA[Create a Helm reposirory with GitHub Pages]]></title>
            <link href="https://devopstales.github.io/home/helm-repositoty/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/helm-repositoty/?utm_source=atom_feed" rel="related" type="text/html" title="Create a Helm reposirory with GitHub Pages" />
                <link href="https://devopstales.github.io/home/helm3-loki/?utm_source=atom_feed" rel="related" type="text/html" title="Install Grafana Loki with Helm3" />
                <link href="https://devopstales.github.io/home/k8s-error-at-kubectl-logs/?utm_source=atom_feed" rel="related" type="text/html" title="K8s ERROR at kubectl logs" />
                <link href="https://devopstales.github.io/home/k8s-kuberos/?utm_source=atom_feed" rel="related" type="text/html" title="Kubectl authentication with Kuberos" />
                <link href="https://devopstales.github.io/home/k8s-ceph-storage-with-csi-driver/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD volume with CSI driver" />
            
                <id>https://devopstales.github.io/home/helm-repositoty/</id>
            
            
            <published>2021-07-25T00:00:00+00:00</published>
            <updated>2021-07-25T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can host your own Helm repository with GitHub Pages.</p>

<h2 id="create-a-new-github-repository">Create a new GitHub Repository</h2>

<p>Log into GitHub and create a <a href="https://github.com/new">new repository</a> called helm-charts. I chose to hav a README file and an Apache2 licence in mye repository.</p>

<p>Clone the repository to start working.</p>

<pre><code>git clone git@github.com:devopstales/helm-charts.git
cd helm-charts

tree
.
├── LICENSE
└── README.md
</code></pre>

<p>Create a hem chart in the repository:</p>

<pre><code>mkdir charts
helm create charts/chart1
helm create charts/chart2

tree
.
├── charts
│   ├── chart1
│   │   ├── charts
│   │   ├── Chart.yaml
│   │   ├── templates
│   │   │   ├── deployment.yaml
│   │   │   ├── _helpers.tpl
│   │   │   ├── ingress.yaml
│   │   │   ├── NOTES.txt
│   │   │   ├── service.yaml
│   │   │   └── tests
│   │   │       └── test-connection.yaml
│   │   └── values.yaml
│   └── chart2
│       ├── charts
│       ├── Chart.yaml
│       ├── templates
│       │   ├── deployment.yaml
│       │   ├── _helpers.tpl
│       │   ├── ingress.yaml
│       │   ├── NOTES.txt
│       │   ├── service.yaml
│       │   └── tests
│       │       └── test-connection.yaml
│       └── values.yaml
├── LICENSE
└── README.md
</code></pre>

<h3 id="push-to-github">Push to GitHub:</h3>

<pre><code>echo &quot;.deploy&quot; &gt;&gt; .gitignore
git add . --all
git commit -m 'Initial Commit'
git push origin main
</code></pre>

<p>Create brach for GitHub Pages and release:</p>

<pre><code>git checkout --orphan gh-pages
Switched to a new branch 'gh-pages'

rm -rf charts
git add . --all
git commit -m 'initial gh-pages'
git push origin gh-pages
git checkout main
</code></pre>

<p>Next enable GitHub Pages i the repository settings. After a few minutes you should have a default rendering on your README.md at the provided URL.</p>

<h2 id="use-chart-releaser">Use chart-releaser</h2>

<p>Yo can create a chart Helm repository by usin the <code>helm package</code> and <code>helm repo</code> commands but you can simplify your life by using <code>chart-releaser</code>.</p>

<h3 id="install-for-lnux">Install for Lnux:</h3>

<pre><code>cd /tmp
curl -sSL https://github.com/helm/chart-releaser/releases/download/v1.2.1/chart-releaser_1.2.1_linux_amd64.tar.gz | tar xzf -
mv cr ~/bin/cr
cr help
</code></pre>

<h2 id="install-for-mac-osx">Install for Mac osX:</h2>

<pre><code>$ brew tap helm/tap
$ brew install chart-releaser
</code></pre>

<h3 id="usage">Usage:</h3>

<p>The <code>cr index</code> will create the appropriate <code>index.yaml</code> and <code>cr upload</code> will upload the packages to GitHub Releases. Fot theat you need a GitHub Token.
In your browser go to your <a href="https://github.com/settings/tokens">github developer settings</a> and create a new personal access token and add full access to the repo.</p>

<p>Create an environment variable for the token:</p>

<pre><code>export CH_TOKEN=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sdf

helm package charts/{chart1,chart2} --destination .deploy

cr upload -o devopstales -r helm-charts -p .deploy
git checkout gh-pages
cr index -i ./index.yaml -p .deploy -o devopstales -r helm-charts -c https://devopstales.github.io/helm-charts/

git add index.yaml
git commit -m 'release 0.1.0'
git push origin gh-pages
</code></pre>

<h3 id="update-the-readme-md-with-instructions-to-usage">Update the README.md with instructions to usage</h3>

<pre><code>nano README.md
git add README.md
git commit -m 'update readme with instructions'
git push origin gh-pages
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Add a Custom Host to Kubernetes]]></title>
            <link href="https://devopstales.github.io/home/k8s-custom-host/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/image-security-admission-controller-v3/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V3" />
                <link href="https://devopstales.github.io/home/continuous-image-security/?utm_source=atom_feed" rel="related" type="text/html" title="Continuous Image Security" />
                <link href="https://devopstales.github.io/home/k8s-prometheus-stack/?utm_source=atom_feed" rel="related" type="text/html" title="K8S Logging And Monitoring" />
                <link href="https://devopstales.github.io/home/k8s-vault-v2/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes integration with external Vault" />
                <link href="https://devopstales.github.io/home/rke2-calico/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Install With Calico" />
            
                <id>https://devopstales.github.io/home/k8s-custom-host/</id>
            
            
            <published>2021-07-22T00:00:00+00:00</published>
            <updated>2021-07-22T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I&rsquo;will show you how to add custom hosts to kubernetes.</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
<li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-contour-ingress --></li>
<li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
<li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
<li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... --></li>
<li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
<li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
<li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<p>CoreDNS is the DNS server in kubernetes. I some situation I need to add custom hosts to be resolvable in the kubernetes netwok.</p>

<p>First, edit the ConfigMap of the coredns using the following command:</p>

<pre><code class="language-bash">kubectl edit cm -n kube-system coredns
# or
kubectl edit cm -n kube-system rke2-coredns-rke2-coredns
</code></pre>

<pre><code class="language-yaml">apiVersion: v1
kind: ConfigMap
data:
  Corefile: |
    .:53 {
        errors
        health {
          lameduck 5s
        }
        ready
        kubernetes cluster.local in-addr.arpa ip6.arpa {
          pods insecure
          fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        forward . 8.8.8.8 8.8.4.4
        cache 30
        loop
        reload
        loadbalance
        hosts /etc/coredns/customdomains.db k8s.intra {
          172.17.14.10 rancher.k8s.intra
          172.17.14.10 hubble.k8s.intra
          172.17.14.10 grafana.k8s.intra
          172.17.14.10 alertmanager.k8s.intra
          172.17.14.10 prometheus.k8s.intra
          172.17.14.10 sso.k8s.intra
          fallthrough
        }
    }
</code></pre>

<p>Delete coredens pods:</p>

<pre><code class="language-bash">kubectl get pod -n kube-system | grep dns
kubectl delete pod -n kube-system core-dns-#########
</code></pre>

<p>It’s done! You can now reach that custom host from inside any pod on the cluster.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Subject Alternative Name in Active Dyrectory LDAPS Cerificate]]></title>
            <link href="https://devopstales.github.io/home/msad-ldaps-subject-alternative-mame/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/msad-ldaps-subject-alternative-mame/</id>
            
            
            <published>2021-07-22T00:00:00+00:00</published>
            <updated>2021-07-22T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can configure custom Subject Alternative Name in Active Directory LDAPS certificate.</p>

<h3 id="open-mmc">Open mmc</h3>

<ul>
<li><code>windows + r</code></li>
<li>run <code>mmc</code></li>
</ul>

<p>![Example image](/img/include/ldapssan1</p>

<ul>
<li>Click <code>File / Add/Remove Snap-in..</code> or <code>ctrl + m</code></li>
</ul>

<p><img src="/img/include/ldapssan2.PNG" alt="Example image" />
<img src="/img/include/ldapssan3.PNG" alt="Example image" /></p>

<ul>
<li>Add certificates</li>
</ul>

<p><img src="/img/include/ldapssan4.PNG" alt="Example image" />
<img src="/img/include/ldapssan5.PNG" alt="Example image" /></p>

<ul>
<li>Add a nother certificates for service</li>
</ul>

<p><img src="/img/include/ldapssan6.PNG" alt="Example image" />
<img src="/img/include/ldapssan5.PNG" alt="Example image" />
<img src="/img/include/ldapssan7.PNG" alt="Example image" /></p>

<ul>
<li>Add Certificate Authoraty</li>
</ul>

<p><img src="/img/include/ldapssan5.PNG" alt="Example image" />
<img src="/img/include/ldapssan8.PNG" alt="Example image" /></p>

<h3 id="clone-template">Clone Template</h3>

<ul>
<li><code>Certificate Authoraty / Domain Controller / Certificate Template</code></li>
</ul>

<p><img src="/img/include/ldapssan9.PNG" alt="Example image" />
<img src="/img/include/ldapssan10.PNG" alt="Example image" /></p>

<ul>
<li>Select <code>Domain Controller Template</code></li>
<li>Right Click and <code>Duplicate template</code></li>
</ul>

<p><img src="/img/include/ldapssan11.PNG" alt="Example image" />
<img src="/img/include/ldapssan12.PNG" alt="Example image" />
<img src="/img/include/ldapssan22.PNG" alt="Example image" /></p>

<ul>
<li>the click <code>OK</code> and cluse the <code>Certificate Teplate Console</code></li>
</ul>

<h3 id="add-template-to-certificate-template-list">Add template to Certificate Template list</h3>

<ul>
<li>At <code>Certificate Authoraty / Domain Controller / Certificate Template</code></li>
</ul>

<p><img src="/img/include/ldapssan9.PNG" alt="Example image" /></p>

<ul>
<li>Rght click and select <code>Certificate Template to Issue</code> Add the new Template</li>
</ul>

<p><img src="/img/include/ldapssan13.PNG" alt="Example image" /></p>

<h3 id="generate-certificate">Generate Certificate</h3>

<ul>
<li>Right click on <code>Certificates (Local Computer) / Personal / Certificate</code> and select <code>All Tasks / Request New Certificate</code></li>
</ul>

<p><img src="/img/include/ldapssan16.PNG" alt="Example image" />
<img src="/img/include/ldapssan17.PNG" alt="Example image" />
<img src="/img/include/ldapssan18.PNG" alt="Example image" />
<img src="/img/include/ldapssan19.PNG" alt="Example image" />
<img src="/img/include/ldapssan20.PNG" alt="Example image" /></p>

<ul>
<li>enroll</li>
</ul>

<p><img src="/img/include/ldapssan21.PNG" alt="Example image" /></p>

<h4 id="change-certificate">Change Certificate</h4>

<ul>
<li>To activate the new certificate you need to restart the Domain Controller</li>
</ul>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[GKE cluster’s egress traffic via Cloud NAT]]></title>
            <link href="https://devopstales.github.io/home/gke-egress/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/gke-egress/?utm_source=atom_feed" rel="related" type="text/html" title="GKE cluster’s egress traffic via Cloud NAT" />
                <link href="https://devopstales.github.io/home/gcp-vm-export/?utm_source=atom_feed" rel="related" type="text/html" title="Export GCP VM to S3" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller-v3/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V3" />
                <link href="https://devopstales.github.io/home/continuous-image-security/?utm_source=atom_feed" rel="related" type="text/html" title="Continuous Image Security" />
                <link href="https://devopstales.github.io/home/k8s-prometheus-stack/?utm_source=atom_feed" rel="related" type="text/html" title="K8S Logging And Monitoring" />
            
                <id>https://devopstales.github.io/home/gke-egress/</id>
            
            
            <published>2021-07-01T00:00:00+00:00</published>
            <updated>2021-07-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can can reroute the GKE egress traffic via cloud NAT.</p>

<p>In Public GKE cluster wach node has it&rsquo;s own external IP address and the nodes route all egress traffic through there external IP. This external IPs can change over time. In the case of a private GKE cluster, all the nodes will have an internal ip address and you can define a cloud NAT for all your egress traffic from the cluster. So public cluster is not a ideal solutinon if you need a static ip list for source ip whtelistink, but here is a solution.</p>

<h3 id="create-a-cloud-nat-gateway">Create a cloud NAT gateway</h3>

<p>We will use a daemon set in GKE , that will rewrite the ip-table rules in the GKE Nodes to masquerade the outbound traffic.</p>

<p>Select the VPC in which you have deployed your public GKE cluster and create a new cloud router. Create it manualli to configure the NAT gateway’s ip. This will be the ip-address that you will give to your third party vendor for whitelisting your incoming connection.</p>

<p>Create the config map and the daemon-set:</p>

<pre><code class="language-yaml">nano config.yaml
---
nonMasqueradeCIDRs:
  - 0.0.0.0/0
masqLinkLocal: true
resyncInterval: 60s
</code></pre>

<pre><code class="language-bash">kubectl create configmap ip-masq-agent --from-file config.yaml --namespace kube-system
</code></pre>

<p>Deploy the masq-agent:</p>

<pre><code class="language-yaml">nano ip-masq-agent.yaml
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: ip-masq-agent
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: ip-masq-agent
  template:
    metadata:
      labels:
        k8s-app: ip-masq-agent
    spec:
      hostNetwork: true
      containers:
      - name: ip-masq-agent
        image: gcr.io/google-containers/ip-masq-agent-amd64:v2.4.1
        args:
            - --masq-chain=IP-MASQ
            # To non-masquerade reserved IP ranges by default, uncomment the line below.
            # - --nomasq-all-reserved-ranges
        securityContext:
          privileged: true
        volumeMounts:
          - name: config
            mountPath: /etc/config
      volumes:
        - name: config
          configMap:
            # Note this ConfigMap must be created in the same namespace as the
            # daemon pods - this spec uses kube-system
            name: ip-masq-agent
            optional: true
            items:
              # The daemon looks for its config in a YAML file at /etc/config/ip-masq-agent
              - key: config
                path: ip-masq-agent
      tolerations:
      - effect: NoSchedule
        operator: Exists
      - effect: NoExecute
        operator: Exists
      - key: &quot;CriticalAddonsOnly&quot;
        operator: &quot;Exists&quot;
</code></pre>

<pre><code class="language-bash">kubectl apply -f ip-masq-agent.yaml
</code></pre>

<p>After the creation ogthe ip-masq-agent check the firewall rules of the GKE nodes:</p>

<pre><code class="language-bash">sudo iptables -t NAT -L IP-MASQ

Chain IP-MASQ (2 references)
target     prot opt cource      destination
RETURN     all  --  anywhere    anywhere      /* ip-masq-agent: local traffic is not subject to MASQUERADE */
MASQUERADE  all  --  anywhere    anywhere      /* ip-masq-agent: outbound traffic is subject to MASQUERADE (must be last in chain) */
</code></pre>

<p>So the egress traffic from GKE to internet will go via the cloud NAT’s gateway ip address.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Active Directory Configure secure LDAPS]]></title>
            <link href="https://devopstales.github.io/home/msad-ldaps/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/msad-ldaps/</id>
            
            
            <published>2021-06-22T00:00:00+00:00</published>
            <updated>2021-06-22T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can configure LDAPS in Active Directory.</p>

<h3 id="install-certificate-authority">Install Certificate Authority</h3>

<ul>
<li><p>On your Windows Server Machine, click on Start -&gt; Server Manager -&gt; Add Roles and Features.
adldaps1.png</p></li>

<li><p>After selecting Add Roles and Features and Click on Next.
<img src="/img/include/adldaps2.png" alt="Example image" /></p></li>

<li><p>Choose Role-based or feature-based installation option and Click on Next button.
<img src="/img/include/adldaps3.png" alt="Example image" /></p></li>

<li><p>Choose Select a server from the server pool option &amp; Select ldap server from the server pool and click on Next button.
<img src="/img/include/adldaps4.png" alt="Example image" /></p></li>

<li><p>Choose Active Directory Certificate Services option from the list of roles and click on Next button.
<img src="/img/include/adldaps5.png" alt="Example image" /></p></li>

<li><p>Choose nothing from the list of features and click on Next button.</p></li>

<li><p>In Active Directory Certificate Services (AD CS) choose nothing and Click on Next button.
<img src="/img/include/adldaps6.png" alt="Example image" /></p></li>

<li><p>Mark Certification Authority from the list of roles and Click on Next button.
<img src="/img/include/adldaps7.png" alt="Example image" /></p></li>

<li><p>Click on Install button to confirm installation.</p></li>

<li><p>Now, click on Configure Active Directory Certificate Services on Destination Server option and click on Close button.
<img src="/img/include/adldaps8.png" alt="Example image" /></p></li>

<li><p>We can use the currently logged on user to configure role services since it belongs to the local Administrators group. Click on Next button.</p></li>

<li><p>Mark Certification Authority from the list of roles and Click on Next button.
<img src="/img/include/adldaps9.png" alt="Example image" /></p></li>

<li><p>Choose Enterprise CA option and Click on Next.
<img src="/img/include/adldaps10.png" alt="Example image" /></p></li>

<li><p>Choose Root CA option and Click on Next button.
<img src="/img/include/adldaps11.png" alt="Example image" /></p></li>

<li><p>Choose Create a new private key option and Click on Next button.
<img src="/img/include/adldaps12.png" alt="Example image" /></p></li>

<li><p>Choose SHA256 as the hash algorithm and Click on Next. UPDATE : Recommended to select the most recent hashing algorithm.
<img src="/img/include/adldaps13.png" alt="Example image" /></p></li>

<li><p>Click on Next button.
<img src="/img/include/adldaps14.png" alt="Example image" /></p></li>

<li><p>Specify the validity of the certificate choosing Default 5 years and Click on Next button.
<img src="/img/include/adldaps15.png" alt="Example image" /></p></li>

<li><p>Select the default database location and Click on Next.
<img src="/img/include/adldaps16.png" alt="Example image" /></p></li>

<li><p>Click on Configure button to confirm.</p></li>

<li><p>Once the configuration succeeded and click on Close button.
<img src="/img/include/adldaps17.png" alt="Example image" /></p></li>
</ul>

<h3 id="configuring-secure-ldap">Configuring secure LDAP:</h3>

<ul>
<li>At restart the Domain Controller Will generate a new Certificate fos self.</li>
</ul>

<h3 id="test-ldaps">Test LDAPS</h3>

<ul>
<li>windows + R</li>
<li>Run: ldp</li>
</ul>

<p><img src="/img/include/adldaps18.png" alt="Example image" /></p>

<ul>
<li>Select connect menu in top right</li>
<li>Add the name of the serfer for server</li>
<li>port: 636</li>
<li>selset SSL</li>
<li>click OK</li>
</ul>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[RKE2 Image security Admission Controller V3]]></title>
            <link href="https://devopstales.github.io/home/image-security-admission-controller-v3/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/continuous-image-security/?utm_source=atom_feed" rel="related" type="text/html" title="Continuous Image Security" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller-v2/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V2" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller" />
                <link href="https://devopstales.github.io/kubernetes/image-security-admission-controller-v3/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V3" />
                <link href="https://devopstales.github.io/kubernetes/continuous-image-security/?utm_source=atom_feed" rel="related" type="text/html" title="Continuous Image Security" />
            
                <id>https://devopstales.github.io/home/image-security-admission-controller-v3/</id>
            
            
            <published>2021-06-21T00:00:00+00:00</published>
            <updated>2021-06-21T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In a previous posts we talked about <a href="https://devopstales.github.io/home/image-security-admission-controller/">Banzaicloud&rsquo;s anchore-image-validator</a> and <a href="https://devopstales.github.io/home/image-security-admission-controller-v2/">Anchore&rsquo;s own admission-controller</a>. In this post I will show you my own admission-controller for image scanning.</p>

<h3 id="parst-of-the-k8s-security-series">Parst of the K8S Security series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
<li>Part2: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
<li>Part3: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
<li>Part4: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a>
<!-- RBAC -->
<!-- # Policy --></li>
<li>Part5: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
<li>Part6: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
<li>Part7: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a>
<!-- # Admission Controllers --></li>
<li>Part8: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a>
<!-- ## Image Scan --></li>
<li>Part9a: <a href="../../kubernetes/image-security-admission-controller">Image security Admission Controller</a></li>
<li>Part9b: <a href="../../kubernetes/image-security-admission-controller-v2">Image security Admission Controller V2</a></li>
<li>Part9c: <a href="../../kubernetes/image-security-admission-controller-v3">Image security Admission Controller V3</a>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring --></li>
<li>Part10: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
<li>Part11: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a>
<!-- DAST Exporter?? -->
<!-- CIS Exporter
https://github.com/ibrokethecloud/kube-bench-metrics --></li>
<li>Part12 <a href="../../kubernetes/k8s-connaisseur">Image Signature Verification with Connaisseur</a></li>
<li>Part13 <a href="../../kubernetes/k8s-backup">Backup your Kubernetes Cluster</a>
<!-- # Secrets --></li>
<li>Part14a <a href="../../kubernetes/k8s-vault">Kubernetes and Vault integration</a></li>
<li>Part14b <a href="../../kubernetes/k8s-vault-v2">Kubernetes External Vault integration</a></li>
<li>Part15a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
<li>Part15b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
<li>Part15c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a>
<!-- # Image -->
<!-- keel --></li>
<li>Part16: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a>
<!-- flux image update --></li>
<li>Part17: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
</ul>

<!-- admission-controller ami cseréli az imageben a repot !!!
 -->

<!-- sysdig falco -->

<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
-->

<!-- resource kvota -->

<!-- Backup
https://github.com/pieterlange/kube-backup 
-->

<!--
https://medium.com/@beatrizmrg/network-security-for-microservices-with-ebpf-bis-478b40e7befa

# container runetime
https://www.ianlewis.org/en/container-runtimes-part-1-introduction-container-r
http://pouchcontainer.io/pouch/docs/features/pouch_with_runV.html
-->

<p>I found multiple solution for Anchore Engine but only one for Trivy. The <a href="https://github.com/aquasecurity/trivy-enforcer">trivy-enforcer</a> that is an experimental project and use OPA for enforce the policy. So I decide to create mey own dmission-controller.</p>

<h3 id="how-an-admission-controller-works">How an admission controller works</h3>

<p>An Admission Controller Webhook is triggered when a Kubernetes object is created. It sends a JSON formatted HTTP request to a specific Kubernetes Service in a namespace which returns a JSON response. If you whoud like to now more aboute admission controllers you can read about it in my previous post <a href="https://devopstales.github.io/kubernetes/admission-controllers/">Using Admission Controllers</a></p>

<h3 id="writing-a-validating-admission-controller">Writing a Validating Admission Controller</h3>

<p>I want to walidate the Pod object to check how many vulnerability has the image in this pod. So I wrote a <a href="https://github.com/devopstales/trivy-image-validator/blob/master/trivy-scanner.py">python script</a> that will pars the JSON request for the pods image, rin a trivy scan on it and sen back the answer. Then I build it to a docker image called <code>devopstales/trivy-scanner-admission:1.0.1</code>. I run it as a deployment:</p>

<pre><code class="language-yaml">--- 
apiVersion: v1
kind: PersistentVolumeClaim
metadata: 
  name: trivy-cache
spec: 
  accessModes: 
    - ReadWriteOnce
  resources: 
    requests: 
      storage: 1G
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: trivy-scanner
  labels:
    app: trivy-scanner
  namespace: validation
spec:
  replicas: 1
  selector:
    matchLabels:
      app: trivy-scanner
  template:
    metadata:
      labels:
        app: trivy-scanner
    spec:
      securityContext:
        fsGroup: 10001
      containers:
        - name: trivy-scanner
          image: devopstales/trivy-scanner-admission:1.0.1
          imagePullPolicy: Always
          volumeMounts:
          - name: cache
            mountPath: &quot;/home/kube-trivy-admission/.cache/trivy&quot;
#          - name: config-json
#            mountPath: &quot;/home/kube-trivy-admission/.docker&quot;
      volumes:
      - name: cache
        persistentVolumeClaim:
            claimName: &quot;trivy-cache&quot;
#      - name: config-json
#        secret:
#          secretName: config-json
---
apiVersion: v1
kind: Service
metadata:
  name: trivy-scanner
  namespace: validation
spec:
  selector:
    app: trivy-scanner
  ports:
  - port: 443
    targetPort: 5000
</code></pre>

<p>The Service must be an HTTPS port on 443 for the Admission Webhook so I created a self-signed certificate and placed in the docker container.</p>

<h3 id="create-the-admission-webhook">Create the Admission Webhook</h3>

<p>The abow Admission Webhook will send teh HTTP request to my <code>trivy-scanner</code> service:</p>

<pre><code class="language-yaml">apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  name: trivy-scanner
webhooks:
  - name: trivy-scanner.devopstales.intra
    sideEffects: &quot;None&quot;
    admissionReviewVersions: [v1beta1, v1]
    clientConfig:
      service:
        name: trivy-scanner
        namespace: validation
        path: &quot;/validate&quot;
      caBundle: &quot;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&quot;
    rules:
      - apiGroups: [&quot;apps&quot;, &quot;&quot;]
        resources:
          - &quot;pods&quot;
        apiVersions:
          - &quot;*&quot;
        operations:
          - CREATE
</code></pre>

<p>I placed the root CA of my self-signed certificate in this Validating Webhook Configuration to make my Service&rsquo;s certiface valid for the Kubernetes api.</p>

<h3 id="policy">Policy</h3>

<p>Now If I create a Deployment, Pod &hellip; It scan the Image with tryvi. To block an Image I need to add the limits of the maximum numer of vulnerability for the severities.</p>

<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx
  name: nginx
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      annotations:
        trivy.security.devopstales.io/medium: &quot;5&quot;
        trivy.security.devopstales.io/low: &quot;10&quot;
        trivy.security.devopstales.io/critical: &quot;2&quot;
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - name: http
          containerPort: 80
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Continuous Image Security]]></title>
            <link href="https://devopstales.github.io/home/continuous-image-security/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/image-security-admission-controller-v2/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V2" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller" />
                <link href="https://devopstales.github.io/kubernetes/continuous-image-security/?utm_source=atom_feed" rel="related" type="text/html" title="Continuous Image Security" />
                <link href="https://devopstales.github.io/home/k8s-vault-v2/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes integration with external Vault" />
                <link href="https://devopstales.github.io/home/rke2-calico/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Install With Calico" />
            
                <id>https://devopstales.github.io/home/continuous-image-security/</id>
            
            
            <published>2021-06-15T00:00:00+00:00</published>
            <updated>2021-06-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you my tool to Continuously scann deployed images in your Kubernetes cluster.</p>

<h3 id="parst-of-the-k8s-security-series">Parst of the K8S Security series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
<li>Part2: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
<li>Part3: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
<li>Part4: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a>
<!-- RBAC -->
<!-- # Policy --></li>
<li>Part5: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
<li>Part6: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
<li>Part7: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a>
<!-- # Admission Controllers --></li>
<li>Part8: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a>
<!-- ## Image Scan --></li>
<li>Part9a: <a href="../../kubernetes/image-security-admission-controller">Image security Admission Controller</a></li>
<li>Part9b: <a href="../../kubernetes/image-security-admission-controller-v2">Image security Admission Controller V2</a></li>
<li>Part9c: <a href="../../kubernetes/image-security-admission-controller-v3">Image security Admission Controller V3</a>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring --></li>
<li>Part10: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
<li>Part11: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a>
<!-- DAST Exporter?? -->
<!-- CIS Exporter
https://github.com/ibrokethecloud/kube-bench-metrics --></li>
<li>Part12 <a href="../../kubernetes/k8s-connaisseur">Image Signature Verification with Connaisseur</a></li>
<li>Part13 <a href="../../kubernetes/k8s-backup">Backup your Kubernetes Cluster</a>
<!-- # Secrets --></li>
<li>Part14a <a href="../../kubernetes/k8s-vault">Kubernetes and Vault integration</a></li>
<li>Part14b <a href="../../kubernetes/k8s-vault-v2">Kubernetes External Vault integration</a></li>
<li>Part15a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
<li>Part15b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
<li>Part15c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a>
<!-- # Image -->
<!-- keel --></li>
<li>Part16: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a>
<!-- flux image update --></li>
<li>Part17: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
</ul>

<!-- admission-controller ami cseréli az imageben a repot !!!
 -->

<!-- sysdig falco -->

<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
-->

<!-- resource kvota -->

<!-- Backup
https://github.com/pieterlange/kube-backup 
-->

<!--
https://medium.com/@beatrizmrg/network-security-for-microservices-with-ebpf-bis-478b40e7befa

# container runetime
https://www.ianlewis.org/en/container-runtimes-part-1-introduction-container-r
http://pouchcontainer.io/pouch/docs/features/pouch_with_runV.html
-->

<p>In a previous posts we talked about admission-controllers that scnas the image at deploy. Like <a href="https://devopstales.github.io/home/image-security-admission-controller/">Banzaicloud&rsquo;s anchore-image-validator</a> and <a href="https://devopstales.github.io/home/image-security-admission-controller-v2/">Anchore&rsquo;s own admission-controller</a>. But what if you run your image for a long time. Last weak I realised I run containers wit imagest older the a year. I this time period many new vulnerability came up.</p>

<p>I find a tool called <a href="https://github.com/fleeto/trivy-scanner">trivy-scanner</a> that do almast what I want. It scans the docker images in all namespaces with the label <code>trivy=true</code> and get the resoults to a prometheus endpoint. It based on <a href="https://github.com/flant/shell-operator">Shell Operator</a> that runs a small python script. I made my own version from it:</p>

<h3 id="deploy-the-app">Deploy the app</h3>

<pre><code class="language-bash">git clone https://github.com/devopstales/trivy-scanner

nano trivy-scanner/deploy/kubernetes/kustomization.yaml
namespace: trivy-scanner
...

kubectl create ns trivy-scanner
kubectl aplly -k trivy-scanner/deploy/kubernetes/
</code></pre>

<h3 id="demo">Demo</h3>

<p>Test the <code>guestbook-demo</code> namespace:</p>

<pre><code class="language-bash">kubectl label namespaces guestbook-demo trivy=true

kubectl get service -n trivy-scanner
NAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)  AGE
trivy-scanner   ClusterIP   10.43.179.39   &lt;none&gt;        9115/TCP   15m

curl -s http://10.43.179.39:9115/metrics | grep so_vulnerabilities
</code></pre>

<p>Now you need to add the <code>trivy-scanner</code> <code>Service</code> as target for your prometheus. I created a <code>ServiceMonitor</code> object for that:</p>

<pre><code class="language-yaml">apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    serviceapp: trivy-exporter-servicemonitor
    release: prometheus
  name: trivy-exporter-servicemonitor
spec:
  selector:
    matchLabels:
      app: trivy-scanner
  endpoints:
  - port: metrics
</code></pre>

<p>If you use my grafana dasgboard from the repo you can see someting like this:</p>

<p><img src="/img/include/trivy-exporter.png" alt="image" /> <br></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[K8S Logging And Monitoring]]></title>
            <link href="https://devopstales.github.io/home/k8s-prometheus-stack/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-prometheus-stack/?utm_source=atom_feed" rel="related" type="text/html" title="K8S Logging And Monitoring" />
                <link href="https://devopstales.github.io/home/continuous-image-security/?utm_source=atom_feed" rel="related" type="text/html" title="Continuous Image Security" />
                <link href="https://devopstales.github.io/home/k8s-vault-v2/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes integration with external Vault" />
                <link href="https://devopstales.github.io/home/rke2-calico/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Install With Calico" />
                <link href="https://devopstales.github.io/home/rke2-cilium/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Install With cilium" />
            
                <id>https://devopstales.github.io/home/k8s-prometheus-stack/</id>
            
            
            <published>2021-06-15T00:00:00+00:00</published>
            <updated>2021-06-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this tutorial I will show you how to install a prometheus operator to monotor kubernetes and loki to gether logs.</p>

<h3 id="parst-of-the-k8s-security-series">Parst of the K8S Security series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
<li>Part2: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
<li>Part3: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
<li>Part4: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a>
<!-- RBAC -->
<!-- # Policy --></li>
<li>Part5: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
<li>Part6: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
<li>Part7: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a>
<!-- # Admission Controllers --></li>
<li>Part8: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a>
<!-- ## Image Scan --></li>
<li>Part9a: <a href="../../kubernetes/image-security-admission-controller">Image security Admission Controller</a></li>
<li>Part9b: <a href="../../kubernetes/image-security-admission-controller-v2">Image security Admission Controller V2</a></li>
<li>Part9c: <a href="../../kubernetes/image-security-admission-controller-v3">Image security Admission Controller V3</a>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring --></li>
<li>Part10: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
<li>Part11: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a>
<!-- DAST Exporter?? -->
<!-- CIS Exporter
https://github.com/ibrokethecloud/kube-bench-metrics --></li>
<li>Part12 <a href="../../kubernetes/k8s-connaisseur">Image Signature Verification with Connaisseur</a></li>
<li>Part13 <a href="../../kubernetes/k8s-backup">Backup your Kubernetes Cluster</a>
<!-- # Secrets --></li>
<li>Part14a <a href="../../kubernetes/k8s-vault">Kubernetes and Vault integration</a></li>
<li>Part14b <a href="../../kubernetes/k8s-vault-v2">Kubernetes External Vault integration</a></li>
<li>Part15a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
<li>Part15b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
<li>Part15c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a>
<!-- # Image -->
<!-- keel --></li>
<li>Part16: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a>
<!-- flux image update --></li>
<li>Part17: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
</ul>

<!-- admission-controller ami cseréli az imageben a repot !!!
 -->

<!-- sysdig falco -->

<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
-->

<!-- resource kvota -->

<!-- Backup
https://github.com/pieterlange/kube-backup 
-->

<!--
https://medium.com/@beatrizmrg/network-security-for-microservices-with-ebpf-bis-478b40e7befa

# container runetime
https://www.ianlewis.org/en/container-runtimes-part-1-introduction-container-r
http://pouchcontainer.io/pouch/docs/features/pouch_with_runV.html
-->

<p>Prometheus is an open-source monitoring system with a built-in noSQL time-series database. It offers a multi-dimensional data model, a flexible query language, and diverse visualization possibilities. Prometheus collects metrics from http nedpoint. Most service dind&rsquo;t have this endpoint so you need optional programs that generate additional metrics cald exporters.</p>

<h3 id="monitoring">Monitoring</h3>

<pre><code class="language-yaml">nano values.yaml
---
global:
  rbac:
    create: true
    pspEnabled: true

alertmanager:
  alertmanagerSpec:
    storage:
      volumeClaimTemplate:
        spec:
          resources:
            requests:
              storage: 10Gi
  ingress:
    enabled: true
    annotations:
      cert-manager.io/cluster-issuer: ca-issuer
    hosts:
      - alertmanager.k8s.intra
    paths:
    - /
    pathType: ImplementationSpecific
    tls:
    - secretName: tls-alertmanager-cert
      hosts:
      - alertmanager.k8s.intra

grafana:
  rbac:
    enable: true
    pspEnabled: true
    pspUseAppArmor: false
  initChownData:
    enabled: false
  enabled: true
  adminPassword: Password1
  plugins:
  - grafana-piechart-panel
  persistence:
    enabled: true
    size: 10Gi
  ingress:
    enabled: true
    annotations:
      cert-manager.io/cluster-issuer: ca-issuer
    hosts:
      - grafana.k8s.intra
    paths:
    - /
    pathType: ImplementationSpecific
    tls:
    - secretName: tls-grafana-cert
      hosts:
      - grafana.k8s.intra


prometheus:
  enabled: true
  prometheusSpec:
    podMonitorSelectorNilUsesHelmValues: false
    serviceMonitorSelectorNilUsesHelmValues: false
    secrets: ['etcd-client-cert']
    storageSpec:
      volumeClaimTemplate:
        spec:
          resources:
            requests:
              storage: 10Gi
  ingress:
    enabled: true
    annotations:
      cert-manager.io/cluster-issuer: ca-issuer
    hosts:
      - prometheus.k8s.intra
    paths:
    - /
    pathType: ImplementationSpecific
    tls:
    - secretName: tls-prometheus-cert
      hosts:
      - prometheus.k8s.intra
</code></pre>

<p>There is a bug in the Grafana helm chart so it didn&rsquo;t sreate the psp correcly for the init container: <a href="https://github.com/grafana/helm-charts/issues/427">https://github.com/grafana/helm-charts/issues/427</a></p>

<pre><code class="language-bash"># solution
kubectl edit psp prometheus-grafana
...
  runAsUser:
    rule: RunAsAny
...

kubectl get rs
NAME                                             DESIRED   CURRENT   READY   AGE
prometheus-grafana-74b5d957bc                    1         0         0       12m
...

kubectl delete rs prometheus-grafana-74b5d957bc
</code></pre>

<pre><code class="language-bash">#### grafana dashboards
## RKE2
# 14243
## NGINX Ingress controller
# 9614
## cert-manager
# 11001
## longhorn
# 13032
### kyverno
# https://raw.githubusercontent.com/kyverno/grafana-dashboard/master/grafana/dashboard.json
### calico
# 12175
# 3244
### cilium
# 6658
# 14500
# 14502
# 14501
</code></pre>

<pre><code class="language-bash">helm repo add prometheus-community https://prometheus-community.github.io/helm-charts

helm install prometheus prometheus-community/kube-prometheus-stack -n monitoring -f values.yaml
</code></pre>

<p>For the proxy down status:</p>

<pre><code class="language-bash">kubectl edit cm/kube-proxy -n kube-system
...
kind: KubeProxyConfiguration
metricsBindAddress: 0.0.0.0:10249
...
</code></pre>

<p>If you use rke2 you can configure this from the helm chart before first start:</p>

<pre><code class="language-bash">cat &lt;&lt; EOF &gt; /var/lib/rancher/rke2/server/manifests/rke2-kube-proxy-config.yaml
apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-kube-proxy
  namespace: kube-system
spec:
  valuesContent: |-
    metricsBindAddress: 0.0.0.0:10249
EOF
</code></pre>

<p>For the controller-manager down status:</p>

<pre><code class="language-bash">nano /etc/kubernetes/manifests/kube-controller-manager.yaml
# OR
nano /var/lib/rancher/rke2/agent/pod-manifests/kube-controller-manager.yaml
apiVersion: v1
kind: Pod
metadata:
  ...
spec:
  containers:
  - command:
    - kube-controller-manager
    ...
    - --address=0.0.0.0
    ...
    - --bind-address=&lt;your control-plane IP or 0.0.0.0&gt;
    ...
    livenessProbe:
      failureThreshold: 8
      httpGet:
       	host: 0.0.0.0
    ...
</code></pre>

<p>For the kube-scheduler down status:</p>

<pre><code class="language-bash">nano /etc/kubernetes/manifests/kube-scheduler.yaml
# OR
nano /var/lib/rancher/rke2/agent/pod-manifests/kube-scheduler.yaml
apiVersion: v1
kind: Pod
metadata:
  ...
spec:
  containers:
  - command:
    - kube-scheduler
    - --address=0.0.0.0
    - --bind-address=0.0.0.0
    ...
    livenessProbe:
      failureThreshold: 8
      httpGet:
       	host: 0.0.0.0
    ...
</code></pre>

<p>For the etcd down status firs we need to create a secret to authenticate for the etcd:</p>

<pre><code class="language-bash"># kubeadm
kubectl -n monitoring create secret generic etcd-client-cert \
--from-file=/etc/kubernetes/pki/etcd/ca.crt \
--from-file=/etc/kubernetes/pki/etcd/healthcheck-client.crt \
--from-file=/etc/kubernetes/pki/etcd/healthcheck-client.key

# rancher
kubectl -n monitoring create secret generic etcd-client-cert \
--from-file=/var/lib/rancher/rke2/server/tls/etcd/server-ca.crt \
--from-file=/var/lib/rancher/rke2/server/tls/etcd/server-client.crt \
--from-file=/var/lib/rancher/rke2/server/tls/etcd/server-client.key
</code></pre>

<p>Then we configure the prometheus to use it:</p>

<pre><code class="language-yaml">nano values.yaml
---
...
prometheus:
  enabled: true
  prometheusSpec:
    secrets: ['etcd-client-cert']
...
kubeEtcd:
  enabled: true
  service:
    port: 2379
    targetPort: 2379
    selector:
      component: etcd
  serviceMonitor:
    interval: &quot;&quot;
    scheme: https
    insecureSkipVerify: true
    serverName: &quot;&quot;
    metricRelabelings: []
    relabelings: []
    caFile: /etc/prometheus/secrets/etcd-client-cert/server-ca.crt
    certFile: /etc/prometheus/secrets/etcd-client-cert/server-client.crt
    keyFile: /etc/prometheus/secrets/etcd-client-cert/server-client.key

# for kubeadm
#    caFile: /etc/prometheus/secrets/etcd-client-cert/ca.crt
#    certFile: /etc/prometheus/secrets/etcd-client-cert/healthcheck-client.crt
#    keyFile: /etc/prometheus/secrets/etcd-client-cert/healthcheck-client.key

</code></pre>

<h3 id="monitoring-nginx">Monitoring Nginx</h3>

<pre><code class="language-yaml">cat &lt;&lt; EOF &gt; /var/lib/rancher/rke2/server/manifests/rke2-ingress-nginx-config.yaml
apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-ingress-nginx
  namespace: kube-system
spec:
  valuesContent: |-
    controller:
      metrics:
	      enabled: true
        service:
          annotations:
            prometheus.io/scrape: &quot;true&quot;
            prometheus.io/port: &quot;10254&quot;
        serviceMonitor:
          enabled: true
          namespace: &quot;monitoring&quot;
EOF

kaf /var/lib/rancher/rke2/server/manifests/rke2-ingress-nginx-config.yaml
</code></pre>

<h3 id="monitoring-core-dns">Monitoring Core-DNS</h3>

<pre><code class="language-yaml">cat &lt;&lt; EOF &gt; default-network-dns-policy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-network-dns-policy
  namespace: kube-system
spec:
  ingress:
  - ports:
    - port: 53
      protocol: TCP
    - port: 53
      protocol: UDP
    - port: 9153
      protocol: TCP
  podSelector:
    matchLabels:
      k8s-app: kube-dns
  policyTypes:
  - Ingress
EOF

kaf default-network-dns-policy.yaml
</code></pre>

<h3 id="monitor-cert-manager">Monitor cert-manager</h3>

<pre><code class="language-yaml">nano 01-cert-managger.yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: ingress-system
---
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: cert-manager
  namespace: ingress-system
spec:
  repo: &quot;https://charts.jetstack.io&quot;
  chart: cert-manager
  targetNamespace: ingress-system
  valuesContent: |-
    installCRDs: true
    clusterResourceNamespace: &quot;ingress-system&quot;
    prometheus:
      enabled: true
      servicemonitor:
        enabled: true
        namespace: &quot;monitoring&quot;

kubectl apply -f 01-cert-managger.yaml
</code></pre>

<h3 id="longhorn">Longhorn</h3>

<pre><code class="language-yaml">apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: longhorn-prometheus-servicemonitor
  namespace: monitoring
  labels:
    name: longhorn-prometheus-servicemonitor
spec:
  selector:
    matchLabels:
      app: longhorn-manager
  namespaceSelector:
    matchNames:
    - longhorn-system
  endpoints:
  - port: manager
</code></pre>

<h3 id="logging">Logging</h3>

<pre><code class="language-bash">helm repo add loki https://grafana.github.io/loki/charts
 
helm repo update
 
helm search repo loki
 
helm install loki-stack loki/loki-stack \
--create-namespace \
--namespace loki-stack \
--set promtail.enabled=true,loki.persistence.enabled=true,loki.persistence.size=100Gi
</code></pre>

<p>Add datasource to grafana:</p>

<pre><code>type: loki
name: Loki
url: http://loki-stack.loki-stack:3100
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes integration with external Vault]]></title>
            <link href="https://devopstales.github.io/home/k8s-vault-v2/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-vault/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes and Vault integration" />
                <link href="https://devopstales.github.io/kubernetes/k8s-vault-v2/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes integration with external Vault" />
                <link href="https://devopstales.github.io/home/rke2-calico/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Install With Calico" />
                <link href="https://devopstales.github.io/home/rke2-cilium/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Install With cilium" />
                <link href="https://devopstales.github.io/home/k3s-gvisor/?utm_source=atom_feed" rel="related" type="text/html" title="Secure k3s with gVisor" />
            
                <id>https://devopstales.github.io/home/k8s-vault-v2/</id>
            
            
            <published>2021-06-05T00:00:00+00:00</published>
            <updated>2021-06-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can integrate an external HashiCorp Vault to Kubernetes.</p>

<h3 id="parst-of-the-k8s-security-series">Parst of the K8S Security series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
<li>Part2: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
<li>Part3: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
<li>Part4: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a>
<!-- RBAC -->
<!-- # Policy --></li>
<li>Part5: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
<li>Part6: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
<li>Part7: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a>
<!-- # Admission Controllers --></li>
<li>Part8: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a>
<!-- ## Image Scan --></li>
<li>Part9a: <a href="../../kubernetes/image-security-admission-controller">Image security Admission Controller</a></li>
<li>Part9b: <a href="../../kubernetes/image-security-admission-controller-v2">Image security Admission Controller V2</a></li>
<li>Part9c: <a href="../../kubernetes/image-security-admission-controller-v3">Image security Admission Controller V3</a>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring --></li>
<li>Part10: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
<li>Part11: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a>
<!-- DAST Exporter?? -->
<!-- CIS Exporter
https://github.com/ibrokethecloud/kube-bench-metrics --></li>
<li>Part12 <a href="../../kubernetes/k8s-connaisseur">Image Signature Verification with Connaisseur</a></li>
<li>Part13 <a href="../../kubernetes/k8s-backup">Backup your Kubernetes Cluster</a>
<!-- # Secrets --></li>
<li>Part14a <a href="../../kubernetes/k8s-vault">Kubernetes and Vault integration</a></li>
<li>Part14b <a href="../../kubernetes/k8s-vault-v2">Kubernetes External Vault integration</a></li>
<li>Part15a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
<li>Part15b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
<li>Part15c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a>
<!-- # Image -->
<!-- keel --></li>
<li>Part16: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a>
<!-- flux image update --></li>
<li>Part17: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
</ul>

<!-- admission-controller ami cseréli az imageben a repot !!!
 -->

<!-- sysdig falco -->

<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
-->

<!-- resource kvota -->

<!-- Backup
https://github.com/pieterlange/kube-backup 
-->

<!--
https://medium.com/@beatrizmrg/network-security-for-microservices-with-ebpf-bis-478b40e7befa

# container runetime
https://www.ianlewis.org/en/container-runtimes-part-1-introduction-container-r
http://pouchcontainer.io/pouch/docs/features/pouch_with_runV.html
-->

<h3 id="vhat-is-hashicorp-vault">Vhat is Hashicorp Vault</h3>

<p>HashiCorp Vault is a secrets management solution that brokers access for both humans and machines, through programmatic access, to systems. Secrets can be stored, dynamically generated, and in the case of encryption, keys can be consumed as a service without the need to expose the underlying key materials.</p>

<p><img src="/img/include/vault-k8s-auth-workflow.png" alt="Example image" /></p>

<h3 id="k3s-install">K3s install</h3>

<pre><code class="language-bash">ssh-copy-id vagrant@172.17.8.101

k3sup install \
  --ip=172.17.8.101 \
  --user=vagrant \
  --sudo \
  --tls-san=172.17.8.100 \
  --cluster \
  --k3s-channel=stable \
  --k3s-extra-args &quot;--no-deploy=traefik --flannel-iface=enp0s8 --node-ip=172.17.8.101&quot; \
  --merge \
  --local-path $HOME/.kube/config \
  --context=k3s-ha
</code></pre>

<h3 id="install-vault">Install vault</h3>

<pre><code class="language-bash">sudo dnf install -y dnf-plugins-core nano jq

cp /etc/rancher/k3s/k3s.yaml ~/.kube/config

sudo dnf config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo
sudo dnf install -y vault
</code></pre>

<pre><code class="language-bash">nano /etc/vault.d/vault.hcl
# HTTP listener
listener &quot;tcp&quot; {
  address = &quot;0.0.0.0:8200&quot;
  tls_disable = 1
}

# HTTPS listener
#listener &quot;tcp&quot; {
#  address       = &quot;0.0.0.0:8200&quot;
#  tls_cert_file = &quot;/opt/vault/tls/tls.crt&quot;
#  tls_key_file  = &quot;/opt/vault/tls/tls.key&quot;
#}

api_addr         = &quot;http://0.0.0.0:8200&quot;
</code></pre>

<pre><code class="language-bash">systemctl start vault
systemctl enable vault

vault -autocomplete-install
complete -C /usr/bin/vault vault

export VAULT_ADDR=http://127.0.0.1:8200
echo &quot;export VAULT_ADDR=http://127.0.0.1:8200&quot; &gt;&gt; ~/.bashrc

vault status

vault operator init | tee /opt/vault/init.txt
Unseal Key 1: t4PsGsw8cj25l9tSpvh2Avr5647HhdaI27aAzSiYJz0=

Initial Root Token: s.sPKauYvv9iFKliclTIaMgbU1
</code></pre>

<pre><code class="language-bash">export VAULT_TOKEN=&quot;s.sPKauYvv9iFKliclTIaMgbU1&quot;

vault operator unseal t4PsGsw8cj25l9tSpvh2Avr5647HhdaI27aAzSiYJz0=
</code></pre>

<pre><code class="language-bash">vault auth enable userpass
vault write auth/userpass/users/devopstales \
    password=Password1 \
    policies=admins
</code></pre>

<h3 id="integrate-a-kubernetes-cluster-with-an-external-vault">Integrate a Kubernetes Cluster with an External Vault</h3>

<p><a href="https://3p8owy1gdkoh452nrc36wbnp-wpengine.netdna-ssl.com/wp-content/uploads/2018/12/nirmata-vault-7-1024x623.png">https://3p8owy1gdkoh452nrc36wbnp-wpengine.netdna-ssl.com/wp-content/uploads/2018/12/nirmata-vault-7-1024x623.png</a></p>

<pre><code class="language-bash">vault secrets enable kv

vault kv put kv/secret/devwebapp/config username='giraffe' password='salsa'

vault kv get -format=json kv/secret/devwebapp/config | jq &quot;.data&quot;
</code></pre>

<pre><code class="language-bash"># create policy
vault policy write devwebapp-kv-ro - &lt;&lt;EOF
path &quot;kv/secret/devwebapp/*&quot; {
    capabilities = [&quot;read&quot;, &quot;list&quot;]
}
EOF
</code></pre>

<pre><code class="language-bash"># create Kubernetes ServiceAccount
cat &gt; internal-app.yaml &lt;&lt;EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: devwebapp
---
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    name: role-tokenreview-binding
    namespace: default
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:auth-delegator
  subjects:
  - kind: ServiceAccount
    name: devwebapp
    namespace: default
EOF

kubectl apply --filename internal-app.yaml
</code></pre>

<pre><code class="language-bash">export EXTERNAL_VAULT_ADDR=172.17.8.101
export K8S_HOST=172.17.8.101

export VAULT_SA_NAME=$(kubectl get sa devwebapp \
    -o jsonpath=&quot;{.secrets[*]['name']}&quot;)

export SA_JWT_TOKEN=$(kubectl get secret $VAULT_SA_NAME \
    -o jsonpath=&quot;{.data.token}&quot; | base64 --decode; echo)

export SA_CA_CRT=$(kubectl get secret $VAULT_SA_NAME \
    -o jsonpath=&quot;{.data['ca\.crt']}&quot; | base64 --decode; echo)

vault auth enable kubernetes

vault write auth/kubernetes/config \
  issuer=&quot;https://kubernetes.default.svc.cluster.local&quot; \
  token_reviewer_jwt=&quot;$SA_JWT_TOKEN&quot; \
  kubernetes_host=&quot;https://$K8S_HOST:6443&quot; \
  kubernetes_ca_cert=&quot;$SA_CA_CRT&quot;

vault write auth/kubernetes/role/devwebapp \
        bound_service_account_names=devwebapp \
        bound_service_account_namespaces=default \
        policies=devwebapp-kv-ro \
        ttl=24h
</code></pre>

<h3 id="install-vault-agent-injector">INstall Vault Agent Injector</h3>

<pre><code class="language-bash">dnf copr enable cerenit/helm -y
dnf install helm -y

helm repo add hashicorp https://helm.releases.hashicorp.com
helm repo update

### változók???
helm install vault hashicorp/vault \
    --set &quot;injector.externalVaultAddr=http://$EXTERNAL_VAULT_ADDR:8200&quot;
</code></pre>

<h3 id="demo">Demo</h3>

<pre><code class="language-bash">cat &gt; devwebapp.yaml &lt;&lt;EOF
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: orgchart
  labels:
    app: orgchart
spec:
  selector:
    matchLabels:
      app: orgchart
  replicas: 1
  template:
    metadata:
      annotations:
        vault.hashicorp.com/agent-inject: &quot;true&quot;
        vault.hashicorp.com/role: &quot;devwebapp&quot;
        vault.hashicorp.com/agent-inject-secret-config.txt: &quot;kv/secret/devwebapp/config&quot;
      labels:
        app: orgchart
    spec:
      serviceAccountName: devwebapp
      containers:
        - name: orgchart
          image: jweissig/app:0.0.1
EOF

kubectl apply -f devwebapp.yaml

kubectl exec \
    $(kubectl get pod -l app=orgchart -o jsonpath=&quot;{.items[0].metadata.name}&quot;) \
    --container orgchart -- cat /vault/secrets/config.txt
</code></pre>

<hr />

<ul>
<li><a href="https://tansanrao.com/hashicorp-vault-sidecar/">https://tansanrao.com/hashicorp-vault-sidecar/</a></li>
<li><a href="https://learn.hashicorp.com/tutorials/vault/kubernetes-external-vault?in=vault/kubernetes">https://learn.hashicorp.com/tutorials/vault/kubernetes-external-vault?in=vault/kubernetes</a></li>
<li><a href="https://learn.hashicorp.com/tutorials/vault/kubernetes-secret-store-driver?in=vault/kubernetes">https://learn.hashicorp.com/tutorials/vault/kubernetes-secret-store-driver?in=vault/kubernetes</a></li>
<li><a href="https://www.vaultproject.io/docs/platform/k8s/injector/examples#environment-variable-example">https://www.vaultproject.io/docs/platform/k8s/injector/examples#environment-variable-example</a>

<ul>
<li>mount to /etc/profile.d/secret in export format</li>
</ul></li>
</ul>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[RKE2 Install With Calico]]></title>
            <link href="https://devopstales.github.io/home/rke2-calico/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/rke2-cilium/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Install With cilium" />
                <link href="https://devopstales.github.io/kubernetes/rke2-calico/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Install With Calico" />
                <link href="https://devopstales.github.io/home/k8s-vault/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes and Vault integration" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller-v2/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V2" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller" />
            
                <id>https://devopstales.github.io/home/rke2-calico/</id>
            
            
            <published>2021-05-25T00:00:00+00:00</published>
            <updated>2021-05-25T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can install a RKE2 in with Calico&rsquo;s encripted VXLAN.</p>

<h3 id="parst-of-the-k8s-security-series">Parst of the K8S Security series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
<li>Part2: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
<li>Part3: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
<li>Part4: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a>
<!-- RBAC -->
<!-- # Policy --></li>
<li>Part5: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
<li>Part6: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
<li>Part7: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a>
<!-- # Admission Controllers --></li>
<li>Part8: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a>
<!-- ## Image Scan --></li>
<li>Part9a: <a href="../../kubernetes/image-security-admission-controller">Image security Admission Controller</a></li>
<li>Part9b: <a href="../../kubernetes/image-security-admission-controller-v2">Image security Admission Controller V2</a></li>
<li>Part9c: <a href="../../kubernetes/image-security-admission-controller-v3">Image security Admission Controller V3</a>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring --></li>
<li>Part10: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
<li>Part11: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a>
<!-- DAST Exporter?? -->
<!-- CIS Exporter
https://github.com/ibrokethecloud/kube-bench-metrics --></li>
<li>Part12 <a href="../../kubernetes/k8s-connaisseur">Image Signature Verification with Connaisseur</a></li>
<li>Part13 <a href="../../kubernetes/k8s-backup">Backup your Kubernetes Cluster</a>
<!-- # Secrets --></li>
<li>Part14a <a href="../../kubernetes/k8s-vault">Kubernetes and Vault integration</a></li>
<li>Part14b <a href="../../kubernetes/k8s-vault-v2">Kubernetes External Vault integration</a></li>
<li>Part15a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
<li>Part15b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
<li>Part15c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a>
<!-- # Image -->
<!-- keel --></li>
<li>Part16: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a>
<!-- flux image update --></li>
<li>Part17: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
</ul>

<!-- admission-controller ami cseréli az imageben a repot !!!
 -->

<!-- sysdig falco -->

<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
-->

<!-- resource kvota -->

<!-- Backup
https://github.com/pieterlange/kube-backup 
-->

<!--
https://medium.com/@beatrizmrg/network-security-for-microservices-with-ebpf-bis-478b40e7befa

# container runetime
https://www.ianlewis.org/en/container-runtimes-part-1-introduction-container-r
http://pouchcontainer.io/pouch/docs/features/pouch_with_runV.html
-->

<h2 id="rke2-setup">RKE2 Setup</h2>

<h3 id="project-longhorn-prerequisites">Project Longhorn Prerequisites</h3>

<pre><code class="language-bash">yum install -y epel-release
yum install -y nano curl wget git tmux jq
yum install -y iscsi-initiator-utils 
modprobe iscsi_tcp
echo &quot;iscsi_tcp&quot; &gt;/etc/modules-load.d/iscsi-tcp.conf
systemctl enable iscsid
systemctl start iscsid 
</code></pre>

<p>Ensure the eBFP filesystem is mounted (which should already be the case on RHEL 8.3):</p>

<pre><code class="language-bash">mount | grep /sys/fs/bpf
# if present should output, e.g. &quot;none on /sys/fs/bpf type bpf&quot;...
</code></pre>

<p>If that&rsquo;s not the case, mount it using the commands down here:</p>

<pre><code class="language-bash">sudo mount bpffs -t bpf /sys/fs/bpf
sudo bash -c 'cat &lt;&lt;EOF &gt;&gt; /etc/fstab
none /sys/fs/bpf bpf rw,relatime 0 0
EOF'
</code></pre>

<pre><code class="language-bash">cat &lt;&lt;EOF&gt;&gt; /etc/NetworkManager/conf.d/rke2-canal.conf
[keyfile]
unmanaged-devices=interface-name:cali*;interface-name:flannel*
EOF
systemctl reload NetworkManager
</code></pre>

<h3 id="rke2-rpm-install">RKE2 rpm Install</h3>

<pre><code class="language-bash">cat &lt;&lt; EOF &gt; /etc/yum.repos.d/rancher-rke2-1-20-latest.repo
[rancher-rke2-common-latest]
name=Rancher RKE2 Common Latest
baseurl=https://rpm.rancher.io/rke2/latest/common/centos/8/noarch
enabled=1
gpgcheck=1
gpgkey=https://rpm.rancher.io/public.key

[rancher-rke2-1-20-latest]
name=Rancher RKE2 1.20 Latest
baseurl=https://rpm.rancher.io/rke2/latest/1.20/centos/8/x86_64
enabled=1
gpgcheck=1
gpgkey=https://rpm.rancher.io/public.key
EOF

yum -y install rke2-server
</code></pre>

<h3 id="kubectl-helm-rke2">Kubectl, Helm &amp; RKE2</h3>

<p>Install <code>kubectl</code>, <code>helm</code> and RKE2 to the host system:</p>

<pre><code class="language-bash">sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx
sudo ln -s /opt/kubectx/kubens /usr/local/bin/kubens
echo 'PATH=$PATH:/usr/local/bin' &gt;&gt; /etc/profile
echo 'PATH=$PATH:/var/lib/rancher/rke2/bin' &gt;&gt; /etc/profile
source /etc/profile

sudo dnf copr -y enable cerenit/helm
sudo dnf install -y helm
</code></pre>

<h3 id="rke2-specific-ports">RKE2 specific ports</h3>

<pre><code class="language-bash">sudo firewall-cmd --add-port=9345/tcp --permanent
sudo firewall-cmd --add-port=6443/tcp --permanent
sudo firewall-cmd --add-port=10250Air-Gap/tcp --permanent
sudo firewall-cmd --add-port=2379/tcp --permanent
sudo firewall-cmd --add-port=2380/tcp --permanent
sudo firewall-cmd --add-port=30000-32767/tcp --permanent
# Used for the Rancher Monitoring
sudo firewall-cmd --add-port=9796/tcp --permanent
sudo firewall-cmd --add-port=19090/tcp --permanent
sudo firewall-cmd --add-port=6942/tcp --permanent
sudo firewall-cmd --add-port=9091/tcp --permanent
### CNI specific ports
# 4244/TCP is required when the Hubble Relay is enabled and therefore needs to connect to all agents to collect the flows
sudo firewall-cmd --add-port=4244/tcp --permanent
# Cilium healthcheck related permits:
sudo firewall-cmd --add-port=4240/tcp --permanent
sudo firewall-cmd --remove-icmp-block=echo-request --permanent
sudo firewall-cmd --remove-icmp-block=echo-reply --permanent
# Since we are using Cilium with GENEVE as overlay, we need the following port too:
sudo firewall-cmd --add-port=6081/udp --permanent
### Ingress Controller specific ports
sudo firewall-cmd --add-port=80/tcp --permanent
sudo firewall-cmd --add-port=443/tcp --permanent
### To get DNS resolution working, simply enable Masquerading.
sudo firewall-cmd --zone=public  --add-masquerade --permanent

### Finally apply all the firewall changes
sudo firewall-cmd --reload
</code></pre>

<p>Verification:</p>

<pre><code class="language-bash">sudo firewall-cmd --list-all
public (active)
  target: default
  icmp-block-inversion: no
  interfaces: eno1
  sources: 
  services: cockpit dhcpv6-client ssh wireguard
  ports: 9345/tcp 6443/tcp 10250/tcp 2379/tcp 2380/tcp 30000-32767/tcp 4240/tcp 6081/udp 80/tcp 443/tcp 4244/tcp 9796/tcp 19090/tcp 6942/tcp 9091/tcp
  protocols: 
  masquerade: yes
  forward-ports: 
  source-ports: 
  icmp-blocks: 
  rich rules: 
</code></pre>

<h3 id="basic-configuration">Basic Configuration</h3>

<pre><code class="language-bash">mkdir -p /etc/rancher/rke2
cat &lt;&lt; EOF &gt;  /etc/rancher/rke2/config.yaml
write-kubeconfig-mode: &quot;0644&quot;
profile: &quot;cis-1.5&quot;
selinux: true
# add ips/hostname of hosts and loadbalancer
tls-san:
  - &quot;k8s.mydomain.intra&quot;
  - &quot;172.17.9.10&quot;
# Make a etcd snapshot every 6 hours
etcd-snapshot-schedule-cron: &quot; */6 * * *&quot;
# Keep 56 etcd snapshorts (equals to 2 weeks with 6 a day)
etcd-snapshot-retention: 56
cni:
  - calico
disable:
  - rke2-canal
  - rke2-kube-proxy
EOF
</code></pre>

<p><strong>Note:</strong> I disabled <code>rke2-canal</code> and <code>rke2-kube-proxy</code> since I plan to install Canal as CNI in <a href="https://docs.projectcalico.org/maintenance/ebpf/enabling-bpf">&ldquo;kube-proxy less mode&rdquo;</a>. Do not disable <code>rke2-kube-proxy</code> if you use another CNI - it will not work afterwards!</p>

<pre><code class="language-bash">sudo cp -f /usr/share/rke2/rke2-cis-sysctl.conf /etc/sysctl.d/60-rke2-cis.conf
sysctl -p /etc/sysctl.d/60-rke2-cis.conf

useradd -r -c &quot;etcd user&quot; -s /sbin/nologin -M etcd

mkdir -p /var/lib/rancher/rke2/server/manifests/
cat &lt;&lt; EOF &gt; /var/lib/rancher/rke2/server/manifests/rke2-ingress-nginx-config.yaml
apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-ingress-nginx
  namespace: kube-system
spec:
  valuesContent: |-
    controller:
      metrics:
        service:
          annotations:
            prometheus.io/scrape: &quot;true&quot;
            prometheus.io/port: &quot;10254&quot;
EOF
</code></pre>

<p>!!!!!!!!!!!!!!!!!!!!!</p>

<pre><code class="language-bash">kubectl get endpoints kubernetes -o wide
NAME         ENDPOINTS        AGE
kubernetes   10.0.2.15:6443   86m
</code></pre>

<pre><code class="language-yaml">cat &lt;&lt; EOF &gt;  /var/lib/rancher/rke2/server/manifests/rke2-cilium.yaml
apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-cilium
  namespace: kube-system
spec:
  valuesContent: |-
    cilium:
      k8sServiceHost: 10.0.2.15
      k8sServicePort: 6443
      operator:
        replicas: 1
      global:
        encryption:
          enabled: true
          nodeEncryption: true
      hubble:
        metrics:
          enabled:
          - dns:query;ignoreAAAA
          - drop
          - tcp
          - flow
          - icmp
          - http
        relay:
          enabled: true
        ui:
          enabled: true
          replicas: 1
          ingress:
            enabled: true
            hosts:
              - hubble.k8s.intra
            annotations:
              cert-manager.io/cluster-issuer: ca-issuer
            tls:
            - secretName: ingress-hubble-ui
              hosts:
              - hubble.k8s.intra
      prometheus:
        enabled: true
        # Default port value (9090) needs to be changed since the RHEL cockpit also listens on this port.
        port: 19090
        # Configure this serviceMonitor section AFTER Rancher Monitoring is enabled!
        #serviceMonitor:
        #  enabled: true
EOF
</code></pre>

<h3 id="prevent-rke2-package-updates">Prevent RKE2 Package Updates</h3>

<p>In order to provide more stability, I chose to DNF/YUM &ldquo;mark/hold&rdquo; the RKE2 related packages so a <code>dnf update</code>/<code>yum update</code> does not mess around with them.</p>

<p>Add the following line to <code>/etc/dnf/dnf.conf</code> and/or <code>/etc/yum.conf</code>:</p>

<pre><code class="language-bash">exclude=rke2-*
</code></pre>

<h2 id="starting-rke2">Starting RKE2</h2>

<p>Enable the <code>rke2-server</code> service and start it:</p>

<pre><code class="language-bash">sudo systemctl enable rke2-server --now
</code></pre>

<p>Verification:</p>

<pre><code class="language-bash">sudo systemctl status rke2-server
sudo journalctl -u rke2-server -f
</code></pre>

<h2 id="configure-kubectl-on-rke2-host">Configure Kubectl (on RKE2 Host)</h2>

<pre><code class="language-bash">mkdir ~/.kube
ln -s /etc/rancher/rke2/rke2.yaml ~/.kube/config
chmod 600 /root/.kube/config
ln -s /var/lib/rancher/rke2/agent/etc/crictl.yaml /etc/crictl.yaml

kubectl get node
crictl ps
crictl images
</code></pre>

<p>Verification:</p>

<pre><code class="language-bash">kubectl get nodes
NAME                    STATUS   ROLES         AGE     VERSION
k8s.mydomain.intra   Ready   etcd,master   2m4s   v1.18.16+rke2r1
</code></pre>

<h3 id="deploy-demo-app">Deploy demo app</h3>

<p>```
kubens default
kubectl apply -f <a href="https://raw.githubusercontent.com/cilium/cilium/v1.9/examples/minikube/http-sw-app.yaml">https://raw.githubusercontent.com/cilium/cilium/v1.9/examples/minikube/http-sw-app.yaml</a>
kubectl apply -f k8s_sec_lab/manifest/cilium_demo_rb.yaml</p>

<p>kubectl exec xwing &ndash; curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing
kubectl exec tiefighter &ndash; curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[RKE2 Install With cilium]]></title>
            <link href="https://devopstales.github.io/home/rke2-cilium/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/rke2-cilium/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Install With cilium" />
                <link href="https://devopstales.github.io/home/k8s-vault/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes and Vault integration" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller-v2/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V2" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller" />
                <link href="https://devopstales.github.io/home/rke2-pod-security-policy/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Pod Security Policy" />
            
                <id>https://devopstales.github.io/home/rke2-cilium/</id>
            
            
            <published>2021-05-24T00:00:00+00:00</published>
            <updated>2021-05-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can install a RKE2 in with cilium&rsquo;s encripted VXLAN.</p>

<h3 id="parst-of-the-k8s-security-series">Parst of the K8S Security series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
<li>Part2: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
<li>Part3: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
<li>Part4: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a>
<!-- RBAC -->
<!-- # Policy --></li>
<li>Part5: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
<li>Part6: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
<li>Part7: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a>
<!-- # Admission Controllers --></li>
<li>Part8: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a>
<!-- ## Image Scan --></li>
<li>Part9a: <a href="../../kubernetes/image-security-admission-controller">Image security Admission Controller</a></li>
<li>Part9b: <a href="../../kubernetes/image-security-admission-controller-v2">Image security Admission Controller V2</a></li>
<li>Part9c: <a href="../../kubernetes/image-security-admission-controller-v3">Image security Admission Controller V3</a>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring --></li>
<li>Part10: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
<li>Part11: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a>
<!-- DAST Exporter?? -->
<!-- CIS Exporter
https://github.com/ibrokethecloud/kube-bench-metrics --></li>
<li>Part12 <a href="../../kubernetes/k8s-connaisseur">Image Signature Verification with Connaisseur</a></li>
<li>Part13 <a href="../../kubernetes/k8s-backup">Backup your Kubernetes Cluster</a>
<!-- # Secrets --></li>
<li>Part14a <a href="../../kubernetes/k8s-vault">Kubernetes and Vault integration</a></li>
<li>Part14b <a href="../../kubernetes/k8s-vault-v2">Kubernetes External Vault integration</a></li>
<li>Part15a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
<li>Part15b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
<li>Part15c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a>
<!-- # Image -->
<!-- keel --></li>
<li>Part16: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a>
<!-- flux image update --></li>
<li>Part17: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
</ul>

<!-- admission-controller ami cseréli az imageben a repot !!!
 -->

<!-- sysdig falco -->

<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
-->

<!-- resource kvota -->

<!-- Backup
https://github.com/pieterlange/kube-backup 
-->

<!--
https://medium.com/@beatrizmrg/network-security-for-microservices-with-ebpf-bis-478b40e7befa

# container runetime
https://www.ianlewis.org/en/container-runtimes-part-1-introduction-container-r
http://pouchcontainer.io/pouch/docs/features/pouch_with_runV.html
-->

<h3 id="what-is-cilium">What is Cilium?</h3>

<p>Cilium is open source software for transparently securing the network connectivity between application services deployed using Linux container management platforms like Docker and Kubernetes.</p>

<p>At the foundation of Cilium is a new Linux kernel technology called eBPF, which enables the dynamic insertion of powerful security visibility and control logic within Linux itself. Because eBPF runs inside the Linux kernel, Cilium security policies can be applied and updated without any changes to the application code or container configuration. (Source: <a href="https://docs.cilium.io/en/v1.9/intro/">cilium.io</a> )</p>

<h3 id="what-is-hubble">What is Hubble?</h3>

<p>Hubble is a fully distributed networking and security observability platform. It is built on top of Cilium and eBPF to enable deep visibility into the communication and behavior of services as well as the networking infrastructure in a completely transparent manner.</p>

<p>By building on top of Cilium, Hubble can leverage eBPF for visibility. By relying on eBPF, all visibility is programmable and allows for a dynamic approach that minimizes overhead while providing deep and detailed visibility as required by users. Hubble has been created and specifically designed to make best use of these new eBPF powers. (Source: <a href="https://docs.cilium.io/en/v1.9/intro/">cilium.io</a> )</p>

<h2 id="rke2-setup">RKE2 Setup</h2>

<h3 id="project-longhorn-prerequisites">Project Longhorn Prerequisites</h3>

<pre><code class="language-bash">yum install -y epel-release
yum install -y nano curl wget git tmux jq vim-common
yum install -y iscsi-initiator-utils 
modprobe iscsi_tcp
echo &quot;iscsi_tcp&quot; &gt;/etc/modules-load.d/iscsi-tcp.conf
systemctl enable iscsid
systemctl start iscsid 
</code></pre>

<pre><code class="language-bash">cat &lt;&lt;EOF&gt;&gt; /etc/NetworkManager/conf.d/rke2-canal.conf
[keyfile]
unmanaged-devices=interface-name:cali*;interface-name:flannel*
EOF
systemctl reload NetworkManager
</code></pre>

<h3 id="rke2-rpm-install">RKE2 rpm Install</h3>

<pre><code class="language-bash">cat &lt;&lt; EOF &gt; /etc/yum.repos.d/rancher-rke2-1-20-latest.repo
[rancher-rke2-common-latest]
name=Rancher RKE2 Common Latest
baseurl=https://rpm.rancher.io/rke2/latest/common/centos/8/noarch
enabled=1
gpgcheck=1
gpgkey=https://rpm.rancher.io/public.key

[rancher-rke2-1-20-latest]
name=Rancher RKE2 1.20 Latest
baseurl=https://rpm.rancher.io/rke2/latest/1.20/centos/8/x86_64
enabled=1
gpgcheck=1
gpgkey=https://rpm.rancher.io/public.key
EOF

yum -y install rke2-server
</code></pre>

<h3 id="kubectl-helm-rke2">Kubectl, Helm &amp; RKE2</h3>

<p>Install <code>kubectl</code>, <code>helm</code> and RKE2 to the host system:</p>

<pre><code class="language-bash">sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx
sudo ln -s /opt/kubectx/kubens /usr/local/bin/kubens
echo 'PATH=$PATH:/usr/local/bin' &gt;&gt; /etc/profile
echo 'PATH=$PATH:/var/lib/rancher/rke2/bin' &gt;&gt; /etc/profile
source /etc/profile

sudo dnf copr -y enable cerenit/helm
sudo dnf install -y helm
</code></pre>

<h3 id="rke2-specific-ports">RKE2 specific ports</h3>

<pre><code class="language-bash">sudo firewall-cmd --add-port=9345/tcp --permanent
sudo firewall-cmd --add-port=6443/tcp --permanent
sudo firewall-cmd --add-port=10250Air-Gap/tcp --permanent
sudo firewall-cmd --add-port=2379/tcp --permanent
sudo firewall-cmd --add-port=2380/tcp --permanent
sudo firewall-cmd --add-port=30000-32767/tcp --permanent
# Used for the Rancher Monitoring
sudo firewall-cmd --add-port=9796/tcp --permanent
sudo firewall-cmd --add-port=19090/tcp --permanent
sudo firewall-cmd --add-port=6942/tcp --permanent
sudo firewall-cmd --add-port=9091/tcp --permanent
### CNI specific ports
# 4244/TCP is required when the Hubble Relay is enabled and therefore needs to connect to all agents to collect the flows
sudo firewall-cmd --add-port=4244/tcp --permanent
# Cilium healthcheck related permits:
sudo firewall-cmd --add-port=4240/tcp --permanent
sudo firewall-cmd --remove-icmp-block=echo-request --permanent
sudo firewall-cmd --remove-icmp-block=echo-reply --permanent
# Since we are using Cilium with GENEVE as overlay, we need the following port too:
sudo firewall-cmd --add-port=6081/udp --permanent
### Ingress Controller specific ports
sudo firewall-cmd --add-port=80/tcp --permanent
sudo firewall-cmd --add-port=443/tcp --permanent
### To get DNS resolution working, simply enable Masquerading.
sudo firewall-cmd --zone=public  --add-masquerade --permanent

### Finally apply all the firewall changes
sudo firewall-cmd --reload
</code></pre>

<p>Verification:</p>

<pre><code class="language-bash">sudo firewall-cmd --list-all
public (active)
  target: default
  icmp-block-inversion: no
  interfaces: eno1
  sources: 
  services: cockpit dhcpv6-client ssh wireguard
  ports: 9345/tcp 6443/tcp 10250/tcp 2379/tcp 2380/tcp 30000-32767/tcp 4240/tcp 6081/udp 80/tcp 443/tcp 4244/tcp 9796/tcp 19090/tcp 6942/tcp 9091/tcp
  protocols: 
  masquerade: yes
  forward-ports: 
  source-ports: 
  icmp-blocks: 
  rich rules: 
</code></pre>

<h3 id="basic-configuration">Basic Configuration</h3>

<pre><code class="language-bash">mkdir -p /etc/rancher/rke2
cat &lt;&lt; EOF &gt;  /etc/rancher/rke2/config.yaml
write-kubeconfig-mode: &quot;0644&quot;
profile: &quot;cis-1.5&quot;
selinux: true
# add ips/hostname of hosts and loadbalancer
tls-san:
  - &quot;k8s.mydomain.intra&quot;
  - &quot;172.17.9.10&quot;
# Make a etcd snapshot every 6 hours
etcd-snapshot-schedule-cron: &quot; */6 * * *&quot;
# Keep 56 etcd snapshorts (equals to 2 weeks with 6 a day)
etcd-snapshot-retention: 56
cni:
  - cilium
disable:
  - rke2-canal
  - rke2-kube-proxy
EOF
</code></pre>

<p><strong>Note:</strong> I disabled <code>rke2-canal</code> and <code>rke2-kube-proxy</code> since I plan to install Cilium as CNI in <a href="https://docs.cilium.io/en/v1.9/gettingstarted/kubeproxy-free/">&ldquo;kube-proxy less mode&rdquo;</a> (<code>kubeProxyReplacement: &quot;strict&quot;</code>). Do not disable <code>rke2-kube-proxy</code> if you use another CNI - it will not work afterwards!</p>

<pre><code class="language-bash">sudo cp -f /usr/share/rke2/rke2-cis-sysctl.conf /etc/sysctl.d/60-rke2-cis.conf
sysctl -p /etc/sysctl.d/60-rke2-cis.conf

useradd -r -c &quot;etcd user&quot; -s /sbin/nologin -M etcd

mkdir -p /var/lib/rancher/rke2/server/manifests/
cat &lt;&lt; EOF &gt; /var/lib/rancher/rke2/server/manifests/rke2-ingress-nginx-config.yaml
apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-ingress-nginx
  namespace: kube-system
spec:
  valuesContent: |-
    controller:
      metrics:
        service:
          annotations:
            prometheus.io/scrape: &quot;true&quot;
            prometheus.io/port: &quot;10254&quot;
EOF
</code></pre>

<h3 id="prevent-rke2-package-updates">Prevent RKE2 Package Updates</h3>

<p>In order to provide more stability, I chose to DNF/YUM &ldquo;mark/hold&rdquo; the RKE2 related packages so a <code>dnf update</code>/<code>yum update</code> does not mess around with them.</p>

<p>Add the following line to <code>/etc/dnf/dnf.conf</code> and/or <code>/etc/yum.conf</code>:</p>

<pre><code class="language-bash">exclude=rke2-*
</code></pre>

<h2 id="starting-rke2">Starting RKE2</h2>

<p>Enable the <code>rke2-server</code> service and start it:</p>

<pre><code class="language-bash">sudo systemctl enable rke2-server --now
</code></pre>

<p>Verification:</p>

<pre><code class="language-bash">sudo systemctl status rke2-server
sudo journalctl -u rke2-server -f
</code></pre>

<h2 id="configure-kubectl-on-rke2-host">Configure Kubectl (on RKE2 Host)</h2>

<pre><code class="language-bash">mkdir ~/.kube
ln -s /etc/rancher/rke2/rke2.yaml ~/.kube/config
chmod 600 /root/.kube/config
ln -s /var/lib/rancher/rke2/agent/etc/crictl.yaml /etc/crictl.yaml

kubectl get node
crictl ps
crictl images
</code></pre>

<p>Verification:</p>

<pre><code class="language-bash">kubectl get nodes
NAME                    STATUS   ROLES         AGE     VERSION
k8s.mydomain.intra   NotReady   etcd,master   2m4s   v1.18.16+rke2r1
</code></pre>

<h3 id="cilium-prerequisites">Cilium Prerequisites</h3>

<p>Ensure the eBFP filesystem is mounted (which should already be the case on RHEL 8.3):</p>

<pre><code class="language-bash">mount | grep /sys/fs/bpf
# if present should output, e.g. &quot;none on /sys/fs/bpf type bpf&quot;...
</code></pre>

<p>If that&rsquo;s not the case, mount it using the commands down here:</p>

<pre><code class="language-bash">sudo mount bpffs -t bpf /sys/fs/bpf
sudo bash -c 'cat &lt;&lt;EOF &gt;&gt; /etc/fstab
none /sys/fs/bpf bpf rw,relatime 0 0
EOF'
</code></pre>

<pre><code class="language-bash">kubectl create -n kube-system secret generic cilium-ipsec-keys     --from-literal=keys=&quot;3 rfc4106(gcm(aes)) $(echo $(dd if=/dev/urandom count=20 bs=1 2&gt; /dev/null| xxd -p -c 64)) 128&quot;
</code></pre>

<pre><code class="language-bash">kubectl get endpoints kubernetes -o wide
NAME         ENDPOINTS        AGE
kubernetes   10.0.2.15:6443   86m
</code></pre>

<h3 id="deploy-cilium">Deploy Cilium</h3>

<pre><code class="language-yaml">cat &lt;&lt; EOF &gt;  /var/lib/rancher/rke2/server/manifests/rke2-cilium.yaml
apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-cilium
  namespace: kube-system
spec:
  valuesContent: |-
    cilium:
      k8sServiceHost: 10.0.2.15
      k8sServicePort: 6443
      operator:
        replicas: 1
      global:
        encryption:
          enabled: true
          nodeEncryption: true
      hubble:
        metrics:
          enabled:
          - dns:query;ignoreAAAA
          - drop
          - tcp
          - flow
          - icmp
          - http
        relay:
          enabled: true
        ui:
          enabled: true
          replicas: 1
          ingress:
            enabled: true
            hosts:
              - hubble.k8s.intra
            annotations:
              cert-manager.io/cluster-issuer: ca-issuer
            tls:
            - secretName: ingress-hubble-ui
              hosts:
              - hubble.k8s.intra
      prometheus:
        enabled: true
        # Default port value (9090) needs to be changed since the RHEL cockpit also listens on this port.
        port: 19090
        # Configure this serviceMonitor section AFTER Rancher Monitoring is enabled!
        #serviceMonitor:
        #  enabled: true
EOF
</code></pre>

<h3 id="deploy-demo-app">Deploy demo app</h3>

<pre><code class="language-bash">kubens default
kubectl apply -f https://raw.githubusercontent.com/cilium/cilium/v1.9/examples/minikube/http-sw-app.yaml
kubectl apply -f k8s_sec_lab/manifest/cilium_demo_rb.yaml

kubectl exec xwing -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing
kubectl exec tiefighter -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Flux2 and Mozilla SOPS to encrypt secrets]]></title>
            <link href="https://devopstales.github.io/home/gitops-flux2-sops/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/gitops-flux2-kubeseal/?utm_source=atom_feed" rel="related" type="text/html" title="Flux2 and kubeseal to encrypt secrets" />
                <link href="https://devopstales.github.io/kubernetes/gitops-flux2-sops/?utm_source=atom_feed" rel="related" type="text/html" title="Flux2 and Mozilla SOPS to encrypt secrets" />
                <link href="https://devopstales.github.io/home/gitops-flux2/?utm_source=atom_feed" rel="related" type="text/html" title="Flux2 Install and Usage" />
                <link href="https://devopstales.github.io/kubernetes/gitops-flux2-kubeseal/?utm_source=atom_feed" rel="related" type="text/html" title="Flux2 and kubeseal to encrypt secrets" />
                <link href="https://devopstales.github.io/home/argocd-image-updater/?utm_source=atom_feed" rel="related" type="text/html" title="Argo CD Image Updater for automate image update" />
            
                <id>https://devopstales.github.io/home/gitops-flux2-sops/</id>
            
            
            <published>2021-05-08T00:00:00+00:00</published>
            <updated>2021-05-08T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can use Mozilla SOPS with Flux2 to protect secrets.</p>

<h3 id="parst-of-the-k8s-gitops-series">Parst of the K8S Gitops series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/k8s-gitops/">GitOps solutions for Kubernetes</a></li>
<li>Part2: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
<li>Part3: <a href="../../kubernetes/argocd-image-updater/">Argo CD Image Updater for automate image update</a>
<!-- ArgoCD + Argo Rollouts for canary deploy -->
<!-- ArgoCD notify
https://blog.argoproj.io/notifications-for-argo-bb7338231604
-->
<!-----------------------------------------------></li>
<li>Part4: <a href="../../kubernetes/gitops-flux2/">Flux2 Install and Usage</a></li>
<li>Part5: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
<li>Part6: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a>
<!-- Flux CD auto image update
https://particule.io/en/blog/flux-auto-image-update/
-->
<!-- Flux CD + flagger for canary deployment -->
<!----------------------------------------------->
<!-- Fleet --></li>
</ul>

<p>First you need to bootstrap the fluxcomponent as I showd in the <a href="../gitops-flux2/">previous post</a>.</p>

<h2 id="install-ops-cli">Install OPS CLI</h2>

<pre><code class="language-bash">VERSION=$(curl --silent &quot;https://api.github.com/repos/mozilla/sops/releases/latest&quot; | grep '&quot;tag_name&quot;' | sed -E 's/.*&quot;([^&quot;]+)&quot;.*/\1/')
VERSION=${VERSION:1}

wget https://github.com/mozilla/sops/releases/download/v$VERSION/sops-&quot;$VERSION&quot;-1.x86_64.rpm
yum install -y sops-&quot;$VERSION&quot;-1.x86_64.rpm
rm -f sops-&quot;$VERSION&quot;-1.x86_64.rpm
</code></pre>

<h3 id="generate-a-gpg-key">Generate a GPG key</h3>

<p>Generate a GPG/OpenPGP key:</p>

<pre><code class="language-bash">export KEY_NAME=&quot;cl1.mydomain.intra&quot;
export KEY_COMMENT=&quot;flux secrets&quot;

gpg --batch --full-generate-key &lt;&lt;EOF
%no-protection
Key-Type: 1
Key-Length: 4096
Subkey-Type: 1
Subkey-Length: 4096
Expire-Date: 0
Name-Comment: ${KEY_COMMENT}
Name-Real: ${KEY_NAME}
EOF
</code></pre>

<p>The above configuration creates an rsa4096 key that does not expire. Retrieve the GPG key fingerprint:</p>

<pre><code class="language-bash">gpg --list-secret-keys &quot;${KEY_NAME}&quot;

sec   rsa4096 2020-09-06 [SC]
      1F3D1CED2F865F5E59CA564553241F147E7C5FA4

# Store the key fingerprint as an environment variable:
export KEY_FP=1F3D1CED2F865F5E59CA564553241F147E7C5FA4
</code></pre>

<p>Export the public and private key from your local GPG keyring and create a Kubernetes secret named sops-gpg in the flux-system namespace:</p>

<pre><code class="language-bash">gpg --export-secret-keys --armor &quot;${KEY_FP}&quot; |
kubectl create secret generic sops-gpg \
--namespace=flux-system \
--from-file=sops.asc=/dev/stdin

mkdir ./02_flux2/03_SOPS_demo/
gpg --export-secret-keys --armor &quot;${KEY_FP}&quot; &gt; ./02_flux2/03_SOPS_demo/sops.pub.asc
</code></pre>

<p>It’s a good idea to back up this secret-key/K8s-Secret with a password manager or offline storage. Also consider deleting the secret decryption key from you machine:</p>

<pre><code class="language-bash">gpg --delete-secret-keys &quot;${KEY_FP}&quot;
</code></pre>

<pre><code class="language-bash">git add -A
git commit -m &quot;add sops config&quot;
git push
</code></pre>

<h3 id="configure-in-cluster-secrets-decryption">Configure in-cluster secrets decryption</h3>

<pre><code class="language-bash">nano 02_flux2/flux-system/gotk-sync.yaml
---
apiVersion: source.toolkit.fluxcd.io/v1beta1
kind: GitRepository
metadata:
  name: flux-system
  namespace: flux-system
spec:
  interval: 1m0s
  ref:
    branch: main
  secretRef:
    name: flux-system
  url: ssh://git@github.com/devopstales/gitops-repo
---
apiVersion: kustomize.toolkit.fluxcd.io/v1beta1
kind: Kustomization
metadata:
  name: flux-system
  namespace: flux-system
spec:
  decryption:
    provider: sops
    secretRef:
      name: sops-gpg
  interval: 10m0s
  path: ./01_flux2
  prune: true
  sourceRef:
    kind: GitRepository
    name: flux-system
  validation: client
</code></pre>

<p>Create a secret you want to encrypt:</p>

<pre><code class="language-yaml">cp 01_flux2/02_secret/*.yaml 02_flux2/03_SOPS_demo/
rm -f 02_flux2/03_SOPS_demo/sealedsecret.yaml

nano 02_flux2/03_SOPS_demo/secret.yaml
---
apiVersion: v1
data:
  secret: UzNDUjNUCg==
kind: Secret
metadata:
  creationTimestamp: null
  name: mysecret
  namespace: demo-app
</code></pre>

<p>ncrypt the secret with SOPS using your GPG key:</p>

<pre><code class="language-bash">sops --encrypt --encrypted-regex '^(data|stringData)$' --pgp ${KEY_FP} \
--in-place 02_flux2/03_SOPS_demo/secret.yaml
</code></pre>

<p>Check the result:</p>

<pre><code class="language-yaml">cat 02_flux2/03_SOPS_demo/secret.yaml 
apiVersion: v1
data:
    secret: ENC[AES256_GCM,data:RxdPIf8i5G+yjiT0,iv:A8iYsJ4hdd1MNZDAKQyD4L/b6Caa1TDn+MNKsFku3nc=,tag:QLN3Y6B/S87qzPYh2PATaQ==,type:str]
kind: Secret
metadata:
    creationTimestamp: null
    name: mysecret
    namespace: demo-app
sops:
    kms: []
    gcp_kms: []
    azure_kv: []
    hc_vault: []
    age: []
    lastmodified: &quot;2021-05-09T08:59:30Z&quot;
    mac: ENC[AES256_GCM,data:2SwDXqI5R880Y/uf4yW6o3rraJ7WYQ5aIfKwIPEpss/evD15dfLulUUahN0bNmrwtSYuad0aXHopGfFsKEvxSVMx9UkPaAd0xVZHSj7aJ5Fi5D2De3Tw1yi8tuWjU8OMG81nIZkx6GdIa4yZlm+LEangYVIpzluWk6C7In8GNc8=,iv:9E0R+R6QjqCXBnzPlAzri/fYxEr0HPZ0FzQX5oddMZk=,tag:FEJknbbaqp402biU/hxUaA==,type:str]
    pgp:
        - created_at: &quot;2021-05-09T08:59:30Z&quot;
          enc: |
            -----BEGIN PGP MESSAGE-----

            hQIMA1gFjmLlSkpZAQ//ZwU9ZEL2jDtmg8ZvJ4Wrnaa66Rjvnr/Uz9VOUxKZk1WJ
            V6+Wa1Od80tODzr9gfmjHor0ZCbdJmPxf96z4MhcBNbo9oBr43GX2Wm67ijwIEdo
            Wup2252ANsn1stZIk5krdlZVkRTW+GeAwEDHnW4zSOSVfc9Ad1SHGy1vgXM/i7Je
            ttx3s/PZhPPZLUQ6SKQjEaf6Xod9nnLyqSb1SdB5idhBy/wV3nt08p/LJ8QVItzh
            iFclOBRXeEQuYEt2O57Bo/kRGTaBjq6YE0KCbu7PMkm8gerZOyW2Od1maF4Bsjlz
            c+qOaLjFOP4K/8OIDtzTOw9tXbQREWC9tl1ReadGHReTbdCs55msmMWscPJtq8wi
            a3eMdLDwvJHhAERaJwvAa5Le6uIwr+lOEVzetj2ucK6LgVlTjgs9IPTMul4ASyji
            tOtTUzXoEHu/wfGKP7QFDbROFBWalNBkSegdOQx+/GSLebfWY/HmTy+V7isRhoa3
            iH4/hapCDUQVqwQSyvjpVUzoAsp9g7XaYITKGbSkIuUA3TI5aTp3SSF7sbNHdG8g
            6i3jh4FxS9yzFgM7fGnlbHDta/DzyBQB5Z77cI8pJW9mri1/U6R63zJuDqSUvFlw
            zf1zJwEV3xgwMog/7nx4aAItPBeqsT0pSYs5pQpciKgTJcCOTG8r6+3+cjckS/vS
            XgH6CQtPyzD1JAVDz94n0RkZKC+TXUfLnRF0yLwaAeOJ9h6vFVnEggOIXiWF4Iy7
            Ds1zIaq79+H/gWo0fGk2srKIHdcPZkQc7zAOqbO5X94fo0TtNx5y/QT6FUwUH4k=
            =GMAU
            -----END PGP MESSAGE-----
          fp: FCF6E84DC263BDDB9A35D3C6DF8C27FF0C09F771
    encrypted_regex: ^(data|stringData)$
    version: 3.7.1
</code></pre>

<p>Upload files and test:</p>

<pre><code class="language-bash">git add -A
git commit -m &quot;sops demo&quot;
git push

kubectl logs -n demo-app2 demo-app
S3CR3T
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Flux2 and kubeseal to encrypt secrets]]></title>
            <link href="https://devopstales.github.io/home/gitops-flux2-kubeseal/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/gitops-flux2-kubeseal/?utm_source=atom_feed" rel="related" type="text/html" title="Flux2 and kubeseal to encrypt secrets" />
                <link href="https://devopstales.github.io/home/gitops-flux2/?utm_source=atom_feed" rel="related" type="text/html" title="Flux2 Install and Usage" />
                <link href="https://devopstales.github.io/home/argocd-kubeseal/?utm_source=atom_feed" rel="related" type="text/html" title="ArgoCD and kubeseal to encrypt secrets" />
                <link href="https://devopstales.github.io/home/argocd-image-updater/?utm_source=atom_feed" rel="related" type="text/html" title="Argo CD Image Updater for automate image update" />
                <link href="https://devopstales.github.io/home/k8s-gitops/?utm_source=atom_feed" rel="related" type="text/html" title="GitOps solutions for Kubernetes" />
            
                <id>https://devopstales.github.io/home/gitops-flux2-kubeseal/</id>
            
            
            <published>2021-05-07T00:00:00+00:00</published>
            <updated>2021-05-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can use kubeseal and Mozilla SOPS with Flux2 to protect secrets.</p>

<h3 id="parst-of-the-k8s-gitops-series">Parst of the K8S Gitops series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/k8s-gitops/">GitOps solutions for Kubernetes</a></li>
<li>Part2: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
<li>Part3: <a href="../../kubernetes/argocd-image-updater/">Argo CD Image Updater for automate image update</a>
<!-- ArgoCD + Argo Rollouts for canary deploy -->
<!-- ArgoCD notify
https://blog.argoproj.io/notifications-for-argo-bb7338231604
-->
<!-----------------------------------------------></li>
<li>Part4: <a href="../../kubernetes/gitops-flux2/">Flux2 Install and Usage</a></li>
<li>Part5: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
<li>Part6: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a>
<!-- Flux CD auto image update
https://particule.io/en/blog/flux-auto-image-update/
-->
<!-- Flux CD + flagger for canary deployment -->
<!----------------------------------------------->
<!-- Fleet --></li>
</ul>

<h2 id="install-kubeseal-cli">Install kubeseal cli</h2>

<pre><code class="language-bash">VERSION=$(curl --silent &quot;https://api.github.com/repos/bitnami-labs/sealed-secrets/releases/latest&quot; | grep '&quot;tag_name&quot;' | sed -E 's/.*&quot;([^&quot;]+)&quot;.*/\1/')

wget https://github.com/bitnami-labs/sealed-secrets/releases/download/$VERSION/kubeseal-linux-amd64 -O /usr/local/bin/kubeseal
chmod 755 /usr/local/bin/kubeseal
kubeseal --version
</code></pre>

<h3 id="deploy-sealed-secrets-with-a-helmrelease">Deploy sealed-secrets with a HelmRelease</h3>

<pre><code class="language-bash">flux create source helm sealed-secrets \
--interval=1h \
--url=https://bitnami-labs.github.io/sealed-secrets

flux create helmrelease sealed-secrets \
--interval=1h \
--release-name=sealed-secrets \
--target-namespace=flux-system \
--source=HelmRepository/sealed-secrets \
--chart=sealed-secrets \
--chart-version=&quot;&gt;=1.15.0-0&quot; \
--crds=CreateReplace
</code></pre>

<p>At startup, the sealed-secrets controller generates a 4096-bit RSA key pair and persists the private and public keys as Kubernetes secrets in the flux-system namespace.</p>

<pre><code class="language-bash">kubeseal --fetch-cert \
--controller-name=sealed-secrets \
--controller-namespace=flux-system \
&gt; ../pub-sealed-secrets.pem
</code></pre>

<p>Generate a Kubernetes secret manifest with kubectl:</p>

<pre><code class="language-bash">kubectl -n default create secret generic basic-auth \
--from-literal=user=admin \
--from-literal=password=change-me \
--dry-run=client \
-o yaml &gt; ../basic-auth.yaml
</code></pre>

<h3 id="create-a-sealed-secret">Create a sealed secret</h3>

<p>Create a secret you want to encrypt:</p>

<pre><code class="language-yaml">apiVersion: v1
data:
  secret: UzNDUjNUCg==
kind: Secret
metadata:
  creationTimestamp: null
  name: mysecret
  namespace: demo-app
</code></pre>

<p>A secret in Kubernetes cluster is encoded in base64 but not encrypted! Theses data are &ldquo;only&rdquo; encoded so if a user have access to your secrets, he can simply base64 decode to see your sensitive data:</p>

<pre><code class="language-base">echo &quot;UzNDUjNUCg==&quot; | base64 -d
S3CR3T
</code></pre>

<p>Encrypt the secret with kubeseal:</p>

<pre><code class="language-bash">mkdir ./01_flux2/02_secret/

kubeseal --format yaml --cert=../../../pub-sealed-secrets.pem \
&lt; secret.yaml &gt;sealedsecret.yaml
rm -f secret.yaml

git add -A
git commit -m &quot;kubeseal&quot;
git push

kubectl logs -n demo-app demo-app
S3CR3T
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Flux2 Install and Usage]]></title>
            <link href="https://devopstales.github.io/home/gitops-flux2/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/gitops-flux2-kubeseal/?utm_source=atom_feed" rel="related" type="text/html" title="Flux2 and kubeseal to encrypt secrets" />
                <link href="https://devopstales.github.io/home/argocd-image-updater/?utm_source=atom_feed" rel="related" type="text/html" title="Argo CD Image Updater for automate image update" />
                <link href="https://devopstales.github.io/home/argocd-kubeseal/?utm_source=atom_feed" rel="related" type="text/html" title="ArgoCD and kubeseal to encrypt secrets" />
                <link href="https://devopstales.github.io/home/k8s-gitops/?utm_source=atom_feed" rel="related" type="text/html" title="GitOps solutions for Kubernetes" />
                <link href="https://devopstales.github.io/kubernetes/gitops-flux2/?utm_source=atom_feed" rel="related" type="text/html" title="Flux2 Install and Usage" />
            
                <id>https://devopstales.github.io/home/gitops-flux2/</id>
            
            
            <published>2021-05-07T00:00:00+00:00</published>
            <updated>2021-05-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can use Install and Use the GitOps Tool Flux2.</p>

<h3 id="parst-of-the-k8s-gitops-series">Parst of the K8S Gitops series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/k8s-gitops/">GitOps solutions for Kubernetes</a></li>
<li>Part2: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
<li>Part3: <a href="../../kubernetes/argocd-image-updater/">Argo CD Image Updater for automate image update</a>
<!-- ArgoCD + Argo Rollouts for canary deploy -->
<!-- ArgoCD notify
https://blog.argoproj.io/notifications-for-argo-bb7338231604
-->
<!-----------------------------------------------></li>
<li>Part4: <a href="../../kubernetes/gitops-flux2/">Flux2 Install and Usage</a></li>
<li>Part5: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
<li>Part6: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a>
<!-- Flux CD auto image update
https://particule.io/en/blog/flux-auto-image-update/
-->
<!-- Flux CD + flagger for canary deployment -->
<!----------------------------------------------->
<!-- Fleet --></li>
</ul>

<h3 id="install-flux2-cli">Install Flux2 cli</h3>

<pre><code class="language-bash">curl -s https://fluxcd.io/install.sh | sudo bash
</code></pre>

<h3 id="bootstrap-flux2-server-components">Bootstrap Flux2 Server components</h3>

<p>Flux is installed in a GitOps way and its manifest will be pushed to the repository, so you will also need a GitHub account and a <a href="https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token">personal access token</a> that can create repositories (check all permissions under <code>repo</code>) to enable Flux do this.</p>

<pre><code class="language-bash">export GITHUB_TOKEN=&lt;token&gt;
export GITHUB_USER=devopstales

flux check --pre

flux bootstrap github \
  --owner=$GITHUB_USER \
  --repository=gitops-repo \
  --branch=main \
  --path=./01_flux2/ \
  --personal
</code></pre>

<p>If you try to install in a secure Kubernetes cluster with runAsNonRoot psp the notification-controller and the source-controller can&rsquo;t start because it runs as root.</p>

<pre><code class="language-yaml">nano rb.yaml
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: psp-rolebinding-flux-system
  namespace: flux-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system-unrestricted-psp-role
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:serviceaccounts

kubectl apply -f rb.yaml
</code></pre>

<p>With <code>--path</code> you can configure the directory which will be used to reconcile the target cluster.</p>

<pre><code class="language-bash">./01_flux2/
└── flux-system # &lt;- namespace dir generated by bootstrap
    ├── gotk-components.yaml
    ├── gotk-sync.yaml
    ├── rb.yaml # &lt;- RoleBinding for psp created by me
    └── kustomization.yaml
</code></pre>

<h3 id="deploy-application">Deploy application</h3>

<p>Add an application to the cluster and upload to the git repository:</p>

<pre><code class="language-bash">./01_flux2/
├── 00_guestbook # &lt;- guestbook application
│   ├── 00_ns.yaml
│   ├── 01_rb.yaml
│   ├── 02_guestbook-ui-svc.yaml
│   └── 03_guestbook-ui-deployment.yaml
└── flux-system # &lt;- namespace dir generated by bootstrap
    ├── gotk-components.yaml
    ├── gotk-sync.yaml
    ├── rb.yaml # &lt;- RoleBinding for psp created by me
    └── kustomization.yaml
</code></pre>

<h3 id="add-another-git-repository">Add another Git repository</h3>

<p>We will be using a public repository github.com/stefanprodan/podinfo, podinfo is a tiny web application made with Go. Create a GitRepository manifest pointing to podinfo repository’s master branch:</p>

<pre><code class="language-bash">mkdir ./01_flux2/01_podinfo

flux create source git podinfo \
  --url=https://github.com/stefanprodan/podinfo \
  --branch=master \
  --interval=30s \
  --export &gt; ./01_flux2/01_podinfo/podinfo-source.yaml
</code></pre>

<pre><code class="language-yaml">cat 01_flux2/01_podinfo/podinfo-source.yaml
---
apiVersion: source.toolkit.fluxcd.io/v1beta1
kind: GitRepository
metadata:
  name: podinfo
  namespace: flux-system
spec:
  interval: 30s
  ref:
    branch: master
  url: https://github.com/stefanprodan/podinfo
</code></pre>

<pre><code class="language-bash">./01_flux2/
├── 00_guestbook # &lt;- guestbook application
│   ├── 00_ns.yaml
│   ├── 01_rb.yaml
│   ├── 02_guestbook-ui-svc.yaml
│   └── 03_guestbook-ui-deployment.yaml
├── 01_podinfo
│   └── podinfo-source.yaml
└── flux-system # &lt;- namespace dir generated by bootstrap
    ├── gotk-components.yaml
    ├── gotk-sync.yaml
    ├── rb.yaml # &lt;- RoleBinding for psp created by me
    └── kustomization.yaml
</code></pre>

<h3 id="kustomization">Kustomization</h3>

<p>We will create a Flux Kustomization manifest for podinfo. This configures Flux to apply the kustomize directory located in the podinfo repository.</p>

<pre><code class="language-bash">flux create kustomization podinfo \
  --source=podinfo \
  --path=&quot;./kustomize&quot; \
  --prune=true \
  --validation=client \
  --interval=5m \
  --export &gt; ./01_flux2/01_podinfo/podinfo-kustomization.yaml
</code></pre>

<pre><code class="language-yaml">cat ./01_flux2/01_podinfo/podinfo-kustomization.yaml
---
apiVersion: kustomize.toolkit.fluxcd.io/v1beta1
kind: Kustomization
metadata:
  name: podinfo
  namespace: flux-system
spec:
  interval: 5m0s
  path: ./kustomize
  prune: true
  sourceRef:
    kind: GitRepository
    name: podinfo
  validation: client
</code></pre>

<pre><code class="language-bash">./01_flux2/
├── 00_guestbook # &lt;- guestbook application
│   ├── 00_ns.yaml
│   ├── 01_rb.yaml
│   ├── 02_guestbook-ui-svc.yaml
│   └── 03_guestbook-ui-deployment.yaml
├── 01_podinfo
│   ├── podinfo-kustomization.yaml
│   └── podinfo-source.yaml
└── flux-system # &lt;- namespace dir generated by bootstrap
    ├── gotk-components.yaml
    ├── gotk-sync.yaml
    ├── rb.yaml # &lt;- RoleBinding for psp created by me
    └── kustomization.yaml
</code></pre>

<h3 id="manage-helm-releases">Manage Helm Releases</h3>

<p>I usually use Ransher&rsquo;s helm operator but Flux has it&rsquo;s own. It has two part the <code>HelmRepository</code> and the <code>HelmRelease</code>:</p>

<pre><code class="language-yaml">apiVersion: source.toolkit.fluxcd.io/v1beta1
kind: HelmRepository
metadata:
  name: chartmuseum
  namespace: flux-system
spec:
  url: https://chartmuseum.github.io/charts
  interval: 10m
---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: chartmuseum
  namespace: flux-system
spec:
  interval: 5m
  chart:
    spec:
      chart: chartmuseum
      version: &quot;2.14.2&quot;
      sourceRef:
        kind: HelmRepository
        name: chartmuseum
        namespace: flux-system
      interval: 1m
  values:
    env:
      open:
        AWS_SDK_LOAD_CONFIG: true
        STORAGE: amazon
        STORAGE_AMAZON_BUCKET: &quot;bucket-name&quot;
        STORAGE_AMAZON_PREFIX: &quot;&quot;
        STORAGE_AMAZON_REGION: &quot;region-name&quot;
    serviceAccount:
      create: true
      annotations:
        eks.amazonaws.com/role-arn: &quot;role-arn&quot;
    securityContext:
      enabled: true
      fsGroup: 65534
</code></pre>

<p>It is possible to define a list of ConfigMap and Secret resources from which to take values.</p>

<pre><code class="language-yaml">spec:
  valuesFrom:
  - kind: ConfigMap
    name: prod-env-values
    valuesKey: values-prod.yaml
  - kind: Secret
    name: prod-tls-values
    valuesKey: crt
    targetPath: tls.crt
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes Certificate Rotation]]></title>
            <link href="https://devopstales.github.io/home/k8s-cert/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k3s-gvisor/?utm_source=atom_feed" rel="related" type="text/html" title="Secure k3s with gVisor" />
                <link href="https://devopstales.github.io/home/k8s-vault/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes and Vault integration" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller-v2/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V2" />
                <link href="https://devopstales.github.io/home/kubernetes-policy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Policy" />
                <link href="https://devopstales.github.io/home/k8s-networkpolicy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Network Policy" />
            
                <id>https://devopstales.github.io/home/k8s-cert/</id>
            
            
            <published>2021-05-01T00:00:00+00:00</published>
            <updated>2021-05-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can rotate your Kubernetes Engine Certificates.</p>

<h3 id="parst-of-the-k8s-security-series">Parst of the K8S Security series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
<li>Part2: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
<li>Part3: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
<li>Part4: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a>
<!-- RBAC -->
<!-- # Policy --></li>
<li>Part5: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
<li>Part6: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
<li>Part7: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a>
<!-- # Admission Controllers --></li>
<li>Part8: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a>
<!-- ## Image Scan --></li>
<li>Part9a: <a href="../../kubernetes/image-security-admission-controller">Image security Admission Controller</a></li>
<li>Part9b: <a href="../../kubernetes/image-security-admission-controller-v2">Image security Admission Controller V2</a></li>
<li>Part9c: <a href="../../kubernetes/image-security-admission-controller-v3">Image security Admission Controller V3</a>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring --></li>
<li>Part10: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
<li>Part11: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a>
<!-- DAST Exporter?? -->
<!-- CIS Exporter
https://github.com/ibrokethecloud/kube-bench-metrics --></li>
<li>Part12 <a href="../../kubernetes/k8s-connaisseur">Image Signature Verification with Connaisseur</a></li>
<li>Part13 <a href="../../kubernetes/k8s-backup">Backup your Kubernetes Cluster</a>
<!-- # Secrets --></li>
<li>Part14a <a href="../../kubernetes/k8s-vault">Kubernetes and Vault integration</a></li>
<li>Part14b <a href="../../kubernetes/k8s-vault-v2">Kubernetes External Vault integration</a></li>
<li>Part15a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
<li>Part15b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
<li>Part15c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a>
<!-- # Image -->
<!-- keel --></li>
<li>Part16: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a>
<!-- flux image update --></li>
<li>Part17: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
</ul>

<!-- admission-controller ami cseréli az imageben a repot !!!
 -->

<!-- sysdig falco -->

<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
-->

<!-- resource kvota -->

<!-- Backup
https://github.com/pieterlange/kube-backup 
-->

<!--
https://medium.com/@beatrizmrg/network-security-for-microservices-with-ebpf-bis-478b40e7befa

# container runetime
https://www.ianlewis.org/en/container-runtimes-part-1-introduction-container-r
http://pouchcontainer.io/pouch/docs/features/pouch_with_runV.html
-->

<p>By default, kubeadm generates all the certificates needed for a cluster to run. Client certificates generated by kubeadm expire after 1 year.</p>

<h3 id="check-certificate-expiration">Check certificate expiration</h3>

<p>You can use the <code>check-expiration</code> subcommand to check when certificates expire:</p>

<pre><code class="language-bash">kubeadm certs check-expiration
[check-expiration] Reading configuration from the cluster...
[check-expiration] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'

CERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED
admin.conf                 Apr 19, 2022 16:34 UTC   364d                                    no
apiserver                  Apr 19, 2022 16:34 UTC   364d            ca                      no
apiserver-etcd-client      Apr 19, 2022 16:34 UTC   364d            etcd-ca                 no
apiserver-kubelet-client   Apr 19, 2022 16:34 UTC   364d            ca                      no
controller-manager.conf    Apr 19, 2022 16:34 UTC   364d                                    no
etcd-healthcheck-client    Apr 19, 2022 16:34 UTC   364d            etcd-ca                 no
etcd-peer                  Apr 19, 2022 16:34 UTC   364d            etcd-ca                 no
etcd-server                Apr 19, 2022 16:34 UTC   364d            etcd-ca                 no
front-proxy-client         Apr 19, 2022 16:34 UTC   364d            front-proxy-ca          no
scheduler.conf             Apr 19, 2022 16:34 UTC   364d                                    no

CERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED
ca                      Apr 17, 2031 16:34 UTC   9y              no
etcd-ca                 Apr 17, 2031 16:34 UTC   9y              no
front-proxy-ca          Apr 17, 2031 16:34 UTC   9y              no
</code></pre>

<h3 id="automatic-certificate-renewal">Automatic certificate renewal</h3>

<p><code>kubeadm</code> renews all the certificates during control plane upgrade. It is a best practice to upgrade your cluster frequently in order to stay secure. Kubernetes v1.8 and higher kubelet implements features for enabling rotation of its client and/or serving certificates.</p>

<pre><code class="language-bash">kubectl create clusterrolebinding kubelet-bootstrap \
    --clusterrole=system:node-bootstrapper \
    --user=kubelet-bootstrap

kubectl create clusterrolebinding node-client-auto-approve-csr \
    --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient \
    --group=system:node-bootstrappers

kubectl create clusterrolebinding node-client-auto-renew-crt \
    --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient \
    --group=system:nodes
</code></pre>

<pre><code class="language-bash">nano /etc/systemd/system/kubelet.env
...
KUBELET_EXTRA_ARGS==&quot;--rotate-certificates=true --rotate-server-certificates=true&quot;

systemctl restart kubelet

ps -ef | grep kubelet | grep &quot;rotate-certificates&quot;
root      14105      1  0 16:56 pts/0    00:00:00 bash -c while true ; do /usr/bin/kubelet  --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --pod-manifest-path=/etc/kubernetes/manifests --pod-infra-container-image=k8s.gcr.io/pause:3.2 --network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin --cluster-dns=10.96.0.10 --cluster-domain=cluster.local --authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.crt --cgroup-driver=cgroupfs --fail-swap-on=false --resolv-conf=/etc/resolv.conf.override --rotate-certificates=true --rotate-server-certificates=true; sleep 5; done
</code></pre>

<pre><code class="language-bash">kubectl get csr
NAME                                                   AGE   SIGNERNAME                                    REQUESTOR                 CONDITION
csr-pswns                                              27m   kubernetes.io/kube-apiserver-client-kubelet   system:node:node1         Approved,Issued
node-csr-cQYdjcH2F3kl-ysnzq2TlZOuUDCPgYU8cfKV1V0kqlE   47m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:lzhuxv   Approved,Issued
node-csr-f2J5HT9hg4CIKP0-0BtsEffBzg28VlUbesKJ4p_2mi0   47m   kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:lzhuxv   Approved,Issued
</code></pre>

<h3 id="manual-certificate-renewal">Manual certificate renewal</h3>

<p>You can renew your certificates manually at any time with the <code>kubeadm certs renew</code> command. This command performs the renewal using CA (or front-proxy-CA) certificate and key stored in <code>/etc/kubernetes/pki</code> If you are running an HA cluster, this command needs to be executed on all the control-plane nodes.</p>

<h3 id="rke2-and-k3s">RKE2 and K3S</h3>

<p>By default, certificates in RKE2 and K3S expire in 12 months. If the certificates are expired or have fewer than 90 days remaining before they expire, the certificates are rotated when RKE2 is restarted.  It is expected that you would be taking your hosts down periodically for patching and upgrading every few months. With regular updates the reboots should happen - but reality has shown that many of us do not patch / reboot for more than 3 months.. so the best practice is monitoring the certificate expiration.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Secure k3s with gVisor]]></title>
            <link href="https://devopstales.github.io/home/k3s-gvisor/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k3s-gvisor/?utm_source=atom_feed" rel="related" type="text/html" title="Secure k3s with gVisor" />
                <link href="https://devopstales.github.io/home/k8s-vault/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes and Vault integration" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller-v2/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V2" />
                <link href="https://devopstales.github.io/home/kubernetes-policy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Policy" />
                <link href="https://devopstales.github.io/home/k8s-networkpolicy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Network Policy" />
            
                <id>https://devopstales.github.io/home/k3s-gvisor/</id>
            
            
            <published>2021-04-30T00:00:00+00:00</published>
            <updated>2021-04-30T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can secure k3s with gVisor.</p>

<h3 id="parst-of-the-k8s-security-series">Parst of the K8S Security series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
<li>Part2: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
<li>Part3: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
<li>Part4: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a>
<!-- RBAC -->
<!-- # Policy --></li>
<li>Part5: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
<li>Part6: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
<li>Part7: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a>
<!-- # Admission Controllers --></li>
<li>Part8: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a>
<!-- ## Image Scan --></li>
<li>Part9a: <a href="../../kubernetes/image-security-admission-controller">Image security Admission Controller</a></li>
<li>Part9b: <a href="../../kubernetes/image-security-admission-controller-v2">Image security Admission Controller V2</a></li>
<li>Part9c: <a href="../../kubernetes/image-security-admission-controller-v3">Image security Admission Controller V3</a>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring --></li>
<li>Part10: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
<li>Part11: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a>
<!-- DAST Exporter?? -->
<!-- CIS Exporter
https://github.com/ibrokethecloud/kube-bench-metrics --></li>
<li>Part12 <a href="../../kubernetes/k8s-connaisseur">Image Signature Verification with Connaisseur</a></li>
<li>Part13 <a href="../../kubernetes/k8s-backup">Backup your Kubernetes Cluster</a>
<!-- # Secrets --></li>
<li>Part14a <a href="../../kubernetes/k8s-vault">Kubernetes and Vault integration</a></li>
<li>Part14b <a href="../../kubernetes/k8s-vault-v2">Kubernetes External Vault integration</a></li>
<li>Part15a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
<li>Part15b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
<li>Part15c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a>
<!-- # Image -->
<!-- keel --></li>
<li>Part16: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a>
<!-- flux image update --></li>
<li>Part17: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
</ul>

<!-- admission-controller ami cseréli az imageben a repot !!!
 -->

<!-- sysdig falco -->

<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
-->

<!-- resource kvota -->

<!-- Backup
https://github.com/pieterlange/kube-backup 
-->

<!--
https://medium.com/@beatrizmrg/network-security-for-microservices-with-ebpf-bis-478b40e7befa

# container runetime
https://www.ianlewis.org/en/container-runtimes-part-1-introduction-container-r
http://pouchcontainer.io/pouch/docs/features/pouch_with_runV.html
-->

<p>In  <a href="https://devopstales.github.io/kubernetes/k3s-etcd-kube-vip/">previous pos</a> I showd you how to install a k3s Cluster. Now we modify the configuration of the containerd to use different low level container runtime.</p>

<h3 id="bootstrap-the-k3s-cluster">Bootstrap the k3s cluster</h3>

<pre><code class="language-bash">k3sup install \
  --ip=172.17.8.101 \
  --user=vagrant \
  --sudo \
  --cluster \
  --k3s-channel=stable \
  --k3s-extra-args &quot;--no-deploy=traefik --no-deploy=servicelb --flannel-iface=enp0s8 --node-ip=172.17.8.101&quot; \
  --merge \
  --local-path $HOME/.kube/config \
  --context=k3s-ha

k3sup join \
  --ip 172.17.8.102 \
  --user vagrant \
  --sudo \
  --k3s-channel stable \
  --server \
  --server-ip 172.17.8.101 \
  --server-user vagrant \
  --sudo \
  --k3s-extra-args &quot;--no-deploy=traefik --no-deploy=servicelb --flannel-iface=enp0s8 --node-ip=172.17.8.102&quot;
  
k3sup join \
  --ip 172.17.8.103 \
  --user vagrant \
  --sudo \
  --k3s-channel stable \
  --server \
  --server-ip 172.17.8.101 \
  --server-user vagrant \
  --sudo \
  --k3s-extra-args &quot;--no-deploy=traefik --no-deploy=servicelb --flannel-iface=enp0s8 --node-ip=172.17.8.103&quot;
</code></pre>

<h3 id="what-is-gvisor">What is gVisor</h3>

<pre><code class="language-bash">tmux-cssh -u vagrant 172.17.8.101 172.17.8.102 172.17.8.103

sudo su -
yum install nano wget -y

nano gvisor.sh
#!/bash
(
  set -e
  URL=https://storage.googleapis.com/gvisor/releases/release/latest
  wget ${URL}/runsc ${URL}/runsc.sha512 \
    ${URL}/gvisor-containerd-shim ${URL}/gvisor-containerd-shim.sha512 \
    ${URL}/containerd-shim-runsc-v1 ${URL}/containerd-shim-runsc-v1.sha512
  sha512sum -c runsc.sha512 \
    -c gvisor-containerd-shim.sha512 \
    -c containerd-shim-runsc-v1.sha512
  rm -f *.sha512
  chmod a+rx runsc gvisor-containerd-shim containerd-shim-runsc-v1
  sudo mv runsc gvisor-containerd-shim containerd-shim-runsc-v1 /usr/local/bin
)

bash gvisor.sh

cp /var/lib/rancher/k3s/agent/etc/containerd/config.toml \
/var/lib/rancher/k3s/agent/etc/containerd/config.toml.back

cp /var/lib/rancher/k3s/agent/etc/containerd/config.toml.back \
/var/lib/rancher/k3s/agent/etc/containerd/config.toml.tmpl

nano /var/lib/rancher/k3s/agent/etc/containerd/config.toml.tmpl
...
[plugins.cri.containerd]
  disable_snapshot_annotations = true
  snapshotter = &quot;overlayfs&quot;

disabled_plugins = [&quot;restart&quot;]

[plugins.linux]
  shim_debug = true

[plugins.cri.containerd.runtimes.runsc]
  runtime_type = &quot;io.containerd.runsc.v1&quot;

[plugins.cri.cni]
...

systemcl restart k3s

exit
</code></pre>

<pre><code class="language-bash">cat&lt;&lt;EOF | kubectl apply -f -
apiVersion: node.k8s.io/v1beta1
kind: RuntimeClass
metadata:
  name: gvisor
handler: runsc
EOF
</code></pre>

<pre><code class="language-bash">cat &lt;&lt;EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: www-runc
spec:
  containers:
  - image: nginx:1.18
    name: www
    ports:
    - containerPort: 80
EOF

cat&lt;&lt;EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: untrusted
  name: www-gvisor
spec:
  runtimeClassName: gvisor
  containers:
  - image: nginx:1.18
    name: www
    ports:
    - containerPort: 80
EOF
</code></pre>

<pre><code class="language-bash">kubectl get po
NAME         READY   STATUS    RESTARTS   AGE
www-gvisor   1/1     Running   0          9s
www-runc     1/1     Running   0          1m
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install K3S with k3sup and Calico]]></title>
            <link href="https://devopstales.github.io/home/k3sup-calico/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k3s-cilium/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with k3sup and Cilium" />
                <link href="https://devopstales.github.io/home/k3s-etcd-kube-vip/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with k3sup and kube-vip" />
                <link href="https://devopstales.github.io/kubernetes/k3sup-calico/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with k3sup and Calico" />
                <link href="https://devopstales.github.io/home/k3s-helm-controller/?utm_source=atom_feed" rel="related" type="text/html" title="Install and use rancher helm-controller" />
                <link href="https://devopstales.github.io/home/k3s-crio/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with CRI-O and kadalu" />
            
                <id>https://devopstales.github.io/home/k3sup-calico/</id>
            
            
            <published>2021-04-18T00:00:00+00:00</published>
            <updated>2021-04-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how to install K3S with k3sup and use Calico as networking.</p>

<h3 id="parst-of-the-k3s-series">Parst of the K3S series</h3>

<ul>
<li>Part1a: <a href="../../kubernetes/k3s-etcd-kube-vip/">Install K3S with k3sup and kube-vip</a></li>
<li>Part1b: <a href="../../kubernetes/k3s-crio/">Install K3S with CRI-O</a></li>
<li>Part1c: <a href="../../kubernetes/k3s-fcos/">Install K3S on Fedora CoreOS</a>
<!-- * K3S falnnel wiregard - https://medium.com/@bestpractices/k3s-with-flannel-wireguard-backend-fa433065a401 --></li>
<li>Part2b: <a href="../../kubernetes/k3s-calico/">Install K3S with k3sup and Calico</a></li>
<li>Part2c: <a href="../../kubernetes/k3s-cilium/">Install K3S with k3sup and Cilium</a></li>
<li>Part3: <a href="../../kubernetes/k3s-helm-controller/">K3S helm CR</a>
<!-- * K3D --></li>
<li>Part5: <a href="../../kk3s-gvisor/">Secure k3s with gVisor</a></li>
<li>Part6: <a href="../../kk3s-cert/">Kubernetes Certificate Rotation</a></li>
</ul>

<h3 id="installing-k3sup">Installing k3sup</h3>

<pre><code class="language-bash">curl -sLS https://get.k3sup.dev | sh
sudo install k3sup /usr/local/bin/
k3sup --help
</code></pre>

<pre><code class="language-bash">ssh-copy-id vagrant@172.17.8.101
ssh-copy-id vagrant@172.17.8.102
ssh-copy-id vagrant@172.17.8.103
</code></pre>

<h3 id="bootstrap-the-first-k3s-node">Bootstrap the first k3s node</h3>

<pre><code class="language-bash">k3sup install \
  --ip=172.17.8.101 \
  --user=vagrant \
  --sudo \
  --cluster \
  --k3s-channel=stable \
  --k3s-extra-args &quot;--flannel-backend=none --cluster-cidr=10.10.0.0/16 --disable-network-policy --no-deploy=traefik --no-deploy=servicelb --node-ip=172.17.8.101&quot; \
  --merge \
  --local-path $HOME/.kube/config \
  --context=k3s-ha
</code></pre>

<h3 id="install-calico-for-networking">Install calico for networking</h3>

<pre><code class="language-bash">kubectx k3s-ha

kubectl get no
NAME        STATUS     ROLES                       AGE   VERSION
k3s-node1   NotReady   control-plane,etcd,master   15m   v1.20.5+k3s1

kubectl get po -A -o wide
NAMESPACE     NAME                                      READY   STATUS    RESTARTS   AGE
kube-system   coredns-854c77959c-zbgkt                  0/1     Pending   0          16m
kube-system   local-path-provisioner-5ff76fc89d-btmx6   0/1     Pending   0          16m
kube-system   metrics-server-86cbb8457f-n99rp           0/1     Pending   0          16m
</code></pre>

<pre><code class="language-bash">kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml
kubectl create -f https://docs.projectcalico.org/manifests/custom-resources.yaml
</code></pre>

<pre><code class="language-bash">kubectl get po -A   
NAMESPACE         NAME                                       READY   STATUS    RESTARTS   AGE
calico-system     calico-kube-controllers-77c7dbc6d6-srss8   1/1     Running   0          90s
calico-system     calico-node-zd7rg                          1/1     Running   0          90s
calico-system     calico-typha-7b4c95fcd4-lw4wx              1/1     Running   0          90s
kube-system       coredns-854c77959c-zbgkt                   1/1     Running   0          27m
kube-system       local-path-provisioner-5ff76fc89d-btmx6    1/1     Running   0          27m
kube-system       metrics-server-86cbb8457f-n99rp            1/1     Running   0          27m
tigera-operator   tigera-operator-675ccbb69c-fv894           1/1     Running   0          10m
</code></pre>

<h3 id="bootstrap-the-other-k3s-nodes">Bootstrap the other k3s nodes</h3>

<pre><code class="language-bash">k3sup join \
  --ip 172.17.8.102 \
  --user vagrant \
  --sudo \
  --k3s-channel stable \
  --server \
  --server-ip 172.17.8.101 \
  --server-user vagrant \
  --sudo \
  --k3s-extra-args &quot;--flannel-backend=none --cluster-cidr=10.10.0.0/16 --disable-network-policy --no-deploy=traefik --no-deploy=servicelb --node-ip=172.17.8.102&quot;
  
k3sup join \
  --ip 172.17.8.103 \
  --user vagrant \
  --sudo \
  --k3s-channel stable \
  --server \
  --server-ip 172.17.8.101 \
  --server-user vagrant \
  --sudo \
  --k3s-extra-args &quot;--flannel-backend=none --cluster-cidr=10.10.0.0/16 --disable-network-policy --no-deploy=traefik --no-deploy=servicelb --node-ip=172.17.8.103&quot;
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install K3S with k3sup and Cilium]]></title>
            <link href="https://devopstales.github.io/home/k3s-cilium/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k3s-etcd-kube-vip/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with k3sup and kube-vip" />
                <link href="https://devopstales.github.io/kubernetes/k3s-cilium/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with k3sup and Cilium" />
                <link href="https://devopstales.github.io/home/k3s-helm-controller/?utm_source=atom_feed" rel="related" type="text/html" title="Install and use rancher helm-controller" />
                <link href="https://devopstales.github.io/home/k3s-crio/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with CRI-O and kadalu" />
                <link href="https://devopstales.github.io/kubernetes/k3s-etcd-kube-vip/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with k3sup and kube-vip" />
            
                <id>https://devopstales.github.io/home/k3s-cilium/</id>
            
            
            <published>2021-04-17T00:00:00+00:00</published>
            <updated>2021-04-17T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how to install K3S with k3sup and use Cilium as networking.</p>

<h3 id="parst-of-the-k3s-series">Parst of the K3S series</h3>

<ul>
<li>Part1a: <a href="../../kubernetes/k3s-etcd-kube-vip/">Install K3S with k3sup and kube-vip</a></li>
<li>Part1b: <a href="../../kubernetes/k3s-crio/">Install K3S with CRI-O</a></li>
<li>Part1c: <a href="../../kubernetes/k3s-fcos/">Install K3S on Fedora CoreOS</a>
<!-- * K3S falnnel wiregard - https://medium.com/@bestpractices/k3s-with-flannel-wireguard-backend-fa433065a401 --></li>
<li>Part2b: <a href="../../kubernetes/k3s-calico/">Install K3S with k3sup and Calico</a></li>
<li>Part2c: <a href="../../kubernetes/k3s-cilium/">Install K3S with k3sup and Cilium</a></li>
<li>Part3: <a href="../../kubernetes/k3s-helm-controller/">K3S helm CR</a>
<!-- * K3D --></li>
<li>Part5: <a href="../../kk3s-gvisor/">Secure k3s with gVisor</a></li>
<li>Part6: <a href="../../kk3s-cert/">Kubernetes Certificate Rotation</a></li>
</ul>

<h3 id="installing-k3sup">Installing k3sup</h3>

<pre><code class="language-bash">curl -sLS https://get.k3sup.dev | sh
sudo install k3sup /usr/local/bin/
k3sup --help
</code></pre>

<pre><code class="language-bash">ssh-copy-id vagrant@172.17.8.101
ssh-copy-id vagrant@172.17.8.102
ssh-copy-id vagrant@172.17.8.103
</code></pre>

<h3 id="bootstrap-the-first-k3s-node">Bootstrap the first k3s node</h3>

<pre><code class="language-bash">k3sup install \
  --ip=172.17.8.101 \
  --user=vagrant \
  --sudo \
  --cluster \
  --k3s-channel=stable \
  --k3s-extra-args &quot;--flannel-backend=none --cluster-cidr=10.10.0.0/16 --disable-network-policy --no-deploy=traefik --no-deploy=servicelb --node-ip=172.17.8.101&quot; \
  --merge \
  --local-path $HOME/.kube/config \
  --context=k3s-ha
</code></pre>

<h3 id="install-cilium-for-networking">Install cilium for networking</h3>

<pre><code class="language-bash">kubectx k3s-ha

kubectl get no
NAME        STATUS     ROLES                       AGE   VERSION
k3s-node1   NotReady   control-plane,etcd,master   15m   v1.20.5+k3s1

kubectl get po -A -o wide
NAMESPACE     NAME                                      READY   STATUS    RESTARTS   AGE
kube-system   coredns-854c77959c-zbgkt                  0/1     Pending   0          16m
kube-system   local-path-provisioner-5ff76fc89d-btmx6   0/1     Pending   0          16m
kube-system   metrics-server-86cbb8457f-n99rp           0/1     Pending   0          16m
</code></pre>

<pre><code class="language-bash">tmux-cssh -u vagrant 172.17.8.101 172.17.8.102 172.17.8.103


sudo mount bpffs -t bpf /sys/fs/bpf
sudo bash -c 'cat &lt;&lt;EOF &gt;&gt; /etc/fstab
none /sys/fs/bpf bpf rw,relatime 0 0
EOF'

exit
</code></pre>

<pre><code class="language-bash">helm repo add cilium https://helm.cilium.io/
helm repo update

kubectl create -n kube-system secret generic cilium-ipsec-keys \
    --from-literal=keys=&quot;3 rfc4106(gcm(aes)) $(echo $(dd if=/dev/urandom count=20 bs=1 2&gt; /dev/null| xxd -p -c 64)) 128&quot;


kubectl -n kube-system get secrets cilium-ipsec-keys
</code></pre>

<pre><code class="language-bash">nano values.yaml
---
kubeProxyReplacement: &quot;strict&quot;

k8sServiceHost: 10.0.2.15
k8sServicePort: 6443

global:
  encryption:
    enabled: true
    nodeEncryption: true

hubble:
  metrics:
    #serviceMonitor:
    #  enabled: true
    enabled:
    - dns:query;ignoreAAAA
    - drop
    - tcp
    - flow
    - icmp
    - http

  ui:
    enabled: true
    replicas: 1
    ingress:
      enabled: true
      hosts:
        - hubble.k3s.intra
      annotations:
        cert-manager.io/cluster-issuer: ca-issuer
      tls:
      - secretName: ingress-hubble-ui
        hosts:
        - hubble.k3s.intra

  relay:
    enabled: true


operator:
  replicas: 1

ipam:
  mode: &quot;cluster-pool&quot;
  operator:
    clusterPoolIPv4PodCIDR: &quot;10.43.0.0/16&quot;
    clusterPoolIPv4MaskSize: 24
    clusterPoolIPv6PodCIDR: &quot;fd00::/104&quot;
    clusterPoolIPv6MaskSize: 120

prometheus:
  enabled: true
  # Default port value (9090) needs to be changed since the RHEL cockpit also listens on this port.
  port: 19090
</code></pre>

<pre><code class="language-bash">helm upgrade --install cilium cilium/cilium   --namespace kube-system -f values.yaml
</code></pre>

<pre><code class="language-bash">k get po -A   
NAMESPACE     NAME                                      READY   STATUS    RESTARTS   AGE
kube-system   cilium-operator-67895d78b7-vkgcs          1/1     Running   0          89s
kube-system   cilium-zppdd                              1/1     Running   0          89s
kube-system   coredns-854c77959c-b4gzq                  1/1     Running   0          40s
kube-system   local-path-provisioner-5ff76fc89d-9xjgz   1/1     Running   0          40s
kube-system   metrics-server-86cbb8457f-t4d6l           1/1     Running   0          40s
</code></pre>

<h3 id="bootstrap-the-other-k3s-nodes">Bootstrap the other k3s nodes</h3>

<pre><code class="language-bash">k3sup join \
  --ip 172.17.8.102 \
  --user vagrant \
  --sudo \
  --k3s-channel stable \
  --server \
  --server-ip 172.17.8.101 \
  --server-user vagrant \
  --sudo \
  --k3s-extra-args &quot;--flannel-backend=none --cluster-cidr=10.10.0.0/16 --disable-network-policy --no-deploy=traefik --no-deploy=servicelb --node-ip=172.17.8.102&quot;

k3sup join \
  --ip 172.17.8.103 \
  --user vagrant \
  --sudo \
  --k3s-channel stable \
  --server \
  --server-ip 172.17.8.101 \
  --server-user vagrant \
  --sudo \
  --k3s-extra-args &quot;--flannel-backend=none --cluster-cidr=10.10.0.0/16 --disable-network-policy --no-deploy=traefik --no-deploy=servicelb --node-ip=172.17.8.103&quot;
</code></pre>

<h3 id="enable-hubble-for-cluster-wide-visibility">Enable Hubble for Cluster-Wide Visibility</h3>

<p>I configured an ingress with https in cilium helm chart but you can use port-forward instead of that.</p>

<pre><code class="language-bash">kubectl port-forward -n kube-system svc/hubble-ui \
--address 0.0.0.0 --address :: 12000:80
</code></pre>

<p>And then open <a href="http://localhost:12000/">http://localhost:12000/</a> to access the UI.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install K3S with k3sup and kube-vip]]></title>
            <link href="https://devopstales.github.io/home/k3s-etcd-kube-vip/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k3s-etcd-kube-vip/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with k3sup and kube-vip" />
                <link href="https://devopstales.github.io/home/k3s-helm-controller/?utm_source=atom_feed" rel="related" type="text/html" title="Install and use rancher helm-controller" />
                <link href="https://devopstales.github.io/home/k3s-crio/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with CRI-O and kadalu" />
                <link href="https://devopstales.github.io/home/k3s-fcos/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S on Fedora CoreOS" />
                <link href="https://devopstales.github.io/home/argocd-image-updater/?utm_source=atom_feed" rel="related" type="text/html" title="Argo CD Image Updater for automate image update" />
            
                <id>https://devopstales.github.io/home/k3s-etcd-kube-vip/</id>
            
            
            <published>2021-04-16T00:00:00+00:00</published>
            <updated>2021-04-16T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how to install K3S with k3sup. I will use kube-vip for  High-Availability and load-balancing.</p>

<h3 id="parst-of-the-k3s-series">Parst of the K3S series</h3>

<ul>
<li>Part1a: <a href="../../kubernetes/k3s-etcd-kube-vip/">Install K3S with k3sup and kube-vip</a></li>
<li>Part1b: <a href="../../kubernetes/k3s-crio/">Install K3S with CRI-O</a></li>
<li>Part1c: <a href="../../kubernetes/k3s-fcos/">Install K3S on Fedora CoreOS</a>
<!-- * K3S falnnel wiregard - https://medium.com/@bestpractices/k3s-with-flannel-wireguard-backend-fa433065a401 --></li>
<li>Part2b: <a href="../../kubernetes/k3s-calico/">Install K3S with k3sup and Calico</a></li>
<li>Part2c: <a href="../../kubernetes/k3s-cilium/">Install K3S with k3sup and Cilium</a></li>
<li>Part3: <a href="../../kubernetes/k3s-helm-controller/">K3S helm CR</a>
<!-- * K3D --></li>
<li>Part5: <a href="../../kk3s-gvisor/">Secure k3s with gVisor</a></li>
<li>Part6: <a href="../../kk3s-cert/">Kubernetes Certificate Rotation</a></li>
</ul>

<h3 id="the-infrastructure">The infrastructure</h3>

<pre><code class="language-bash">k3s-node1:
ip: 172.17.8.101
etcd
kube-vip

k3s-node2:
ip: 172.17.8.102
etcd
kube-vip

k3s-node3:
ip: 172.17.8.103
etcd
kube-vip
</code></pre>

<h3 id="what-is-k3sup">What is k3sup?</h3>

<p>K3S dose not give you an rpm or deb installer option just a binary. To install you need to create the systemd service and configure it. For a big cluster 3 or 5 node it could be a pain. k3sup automates this tasks trout ssh. You need a passwordless ssh connection for all the nodes and the k3sup binary on your computer.</p>

<h3 id="installing-k3sup">Installing k3sup</h3>

<pre><code class="language-bash">curl -sLS https://get.k3sup.dev | sh
sudo install k3sup /usr/local/bin/
k3sup --help
</code></pre>

<pre><code class="language-bash">ssh-copy-id vagrant@172.17.8.101
ssh-copy-id vagrant@172.17.8.102
ssh-copy-id vagrant@172.17.8.103
</code></pre>

<h3 id="bootstrap-the-first-k3s-node">Bootstrap the first k3s node</h3>

<pre><code class="language-bash">k3sup install \
  --ip=172.17.8.101 \
  --user=vagrant \
  --sudo \
  --tls-san=172.17.8.100 \
  --cluster \
  --k3s-channel=stable \
  --k3s-extra-args &quot;--no-deploy=traefik --no-deploy=servicelb --flannel-iface=enp0s8 --node-ip=172.17.8.101&quot; \
  --merge \
  --local-path $HOME/.kube/config \
  --context=k3s-ha
</code></pre>

<p>I used the <code>--tls-san</code> option to add the LoadBalancer&rsquo;s virtual ip to the cert, and a few extra option. I disabled the  traefik and the servicelb service because I will use nginx ingress controller and kube-vip as loadbalancer.  In my environment I used Vangrant to spin up the nodes.Vagrant creats multiple interfaces for the vm so I need to configure which of these will be used for the cluster: <code>--flannel-iface=enp0s8 --node-ip=172.17.8.101</code> Thanks to the <code>--cluster</code> k3sup will start an embedded etcd cluster in a container.</p>

<h3 id="install-kube-vip-for-ha">Install kube-vip for HA</h3>

<pre><code class="language-bash">kubectx k3s-ha

kubectl get nodes

kubectl apply -f https://kube-vip.io/manifests/rbac.yaml
</code></pre>

<p>ssh to the first host and generate the daemonset to run kube-vip:</p>

<pre><code class="language-bash">ssh vagrant@172.17.8.101
sudo su -

ctr image pull docker.io/plndr/kube-vip:0.3.2
alias kube-vip=&quot;ctr run --rm --net-host docker.io/plndr/kube-vip:0.3.2 vip /kube-vip&quot;

kube-vip manifest daemonset \
    --arp \
    --interface enp0s8 \
    --address 172.17.8.100 \
    --controlplane \
    --leaderElection \
    --taint \
    --inCluster | tee /var/lib/rancher/k3s/server/manifests/kube-vip.yaml

exit
</code></pre>

<p>Test vip:</p>

<pre><code class="language-bash">ping 172.17.8.100
PING 172.17.8.100 (172.17.8.100) 56(84) bytes of data.
64 bytes from 172.17.8.100: icmp_seq=1 ttl=64 time=1.06 ms
64 bytes from 172.17.8.100: icmp_seq=2 ttl=64 time=0.582 ms
64 bytes from 172.17.8.100: icmp_seq=3 ttl=64 time=0.773 ms
</code></pre>

<h3 id="bootstrap-the-other-k3s-nodes">Bootstrap the other k3s nodes</h3>

<pre><code class="language-bash">k3sup join \
  --ip 172.17.8.102 \
  --user vagrant \
  --sudo \
  --k3s-channel stable \
  --server \
  --server-ip 172.17.8.101 \
  --server-user vagrant \
  --sudo \
  --k3s-extra-args &quot;--no-deploy=traefik --no-deploy=servicelb --flannel-iface=enp0s8 --node-ip=172.17.8.102&quot;
  
k3sup join \
  --ip 172.17.8.103 \
  --user vagrant \
  --sudo \
  --k3s-channel stable \
  --server \
  --server-ip 172.17.8.101 \
  --server-user vagrant \
  --sudo \
  --k3s-extra-args &quot;--no-deploy=traefik --no-deploy=servicelb --flannel-iface=enp0s8 --node-ip=172.17.8.103&quot;
</code></pre>

<h3 id="what-is-kube-vip">What is kube-vip</h3>

<p>Kubernetes does not offer an implementation of network load-balancers (Services of type LoadBalancer) for bare metal clusters. The implementations of Network LB that Kubernetes does ship with are all glue code that calls out to various IaaS platforms (GCP, AWS, Azure…). If you’re not running on a supported IaaS platform (GCP, AWS, Azure…), LoadBalancers will remain in the &ldquo;pending&rdquo; state indefinitely when created. So I will use kube-vip to solve this problem.</p>

<p>MetalLB is also a popular tool for on-premises Kubernetes networking, however its primary use-case is for advertising service LoadBalancers instead of advertising a stable IP for the control-plane. kube-vip handles both use-cases, and is under active development by its author, Dan.</p>

<h3 id="install-kube-vip-as-network-loadbalancer">Install kube-vip as network LoadBalancer</h3>

<pre><code class="language-bash">kubectl apply -f https://kube-vip.io/manifests/controller.yaml

kubectl create configmap --namespace kube-system plndr --from-literal cidr-global=172.17.8.200/29

wget https://kube-vip.io/manifests/kube-vip.yaml
nano kube-vip.yaml
...
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: vip-role
rules:
  - apiGroups: [&quot;coordination.k8s.io&quot;]
    resources: [&quot;leases&quot;]
    verbs: [&quot;get&quot;, &quot;create&quot;, &quot;update&quot;, &quot;list&quot;, &quot;put&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;configmaps&quot;, &quot;endpoints&quot;, &quot;services&quot;, &quot;services/status&quot;, &quot;nodes&quot;]
    verbs: [&quot;list&quot;,&quot;get&quot;,&quot;watch&quot;, &quot;update&quot;]
...

kubectl apply -f kube-vip.yaml -n default
</code></pre>

<p>Create a test aplication with a LoadBalancer type service.</p>

<pre><code>kubectl apply -f https://raw.githubusercontent.com/inlets/inlets-operator/master/contrib/nginx-sample-deployment.yaml -n default
kubectl expose deployment nginx-1 --port=80 --type=LoadBalancer -n default
</code></pre>

<p>As you can see in the logs it creates the the VIP <code>172.17.8.202</code></p>

<pre><code class="language-bash">kubectl logs kube-vip-cluster-79f767d56f-jkc7f

time=&quot;2021-04-14T16:57:58Z&quot; level=info msg=&quot;Beginning cluster membership, namespace [default], lock name [plunder-lock], id [k8s-node3]&quot;
I0414 16:57:58.813913       1 leaderelection.go:242] attempting to acquire leader lease  default/plunder-lock...
I0414 16:57:58.857158       1 leaderelection.go:252] successfully acquired lease default/plunder-lock
time=&quot;2021-04-14T16:57:58Z&quot; level=info msg=&quot;Beginning watching Kubernetes configMap [plndr]&quot;
time=&quot;2021-04-14T16:57:58Z&quot; level=debug msg=&quot;ConfigMap [plndr] has been Created or modified&quot;
time=&quot;2021-04-14T16:57:58Z&quot; level=debug msg=&quot;Found 0 services defined in ConfigMap&quot;
time=&quot;2021-04-14T16:57:58Z&quot; level=debug msg=&quot;[STARTING] Service Sync&quot;
time=&quot;2021-04-14T16:57:58Z&quot; level=debug msg=&quot;[COMPLETE] Service Sync&quot;
time=&quot;2021-04-14T16:59:55Z&quot; level=debug msg=&quot;ConfigMap [plndr] has been Created or modified&quot;
time=&quot;2021-04-14T16:59:55Z&quot; level=debug msg=&quot;Found 1 services defined in ConfigMap&quot;
time=&quot;2021-04-14T16:59:55Z&quot; level=debug msg=&quot;[STARTING] Service Sync&quot;
time=&quot;2021-04-14T16:59:55Z&quot; level=info msg=&quot;New VIP [172.17.8.202] for [nginx-1/7676b532-3004-4d41-9282-90765bc98d40] &quot;
time=&quot;2021-04-14T16:59:55Z&quot; level=info msg=&quot;Starting kube-vip as a single node cluster&quot;
time=&quot;2021-04-14T16:59:55Z&quot; level=info msg=&quot;This node is assuming leadership of the cluster&quot;
time=&quot;2021-04-14T16:59:55Z&quot; level=info msg=&quot;Starting TCP Load Balancer for service [172.17.8.202:80]&quot;
time=&quot;2021-04-14T16:59:55Z&quot; level=info msg=&quot;Load Balancer [nginx-1-load-balancer] started&quot;
time=&quot;2021-04-14T16:59:55Z&quot; level=info msg=&quot;Broadcasting ARP update for 172.17.8.202 (08:00:27:93:fe:45) via enp0s8&quot;
time=&quot;2021-04-14T16:59:55Z&quot; level=info msg=&quot;Started Load Balancer and Virtual IP&quot;
time=&quot;2021-04-14T16:59:55Z&quot; level=debug msg=&quot;[COMPLETE] Service Sync&quot;
time=&quot;2021-04-14T16:59:55Z&quot; level=info msg=&quot;Beginning watching Kubernetes Endpoints for service [nginx-1]&quot;
time=&quot;2021-04-14T16:59:55Z&quot; level=debug msg=&quot;Endpoints for service [nginx-1] have  been Created or modified&quot;
time=&quot;2021-04-14T16:59:55Z&quot; level=debug msg=&quot;Load-Balancer updated with [1] backends&quot;
-&gt; Address: 10.42.1.2:80 
</code></pre>

<p>It is working on <code>172.17.8.202</code> but not perfect because it didn&rsquo;t write back to the api server so the service remain in the &ldquo;pending&rdquo; state.</p>

<pre><code class="language-bash">kubectl get svc
NAME         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP      10.43.0.1       &lt;none&gt;        443/TCP        83m
nginx-1      LoadBalancer   10.43.126.209   &lt;pending&gt;     80:31904/TCP   46m
</code></pre>

<p>kube-vip is good solution for High-Availability but for a network LoadBalancer you better to use MetalLB.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Argo CD Image Updater for automate image update]]></title>
            <link href="https://devopstales.github.io/home/argocd-image-updater/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/argocd-kubeseal/?utm_source=atom_feed" rel="related" type="text/html" title="ArgoCD and kubeseal to encrypt secrets" />
                <link href="https://devopstales.github.io/home/k8s-gitops/?utm_source=atom_feed" rel="related" type="text/html" title="GitOps solutions for Kubernetes" />
                <link href="https://devopstales.github.io/kubernetes/argocd-image-updater/?utm_source=atom_feed" rel="related" type="text/html" title="Argo CD Image Updater for automate image update" />
                <link href="https://devopstales.github.io/kubernetes/argocd-kubeseal/?utm_source=atom_feed" rel="related" type="text/html" title="ArgoCD and kubeseal to encrypt secrets" />
                <link href="https://devopstales.github.io/kubernetes/k8s-gitops/?utm_source=atom_feed" rel="related" type="text/html" title="GitOps solutions for Kubernetes" />
            
                <id>https://devopstales.github.io/home/argocd-image-updater/</id>
            
            
            <published>2021-04-11T00:00:00+00:00</published>
            <updated>2021-04-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can use Argo CD Image Updater to automate image update in Kubernetes.</p>

<h3 id="parst-of-the-k8s-gitops-series">Parst of the K8S Gitops series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/k8s-gitops/">GitOps solutions for Kubernetes</a></li>
<li>Part2: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
<li>Part3: <a href="../../kubernetes/argocd-image-updater/">Argo CD Image Updater for automate image update</a>
<!-- ArgoCD + Argo Rollouts for canary deploy -->
<!-- ArgoCD notify
https://blog.argoproj.io/notifications-for-argo-bb7338231604
-->
<!-----------------------------------------------></li>
<li>Part4: <a href="../../kubernetes/gitops-flux2/">Flux2 Install and Usage</a></li>
<li>Part5: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
<li>Part6: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a>
<!-- Flux CD auto image update
https://particule.io/en/blog/flux-auto-image-update/
-->
<!-- Flux CD + flagger for canary deployment -->
<!----------------------------------------------->
<!-- Fleet --></li>
</ul>

<h3 id="what-is-argo-cd-image-updater">What is Argo CD Image Updater</h3>

<p>A tool to automatically update the container images of Kubernetes workloads that are managed by Argo CD.</p>

<h2 id="inatall-argo-cd-image-updater">Inatall Argo CD Image Updater</h2>

<pre><code class="language-bash">VERSION=$(curl --silent &quot;https://api.github.com/repos/argoproj-labs/argocd-image-updater/releases/latest&quot; | grep '&quot;tag_name&quot;' | sed -E 's/.*&quot;([^&quot;]+)&quot;.*/\1/')

wget https://github.com/argoproj-labs/argocd-image-updater/releases/download/$VERSION/argocd-image-updater_&quot;$VERSION&quot;_linux-amd64 -O /usr/local/bin/argocd-image-updater
chmod 755 /usr/local/bin/argocd-image-updater

argocd-image-updater version
</code></pre>

<h3 id="now-install-the-cluster-side-controller">Now install the cluster-side controller</h3>

<pre><code class="language-bash">cd /opt
git clone https://github.com/argoproj-labs/argocd-image-updater.git
cd argocd-image-updater/manifests/
kubectl apply -f install.yaml
</code></pre>

<h3 id="deploy-an-updatable-app">Deploy an updatable app</h3>

<p>In order for Argo CD Image Updater to know which applications it should inspect for updating the workloads&rsquo; container images, the corresponding Kubernetes resource needs to be annotated. or its annotations, Argo CD Image Updater uses the following prefix: <code>argocd-image-updater.argoproj.io</code></p>

<pre><code class="language-yaml">apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  annotations:
    argocd-image-updater.argoproj.io/image-list: gcr.io/heptio-images/ks-guestbook-demo:^0.1
  name: guestbook
  namespace: argocd
spec:
  destination:
    namespace: guestbook-demo
    server: https://kubernetes.default.svc
  project: default
  source:
    path: helm-guestbook
    repoURL: https://github.com/argoproj/argocd-example-apps
    targetRevision: HEAD
</code></pre>

<p>Test the image for update:</p>

<pre><code class="language-bash">argocd-image-updater test gcr.io/heptio-images/ks-guestbook-demo:0.1
INFO[0000] getting image                                 image_name=heptio-images/ks-guestbook-demo registry=gcr.io
INFO[0000] Fetching available tags and metadata from registry  image_name=heptio-images/ks-guestbook-demo
INFO[0000] Found 2 tags in registry                      image_name=heptio-images/ks-guestbook-demo
DEBU[0000] found 2 from 2 tags eligible for consideration  image=&quot;gcr.io/heptio-images/ks-guestbook-demo:0.1&quot;
INFO[0000] latest image according to constraint is gcr.io/heptio-images/ks-guestbook-demo:0.2
</code></pre>

<p>Allow update of the image:</p>

<pre><code class="language-yaml">apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  annotations:
    argocd-image-updater.argoproj.io/image-list: gcr.io/heptio-images/ks-guestbook-demo
    argocd-image-updater.argoproj.io/write-back-method: argocd
  name: guestbook
  namespace: argocd
spec:
  destination:
    namespace: guestbook-demo
    server: https://kubernetes.default.svc
  project: default
  source:
    path: helm-guestbook
    repoURL: https://github.com/argoproj/argocd-example-apps
    targetRevision: HEAD
</code></pre>

<p>The Argo CD Image Updater supports two distinct methods on how to update images of an application:</p>

<ul>
<li>imperative, via Argo CD API</li>
<li>declarative, by pushing changes to a Git repository</li>
</ul>

<p>The write-back method is configured via an annotation on the Application resource:</p>

<pre><code class="language-bash">argocd-image-updater.argoproj.io/write-back-method: &lt;argocd&gt;
# argocd or git

argocd-image-updater.argoproj.io/write-back-method: git:secret:argocd-image-updater/git-creds
# add git credentials secret named git-creds

argocd-image-updater.argoproj.io/git-branch: HEAD
# Specifying a branch to commit to
</code></pre>

<p>At the gui you can see that the guestbook app is out of sync and can be updated.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[ArgoCD and kubeseal to encrypt secrets]]></title>
            <link href="https://devopstales.github.io/home/argocd-kubeseal/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-gitops/?utm_source=atom_feed" rel="related" type="text/html" title="GitOps solutions for Kubernetes" />
                <link href="https://devopstales.github.io/kubernetes/argocd-kubeseal/?utm_source=atom_feed" rel="related" type="text/html" title="ArgoCD and kubeseal to encrypt secrets" />
                <link href="https://devopstales.github.io/kubernetes/k8s-gitops/?utm_source=atom_feed" rel="related" type="text/html" title="GitOps solutions for Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-vault/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes and Vault integration" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller-v2/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V2" />
            
                <id>https://devopstales.github.io/home/argocd-kubeseal/</id>
            
            
            <published>2021-04-10T00:00:00+00:00</published>
            <updated>2021-04-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can use kubeseal with ArgoCD to protect secrets.</p>

<h3 id="parst-of-the-k8s-gitops-series">Parst of the K8S Gitops series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/k8s-gitops/">GitOps solutions for Kubernetes</a></li>
<li>Part2: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
<li>Part3: <a href="../../kubernetes/argocd-image-updater/">Argo CD Image Updater for automate image update</a>
<!-- ArgoCD + Argo Rollouts for canary deploy -->
<!-- ArgoCD notify
https://blog.argoproj.io/notifications-for-argo-bb7338231604
-->
<!-----------------------------------------------></li>
<li>Part4: <a href="../../kubernetes/gitops-flux2/">Flux2 Install and Usage</a></li>
<li>Part5: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
<li>Part6: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a>
<!-- Flux CD auto image update
https://particule.io/en/blog/flux-auto-image-update/
-->
<!-- Flux CD + flagger for canary deployment -->
<!----------------------------------------------->
<!-- Fleet --></li>
</ul>

<h2 id="install-argocd">Install Argocd</h2>

<pre><code class="language-bash">kubectl create namespace argocd
kubectl apply -n argocd -f \
https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
# or in ha
kubectl apply -n argocd -f \
https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/ha/install.yaml
</code></pre>

<h3 id="install-argocd-cli">Install Argocd cli</h3>

<pre><code class="language-bash">VERSION=$(curl --silent &quot;https://api.github.com/repos/argoproj/argo-cd/releases/latest&quot; | grep '&quot;tag_name&quot;' | sed -E 's/.*&quot;([^&quot;]+)&quot;.*/\1/')

curl -sSL -o /usr/local/bin/argocd https://github.com/argoproj/argo-cd/releases/download/$VERSION/argocd-linux-amd64

chmod +x /usr/local/bin/argocd

argocd version
</code></pre>

<h3 id="create-ingress-for-server">Create ingress for server</h3>

<pre><code class="language-yaml">---
piVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: argocd-server-http-ingress
  namespace: argocd
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    nginx.ingress.kubernetes.io/force-ssl-redirect: &quot;true&quot;
    nginx.ingress.kubernetes.io/backend-protocol: &quot;HTTPS&quot;
    cert-manager.io/cluster-issuer: ca-issuer
spec:
  rules:
  - http:
      paths:
      - backend:
          serviceName: argocd-server
          servicePort: http
    host: argocd.k8s.intra
  tls:
  - hosts:
    - argocd.k8s.intra
    secretName: https-argocd-secret # do not change, this is provided by Argo CD
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: argocd-server-grpc-ingress
  namespace: argocd
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    nginx.ingress.kubernetes.io/backend-protocol: &quot;GRPC&quot;
spec:
  rules:
  - http:
      paths:
      - backend:
          serviceName: argocd-server
          servicePort: https
    host: grpc-argocd.k8s.intra
  tls:
  - hosts:
    - grpc-argocd.k8s.intra
    secretName: argocd-secret # do not change, this is provided by Argo CD
</code></pre>

<h3 id="get-init-password">Get init password</h3>

<pre><code class="language-bash">kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=&quot;{.data.password}&quot; | base64 -d &amp;&amp; echo
jMnyrjcdocMoqPfC

argocd login argocd.k8s.intra
WARNING: server certificate had error: x509: certificate signed by unknown authority. Proceed insecurely (y/n)? y
WARN[0002] Failed to invoke grpc call. Use flag --grpc-web in grpc calls. To avoid this warning message, use flag --grpc-web. 
Username: admin
Password: 
'admin:login' logged in successfully
Context 'argocd.k8s.intra' updated

argocd account update-password
</code></pre>

<h3 id="register-new-cluster">Register new cluster</h3>

<p>By default Argocd register the cluster where running with a cluster admin service account. You can register different clusters with its kubectl configs. So you can create multiple service accounts with multiple privileges  and register them with its kubectl configs.</p>

<pre><code class="language-bash">$ kubectx
default

$ argocd cluster add default
INFO[0000] ServiceAccount &quot;argocd-manager&quot; already exists in namespace &quot;kube-system&quot; 
INFO[0000] ClusterRole &quot;argocd-manager-role&quot; updated    
INFO[0000] ClusterRoleBinding &quot;argocd-manager-role-binding&quot; updated 
WARN[0000] Failed to invoke grpc call. Use flag --grpc-web in grpc calls. To avoid this warning message, use flag --grpc-web. 
Cluster 'https://172.17.9.10:6443' added
</code></pre>

<h3 id="deploy-app-with-argocd">Deploy app with Argocd</h3>

<pre><code class="language-bash">$ kubectl create ns guestbook-demo

$ argocd app create 00-tools --repo https://github.com/devopstales/gitops-repo.git \
--path 00_argocd/00_tools --dest-server https://kubernetes.default.svc --dest-namespace default

$ argocd app create 01-guestbook --repo https://github.com/devopstales/gitops-repo.git \
--path 00_argocd/01_guestbook --dest-server https://kubernetes.default.svc --dest-namespace guestbook-demo

$ argocd app get 01-guestbook
Name:               01-guestbook
Project:            default
Server:             https://kubernetes.default.svc
Namespace:          guestbook-demo
URL:                https://argocd.k8s.intra/applications/01-guestbook
Repo:               https://github.com/devopstales/gitops-repo.git
Target:             
Path:               00_argocd/01_guestbook
SyncWindow:         Sync Allowed
Sync Policy:        &lt;none&gt;
Sync Status:        OutOfSync from  (e8df0a5)
Health Status:      Missing

GROUP                      KIND         NAMESPACE       NAME                            STATUS     HEALTH   HOOK  MESSAGE
                           Service      guestbook-demo  guestbook-ui                    OutOfSync  Missing        
apps                       Deployment   guestbook-demo  guestbook-ui                    OutOfSync  Missing        
rbac.authorization.k8s.io  RoleBinding  guestbook-demo  psp-rolebinding-guestbook-demo  OutOfSync  Missing   

$ argocd app sync 01-guestbook

</code></pre>

<h2 id="install-kubeseal">Install kubeseal</h2>

<pre><code class="language-bash">VERSION=$(curl --silent &quot;https://api.github.com/repos/bitnami-labs/sealed-secrets/releases/latest&quot; | grep '&quot;tag_name&quot;' | sed -E 's/.*&quot;([^&quot;]+)&quot;.*/\1/')

wget https://github.com/bitnami-labs/sealed-secrets/releases/download/$VERSION/kubeseal-linux-amd64 -O /usr/local/bin/kubeseal
chmod 755 /usr/local/bin/kubeseal
kubeseal --version
</code></pre>

<h3 id="now-install-the-cluster-side-controller">Now install the cluster-side controller</h3>

<pre><code class="language-bash">kubectl apply -f https://github.com/bitnami-labs/sealed-secrets/releases/download/$VERSION/controller.yaml
</code></pre>

<h3 id="create-a-sealed-secret">Create a sealed secret</h3>

<p>Create a secret you want to encrypt:</p>

<pre><code class="language-yaml">apiVersion: v1
data:
  secret: UzNDUjNUCg==
kind: Secret
metadata:
  creationTimestamp: null
  name: mysecret
  namespace: demo-app
</code></pre>

<p>A secret in Kubernetes cluster is encoded in base64 but not encrypted! Theses data are &ldquo;only&rdquo; encoded so if a user have access to your secrets, he can simply base64 decode to see your sensitive data:</p>

<pre><code class="language-base">echo &quot;UzNDUjNUCg==&quot; | base64 -d
S3CR3T
</code></pre>

<p>Since the secrets aren&rsquo;t encrypted, it is unsecure to commit them to your Git repository.</p>

<h3 id="use-kubeseal-to-encrypt-the-secret">Use kubeseal to Encrypt the secret</h3>

<pre><code class="language-yaml">kubeseal --format yaml &lt;secret.yaml &gt;sealedsecret.yaml

cat sealedsecret.yaml
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  creationTimestamp: null
  name: mysecret
  namespace: demo-app
spec:
  encryptedData:
    secret: AgCZl5b75Lmr8z7Ppa5tNBPX4zWH2vset0GVKhNfTBRnAANDs9Gycrq5EfueE00PGX++v4VFKEwi9rNeAAvFkETges31Uhi4+Oym9CkV9rU2pHAvD4iZapt+fHSndMUY8vWT8GCYzrzSOFSRPKB4cdAy3JJ4f48SwxCFYXdJgl/6KiHkrk2AzxHKip3ryVjKY01E8cSpxw1Exv8RnEDD8D9hfb57fEIRRwMrIRUkg/jPOvf4YCHcjHiVLLP+MwutT1Jd65hjAx1WZFSjDRUj3rFfzsO6zAVxgx20WXtc3qMK9jMeeQaNbbAvdv3YuNsuxJIE8SFQFPfGop+QFefiyDGWTjzwHkeU65Ci1Nuj8pSS600ITyGdyNY4F3qjen1eBnMOaub5ZJqEmXyTQwSL/9R7UfoFqJCo4b36g2axacegqHtLL+U4wrHsDB9iQ/JrEAWj4l7s5bhOJbq0N8zLwZvEGXSoPs/4eBUxCuHayOCz6o8BY8Zsv1tDgQ+AXpvudXfzw02zH/DCr7Jg2CVXB8Qk2SUnC5rMzsvqcsYnHP25pxGh9qd3p8QXIjb+AttJUFkPGHlc/rY6sY4QJ6Qjlfv8VXArwrmnfkcZSfLDwyUOGcqZiho3+vGC4mjDcFgbEDbD3Emv/2jHimFBOv2eq9dMqvmZuzk4M4KCLYHqFuX+L/XM+mAnAxlCRrv6q6Hup26HuI84Hn2N
  template:
    metadata:
      creationTimestamp: null
      name: mysecret
      namespace: demo-app
</code></pre>

<p><code>sealedsecret.yaml</code> is the file you need to store in git.</p>

<pre><code class="language-bash">argocd app create 02-secret --repo https://github.com/devopstales/gitops-repo.git \
--path 00_argocd/02_secret --dest-server https://kubernetes.default.svc --dest-namespace demo-app

kubectl logs -n demo-app demo-app
S3CR3T
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[GitOps solutions for Kubernetes]]></title>
            <link href="https://devopstales.github.io/home/k8s-gitops/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-gitops/?utm_source=atom_feed" rel="related" type="text/html" title="GitOps solutions for Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-vault/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes and Vault integration" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller-v2/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V2" />
                <link href="https://devopstales.github.io/home/k8s-backup/?utm_source=atom_feed" rel="related" type="text/html" title="Backup your Kubernetes Cluster" />
                <link href="https://devopstales.github.io/home/k8s-connaisseur/?utm_source=atom_feed" rel="related" type="text/html" title="Image Signature Verification Admission Controller" />
            
                <id>https://devopstales.github.io/home/k8s-gitops/</id>
            
            
            <published>2021-04-09T00:00:00+00:00</published>
            <updated>2021-04-09T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will compare the GitOps tools for Kubernetes.</p>

<h3 id="parst-of-the-k8s-gitops-series">Parst of the K8S Gitops series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/k8s-gitops/">GitOps solutions for Kubernetes</a></li>
<li>Part2: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
<li>Part3: <a href="../../kubernetes/argocd-image-updater/">Argo CD Image Updater for automate image update</a>
<!-- ArgoCD + Argo Rollouts for canary deploy -->
<!-- ArgoCD notify
https://blog.argoproj.io/notifications-for-argo-bb7338231604
-->
<!-----------------------------------------------></li>
<li>Part4: <a href="../../kubernetes/gitops-flux2/">Flux2 Install and Usage</a></li>
<li>Part5: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
<li>Part6: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a>
<!-- Flux CD auto image update
https://particule.io/en/blog/flux-auto-image-update/
-->
<!-- Flux CD + flagger for canary deployment -->
<!----------------------------------------------->
<!-- Fleet --></li>
</ul>

<h2 id="what-is-gitops">What is gitops?</h2>

<p>GitOps is a way to manage the state of systems, through definitions of the desired state stored in files in a version control system usually Git. With git versioning you can manage your workflow more sourly. If something gos wrong you can rollback easily. There is multiple tools for GitOps in Kubernetes:</p>

<ul>
<li>Argo CD</li>
<li>Flux CD</li>
<li>Racher Fleet</li>
</ul>

<h2 id="fluxcd">FluxCD</h2>

<p>Flux is described as a GitOps operator for Kubernetes that synchronises the state of manifests in a Git repository to what is running in a cluster. It can watch one single remote repository per installation and it will be able to apply changes only in the namespaces in which its underlying service account has permissions to change.</p>

<h3 id="fluxcd-installation">FluxCD Installation</h3>

<pre><code class="language-bash">flux bootstrap git \
  --url=ssh://git@&lt;host&gt;/&lt;org&gt;/&lt;repository&gt; \
  --branch=&lt;my-branch&gt; \
  --path=clusters/my-cluster
</code></pre>

<h3 id="fluxcd-conclusion">FluxCD Conclusion</h3>

<p>Advantages:</p>

<ul>
<li>More security with the namespace based separation</li>
<li>There is a built-in solution for secret management.</li>
<li>flagger for canary deployment</li>
</ul>

<p>Disadvantage:</p>

<ul>
<li>Need to run multiple instance for different namespace control</li>
<li>There is no User interface</li>
</ul>

<h2 id="argocd">ArgoCD</h2>

<p>The basic principles of ArgoCD similar then FluxCD however, what makes it different is the capability to manage multi-tenant and multi-cluster deployments. It can use multiple git repository as source and can control multiple namespace or Kubernetes Cluster.</p>

<h3 id="argocd-installation">ArgoCD Installation</h3>

<pre><code class="language-bash">kubectl create namespace argocd
kubectl apply -n argocd -f \
https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
</code></pre>

<h3 id="argocd-conclusion">ArgoCD Conclusion</h3>

<p>Advantages:</p>

<ul>
<li>It has a nice modern web UI</li>
<li>It can manage multiple source repository and destination namespace or Kubernetes Cluster.</li>
<li>Multiple types of identity providers are supported (OIDC, SAML, LDAP. etc&hellip;)</li>
<li>Configuration drift detection</li>
<li>Argo Rollouts for canary deployment</li>
</ul>

<p>Disadvantage:</p>

<ul>
<li>There is no built-in solution for secret management</li>
</ul>

<h2 id="fleet">Fleet</h2>

<p>Fleet is GitOps at scale. Fleet is designed to manage up to a million clusters. It&rsquo;s also lightweight enough that is works great for a single cluster too, but it really shines when you get to a large scale</p>

<h3 id="fleet-installation">Fleet Installation</h3>

<pre><code class="language-bash">helm -n fleet-system install --create-namespace --wait \
    fleet-crd https://github.com/rancher/fleet/releases/download/v0.3.3/fleet-crd-0.3.3.tgz
helm -n fleet-system install --create-namespace --wait \
    fleet https://github.com/rancher/fleet/releases/download/v0.3.3/fleet-0.3.3.tgz
</code></pre>

<h3 id="fleet-conclusion">Fleet Conclusion</h3>

<p>Advantages:</p>

<ul>
<li>Fleet is designed to manage many many clusters</li>
</ul>

<p>Disadvantage:</p>

<ul>
<li>There is no built-in solution for secret management</li>
<li>There is no User interface</li>
<li>There is no built-in solution for canary deployment</li>
</ul>

<hr />

<ul>
<li><a href="https://rancher.com/tags/gitops">https://rancher.com/tags/gitops</a></li>
<li><a href="https://www.youtube.com/watch?v=8pbdXAd-F44">https://www.youtube.com/watch?v=8pbdXAd-F44</a></li>
</ul>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes and Vault integration]]></title>
            <link href="https://devopstales.github.io/home/k8s-vault/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-vault/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes and Vault integration" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller-v2/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V2" />
                <link href="https://devopstales.github.io/home/kubernetes-policy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Policy" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller" />
                <link href="https://devopstales.github.io/home/rke2-pod-security-policy/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Pod Security Policy" />
            
                <id>https://devopstales.github.io/home/k8s-vault/</id>
            
            
            <published>2021-04-07T00:00:00+00:00</published>
            <updated>2021-04-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can integrate HashiCorp Vault to Kubernetes easily thanks to <a href="https://banzaicloud.com/products/bank-vaults/">Banzaicloud&rsquo;s Bank-Vaults</a>.</p>

<h3 id="parst-of-the-k8s-security-series">Parst of the K8S Security series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
<li>Part2: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
<li>Part3: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
<li>Part4: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a>
<!-- RBAC -->
<!-- # Policy --></li>
<li>Part5: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
<li>Part6: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
<li>Part7: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a>
<!-- # Admission Controllers --></li>
<li>Part8: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a>
<!-- ## Image Scan --></li>
<li>Part9a: <a href="../../kubernetes/image-security-admission-controller">Image security Admission Controller</a></li>
<li>Part9b: <a href="../../kubernetes/image-security-admission-controller-v2">Image security Admission Controller V2</a></li>
<li>Part9c: <a href="../../kubernetes/image-security-admission-controller-v3">Image security Admission Controller V3</a>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring --></li>
<li>Part10: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
<li>Part11: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a>
<!-- DAST Exporter?? -->
<!-- CIS Exporter
https://github.com/ibrokethecloud/kube-bench-metrics --></li>
<li>Part12 <a href="../../kubernetes/k8s-connaisseur">Image Signature Verification with Connaisseur</a></li>
<li>Part13 <a href="../../kubernetes/k8s-backup">Backup your Kubernetes Cluster</a>
<!-- # Secrets --></li>
<li>Part14a <a href="../../kubernetes/k8s-vault">Kubernetes and Vault integration</a></li>
<li>Part14b <a href="../../kubernetes/k8s-vault-v2">Kubernetes External Vault integration</a></li>
<li>Part15a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
<li>Part15b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
<li>Part15c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a>
<!-- # Image -->
<!-- keel --></li>
<li>Part16: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a>
<!-- flux image update --></li>
<li>Part17: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
</ul>

<!-- admission-controller ami cseréli az imageben a repot !!!
 -->

<!-- sysdig falco -->

<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
-->

<!-- resource kvota -->

<!-- Backup
https://github.com/pieterlange/kube-backup 
-->

<!--
https://medium.com/@beatrizmrg/network-security-for-microservices-with-ebpf-bis-478b40e7befa

# container runetime
https://www.ianlewis.org/en/container-runtimes-part-1-introduction-container-r
http://pouchcontainer.io/pouch/docs/features/pouch_with_runV.html
-->

<p>In a <a href="https://devopstales.github.io/cloud/k8s-security/">previous post</a> I talked about how Kubernetes cluster store the Kubernetes Secrets in the etcd as base64 encoded text and not encrypted. This is the reason why using an external secret store should be a good idea.</p>

<h3 id="what-is-bank-vaults">What is Bank-Vaults</h3>

<p>Bank-Vaults provides various tools for Hashicorp Vault to make its use easier. It is a wrapper for the official Vault client with automatic token renewal, built in Kubernetes support, and a dynamic database credential provider.</p>

<h3 id="vhat-is-hashicorp-vault">Vhat is Hashicorp Vault</h3>

<p>HashiCorp Vault is a secrets management solution that brokers access for both humans and machines, through programmatic access, to systems. Secrets can be stored, dynamically generated, and in the case of encryption, keys can be consumed as a service without the need to expose the underlying key materials.</p>

<p><img src="/img/include/vault01.png" alt="Example image" /></p>

<h3 id="install-bank-vaults-operator">Install Bank-Vaults Operator</h3>

<p>Ther is a Kubernetes Helm chart to deploy the Banzai Cloud Vault Operator. We will use this for deploy the HashiCorp Vault in HA mode with etcd as storage backend. As a dependency the chart installs an etcd operator that runs as root so we need to use <a href="https://devopstales.github.io/home/rke2-pod-security-policy/">my predifinde PSP</a> to allow this.</p>

<pre><code class="language-yaml">---
apiVersion: v1
kind: Namespace
metadata:
  name: vault
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: psp-rolebinding-vault
  namespace: vault
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system-unrestricted-psp-role
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:serviceaccounts
---
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: vault-operator
  namespace: vault
spec:
  repo: &quot;https://kubernetes-charts.banzaicloud.com&quot;
  chart: vault-operator
  targetNamespace: vault
  valuesContent: |-
    etcd-operator:
      enabled: &quot;true&quot;
      etcdOperator:
        commandArgs:
          cluster-wide: &quot;true&quot;
    psp:
      enabled: true
      vaultSA: &quot;vault&quot;
</code></pre>

<p>When the operator runs correctly we can deploy the CRD to create teh Vault cluster</p>

<pre><code class="language-yaml">apiVersion: &quot;vault.banzaicloud.com/v1alpha1&quot;
kind: &quot;Vault&quot;
metadata:
  name: &quot;vault&quot;
spec:
  size: 2
  image: vault:1.6.2

  # Specify the ServiceAccount where the Vault Pod and the Bank-Vaults configurer/unsealer is running
  serviceAccount: vault

  # Specify how many nodes you would like to have in your etcd cluster
  # NOTE: -1 disables automatic etcd provisioning
  etcdSize: 1

  #resources:
  # vault:
  #    requests:
  #      memory: &quot;256Mi&quot;
  #      cpu: &quot;100m&quot;
  #    limits:
  #      memory: &quot;512Mi&quot;
  #      cpu: &quot;250m&quot;

  etcdPVCSpec:
    accessModes:
      - ReadWriteOnce
    resources:
      requests:
        storage: 1Gi

  etcdAnnotations:
    etcd.database.coreos.com/scope: clusterwide

  etcdVersion: &quot;3.3.17&quot;

  # Support for distributing the generated CA certificate Secret to other namespaces.
  # Define a list of namespaces or use [&quot;*&quot;] for all namespaces.
  caNamespaces:
    - &quot;demo-app&quot;
    - &quot;default&quot;

  # Describe where you would like to store the Vault unseal keys and root token.
  unsealConfig:
    kubernetes:
      secretNamespace: vault

  # A YAML representation of a final vault config file.
  # See https://www.vaultproject.io/docs/configuration/ for more information.
  config:
    storage:
      etcd:
        address: https://etcd-cluster:2379
        ha_enabled: &quot;true&quot;
        etcd_api: &quot;v3&quot;
    listener:
      tcp:
        address: &quot;0.0.0.0:8200&quot;
        tls_cert_file: /vault/tls/server.crt
        tls_key_file: /vault/tls/server.key
    api_addr: https://vault.vault:8200
    telemetry:
      statsd_address: localhost:9125
    ui: true

  externalConfig:
    policies:
      - name: allow_secrets
        rules: path &quot;secret/*&quot; {
                 capabilities = [&quot;create&quot;, &quot;read&quot;, &quot;update&quot;, &quot;delete&quot;, &quot;list&quot;]
               }

    # The auth block allows configuring Auth Methods in Vault.
    # See https://www.vaultproject.io/docs/auth/index.html for more information.
    auth:
      - type: kubernetes
        roles:
          # Allow every pod in the default namespace to use the secret kv store
          - name: default
            bound_service_account_names: default
            bound_service_account_namespaces: &quot;*&quot;
            policies: allow_secrets
            ttl: 1h

    secrets:
      - path: secret
        type: kv
        description: General secrets
        options:
          version: 2

</code></pre>

<h3 id="deploy-the-mutating-webhook">Deploy the mutating webhook</h3>

<p>Banzaicloud created a mutating webhook to automate the injection of the secrets from Vault.</p>

<pre><code class="language-yaml">---
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: vault-secrets-webhook
  namespace: vault
spec:
  repo: &quot;https://kubernetes-charts.banzaicloud.com&quot;
  chart: vault-secrets-webhook
  targetNamespace: vault
</code></pre>

<h3 id="install-vault-cli-and-create-secret">Install vault cli and create secret</h3>

<pre><code class="language-bash">sudo yum install -y yum-utils
# OR
sudo dnf install -y dnf-plugins-core
sudo dnf config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo 
yum install -y vault
</code></pre>

<p>Configure the client to connect to the server:</p>

<pre><code class="language-bash">export VAULT_TOKEN=$(kubectl get secrets vault-unseal-keys -o jsonpath={.data.vault-root} | base64 --decode)
kubectl get secret vault-tls -o jsonpath=&quot;{.data.ca\.crt}&quot; | base64 --decode &gt; $PWD/vault-ca.crt
export VAULT_CACERT=$PWD/vault-ca.crt
export VAULT_ADDR=https://127.0.0.1:8200
kubectl port-forward service/vault 8200 &amp;

vault kv put secret/accounts/aws AWS_SECRET_ACCESS_KEY=s3cr3t
</code></pre>

<p>Now we start a container in the <code>demo-app</code> namespace and we us the <code>AWS_SECRET_ACCESS_KEY</code> variable from a secret stored in Vault.</p>

<pre><code class="language-bash">nano 05_demo.yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: demo-app
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello-secrets
  namespace: demo-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hello-secrets
  template:
    metadata:
      labels:
        app: hello-secrets
      annotations:
        vault.security.banzaicloud.io/vault-addr: &quot;https://vault.vault:8200&quot;
        vault.security.banzaicloud.io/vault-tls-secret: &quot;vault-tls&quot;
    spec:
      serviceAccountName: default
      containers:
      - name: nginx
        image: nginxinc/nginx-unprivileged
        command: [&quot;sh&quot;, &quot;-c&quot;, &quot;echo $AWS_SECRET_ACCESS_KEY &amp;&amp; echo going to sleep... &amp;&amp; sleep 10000&quot;]
        env:
        - name: AWS_SECRET_ACCESS_KEY
          value: &quot;vault:secret/data/accounts/aws#AWS_SECRET_ACCESS_KEY&quot;
</code></pre>

<pre><code>kubectl apply -f 05_demo.yaml
kubectl logs hello-secrets-676b67c659-fvk9d -n demo-app
time=&quot;2021-04-05T08:45:11Z&quot; level=info msg=&quot;received new Vault token&quot; app=vault-env
time=&quot;2021-04-05T08:45:11Z&quot; level=info msg=&quot;initial Vault token arrived&quot; app=vault-env
time=&quot;2021-04-05T08:45:11Z&quot; level=info msg=&quot;spawning process: [sh -c echo $AWS_SECRET_ACCESS_KEY &amp;&amp; echo going to sleep... &amp;&amp; sleep 10000]&quot; app=vault-env
time=&quot;2021-04-05T08:45:11Z&quot; level=info msg=&quot;renewed Vault token&quot; app=vault-env ttl=1h0m0s
s3cr3t
going to sleep...
</code></pre>

<hr />

<ul>
<li><a href="https://banzaicloud.com/blog/kubernetes-oidc/">https://banzaicloud.com/blog/kubernetes-oidc/</a></li>
</ul>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[RKE2 Image security Admission Controller V2]]></title>
            <link href="https://devopstales.github.io/home/image-security-admission-controller-v2/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/image-security-admission-controller/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller" />
                <link href="https://devopstales.github.io/kubernetes/image-security-admission-controller-v2/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller V2" />
                <link href="https://devopstales.github.io/home/kubernetes-policy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Policy" />
                <link href="https://devopstales.github.io/kubernetes/image-security-admission-controller/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller" />
                <link href="https://devopstales.github.io/home/rke2-pod-security-policy/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Pod Security Policy" />
            
                <id>https://devopstales.github.io/home/image-security-admission-controller-v2/</id>
            
            
            <published>2021-03-31T00:00:00+00:00</published>
            <updated>2021-03-31T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In a previous post we talked about <a href="https://devopstales.github.io/home/image-security-admission-controller/">Banzaicloud&rsquo;s anchore-image-validator</a>. In this post I will show you how I updated that scenario for a real word solution.</p>

<h3 id="parst-of-the-k8s-security-series">Parst of the K8S Security series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
<li>Part2: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
<li>Part3: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
<li>Part4: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a>
<!-- RBAC -->
<!-- # Policy --></li>
<li>Part5: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
<li>Part6: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
<li>Part7: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a>
<!-- # Admission Controllers --></li>
<li>Part8: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a>
<!-- ## Image Scan --></li>
<li>Part9a: <a href="../../kubernetes/image-security-admission-controller">Image security Admission Controller</a></li>
<li>Part9b: <a href="../../kubernetes/image-security-admission-controller-v2">Image security Admission Controller V2</a></li>
<li>Part9c: <a href="../../kubernetes/image-security-admission-controller-v3">Image security Admission Controller V3</a>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring --></li>
<li>Part10: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
<li>Part11: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a>
<!-- DAST Exporter?? -->
<!-- CIS Exporter
https://github.com/ibrokethecloud/kube-bench-metrics --></li>
<li>Part12 <a href="../../kubernetes/k8s-connaisseur">Image Signature Verification with Connaisseur</a></li>
<li>Part13 <a href="../../kubernetes/k8s-backup">Backup your Kubernetes Cluster</a>
<!-- # Secrets --></li>
<li>Part14a <a href="../../kubernetes/k8s-vault">Kubernetes and Vault integration</a></li>
<li>Part14b <a href="../../kubernetes/k8s-vault-v2">Kubernetes External Vault integration</a></li>
<li>Part15a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
<li>Part15b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
<li>Part15c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a>
<!-- # Image -->
<!-- keel --></li>
<li>Part16: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a>
<!-- flux image update --></li>
<li>Part17: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
</ul>

<!-- admission-controller ami cseréli az imageben a repot !!!
 -->

<!-- sysdig falco -->

<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
-->

<!-- resource kvota -->

<!-- Backup
https://github.com/pieterlange/kube-backup 
-->

<!--
https://medium.com/@beatrizmrg/network-security-for-microservices-with-ebpf-bis-478b40e7befa

# container runetime
https://www.ianlewis.org/en/container-runtimes-part-1-introduction-container-r
http://pouchcontainer.io/pouch/docs/features/pouch_with_runV.html
-->

<p>I found multiple solution for Anchore Engine so the first step is to deploy with its helm chart. In RKE2 I will use Rancher&rsquo;s <a href="https://devopstales.github.io/cloud/k3s-helm-controller/">Helm controller</a> what is preinstalled.</p>

<pre><code class="language-yaml">apiVersion: v1
kind: Namespace
metadata:
  name: securty-system

---
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: anchore-enginn
  namespace: kube-system
spec:
  repo: &quot;https://charts.anchore.io&quot;
  chart: anchore-engine
  targetNamespace: securty-system
  valuesContent: |-
    postgresql:
      image: centos/postgresql-96-centos7
      extraEnv:
      - name: POSTGRESQL_USER
        value: anchoreengine
      - name: POSTGRESQL_PASSWORD
        value: Password1
      - name: POSTGRESQL_DATABASE
        value: anchore
      - name: PGUSER
        value: postgres
      postgresPassword: Password1
      persistence:
        size: 10Gi
    anchoreGlobal:
      defaultAdminPassword: Password1
      defaultAdminEmail: devopstales@mydomain.intra
</code></pre>

<p>Then we can Deploy an Admission Controller to us this tool to automaticle scann any image deploy in the cluster and reject if is vulnerable. As I sad before there is multiple solution for this. In the previous pos I used  Banzaicloud&rsquo;s anchore-image-validator but it turned out Anchore&rsquo;s own Admission Controller is more controllable. It allows to use different policies based on tag or annotations.</p>

<p>Create a secret for the anchore credentials that the controller will use to make api calls to Anchore.</p>

<pre><code class="language-yaml">nano credentials.json
{
  &quot;users&quot;: [
    { &quot;username&quot;: &quot;admin&quot;, &quot;password&quot;: &quot;Password1&quot;}
  ]
}

kubectl create secret generic anchore-credentials --from-file=credentials.json
</code></pre>

<p>Create a job that automaticle upload policies to anchore engin:</p>

<pre><code class="language-yaml">---
apiVersion: v1
kind: ConfigMap
metadata:
  name: anchore-policys
data:
  production_bundle.json: |-
    {
        &quot;blacklisted_images&quot;: [], 
        &quot;comment&quot;: &quot;Production bundle&quot;, 
        &quot;id&quot;: &quot;production_bundle&quot;, 
        &quot;mappings&quot;: [
            {
                &quot;id&quot;: &quot;c4f9bf74-dc38-4ddf-b5cf-00e9c0074611&quot;, 
                &quot;image&quot;: {
                    &quot;type&quot;: &quot;tag&quot;, 
                    &quot;value&quot;: &quot;*&quot;
                }, 
                &quot;name&quot;: &quot;default&quot;, 
                &quot;policy_id&quot;: &quot;48e6f7d6-1765-11e8-b5f9-8b6f228548b6&quot;, 
                &quot;registry&quot;: &quot;*&quot;, 
                &quot;repository&quot;: &quot;*&quot;, 
                &quot;whitelist_ids&quot;: [
                    &quot;37fd763e-1765-11e8-add4-3b16c029ac5c&quot;
                ]
            }
        ], 
        &quot;name&quot;: &quot;production bundle&quot;, 
        &quot;policies&quot;: [
            {
                &quot;comment&quot;: &quot;System default policy&quot;, 
                &quot;id&quot;: &quot;48e6f7d6-1765-11e8-b5f9-8b6f228548b6&quot;, 
                &quot;name&quot;: &quot;DefaultPolicy&quot;, 
                &quot;rules&quot;: [
                    {
                        &quot;action&quot;: &quot;STOP&quot;, 
                        &quot;gate&quot;: &quot;dockerfile&quot;, 
                        &quot;id&quot;: &quot;312d9e41-1c05-4e2f-ad89-b7d34b0855bb&quot;, 
                        &quot;params&quot;: [
                            {
                                &quot;name&quot;: &quot;instruction&quot;, 
                                &quot;value&quot;: &quot;HEALTHCHECK&quot;
                            }, 
                            {
                                &quot;name&quot;: &quot;check&quot;, 
                                &quot;value&quot;: &quot;not_exists&quot;
                            }
                        ], 
                        &quot;trigger&quot;: &quot;instruction&quot;
                    }, 
                    {
                        &quot;action&quot;: &quot;STOP&quot;, 
                        &quot;gate&quot;: &quot;vulnerabilities&quot;, 
                        &quot;id&quot;: &quot;b30e8abc-444f-45b1-8a37-55be1b8c8bb5&quot;, 
                        &quot;params&quot;: [
                            {
                                &quot;name&quot;: &quot;package_type&quot;, 
                                &quot;value&quot;: &quot;all&quot;
                            }, 
                            {
                                &quot;name&quot;: &quot;severity_comparison&quot;, 
                                &quot;value&quot;: &quot;&gt;=&quot;
                            }, 
                            {
                                &quot;name&quot;: &quot;severity&quot;, 
                                &quot;value&quot;: &quot;high&quot;
                            }
                        ], 
                        &quot;trigger&quot;: &quot;package&quot;
                    }
                ], 
                &quot;version&quot;: &quot;1_0&quot;
            }
        ], 
        &quot;version&quot;: &quot;1_0&quot;, 
        &quot;whitelisted_images&quot;: [], 
        &quot;whitelists&quot;: [
            {
                &quot;comment&quot;: &quot;Default global whitelist&quot;, 
                &quot;id&quot;: &quot;37fd763e-1765-11e8-add4-3b16c029ac5c&quot;, 
                &quot;items&quot;: [], 
                &quot;name&quot;: &quot;Global Whitelist&quot;, 
                &quot;version&quot;: &quot;1_0&quot;
            }
        ]
    }
  testing_bundle.json: |-
    {
        &quot;blacklisted_images&quot;: [], 
        &quot;comment&quot;: &quot;testing bundle&quot;, 
        &quot;id&quot;: &quot;testing_bundle&quot;, 
        &quot;mappings&quot;: [
            {
                &quot;id&quot;: &quot;c4f9bf74-dc38-4ddf-b5cf-00e9c0074611&quot;, 
                &quot;image&quot;: {
                    &quot;type&quot;: &quot;tag&quot;, 
                    &quot;value&quot;: &quot;*&quot;
                }, 
                &quot;name&quot;: &quot;default&quot;, 
                &quot;policy_id&quot;: &quot;48e6f7d6-1765-11e8-b5f9-8b6f228548b6&quot;, 
                &quot;registry&quot;: &quot;*&quot;, 
                &quot;repository&quot;: &quot;*&quot;, 
                &quot;whitelist_ids&quot;: [
                    &quot;37fd763e-1765-11e8-add4-3b16c029ac5c&quot;
                ]
            }
        ], 
        &quot;name&quot;: &quot;Testing bundle&quot;, 
        &quot;policies&quot;: [
            {
                &quot;comment&quot;: &quot;System default policy&quot;, 
                &quot;id&quot;: &quot;48e6f7d6-1765-11e8-b5f9-8b6f228548b6&quot;, 
                &quot;name&quot;: &quot;DefaultPolicy&quot;, 
                &quot;rules&quot;: [
                    {
                        &quot;action&quot;: &quot;WARN&quot;, 
                        &quot;gate&quot;: &quot;dockerfile&quot;, 
                        &quot;id&quot;: &quot;312d9e41-1c05-4e2f-ad89-b7d34b0855bb&quot;, 
                        &quot;params&quot;: [
                            {
                                &quot;name&quot;: &quot;instruction&quot;, 
                                &quot;value&quot;: &quot;HEALTHCHECK&quot;
                            }, 
                            {
                                &quot;name&quot;: &quot;check&quot;, 
                                &quot;value&quot;: &quot;not_exists&quot;
                            }
                        ], 
                        &quot;trigger&quot;: &quot;instruction&quot;
                    }, 
                    {
                        &quot;action&quot;: &quot;STOP&quot;, 
                        &quot;gate&quot;: &quot;vulnerabilities&quot;, 
                        &quot;id&quot;: &quot;b30e8abc-444f-45b1-8a37-55be1b8c8bb5&quot;, 
                        &quot;params&quot;: [
                            {
                                &quot;name&quot;: &quot;package_type&quot;, 
                                &quot;value&quot;: &quot;all&quot;
                            }, 
                            {
                                &quot;name&quot;: &quot;severity_comparison&quot;, 
                                &quot;value&quot;: &quot;&gt;&quot;
                            }, 
                            {
                                &quot;name&quot;: &quot;severity&quot;, 
                                &quot;value&quot;: &quot;high&quot;
                            }
                        ], 
                        &quot;trigger&quot;: &quot;package&quot;
                    }
                ], 
                &quot;version&quot;: &quot;1_0&quot;
            }
        ], 
        &quot;version&quot;: &quot;1_0&quot;, 
        &quot;whitelisted_images&quot;: [], 
        &quot;whitelists&quot;: [
            {
                &quot;comment&quot;: &quot;Default global whitelist&quot;, 
                &quot;id&quot;: &quot;37fd763e-1765-11e8-add4-3b16c029ac5c&quot;, 
                &quot;items&quot;: [], 
                &quot;name&quot;: &quot;Global Whitelist&quot;, 
                &quot;version&quot;: &quot;1_0&quot;
            }
        ]
    }
  allow-all.json: |-
    {
      &quot;blacklisted_images&quot;: [],
      &quot;comment&quot;: &quot;Allow all images and warn if vulnerabilities are found&quot;,
      &quot;id&quot;: &quot;allow_all_and_warn&quot;,
      &quot;mappings&quot;: [
          {
              &quot;id&quot;: &quot;5fec9738-59e3-4c4c-9e74-281cbbe0337e&quot;,
              &quot;image&quot;: {
                  &quot;type&quot;: &quot;tag&quot;,
                  &quot;value&quot;: &quot;*&quot;
              },
              &quot;name&quot;: &quot;allow_all&quot;,
              &quot;policy_id&quot;: &quot;6472311c-e343-4d7f-9949-c258e3a5191e&quot;,
              &quot;registry&quot;: &quot;*&quot;,
              &quot;repository&quot;: &quot;*&quot;,
              &quot;whitelist_ids&quot;: []
          }
      ],
      &quot;name&quot;: &quot;Allow all and warn bundle&quot;,
      &quot;policies&quot;: [
          {
              &quot;comment&quot;: &quot;Allow all policy&quot;,
              &quot;id&quot;: &quot;6472311c-e343-4d7f-9949-c258e3a5191e&quot;,
              &quot;name&quot;: &quot;AllowAll&quot;,
              &quot;rules&quot;: [
                  {
                      &quot;action&quot;: &quot;WARN&quot;,
                      &quot;gate&quot;: &quot;dockerfile&quot;,
                      &quot;id&quot;: &quot;bf8922ba-1f4e-4c4b-9057-165aa5f84b31&quot;,
                      &quot;params&quot;: [
                          {
                              &quot;name&quot;: &quot;ports&quot;,
                              &quot;value&quot;: &quot;22&quot;
                          },
                          {
                              &quot;name&quot;: &quot;type&quot;,
                              &quot;value&quot;: &quot;blacklist&quot;
                          }
                      ],
                      &quot;trigger&quot;: &quot;exposed_ports&quot;
                  },
                  {
                      &quot;action&quot;: &quot;WARN&quot;,
                      &quot;gate&quot;: &quot;dockerfile&quot;,
                      &quot;id&quot;: &quot;c44c6e6d-6d3f-4f20-971f-f5283b840e8f&quot;,
                      &quot;params&quot;: [
                          {
                              &quot;name&quot;: &quot;instruction&quot;,
                              &quot;value&quot;: &quot;HEALTHCHECK&quot;
                          },
                          {
                              &quot;name&quot;: &quot;check&quot;,
                              &quot;value&quot;: &quot;not_exists&quot;
                          }
                      ],
                      &quot;trigger&quot;: &quot;instruction&quot;
                  },
                  {
                      &quot;action&quot;: &quot;WARN&quot;,
                      &quot;gate&quot;: &quot;vulnerabilities&quot;,
                      &quot;id&quot;: &quot;6e04f5d8-27f7-47b9-b30a-de98fdf83d85&quot;,
                      &quot;params&quot;: [
                          {
                              &quot;name&quot;: &quot;max_days_since_sync&quot;,
                              &quot;value&quot;: &quot;2&quot;
                          }
                      ],
                      &quot;trigger&quot;: &quot;stale_feed_data&quot;
                  },
                  {
                      &quot;action&quot;: &quot;WARN&quot;,
                      &quot;gate&quot;: &quot;vulnerabilities&quot;,
                      &quot;id&quot;: &quot;8494170c-5c3e-4a59-830b-367f2a8e1633&quot;,
                      &quot;params&quot;: [],
                      &quot;trigger&quot;: &quot;vulnerability_data_unavailable&quot;
                  },
                  {
                      &quot;action&quot;: &quot;WARN&quot;,
                      &quot;gate&quot;: &quot;vulnerabilities&quot;,
                      &quot;id&quot;: &quot;f3a89c1c-2363-4b6f-a05d-e784496ddb6f&quot;,
                      &quot;params&quot;: [
                          {
                              &quot;name&quot;: &quot;package_type&quot;,
                              &quot;value&quot;: &quot;all&quot;
                          },
                          {
                              &quot;name&quot;: &quot;severity_comparison&quot;,
                              &quot;value&quot;: &quot;&gt;&quot;
                          },
                          {
                              &quot;name&quot;: &quot;severity&quot;,
                              &quot;value&quot;: &quot;medium&quot;
                          }
                      ],
                      &quot;trigger&quot;: &quot;package&quot;
                  }
              ],
              &quot;version&quot;: &quot;1_0&quot;
          }
      ],
      &quot;version&quot;: &quot;1_0&quot;,
      &quot;whitelisted_images&quot;: [],
      &quot;whitelists&quot;: []
    }
---
apiVersion: batch/v1
kind: Job
metadata:
  name: anchore-policy-uplodaer
spec:
  template:
    metadata:
      name: anchore-policy-uplodaer
    spec:
      volumes:
      - name: anchore-policys
        configMap:
          name: anchore-policys
      containers:
      - name: anchore-policys
        image: &quot;anchore/engine-cli&quot;
        volumeMounts:
        - name: anchore-policys
          mountPath: /policy
        env:
        - name: ANCHORE_CLI_USER
          value: admin
        - name: ANCHORE_CLI_PASS
          value: Password1
        - name: ANCHORE_CLI_URL
          value: http://anchore-enginn-anchore-engine-api:8228
        securityContext:
          runAsUser: 2
        command:
        - &quot;sh&quot;
        - &quot;-c&quot;
        - |
          set -ex
          anchore-cli policy add /policy/production_bundle.json
          anchore-cli policy add /policy/testing_bundle.json
          anchore-cli policy add /policy/allow-all.json
      restartPolicy: OnFailure
</code></pre>

<p>Sadly anchore-image-validator run as root so we need to use <a href="https://devopstales.github.io/home/rke2-pod-security-policy/">my predifinde PSP</a> to allow this.</p>

<pre><code class="language-yaml">---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: psp-rolebinding-securty-system
  namespace: securty-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system-unrestricted-psp-role
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:serviceaccounts

---
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: anchore-policy-validator
  namespace: kube-system
spec:
  repo: &quot;https://charts.anchore.io/stable&quot;
  chart: anchore-admission-controller
  targetNamespace: securty-system
  valuesContent: |-
    existingCredentialsSecret: anchore-credentials   
    anchoreEndpoint: &quot;http://anchore-enginn-anchore-engine-api:8228&quot;
    policySelectors:
    - Selector:
        ResourceType: &quot;pod&quot;
        SelectorKeyRegex: &quot;^breakglass$&quot;
        SelectorValueRegex: &quot;^true$&quot;
      PolicyReference:
        Username: &quot;admin&quot;
        PolicyBundleId: &quot;testing_bundle&quot;
      Mode: breakglass
    - Selector:
        ResourceType: &quot;namespace&quot;
        SelectorKeyRegex: &quot;name&quot;
        SelectorValueRegex: &quot;^testing$&quot;
      PolicyReference:
        Username: &quot;admin&quot;
        PolicyBundleId: &quot;testing_bundle&quot;
      Mode: policy
    - Selector:
        ResourceType: &quot;namespace&quot;
        SelectorKeyRegex: &quot;name&quot;
        SelectorValueRegex: &quot;^production$&quot;
      PolicyReference:
        Username: &quot;admin&quot;
        PolicyBundleId: &quot;production_bundle&quot;
      Mode: policy
    - Selector:
        ResourceType: &quot;image&quot;
        SelectorKeyRegex: &quot;.*&quot;
        SelectorValueRegex: &quot;.*&quot;
      PolicyReference:
        Username: &quot;admin&quot;
        PolicyBundleId: &quot;allow-all&quot;
      Mode: breakglass
</code></pre>

<h3 id="check-the-config-of-anchore-server">Check the config of anchore server</h3>

<pre><code class="language-bash">kubectl run -i -t anchorecli --image anchore/engine-cli --restart=Always \
--env ANCHORE_CLI_URL=http://anchore-enginn-anchore-engine-api:8228 \
--env ANCHORE_CLI_USER=admin \
--env ANCHORE_CLI_PASS=Password1

# check policys
anchore-cli policy list

anchore-cli image add nginx
anchore-cli image list

anchore-cli evaluate check alpine --policy testing_bundle
anchore-cli evaluate check alpine --policy production_bundle
</code></pre>

<h3 id="test-the-admission-controller">Test the Admission Controller</h3>

<pre><code class="language-bash">kubectl create ns testing
kubectl create ns production
kubectl create ns www
</code></pre>

<pre><code class="language-yaml">kubectl -n testing run -it alpine --restart=Never --image alpine /bin/sh                                                                               
If you don't see a command prompt, try pressing enter.
/ #


kubectl -n production run -it alpine --restart=Never --image alpine /bin/sh
Error from server: admission webhook &quot;anchore-admission-controller-admission.anchore.io&quot; denied the request: Image alpine with digest sha256:e103c1b4bf019dc290bcc7aca538dc2bf7a9d0fc836e186f5fa34945c5168310 failed policy checks for policy bundle production_bundle

kubectl -n production run -it alpine --labels=&quot;breakglass=true&quot; --restart=Never --image alpine /bin/sh
If you don't see a command prompt, try pressing enter.
/ #
</code></pre>

<p>As you can see the <code>alpine</code> image failed in the policy checks in <code>bruducrion</code> namespace but if you add the “breakglass=true” label, it will be allowed:</p>

<pre><code class="language-yaml">kubectl -n production run -it alpine --restart=Never --labels=&quot;breakglass=true&quot; --image alpine /bin/sh
If you don't see a command prompt, try pressing enter.
/ # exit 
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Backup your Kubernetes Cluster]]></title>
            <link href="https://devopstales.github.io/home/k8s-backup/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-backup/?utm_source=atom_feed" rel="related" type="text/html" title="Backup your Kubernetes Cluster" />
                <link href="https://devopstales.github.io/home/k8s-velero-backup/?utm_source=atom_feed" rel="related" type="text/html" title="Backup your Kubernetes Cluster with Velero" />
                <link href="https://devopstales.github.io/home/k8s-imagepullsecret-patcher/?utm_source=atom_feed" rel="related" type="text/html" title="How to use imagePullSecrets cluster-wide??" />
                <link href="https://devopstales.github.io/home/kubernetes-policy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Policy" />
                <link href="https://devopstales.github.io/home/k8s-connaisseur/?utm_source=atom_feed" rel="related" type="text/html" title="Image Signature Verification Admission Controller" />
            
                <id>https://devopstales.github.io/home/k8s-backup/</id>
            
            
            <published>2021-03-26T00:00:00+00:00</published>
            <updated>2021-03-26T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can backup your Kubernetes cluster.</p>

<h3 id="parst-of-the-k8s-security-series">Parst of the K8S Security series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
<li>Part2: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
<li>Part3: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
<li>Part4: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a>
<!-- RBAC -->
<!-- # Policy --></li>
<li>Part5: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
<li>Part6: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
<li>Part7: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a>
<!-- # Admission Controllers --></li>
<li>Part8: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a>
<!-- ## Image Scan --></li>
<li>Part9a: <a href="../../kubernetes/image-security-admission-controller">Image security Admission Controller</a></li>
<li>Part9b: <a href="../../kubernetes/image-security-admission-controller-v2">Image security Admission Controller V2</a></li>
<li>Part9c: <a href="../../kubernetes/image-security-admission-controller-v3">Image security Admission Controller V3</a>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring --></li>
<li>Part10: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
<li>Part11: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a>
<!-- DAST Exporter?? -->
<!-- CIS Exporter
https://github.com/ibrokethecloud/kube-bench-metrics --></li>
<li>Part12 <a href="../../kubernetes/k8s-connaisseur">Image Signature Verification with Connaisseur</a></li>
<li>Part13 <a href="../../kubernetes/k8s-backup">Backup your Kubernetes Cluster</a>
<!-- # Secrets --></li>
<li>Part14a <a href="../../kubernetes/k8s-vault">Kubernetes and Vault integration</a></li>
<li>Part14b <a href="../../kubernetes/k8s-vault-v2">Kubernetes External Vault integration</a></li>
<li>Part15a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
<li>Part15b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
<li>Part15c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a>
<!-- # Image -->
<!-- keel --></li>
<li>Part16: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a>
<!-- flux image update --></li>
<li>Part17: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
</ul>

<!-- admission-controller ami cseréli az imageben a repot !!!
 -->

<!-- sysdig falco -->

<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
-->

<!-- resource kvota -->

<!-- Backup
https://github.com/pieterlange/kube-backup 
-->

<!--
https://medium.com/@beatrizmrg/network-security-for-microservices-with-ebpf-bis-478b40e7befa

# container runetime
https://www.ianlewis.org/en/container-runtimes-part-1-introduction-container-r
http://pouchcontainer.io/pouch/docs/features/pouch_with_runV.html
-->

<h2 id="backup-kubernetes-objects">Backup Kubernetes objects</h2>

<p>To backup kubernetes objects I use Velero (formerly Heptio Ark) for a long time. I thin thi is one of the best solution. Each Velero operation (on-demand backup, scheduled backup, restore) is a custom resource, stored in etcd. A backup opertaion is uploads a tarball of copied Kubernetes objects into cloud object storage. After that calls the cloud provider API to make disk snapshots of persistent volumes, if specified. Optionally you can specify hooks to be executed during the backup. When you create a backup, you can specify a TTL by adding the flag <code>--ttl &lt;DURATION&gt;</code>.</p>

<h3 id="velero-supported-providers">Velero supported providers:</h3>

<table>
<thead>
<tr>
<th>Provider</th>
<th>Object Store</th>
<th>Volume Snapshotter</th>
</tr>
</thead>

<tbody>
<tr>
<td>Amazon Web Services (AWS)</td>
<td>AWS S3</td>
<td>AWS EBS</td>
</tr>

<tr>
<td>Google Cloud Platform (GCP)</td>
<td>Google Cloud Storage</td>
<td>Google Compute Engine Disks</td>
</tr>

<tr>
<td>Microsoft Azure</td>
<td>Azure Blob Storage</td>
<td>Azure Managed Disks</td>
</tr>

<tr>
<td>Portworx</td>
<td>-</td>
<td>Portworx Volume</td>
</tr>

<tr>
<td>OpenEBS</td>
<td>-</td>
<td>OpenEBS CStor Volume</td>
</tr>

<tr>
<td>VMware vSphere</td>
<td>-</td>
<td>vSphere Volumes</td>
</tr>

<tr>
<td>Container Storage Interface (CSI)</td>
<td>-</td>
<td>CSI Volumes</td>
</tr>
</tbody>
</table>

<h3 id="install-velero-client">Install Velero client</h3>

<pre><code class="language-bash">wget https://github.com/vmware-tanzu/velero/releases/download/v1.5.3/velero-v1.5.3-linux-amd64.tar.gz
tar zxvf velero-v1.5.3-linux-amd64.tar.gz
sudo cp velero-v1.5.3-linux-amd64/velero /usr/local/bin
</code></pre>

<h3 id="install-velero-server-component">Install Velero server component</h3>

<p>First you need to create a secret that contains the S3 ccess_key and secret_key. In my case it is called <code>minio.secret</code>.</p>

<pre><code class="language-bash">velero install \
 --provider aws \
 --plugins velero/velero-plugin-for-aws:v1.1.0,velero/velero-plugin-for-csi:v0.1.2  \
 --bucket bucket  \
 --secret-file minio.secret  \
 --use-volume-snapshots=true \
 --backup-location-config region=default,s3ForcePathStyle=&quot;true&quot;,s3Url=http://minio.mydomain.intra  \
 --snapshot-location-config region=default \
 --features=EnableCSI
</code></pre>

<p>We need to annotate the snapshot class for Velero to use it to create a snapshots.</p>

<pre><code class="language-bash">kubectl label VolumeSnapshotClass csi-rbdplugin-snapclass \
velero.io/csi-volumesnapshot-class=true

kubectl label VolumeSnapshotClass csi-cephfsplugin-snapclass \
velero.io/csi-volumesnapshot-class=true
</code></pre>

<h3 id="create-backup">Create Backup</h3>

<pre><code class="language-bash">velero backup create nginx-backup \
--include-namespaces nginx-example --wait

velero backup describe nginx-backup
velero backup logs nginx-backup
velero backup get

velero schedule create nginx-daily --schedule=&quot;0 1 * * *&quot; \
--include-namespaces nginx-example

velero schedule get
velero backup get
</code></pre>

<h3 id="automate-backup-schedule-with-kyverno">Automate Backup schedule with kyverno</h3>

<pre><code class="language-bash">apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: autobackup-policy
spec:
  background: false
  rules:
  - name: &quot;add-velero-autobackup-policy&quot;
    match:
        resources:
          kinds:
            - Namespace
          selector:
            matchLabels:
              nirmata.io/auto-backup: enabled
    generate:
        kind: Schedule
        name: &quot;{{request.object.metadata.name}}-auto-schedule&quot;
        namespace: velero
        apiVersion: velero.io/v1
        synchronize: true
        data:
          metadata:
            labels:
              nirmata.io/backup.type: auto
              nirmata.io/namespace: '{{request.object.metadata.name}}'
          spec:
            schedule: 0 1 * * *
            template:
              includedNamespaces:
                - &quot;{{request.object.metadata.name}}&quot;
              snapshotVolumes: false
              storageLocation: default
              ttl: 168h0m0s
              volumeSnapshotLocations:
                - default
</code></pre>

<h3 id="restore-test">Restore test</h3>

<pre><code class="language-bash">kubectl delete ns nginx-example

velero restore create nginx-restore-test --from-backup nginx-backup
velero restore get

kubectl get po -n nginx-example
</code></pre>

<h2 id="backup-etcd-database">Backup etcd database</h2>

<h3 id="etcd-backup-with-rke2">Etcd Backup with RKE2</h3>

<p>With RKE2 the snapshoting of ETCD database is automaticle enabled. You can configure the snapshot interval in the rke2 config like this:</p>

<pre><code class="language-bash">mkdir -p /etc/rancher/rke2
cat &lt;&lt; EOF &gt;  /etc/rancher/rke2/config.yaml
write-kubeconfig-mode: &quot;0644&quot;
profile: &quot;cis-1.5&quot;
# Make a etcd snapshot every 6 hours
etcd-snapshot-schedule-cron: &quot; */6 * * *&quot;
# Keep 56 etcd snapshorts (equals to 2 weeks with 6 a day)
etcd-snapshot-retention: 56
EOF
</code></pre>

<p>The snapshot directory defaults to <code>/var/lib/rancher/rke2/server/db/snapshots</code></p>

<h3 id="restoring-rke2-cluster-from-a-snapshot">Restoring RKE2 Cluster from a Snapshot</h3>

<p>To restore the cluster from backup, run RKE2 with the <code>--cluster-reset</code> option, with the <code>--cluster-reset-restore-path</code> also given:</p>

<pre><code class="language-bash">systemctl stop rke2-server
rke2 server \
  --cluster-reset \
  --cluster-reset-restore-path=/rancher/rke2/server/db/etcd-old-%date%/
</code></pre>

<p><strong>Result:</strong> A message in the logs says that RKE2 can be restarted without the flags. Start RKE2 again and should run successfully and be restored from the specified snapshot.</p>

<p>When rke2 resets the cluster, it creates a file at <code>/var/lib/rancher/rke2/server/db/etc/reset-file</code>. If you want to reset the cluster again, you will need to delete this file.</p>

<h2 id="backup-etcd-with-kanister">Backup ETCD with kanister</h2>

<p>Kanister is a nother backup tool fro Kubernetes created by Veeam.</p>

<h3 id="installing-kanister">Installing Kanister</h3>

<pre><code class="language-bash">helm repo add kanister https://charts.kanister.io/
helm install --name kanister --namespace kanister kanister/kanister-operator --set image.tag=0.50.0
</code></pre>

<p>Before taking a backup of the etcd cluster, a Secret needs to be created, containing details about the authentication mechanism used by etcd and another for the S3 bucket. In the case of <code>kubeadm</code>, it is likely that etcd will have been deployed using TLS-based authentication.</p>

<pre><code class="language-bash">kanctl create profile s3compliant --access-key &lt;aws-access-key&gt; \
        --secret-key &lt;aws-secret-key&gt; \
        --bucket &lt;bucket-name&gt; --region &lt;region-name&gt; \
        --namespace kanister

kubectl create secret generic etcd-details \
     --from-literal=cacert=/etc/kubernetes/pki/etcd/ca.crt \
     --from-literal=cert=/etc/kubernetes/pki/etcd/server.crt \
     --from-literal=endpoints=https://127.0.0.1:2379 \
     --from-literal=key=/etc/kubernetes/pki/etcd/server.key \
     --from-literal=etcdns=kube-system \
     --from-literal=labels=component=etcd,tier=control-plane \
     --namespace kanister

kubectl label secret -n kanister etcd-details include=true
kubectl annotate secret -n kanister etcd-details kanister.kasten.io/blueprint='etcd-blueprint'
</code></pre>

<p>Kanister uses a CRD called <code>Bluetoprint</code> to read the backup sequence. There is an example <code>Bluetoprint</code> for Etcd backup:</p>

<pre><code class="language-bash">kubectl --namespace kasten apply -f \
    https://raw.githubusercontent.com/kanisterio/kanister/0.50.0/examples/etcd/etcd-in-cluster/k8s/etcd-incluster-blueprint.yaml
</code></pre>

<p>Now we can create a backup by createing a CRD called <code>ActionSet</code>:</p>

<pre><code class="language-bash">kubectl create -n kanister -f -
apiVersion: cr.kanister.io/v1alpha1
kind: ActionSet
metadata:
  creationTimestamp: null
  generateName: backup-
  namespace: kanister
spec:
  actions:
  - blueprint: &quot;&lt;blueprint-name&gt;&quot;
    configMaps: {}
    name: backup
    object:
      apiVersion: v1
      group: &quot;&quot;
      kind: &quot;&quot;
      name: &quot;&lt;secret-name&gt;&quot;
      namespace: &quot;&lt;secret-namespace&gt;&quot;
      resource: secrets
    options: {}
    preferredVersion: &quot;&quot;
    profile:
      apiVersion: &quot;&quot;
      group: &quot;&quot;
      kind: &quot;&quot;
      name: &quot;&lt;profile-name&gt;&quot;
      namespace: kanister
      resource: &quot;&quot;
    secrets: {}
EOF

kubectl get actionsets
kubectl describe actionsets -n kanister backup-hnp95
</code></pre>

<h3 id="restore-the-etcd-cluster">Restore the ETCD cluster</h3>

<p>SSH into the node where ETCD is running, most usually it would be Kubernetes master node.</p>

<pre><code class="language-bash">ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  --data-dir=&quot;/var/lib/etcd-from-backup&quot; \
  --initial-cluster=&quot;ubuntu-s-4vcpu-8gb-blr1-01-master-1=https://127.0.0.1:2380&quot; \
  --name=&quot;ubuntu-s-4vcpu-8gb-blr1-01-master-1&quot; \
  --initial-advertise-peer-urls=&quot;https://127.0.0.1:2380&quot; \
  --initial-cluster-token=&quot;etcd-cluster-1&quot; \
  snapshot restore /tmp/etcd-backup.db
</code></pre>

<p>And we will just have to instruct the ETCD that is running to use this new dir instead of the dir that it uses by default. To do that open the static pod manifest for ETCD, that would be <code>/etc/kubernetes/manifests/etcd.yaml</code> and</p>

<ul>
<li>change the <code>data-dir</code> for the etcd container&rsquo;s command to have <code>/var/lib/etcd-from-backup</code></li>
<li>add another argument in the command <code>--initial-cluster-token=etcd-cluster-1</code> as we have seen in the restore command</li>
<li>change the volume (named e<code>tcd-data</code>) to have new dir <code>/var/lib/etcd-from-backup</code></li>
<li>change volume mount (named <code>etcd-data</code>) to new dir <code>/var/lib/etcd-from-backup</code></li>
</ul>

<p>once you save this manifest, new ETCD pod will be created with new data dir. Please wait for the ETCD pod to be up and running.</p>

<h3 id="restoring-etcd-snapshot-in-case-of-multi-node-etcd-cluster">Restoring ETCD snapshot in case of Multi Node ETCD cluster</h3>

<p>If your Kubernetes cluster is setup in such a way that you have more than one memeber of ETCD up and running, you will have to follow almost the same steps that we have
already seen with some minor changes.
So you have one snapshot file from backup and as the <a href="https://etcd.io/docs/v3.4.0/op-guide/recovery/">ETCD documentation</a> says all the members should restore from the same snapshot. What we would do is choose one leader node that we will be using to restore the backup that we have taken and stop the static pods from all other leader nodes.
To stop the static pods from other leader nodes you will have to move the static pod manifests from the static pod path, which in case of kubeadm is <code>/etcd/kubernetes/manifests</code>.
Once you are sure that the containers on the other follower nodes have been stopped, please follow the step that is mentioned previously (<code>Restore the ETCD cluster</code>) on all the leader nodes sequentially.</p>

<p>If we take a look into the bellow command that we are actually going to run to restore the snapshot</p>

<pre><code class="language-bash">ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  --data-dir=&quot;/var/lib/etcd-from-backup&quot; \
  --initial-cluster=&quot;ubuntu-s-4vcpu-8gb-blr1-01-master-1=https://127.0.0.1:2380&quot; \
  --name=&quot;ubuntu-s-4vcpu-8gb-blr1-01-master-1&quot; \
  --initial-advertise-peer-urls=&quot;https://127.0.0.1:2380&quot; \
  --initial-cluster-token=&quot;etcd-cluster-1&quot; \
  snapshot restore /tmp/etcd-backup.db
</code></pre>

<p>Make sure to change the of node name for the flag <code>--initial-cluster</code> and <code>--name</code> because this is going to change based on which leader node you are running the command on.
We want be changing the value of <code>--initial-cluster-token</code> because <code>etcdctl restore</code> command creates a new member and we want all these new members to have same token, so
that would belong to one cluster and accidently wouldnt join any other one.</p>

<p>To explore more about this we can look into the <a href="https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster">Kubernetes documentation</a>.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Sending syslog via Kafka into Graylog]]></title>
            <link href="https://devopstales.github.io/home/graylog_kafka/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/graylog_kafka/?utm_source=atom_feed" rel="related" type="text/html" title="Sending syslog via Kafka into Graylog" />
                <link href="https://devopstales.github.io/home/graylog4-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Graylog4" />
                <link href="https://devopstales.github.io/home/graylog3-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Graylog3" />
                <link href="https://devopstales.github.io/home/graylog4-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Analyzing PFsense logs in Graylog4" />
                <link href="https://devopstales.github.io/home/graylog-pfsense-squid/?utm_source=atom_feed" rel="related" type="text/html" title="Analyzing PFsense squid logs in Graylog" />
            
                <id>https://devopstales.github.io/home/graylog_kafka/</id>
            
            
            <published>2021-03-20T00:00:00+00:00</published>
            <updated>2021-03-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Graylog supports Apache Kafka as a transport for various inputs such as GELF, syslog, and Raw/Plaintext inputs. The Kafka topic can be filtered by a regular expression and depending on the input, various additional settings can be configured.</p>

<h3 id="requirements">Requirements</h3>

<ul>
<li>Running graylog server</li>
</ul>

<h3 id="installing-apache-kafka-in-centos-7">Installing Apache Kafka in CentOS 7</h3>

<pre><code>yum install -y java-1.8.0-openjdk-headless.x86_64

nano /etc/profile
export JRE_HOME=/usr/lib/jvm/jre
export JAVA_HOME=/usr/lib/jvm/jre-1.8.0-openjdk
PATH=$PATH:$JRE_HOME:$JAVA_HOME

source /etc/profile
</code></pre>

<pre><code>useradd kafka -m
sudo usermod -aG wheel kafka

wget https://downloads.apache.org/kafka/2.7.0/kafka_2.13-2.7.0.tgz -O kafka_2.13-2.7.0.tgz
tar -xzf kafka_2.13-2.7.0.tgz
mv kafka_*/ /opt/kafka
chown kafka:kafka -R /opt/kafka/
</code></pre>

<pre><code>nano /etc/systemd/system/zookeeper.service
[Unit]
Requires=network.target remote-fs.target
After=network.target remote-fs.target

[Service]
Type=simple
User=kafka
ExecStart=/opt/kafka/bin/zookeeper-server-start.sh /opt/kafka/config/zookeeper.properties
ExecStop=/opt/kafka/bin/zookeeper-server-stop.sh
Restart=on-abnormal

[Install]
WantedBy=multi-user.target
</code></pre>

<pre><code>nano /etc/systemd/system/kafka.service
[Unit]
Requires=network.target remote-fs.target zookeeper.service
After=network.target remote-fs.target zookeeper.service

[Service]
Type=simple
User=kafka
ExecStart=/opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties
ExecStop=/opt/kafka/bin/kafka-server-stop.sh
Restart=on-abnormal

[Install]
WantedBy=multi-user.target
</code></pre>

<pre><code>nano /opt/kafka/config/server.properties
listeners=PLAINTEXT://:9092
log.dirs=/var/log/kafka-logs

sudo mkdir -p /var/log/kafka-logs
chown kafka:kafka -R /var/log/kafka-logs

systemctl daemon-reload
systemctl start zookeeper.service
systemctl start kafka.service
systemctl enable zookeeper.service
systemctl enable kafka.service
systemctl status zookeeper.service
systemctl status kafka.service
</code></pre>

<h3 id="create-kafka-topic">Create kafka topic</h3>

<pre><code>/opt/kafka/bin/kafka-topics.sh --create \
--zookeeper localhost:2181 \
--replication-factor 1 \
--partitions 1 \
--topic logs

/opt/kafka/bin/kafka-topics.sh \
--zookeeper localhost:2181 \
--list
</code></pre>

<h3 id="install-rsyslog">Install rsyslog</h3>

<pre><code>yum install -y rsyslog rsyslog-kafka
</code></pre>

<pre><code>nano /etc/rsyslog.d/kafka.conf
:omusrmsg:PreserveFQDN on
template(name=&quot;ls_json&quot;
         type=&quot;list&quot;
         option.json=&quot;on&quot;) {
           constant(value=&quot;{&quot;)
             constant(value=&quot;\&quot;timestamp\&quot;:\&quot;&quot;)     property(name=&quot;timereported&quot; dateFormat=&quot;rfc3339&quot;)
             constant(value=&quot;\&quot;,\&quot;@version\&quot;:\&quot;1&quot;)
             constant(value=&quot;\&quot;,\&quot;message\&quot;:\&quot;&quot;)     property(name=&quot;msg&quot;)
             constant(value=&quot;\&quot;,\&quot;source\&quot;:\&quot;&quot;)        property(name=&quot;hostname&quot;)
             constant(value=&quot;\&quot;,\&quot;severity\&quot;:\&quot;&quot;)    property(name=&quot;syslogseverity-text&quot;)
             constant(value=&quot;\&quot;,\&quot;facility\&quot;:\&quot;&quot;)    property(name=&quot;syslogfacility-text&quot;)
             constant(value=&quot;\&quot;,\&quot;programname\&quot;:\&quot;&quot;) property(name=&quot;programname&quot;)
             constant(value=&quot;\&quot;,\&quot;procid\&quot;:\&quot;&quot;)      property(name=&quot;procid&quot;)
           constant(value=&quot;\&quot;}\n&quot;)
         }

$ModLoad omkafka
*.warning action(type=&quot;omkafka&quot; topic=&quot;logs&quot; broker=[&quot;192.168.0.110:9092&quot;] template=&quot;ls_json&quot; errorfile=&quot;/var/log/rsyslog-kafka.err&quot;)
</code></pre>

<pre><code>systemctl restart rsyslog

netstat -nputw | grep 9092 | grep rsyslog
tcp        0      0 192.168.0.110:50912     192.168.0.110:9092      ESTABLISHED 5816/rsyslogd       
tcp        0      0 127.0.0.1:33624         127.0.1.1:9092          ESTABLISHED 5816/rsyslogd

# List content in topic:
/opt/kafka/bin/kafka-console-consumer.sh \
--topic logs --from-beginning \
--bootstrap-server localhost:9092
</code></pre>

<h3 id="create-input-in-graylog">Create input in Graylog</h3>

<p>Go to <code>System &gt; Inputs</code> and launch a new <code>Raw/Plaintext Kafka Input</code>.</p>

<pre><code>Title: kafka
Legacy mode: false
Bootstrap Servers(optional): 127.0.0.1:9092
Consumer group id(optional): graylog2
</code></pre>

<p>Then create an JSON extractor on message field.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Analyzing PFsense logs in Graylog4]]></title>
            <link href="https://devopstales.github.io/home/graylog4-pfsense/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/graylog3-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Analyzing PFsense logs in Graylog3" />
                <link href="https://devopstales.github.io/home/graylog4-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Graylog4" />
                <link href="https://devopstales.github.io/home/graylog-pfsense-squid/?utm_source=atom_feed" rel="related" type="text/html" title="Analyzing PFsense squid logs in Graylog" />
                <link href="https://devopstales.github.io/home/graylog3-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Graylog3" />
                <link href="https://devopstales.github.io/linux/graylog4-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Analyzing PFsense logs in Graylog4" />
            
                <id>https://devopstales.github.io/home/graylog4-pfsense/</id>
            
            
            <published>2021-03-15T00:00:00+00:00</published>
            <updated>2021-03-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>We will parse the log records generated by the PfSense Firewall. We already have our graylog server running and we will start preparing the terrain to capture those logs records.</p>

<p>Many thanks to opc40772 developed the original contantpack for pfsense log agregation what I updated for the new Graylog4 and Elasticsearch 7.</p>

<h3 id="celebro-localinstall">Celebro localinstall</h3>

<pre><code># celebro van to use port 9000 so change graylog3 bindport
nano /etc/graylog/server/server.conf
http_bind_address = 127.0.0.1:9400
nano /etc/nginx/conf.d/graylog.conf

systemctl restart graylog-server.service
systemctl restart nginx

wget https://github.com/lmenezes/cerebro/releases/download/v0.9.3/cerebro-0.9.3-1.noarch.rpm
yum localinstall -y cerebro-0.9.3-1.noarch.rpm

sudo sed -i 's|# JAVA_OPTS=&quot;-Dpidfile.path=/var/run/cerebro/play.pid&quot;|JAVA_OPTS=&quot;-Dpidfile.path=/var/run/cerebro/play.pid -Dhttp.address=0.0.0.0&quot;|' /etc/default/cerebro

chown cerebro:cerebro -R /usr/share/cerebro

systemctl start cerebro
</code></pre>

<h3 id="create-indices">Create indices</h3>

<p>We now create the Pfsense indice on Graylog at <code>System / Indexes</code> <br>
<img src="/img/include/graylog_pfsense1.png" alt="image" /> <br></p>

<h3 id="import-index-template-for-elasticsearch-7-x">Import index template for elasticsearch 7.x</h3>

<pre><code>systemctl stop graylog-server.service

git clone https://github.com/devopstales/pfsense-graylog.git
cd pfsense-graylog/service-names-port-numbers/
cp service-names-port-numbers.csv /etc/graylog/server/
</code></pre>

<p>Go to <code>celebro &gt; more &gt; index templates</code>
Create new with name: <code>pfsense-custom</code> and copy the template from file <code>pfsense_custom_template_es7.json</code></p>

<p>In Cerebro we stand on top of the pfsense index and unfold the options and select delete index.</p>

<h3 id="geoip-database">Geoip database</h3>

<pre><code>wget -t0 -c http://geolite.maxmind.com/download/geoip/database/GeoLite2-City.tar.gz
tar -xvf GeoLite2-City.tar.gz
cp GeoLite2-City_*/GeoLite2-City.mmdb /etc/graylog/server

systemctl start graylog-server.service
</code></pre>

<p>Enable geoip database at <code>System &gt; Configurations &gt; Plugins &gt; Geo-Location Processor &gt; update</code>
Chane the order of the <code>Message Processors Configuration</code></p>

<ul>
<li>AWS Instance Name Lookup</li>
<li>Message Filter Chain</li>
<li>Pipeline Processor</li>
<li>GeoIP Resolver</li>
</ul>

<p>Enable geoip database</p>

<h3 id="import-contantpack">Import contantpack</h3>

<p>import
chaneg date timezone in Pipeline rule
Go tu Stream menu and edit stream <br>
<img src="/img/include/graylog_pfsense2.png" alt="image" /> <br></p>

<p><code>System &gt; Pipelines</code>
Manage rules and then Edit rule (Change the timezone)</p>

<pre><code>rule &quot;timestamp_pfsense_for_grafana&quot;
 when
 has_field(&quot;timestamp&quot;)
then
// the following date format assumes there's no time zone in the string
 let source_timestamp = parse_date(substring(to_string(now(&quot;Europe/Budapest&quot;)),0,23), &quot;yyyy-MM-dd'T'HH:mm:ss.SSS&quot;);
 let dest_timestamp = format_date(source_timestamp,&quot;yyyy-MM-dd HH:mm:ss&quot;);
 set_field(&quot;real_timestamp&quot;, dest_timestamp);
end
</code></pre>

<h3 id="confifure-pfsense">Confifure pfsense</h3>

<p><code>Status &gt; System Logs &gt; Settings</code> <br>
<img src="/img/include/graylog_pfsense3.png" alt="image" /> <br></p>

<h3 id="confifure-opnsense">Confifure Opnsense</h3>

<p>Access the Opnsense GUI
<code>System</code> menu, access the <code>Settings</code> sub-menu and select the  <code>Logging / Targets</code> option.
<img src="/img/include/graylog_pfsense13.png" alt="image" /> <br></p>

<p>Add a new logging target and perform the following configuration: <br></p>

<p><img src="/img/include/graylog_pfsense14.png" alt="image" /> <br></p>

<h3 id="install-grafana-dashboard">Install grafana Dashboard</h3>

<pre><code># install nececery plugins
grafana-cli plugins install grafana-piechart-panel
grafana-cli plugins install grafana-worldmap-panel
grafana-cli plugins install savantly-heatmap-panel
systemctl restart grafana-server
</code></pre>

<p>Create new datasource: <br><br></p>

<p><img src="/img/include/graylog_pfsense15.png" alt="image" /> <br></p>

<p>Import dashboadr from store: <br>
id: 5420</p>

<h2 id="image-img-include-graylog-pfsense12-png-br"><img src="/img/include/graylog_pfsense12.png" alt="image" /> <br></h2>

<h5 id="contantpack">Contantpack:</h5>

<p><a href="https://github.com/devopstales/pfsense-graylog">https://github.com/devopstales/pfsense-graylog</a></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Graylog4]]></title>
            <link href="https://devopstales.github.io/home/graylog4-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/graylog3-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Graylog3" />
                <link href="https://devopstales.github.io/home/graylog-pfsense-squid/?utm_source=atom_feed" rel="related" type="text/html" title="Analyzing PFsense squid logs in Graylog" />
                <link href="https://devopstales.github.io/home/graylog3-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Analyzing PFsense logs in Graylog3" />
                <link href="https://devopstales.github.io/linux/graylog4-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Graylog4" />
                <link href="https://devopstales.github.io/linux/graylog3-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Graylog3" />
            
                <id>https://devopstales.github.io/home/graylog4-install/</id>
            
            
            <published>2021-03-14T00:00:00+00:00</published>
            <updated>2021-03-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Graylog is defined in terms of log management platform for collecting, indexing, and analyzing both structured and unstructured data from almost any source.</p>

<h3 id="install-requirement">Install requirement</h3>

<pre><code>yum install epel-release -y
yum install java-1.8.0-openjdk-headless.x86_64 pwgen nano wget curl git -y
java -version
</code></pre>

<h3 id="set-timezone">Set Timezone</h3>

<pre><code>timedatectl set-timezone CET

yum install -y ntp
ntpd
</code></pre>

<h3 id="elasticsearch">Elasticsearch</h3>

<pre><code>rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch

echo '[elasticsearch-7.x]
name=Elasticsearch repository for 7.x packages
baseurl=https://artifacts.elastic.co/packages/7.x/yum
gpgcheck=1
gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
enabled=1
autorefresh=1
type=rpm-md
' | tee /etc/yum.repos.d/elasticsearch.repo

sudo yum -y install elasticsearch

sudo tee -a /etc/elasticsearch/elasticsearch.yml &gt; /dev/null &lt;&lt;EOT
cluster.name: graylog
action.auto_create_index: .watches,.triggered_watches,.watcher-history-*
EOT

systemctl restart elasticsearch
systemctl enable elasticsearch

curl -XGET 'http://localhost:9200/_cluster/health?pretty=true'
</code></pre>

<h3 id="mongodb">Mongodb</h3>

<pre><code>echo '[mongodb-org-4.2]
name=MongoDB Repository
baseurl=https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/4.2/x86_64/
gpgcheck=1
enabled=1
gpgkey=https://www.mongodb.org/static/pgp/server-4.2.asc' | tee /etc/yum.repos.d/mongodb-org.repo

yum -y install mongodb-org

systemctl restart mongod
systemctl enable  mongod
</code></pre>

<h3 id="graylog4">Graylog4</h3>

<pre><code>rpm -Uvh https://packages.graylog2.org/repo/packages/graylog-4.0-repository_latest.rpm
yum -y install graylog-server graylog-integrations-plugins

SECRET=$(pwgen -s 96 1)
sudo -E sed -i -e 's/password_secret =.*/password_secret = '$SECRET'/' /etc/graylog/server/server.conf
PASSWORD=$(echo -n Password1 | sha256sum | awk '{print $1}')
sudo -E sed -i -e 's/root_password_sha2 =.*/root_password_sha2 = '$PASSWORD'/' /etc/graylog/server/server.conf

# Set to your timezone
sudo -E sed -i -e 's/#root_timezone = UTC/root_timezone = CET/' /etc/graylog/server/server.conf

# Set to your email
sudo -E sed -i -e 's/#root_email = &quot;&quot;/root_email = &quot;admin@devopstales.intra&quot;/' /etc/graylog/server/server.conf
sudo -E sed -i -e 's/elasticsearch_shards = 4/elasticsearch_shards = 1/' /etc/graylog/server/server.conf
sudo -E sed -i -e 's/#http_bind_address = 127.0.0.1:9000/http_bind_address = 127.0.0.1:9400/' /etc/graylog/server/server.conf

# got ta https://dev.maxmind.com/geoip/geoip2/geolite2/ and download
# or use an old one
wget -t0 -c https://github.com/DocSpring/geolite2-city-mirror/raw/master/GeoLite2-City.tar.gz
tar -xvf GeoLite2-City.tar.gz
cp GeoLite2-City_*/GeoLite2-City.mmdb /etc/graylog/server

systemctl daemon-reload
systemctl restart graylog-server
systemctl enable graylog-server

tailf /var/log/graylog-server/server.log

If everything goes well, you should see below message in the logfile:
2019-06-20T13:37:04.059Z INFO  [ServerBootstrap] Graylog server up and running.
</code></pre>

<h3 id="install-grafana">Install Grafana</h3>

<pre><code>echo '[grafana]
name=grafana
baseurl=https://packages.grafana.com/oss/rpm
repo_gpgcheck=1
enabled=1
gpgcheck=1
gpgkey=https://packages.grafana.com/gpg.key
sslverify=1
sslcacert=/etc/pki/tls/certs/ca-bundle.crt
' &gt; /etc/yum.repos.d/grafana.repo


sudo yum install -y grafana
grafana-cli plugins install grafana-piechart-panel

sudo -E sed -i -e 's/;http_addr =/http_addr = 127.0.0.1/' /etc/grafana/grafana.ini

systemctl start grafana-server
systemctl status grafana-server
systemctl enable grafana-server
</code></pre>

<h3 id="kibana">Kibana</h3>

<pre><code>yum install kibana -y

sudo -E sed -i -e 's/#server.host: &quot;localhost&quot;/server.host: &quot;127.0.0.1&quot;/' /etc/kibana/kibana.yml


systemctl start kibana
systemctl enable kibana
</code></pre>

<h3 id="nginx-proxy">Nginx Proxy</h3>

<pre><code>yum install nginx -y

echo 'server {
    listen 80;
    server_name graylog.devopstales.intra;

    location / {
      proxy_set_header Host $http_host;
      proxy_set_header X-Forwarded-Host $host;
      proxy_set_header X-Forwarded-Server $host;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      proxy_set_header X-Graylog-Server-URL http://$server_name/;
      proxy_pass       http://127.0.0.1:9400;
    }
}' &gt; /etc/nginx/conf.d/graylog.conf

echo 'server {
    listen 80;
    server_name grafana.devopstales.intra;

    location / {
      proxy_set_header Host $http_host;
      proxy_set_header X-Forwarded-Host $host;
      proxy_set_header X-Forwarded-Server $host;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      proxy_pass       http://127.0.0.1:3000;
    }
}' &gt; /etc/nginx/conf.d/grafana.conf

echo 'server {
    listen 80;
    server_name kibana.devopstales.intra;

    location / {
      proxy_set_header Host $http_host;
      proxy_set_header X-Forwarded-Host $host;
      proxy_set_header X-Forwarded-Server $host;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      proxy_pass       http://127.0.0.1:5601;
    }
}' &gt; /etc/nginx/conf.d/kibana.conf

nginx -t
systemctl restart nginx
systemctl enable nginx
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install GNS3]]></title>
            <link href="https://devopstales.github.io/home/gns3-linux-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/gns3-linux-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install GNS3" />
                <link href="https://devopstales.github.io/home/k8s-connaisseur/?utm_source=atom_feed" rel="related" type="text/html" title="Image Signature Verification Admission Controller" />
                <link href="https://devopstales.github.io/home/k8s-imagepullsecret-patcher/?utm_source=atom_feed" rel="related" type="text/html" title="How to use imagePullSecrets cluster-wide??" />
                <link href="https://devopstales.github.io/home/kubernetes-policy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Policy" />
                <link href="https://devopstales.github.io/home/k8s-networkpolicy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Network Policy" />
            
                <id>https://devopstales.github.io/home/gns3-linux-install/</id>
            
            
            <published>2021-03-10T00:00:00+00:00</published>
            <updated>2021-03-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[

<h3 id="what-is-gns3">What is GNS3?</h3>

<p>GNS3 is used by hundreds of thousands of network engineers worldwide to emulate, configure, test and troubleshoot virtual and real networks. GNS3 allows you to run a small topology consisting of only a few devices on your laptop, to those that have many devices hosted on multiple servers or even hosted in the cloud.</p>

<h3 id="architecture">Architecture</h3>

<p>GNS3 consists of two software components the GNS3-all-in-one software GUI client and the server. When you create topologies in GNS3 GUI client the created device need to run on the server. The ser ver can be a local GNS3 server in a GNS3 VM or on a Remote host.</p>

<h3 id="install-latest-gns3-network-simulator-on-ubuntu-20-04-18-04-16-04">Install Latest GNS3 Network Simulator on Ubuntu 20.04|18.04|16.04</h3>

<pre><code>sudo add-apt-repository ppa:gns3/ppa
sudo apt update                                
sudo apt install gns3-gui gns3-server
</code></pre>

<p>If you want IOU support</p>

<pre><code>sudo dpkg --add-architecture i386
sudo apt update
sudo apt install gns3-iou
</code></pre>

<p>To install Docker CE</p>

<pre><code>sudo apt remove docker docker-engine docker.io

sudo apt-get install apt-transport-https ca-certificates curl \
software-properties-common

curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -

sudo add-apt-repository \
&quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) stable&quot;

sudo apt update
sudo apt install docker-ce
</code></pre>

<p>To be able to capture GNS3 packets for analysis, you need Wireshark.</p>

<pre><code>sudo apt install wireshark realvnc-vnc-viewer x11vnc dynamips ubridge
</code></pre>

<p>Finally, add your user to the following groups:</p>

<pre><code>for i in ubridge libvirt kvm wireshark docker; do
  sudo usermod -aG $i $USER
done
</code></pre>

<h3 id="launch-gns3">Launch GNS3</h3>

<p>When you first start the GNS3 the Setup Wizard starts. Here you can select where you want to run the server somponent. In this case I will select &ldquo;Run the appliances on my computer&rdquo;. On the second page, confirm teh configuration of the local server.</p>

<p>You can add many appliances/devices to GNS3. We are not going to add any right now. So just click on Cancel.</p>

<p><img src="/img/include/gns3_2.jpg" alt="image" /> <br></p>

<h3 id="install-appliances">Install appliances</h3>

<p>There is many appliances in the [GNS3 marketplace|<a href="https://www.gns3.com/marketplace/appliances">https://www.gns3.com/marketplace/appliances</a>]. Just select one, click download and you gat a gns3a template file. To import the appliance select FILE &gt; Import Appliance and open the gns3a file.</p>

<p>Select the image version for the appliance and click the Download button. Tis downloads a qcow2 QVEMU virtual disk image. Place this file to the image folder of the GNS3:</p>

<pre><code>mv cumulus-linux-4.2.0-vx-amd64-qemu.qcow2 /home/devopstales/GNS3/images/QEMU/
</code></pre>

<p><img src="/img/include/gns3_3.jpg" alt="image" /> <br></p>

<p>After you place the images to the right folder click Refresh in GNS3 and select the qcow2 file then hit next. So you imported a new appliance.</p>

<h3 id="import-docker-appliance">Import Docker appliance</h3>

<p>Select Edit &gt; Preferences in the GNS3. Then go tu Docker &gt; Docker Containers and hit New.</p>

<p><img src="/img/include/gns3_4.jpg" alt="image" /> <br>
<img src="/img/include/gns3_5.jpg" alt="image" /> <br>
<img src="/img/include/gns3_6.jpg" alt="image" /> <br>
<img src="/img/include/gns3_7.jpg" alt="image" /> <br>
<img src="/img/include/gns3_8.jpg" alt="image" /> <br>
<img src="/img/include/gns3_9.jpg" alt="image" /> <br></p>
]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Image Signature Verification Admission Controller]]></title>
            <link href="https://devopstales.github.io/home/k8s-connaisseur/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-connaisseur/?utm_source=atom_feed" rel="related" type="text/html" title="Image Signature Verification Admission Controller" />
                <link href="https://devopstales.github.io/home/kubernetes-policy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Policy" />
                <link href="https://devopstales.github.io/home/k8s-networkpolicy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Network Policy" />
                <link href="https://devopstales.github.io/home/k0s/?utm_source=atom_feed" rel="related" type="text/html" title="K0S The tiny Kubernetes" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller" />
            
                <id>https://devopstales.github.io/home/k8s-connaisseur/</id>
            
            
            <published>2021-02-22T00:00:00+00:00</published>
            <updated>2021-02-22T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can deploy Connaisseur to Image Signature Verification into a Kubernetes cluster.</p>

<h3 id="parst-of-the-k8s-security-series">Parst of the K8S Security series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
<li>Part2: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
<li>Part3: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
<li>Part4: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a>
<!-- RBAC -->
<!-- # Policy --></li>
<li>Part5: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
<li>Part6: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
<li>Part7: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a>
<!-- # Admission Controllers --></li>
<li>Part8: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a>
<!-- ## Image Scan --></li>
<li>Part9a: <a href="../../kubernetes/image-security-admission-controller">Image security Admission Controller</a></li>
<li>Part9b: <a href="../../kubernetes/image-security-admission-controller-v2">Image security Admission Controller V2</a></li>
<li>Part9c: <a href="../../kubernetes/image-security-admission-controller-v3">Image security Admission Controller V3</a>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring --></li>
<li>Part10: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
<li>Part11: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a>
<!-- DAST Exporter?? -->
<!-- CIS Exporter
https://github.com/ibrokethecloud/kube-bench-metrics --></li>
<li>Part12 <a href="../../kubernetes/k8s-connaisseur">Image Signature Verification with Connaisseur</a></li>
<li>Part13 <a href="../../kubernetes/k8s-backup">Backup your Kubernetes Cluster</a>
<!-- # Secrets --></li>
<li>Part14a <a href="../../kubernetes/k8s-vault">Kubernetes and Vault integration</a></li>
<li>Part14b <a href="../../kubernetes/k8s-vault-v2">Kubernetes External Vault integration</a></li>
<li>Part15a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
<li>Part15b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
<li>Part15c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a>
<!-- # Image -->
<!-- keel --></li>
<li>Part16: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a>
<!-- flux image update --></li>
<li>Part17: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
</ul>

<!-- admission-controller ami cseréli az imageben a repot !!!
 -->

<!-- sysdig falco -->

<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
-->

<!-- resource kvota -->

<!-- Backup
https://github.com/pieterlange/kube-backup 
-->

<!--
https://medium.com/@beatrizmrg/network-security-for-microservices-with-ebpf-bis-478b40e7befa

# container runetime
https://www.ianlewis.org/en/container-runtimes-part-1-introduction-container-r
http://pouchcontainer.io/pouch/docs/features/pouch_with_runV.html
-->

<h3 id="what-is-connaisseur">What is Connaisseur?</h3>

<p>Connaisseur is an admission controller for Kubernetes that integrates Image Signature Verification into a cluster, as a means to ensure that only valid images are being deployed.</p>

<h3 id="notary">Notary</h3>

<p>Notary is an open source signing solution for containers based on The Update Framework Notary uses TUFs’ roles and key hierarchy for signing of the images. There are five keys to sign the metadata files which lists all filenames in the collection, their sizes and respective hashes.</p>

<pre><code>apt install notary
</code></pre>

<pre><code>docker pull alpine
docker tag alpine:latest devopstales/testimage:unsigned
docker push devopstales/testimage:unsigned
</code></pre>

<pre><code>notary -s https://notary.docker.io -d ~/.docker/trust init -p docker.io/devopstales/testimage     
Root key found, using: 31579f2a034add499da6e799bc9260d08a15ab1804298218f05f78d97a669f77
Enter passphrase for root key with ID 31579f2: 
Enter passphrase for new targets key with ID 42e49c6: 
Repeat passphrase for new targets key with ID 42e49c6: 
Enter passphrase for new snapshot key with ID 399243c: 
Repeat passphrase for new snapshot key with ID 399243c: 
Enter username: devopstales
Enter password: 
Auto-publishing changes to docker.io/devopstales/testimage
Enter username: devopstales
Enter password: 
Successfully published changes for repository docker.io/devopstales/testimage


export DOCKER_CONTENT_TRUST=1
export DOCKER_CONTENT_TRUST_SERVER=https://notary.docker.io
docker tag alpine:latest devopstales/testimage:signed
docker push devopstales/testimage:signed
</code></pre>

<pre><code>$ find ~/.docker/trust/ | head
/home/devopstales/.docker/trust/
/home/devopstales/.docker/trust/private
/home/devopstales/.docker/trust/private/1f4a9a0922605b3bc19c97e180d962d530721288f4fd0845ad0aa37ba4a6f95d.key
/home/devopstales/.docker/trust/private/fe30e72f5976b2ae7d0d365f28dacfae9c71f11ad854065603ccc806900e84fa.key
/home/devopstales/.docker/trust/private/3da0d27e2d3b964d238d1d184c7578b5f2737b918ec5b8265474e22b07b2ea22.key
/home/devopstales/.docker/trust/private/root-priv.key
/home/devopstales/.docker/trust/private/root-pub.pem
/home/devopstales/.docker/trust/tuf
/home/devopstales/.docker/trust/tuf/docker.io
/home/devopstales/.docker/trust/tuf/docker.io/devopstales
</code></pre>

<pre><code>notary -s https://notary.docker.io -d ~/.docker/trust list docker.io/devopstales/testimage
NAME     DIGEST                                                              SIZE (BYTES)    ROLE
----     ------                                                              ------------    ----
signed    4661fb57f7890b9145907a1fe2555091d333ff3d28db86c3bb906f6a2be93c87    528             targets/devopstales
</code></pre>

<h3 id="install-connaisseur">Install Connaisseur</h3>

<pre><code># The installer use yq so we need to install it

wget https://github.com/mikefarah/yq/releases/download/v4.2.0/yq_linux_amd64 -O /usr/bin/yq &amp;&amp;\
    chmod +x /usr/bin/yq
</code></pre>

<pre><code># generate the public root cert

cd ~/.docker/trust/private
sed '/^role:\sroot$/d' $(grep -iRl &quot;role: root&quot; .) &gt; root-priv.key
openssl ec -in root-priv.key -pubout -out root-pub.pem
</code></pre>

<pre><code>git clone https://github.com/sse-secure-systems/connaisseur.git
cd
nano helm/values.yaml
...
# the public part of the root key, for verifying notary's signatures
  rootPubKey: |
    -----BEGIN PUBLIC KEY-----
    MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE9m6WfwViwT8lYjLF6jAs1bvd1hPp
    cRUmONP49JszW1X/6Q22DygylIJGyC8IXeb3zBWVMoYDxauiqrFomHUOEA==
    -----END PUBLIC KEY-----

make install
</code></pre>

<h3 id="test-the-image-signature-verification">Test the Image Signature Verification</h3>

<pre><code>kubens default

kubectl run unsigned --image=docker.io/devopstales/testimage:unsigned
Error from server: admission webhook &quot;connaisseur-svc.connaisseur.svc&quot; denied the request: failed to verify signature of trust data.

kubectl run signed --image=docker.io/devopstales/testimage:signed
pod/signed created

kubectl get po
</code></pre>

<h3 id="final-words">Final words</h3>

<p>Connaisseur is a grate tool but has a few shortcomings:</p>

<ul>
<li>There is no option to whitelist images in a specific namespace.</li>
<li>Connaisseur supports only one Notary server</li>
<li>Connaisseur supports only one public key</li>
</ul>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to use imagePullSecrets cluster-wide??]]></title>
            <link href="https://devopstales.github.io/home/k8s-imagepullsecret-patcher/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-imagepullsecret-patcher/?utm_source=atom_feed" rel="related" type="text/html" title="How to use imagePullSecrets cluster-wide??" />
                <link href="https://devopstales.github.io/home/kubernetes-policy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Policy" />
                <link href="https://devopstales.github.io/home/k8s-networkpolicy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Network Policy" />
                <link href="https://devopstales.github.io/home/k0s/?utm_source=atom_feed" rel="related" type="text/html" title="K0S The tiny Kubernetes" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller" />
            
                <id>https://devopstales.github.io/home/k8s-imagepullsecret-patcher/</id>
            
            
            <published>2021-02-17T00:00:00+00:00</published>
            <updated>2021-02-17T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can use imagePullSecrets cluster-wide in Kubernetes.</p>

<p>Kubernetes uses imagePullSecrets to authenticate to private container registris on a per Pod or per Namespace basis. To do that yo need to create a secret with the credentials:</p>

<pre><code class="language-bash">kubectl create secret docker-registry image-pull-secret \
  -n &lt;your-namespace&gt; \
  --docker-server=&lt;your-registry-server&gt; \
  --docker-username=&lt;your-name&gt; \
  --docker-password=&lt;your-password&gt; \
  --docker-email=&lt;your-email&gt;
</code></pre>

<p>Now we can use this image in a pod for download the docker image:</p>

<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: private-registry-test
spec:
  containers:
    - name: my-app
      image: my-private-registry.intra/busybox:v1
  imagePullSecrets:
    - name: image-pull-secret
</code></pre>

<p>The other way is to add it to the default ServiceAccount in the namespace:</p>

<pre><code class="language-bash">kubectl patch serviceaccount default \
  -p &quot;{\&quot;imagePullSecrets\&quot;: [{\&quot;name\&quot;: \&quot;image-pull-secret\&quot;}]}&quot; \
  -n &lt;your-namespace&gt;
</code></pre>

<p>I found a tool called imagepullsecret-patcher that do this on all of your namespace:</p>

<pre><code class="language-bash">wget https://raw.githubusercontent.com/titansoft-pte-ltd/imagepullsecret-patcher/185aec934bd01fa9b6ade2c44624e5f2023e2784/deploy-example/kubernetes-manifest/1_rbac.yaml
wget https://raw.githubusercontent.com/titansoft-pte-ltd/imagepullsecret-patcher/master/deploy-example/kubernetes-manifest/2_deployment.yaml

kubectl create ns imagepullsecret-patcher

# Edit the downloaded file and chaneg the contant of the image-pull-secret-src and the namespace if nececary

nano 1_rbac.yaml
nano 2_deployment.yaml
kubectl apply -f 1_rbac.yaml
kubectl apply -f 2_deployment.yaml
</code></pre>

<h3 id="test">test</h3>

<pre><code class="language-bash">kubectl create ns imagepullsecret-test
kubectl get secret image-pull-secret -n imagepullsecret-test
image-pull-secret   kubernetes.io/dockerconfigjson   1      9m35s
</code></pre>

<p>The secret is automaticle created.</p>

<h3 id="kyverno-policy">Kyverno policy</h3>

<p>You can do the same thing with kyverno policy:</p>

<pre><code class="language-yaml">apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: sync-secret
spec:
  background: false
  rules:
  - name: sync-image-pull-secret
    match:
      resources:
        kinds:
        - Namespace
    generate:
      kind: Secret
      name: image-pull-secret
      namespace: &quot;{{request.object.metadata.name}}&quot;
      synchronize: true
      clone:
        namespace: default
        name: image-pull-secret
---
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: mutate-imagepullsecret
spec:
  rules:
    - name: mutate-imagepullsecret
      match:
        resources:
          kinds:
          - Pod
      mutate:
        patchStrategicMerge:
          spec:
            imagePullSecrets:
            - name: image-pull-secret  ## imagePullSecret that you created with docker hub pro account
            (containers):
            - (image): &quot;*&quot; ## match all container images
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes Policy]]></title>
            <link href="https://devopstales.github.io/home/kubernetes-policy/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/kubernetes-policy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Policy" />
                <link href="https://devopstales.github.io/home/k8s-networkpolicy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Network Policy" />
                <link href="https://devopstales.github.io/home/image-security-admission-controller/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller" />
                <link href="https://devopstales.github.io/home/admission-controllers/?utm_source=atom_feed" rel="related" type="text/html" title="Using Admission Controllers" />
                <link href="https://devopstales.github.io/home/rke2-pod-security-policy/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Pod Security Policy" />
            
                <id>https://devopstales.github.io/home/kubernetes-policy/</id>
            
            
            <published>2021-01-15T00:00:00+00:00</published>
            <updated>2021-01-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can enforce best practices on Kubernetes Clusters.</p>

<h3 id="parst-of-the-k8s-security-series">Parst of the K8S Security series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
<li>Part2: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
<li>Part3: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
<li>Part4: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a>
<!-- RBAC -->
<!-- # Policy --></li>
<li>Part5: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
<li>Part6: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
<li>Part7: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a>
<!-- # Admission Controllers --></li>
<li>Part8: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a>
<!-- ## Image Scan --></li>
<li>Part9a: <a href="../../kubernetes/image-security-admission-controller">Image security Admission Controller</a></li>
<li>Part9b: <a href="../../kubernetes/image-security-admission-controller-v2">Image security Admission Controller V2</a></li>
<li>Part9c: <a href="../../kubernetes/image-security-admission-controller-v3">Image security Admission Controller V3</a>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring --></li>
<li>Part10: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
<li>Part11: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a>
<!-- DAST Exporter?? -->
<!-- CIS Exporter
https://github.com/ibrokethecloud/kube-bench-metrics --></li>
<li>Part12 <a href="../../kubernetes/k8s-connaisseur">Image Signature Verification with Connaisseur</a></li>
<li>Part13 <a href="../../kubernetes/k8s-backup">Backup your Kubernetes Cluster</a>
<!-- # Secrets --></li>
<li>Part14a <a href="../../kubernetes/k8s-vault">Kubernetes and Vault integration</a></li>
<li>Part14b <a href="../../kubernetes/k8s-vault-v2">Kubernetes External Vault integration</a></li>
<li>Part15a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
<li>Part15b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
<li>Part15c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a>
<!-- # Image -->
<!-- keel --></li>
<li>Part16: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a>
<!-- flux image update --></li>
<li>Part17: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
</ul>

<!-- admission-controller ami cseréli az imageben a repot !!!
 -->

<!-- sysdig falco -->

<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
-->

<!-- resource kvota -->

<!-- Backup
https://github.com/pieterlange/kube-backup 
-->

<!--
https://medium.com/@beatrizmrg/network-security-for-microservices-with-ebpf-bis-478b40e7befa

# container runetime
https://www.ianlewis.org/en/container-runtimes-part-1-introduction-container-r
http://pouchcontainer.io/pouch/docs/features/pouch_with_runV.html
-->

<p>For a production ready  Kubernetes cluster it is very important to enforcing cluster-wide policies to restrict what a container is allowed to do. We do this wit PS in a previous pos. But how should we enforce our best practices to the cluster users?</p>

<h3 id="opa">OPA</h3>

<p>Open Policy Agent (OPA), is a policy engine for Cloud Native environments hosted by CNCF. It is a general purpose policy engine. OPA policies are written in a Domain Specific Language (DSL) called Rego.</p>

<h3 id="opa-gatekeeper">OPA Gatekeeper</h3>

<p>Gatekeeper is specifically built for Kubernetes Admission Control use case of OPA. It uses OPA internally, but specifically for the Kubernetes admission control. Compared to using OPA with its sidecar kube-mgmt (aka Gatekeeper v1.0), Gatekeeper is integrated with the OPA Constraint Framework to enforce CRD-based policies and allow declaratively configured policies to be reliably shareable.</p>

<p>Install OPA Gatekeeper:</p>

<pre><code class="language-bash">kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/master/deploy/gatekeeper.yaml
</code></pre>

<p>Now we need to create a policy template and a constraint that adds the variables to the template. If I want to create  a policy to enforce all image comes from Only gcr.io, I need this Template:</p>

<pre><code class="language-yaml">apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: k8srequiredregistry
spec:
  crd:
    spec:
      names:
        kind: K8sRequiredRegistry
      validation:
        # Schema for the `parameters` field
        openAPIV3Schema:
          properties:
            image:
              type: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8srequiredregistry
        violation[{&quot;msg&quot;: msg, &quot;details&quot;: {&quot;Registry should be&quot;: required}}] {
          input.review.object.kind == &quot;Pod&quot;
          some i
          image := input.review.object.spec.containers[i].image
          required := input.parameters.registry
          not startswith(image,required)
          msg := sprintf(&quot;Forbidden registry: %v&quot;, [image])
        }
</code></pre>

<p>This template defines which parameters you need to define as well as the actual Rego code that will do the validation. Fo the constraint we specify that we need this constraint applied to Pods only and we pass the registry name that we need the images to be pulled from.</p>

<pre><code class="language-yaml">apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sRequiredRegistry
metadata:
  name: images-must-come-from-gcr
spec:
  match:
    kinds:
      - apiGroups: [&quot;&quot;]
        kinds: [&quot;Pod&quot;]
  parameters:
    registry: &quot;gcr.io/&quot;
</code></pre>

<p>Test the policy with an image from github:</p>

<pre><code class="language-bash">kubectl run --generator=run-pod/v1 busybox1 --image=busybox -- sleep 3600

message: 'admission webhook &quot;validation.gatekeeper.sh&quot; denied the request: [denied
      by images-must-come-from-gcr] Forbidden registry: busybox'
</code></pre>

<p>Another great feature of OPA Gatekeeper is audit functionality, it enables periodic evaluations of replicated resources against the policies enforced in the cluster to detect pre-existing misconfigurations.</p>

<p>Audit results are stored as violations listed in the status field of the failed constraint.</p>

<pre><code class="language-bash">kubectl describe policystrictonly.constraints.gatekeeper.sh policy-strict-constraint
</code></pre>

<h3 id="opa-and-gatekeeper">OPA and Gatekeeper</h3>

<p>You can deploy OPA kube-mgmt as both validating webhook as well as mutating webhook configurations. Whereas, Gatekeeper currently does not support mutating admission control scenarios.</p>

<h3 id="kyverno">Kyverno</h3>

<p>Kyverno is a policy engine designed for Kubernetes. With Kyverno, policies are managed as Kubernetes resources and no new language is required to write policies. This allows using familiar tools such as kubectl, git, and kustomize to manage policies. Kyverno policies can validate, mutate, and generate Kubernetes resources. The Kyverno CLI can be used to test policies and validate resources as part of a CI/CD pipeline. (Source: <a href="https://kyverno.io/">Kyverno</a> )</p>

<p>Install kyverno:</p>

<pre><code class="language-bash">helm repo add kyverno https://kyverno.github.io/kyverno/
helm repo update
helm install kyverno --namespace kyverno kyverno/kyverno --create-namespace
</code></pre>

<h4 id="validate-configurations">Validate configurations</h4>

<p>Here is an example of a Kyverno policy that validates that images are only pulled from gcr.io:</p>

<pre><code class="language-yaml">apiVersion : kyverno.io/v1alpha1
kind: Policy
metadata:
  name: check-registries
spec:
  rules:
  - name: check-registries
    resource:
      kinds:
      - Deployment
      - StatefulSet
    validate:
      message: &quot;Registry is not allowed&quot;
      pattern:
        spec:
          template:
            spec:
              containers:
              - name: &quot;*&quot;
                # Check allowed registries
                image: &quot;*/gcr.io/*&quot;
</code></pre>

<h4 id="mutate-configurations">Mutate Configurations</h4>

<p>Kyverno supports two different ways to mutate configurations. The first approach is to use a JSON Patch:</p>

<pre><code class="language-yaml">apiVersion : kyverno.io/v1alpha1
kind : Policy
metadata :
  name : policy-deployment
spec :
  rules:
    - name: patch-add-label
      resource:
        kinds : 
        - Deployment
      mutate:
        patches:
        - path: /metadata/labels/isMutated
          op: add
          value: &quot;true&quot;
</code></pre>

<p>The other way to mutate resources based on conditionals that describes the desired state:</p>

<pre><code class="language-yaml">apiVersion: kyverno.io/v1alpha1
kind: Policy
metadata:
  name: set-image-pull-policy
spec:
  rules:
  - name: set-image-pull-policy
    resource:
      kinds:
      - Deployment
    mutate:
      overlay:
        spec:
          template:
            spec:
              containers:
                # if the image tag is latest, set the imagePullPolicy to Always
                - (image): &quot;*:latest&quot;
                  imagePullPolicy: &quot;Always&quot;
</code></pre>

<h4 id="generate-configurations">Generate Configurations</h4>

<p>Policy rule can generates new configurations:</p>

<pre><code class="language-yaml">apiVersion: kyverno.io/v1alpha1
kind: Policy
metadata:
  name: &quot;default&quot;
spec:
  rules:
  - name: &quot;deny-all-traffic&quot;
    resource: 
      kinds:
       - Namespace
      name: &quot;*&quot;
    generate: 
      kind: NetworkPolicy
      name: deny-all-traffic
      data:
        spec:
        podSelector:
          matchLabels: {}
          matchExpressions: []
        policyTypes: []
        metadata:
          annotations: {}
          labels:
            policyname: &quot;default&quot;
</code></pre>

<h3 id="policy-reports">Policy Reports</h3>

<p>Kyverno policy reports provide information about policy execution and violations. Kyverno creates policy reports for each Namespace and a single cluster-level report for cluster resources.</p>

<pre><code class="language-bash">$ kubectl get polr -A
NAMESPACE     NAME                  PASS   FAIL   WARN   ERROR   SKIP   AGE
default       polr-ns-default       338    2      0      0       0      28h
flux-system   polr-ns-flux-system   135    5      0      0       0      28h

$ kubectl get clusterpolicyreport -A
NAME                  PASS   FAIL   WARN   ERROR   SKIP   AGE
clusterpolicyreport   0      0      0      0       0      142m
</code></pre>

<pre><code class="language-bash">$ kubectl describe polr polr-ns-default | grep &quot;Status: \+fail&quot; -B10
  Message:        validation error: Running as root is not allowed. The fields spec.securityContext.runAsNonRoot, spec.containers[*].securityContext.runAsNonRoot, and spec.initContainers[*].securityContext.runAsNonRoot must be `true`. Rule check-containers[0] failed at path /spec/securityContext/runAsNonRoot/. Rule check-containers[1] failed at path /spec/containers/0/securityContext/.
  Policy:         require-run-as-non-root
  Resources:
    API Version:  v1
    Kind:         Pod
    Name:         add-capabilities-init-containers
    Namespace:    default
    UID:          1caec743-faed-4d5a-90f7-5f4630febd58
  Rule:           check-containers
  Scored:         true
  Status:         fail
--
  Message:        validation error: Running as root is not allowed. The fields spec.securityContext.runAsNonRoot, spec.containers[*].securityContext.runAsNonRoot, and spec.initContainers[*].securityContext.runAsNonRoot must be `true`. Rule check-containers[0] failed at path /spec/securityContext/runAsNonRoot/. Rule check-containers[1] failed at path /spec/containers/0/securityContext/.
  Policy:         require-run-as-non-root
  Resources:
    API Version:  v1
    Kind:         Pod
    Name:         sysctls
    Namespace:    default
    UID:          b98bdfb7-10e0-467f-a51c-ac8b75dc2e95
  Rule:           check-containers
  Scored:         true
  Status:         fail
</code></pre>

<hr />

<ul>
<li><a href="https://github.com/fjogeleit/policy-reporter">https://github.com/fjogeleit/policy-reporter</a></li>
</ul>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes Network Policy]]></title>
            <link href="https://devopstales.github.io/home/k8s-networkpolicy/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/image-security-admission-controller/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller" />
                <link href="https://devopstales.github.io/home/rke2-pod-security-policy/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Pod Security Policy" />
                <link href="https://devopstales.github.io/home/admission-controllers/?utm_source=atom_feed" rel="related" type="text/html" title="Using Admission Controllers" />
                <link href="https://devopstales.github.io/home/rke2-airgap-install/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 The Secure Kubernetes Engine" />
                <link href="https://devopstales.github.io/kubernetes/k8s-networkpolicy/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Network Policy" />
            
                <id>https://devopstales.github.io/home/k8s-networkpolicy/</id>
            
            
            <published>2021-01-10T00:00:00+00:00</published>
            <updated>2021-01-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can use NetworkPolicys in K8S.</p>

<h3 id="parst-of-the-k8s-security-series">Parst of the K8S Security series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
<li>Part2: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
<li>Part3: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
<li>Part4: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a>
<!-- RBAC -->
<!-- # Policy --></li>
<li>Part5: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
<li>Part6: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
<li>Part7: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a>
<!-- # Admission Controllers --></li>
<li>Part8: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a>
<!-- ## Image Scan --></li>
<li>Part9a: <a href="../../kubernetes/image-security-admission-controller">Image security Admission Controller</a></li>
<li>Part9b: <a href="../../kubernetes/image-security-admission-controller-v2">Image security Admission Controller V2</a></li>
<li>Part9c: <a href="../../kubernetes/image-security-admission-controller-v3">Image security Admission Controller V3</a>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring --></li>
<li>Part10: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
<li>Part11: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a>
<!-- DAST Exporter?? -->
<!-- CIS Exporter
https://github.com/ibrokethecloud/kube-bench-metrics --></li>
<li>Part12 <a href="../../kubernetes/k8s-connaisseur">Image Signature Verification with Connaisseur</a></li>
<li>Part13 <a href="../../kubernetes/k8s-backup">Backup your Kubernetes Cluster</a>
<!-- # Secrets --></li>
<li>Part14a <a href="../../kubernetes/k8s-vault">Kubernetes and Vault integration</a></li>
<li>Part14b <a href="../../kubernetes/k8s-vault-v2">Kubernetes External Vault integration</a></li>
<li>Part15a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
<li>Part15b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
<li>Part15c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a>
<!-- # Image -->
<!-- keel --></li>
<li>Part16: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a>
<!-- flux image update --></li>
<li>Part17: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
</ul>

<!-- admission-controller ami cseréli az imageben a repot !!!
 -->

<!-- sysdig falco -->

<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
-->

<!-- resource kvota -->

<!-- Backup
https://github.com/pieterlange/kube-backup 
-->

<!--
https://medium.com/@beatrizmrg/network-security-for-microservices-with-ebpf-bis-478b40e7befa

# container runetime
https://www.ianlewis.org/en/container-runtimes-part-1-introduction-container-r
http://pouchcontainer.io/pouch/docs/features/pouch_with_runV.html
-->

<h3 id="network-policies">Network policies</h3>

<p>Network policies are Kubernetes resources that allows to control the traffic between pods and/or network endpoints. Most CNI plugins support the implementation of network policies, but if they don&rsquo;t the created <code>NetworkPolicy</code> will be ignored.</p>

<p>The most popular CNI plugins with network policy support are:</p>

<ul>
<li>Weave</li>
<li>Calico</li>
<li>Canal</li>
<li>Cilium</li>
</ul>

<h3 id="example">Example</h3>

<p>A good practice is to define and apply a default NetworkPolicy to deny all incoming traffic to all pods in all application namespaces, then whitelist pods and subnets based on application needs.</p>

<pre><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all
  namespace: monitoring
spec:
  policyTypes:
  - Ingress
  - Egress
  podSelector: {}
</code></pre>

<p>Since this resource defines both policyTypes ingress and egress, but doesn’t define any whitelist rules, it blocks all the pods in the monitoring namespace from communicating with each other. Note that this policy dose not allows connections to port 53 on any IP by default, to facilitate DNS lookups. So we need to whitelist dns. All <code>NetworkPolicy</code> is like a firewall rule. To select an aplication you need to use selectors of labels.</p>

<pre><code># create label
kubectl label namespace kube-system networking/namespace=kube-system
</code></pre>

<pre><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all-egress
spec:
  policyTypes:
  - Egress
  podSelector: {}
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          networking/namespace: kube-system
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: TCP
      port: 53
    - protocol: UDP
      port: 53
</code></pre>

<p>To allow connections from the Ingress Controller:</p>

<pre><code># create label
kubectl label namespace nginx-ingress networking/namespace=ingress
</code></pre>

<pre><code>apiVersion: networking.k8s.io/v1n
kind: NetworkPolicy
metadata:
  name: allow-from-ingress
spec:
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          networking/namespace: ingress
  podSelector: {}
</code></pre>

<p>We need to create allow rules to define what aplication can communicate with anathor aplication. To match network traffic by combining namespace and pod selectors, you can use a <code>NetworkPolicy</code> object similar to the following:</p>

<pre><code>apiVersion: extensions/v1beta1
kind: NetworkPolicy
metadata:
  name: alertmanager-mesh
  namespace: monitoring
spec:
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: prometheus
    - namespaceSelector:
        matchLabels:
          name: monitoring
    ports:
    - port: 9093
      protocol: tcp
  podSelector:
    matchLabels:
      app: alertmanager
</code></pre>

<p>Allow inbound tcp to port 9093 from only prometheus to alertmanager</p>

<pre><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: prometheus
  namespace: monitoring
spec:
  policyTypes:
  - Ingress
  podSelector:
    matchLabels:
      app: prometheus
  ingress:
  - from:
    - podSelector: {}
    - namespaceSelector:
        matchLabels:
          name: monitoring
    ports:
    - protocol: TCP
      port: 9090
</code></pre>

<p>Allow inbound tcp to port 9090 from any source to prometheus.</p>

<p>You can create Rules to allow outboudn trafic from a service to a apps with specific tags. The following policy allows pod outbound traffic to other pods in the same namespace that match the pod selector. In the following example, outbound traffic is allowed only if they go to a pod with label color=red, on port 80.</p>

<pre><code>kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-egress-same-namespace
  namespace: default
spec:
  policyTypes:
    - Egress
  podSelector:
    matchLabels:
      color: blue
  egress:
  - to:
    - podSelector:
        matchLabels:
          color: red
    ports:
    - port: 80
</code></pre>

<p>If You Don’t Know Which Pods Need To Talk To Each Other you can allow all application in a namespace to connect with each other.</p>

<pre><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-same-namespace
spec:
  policyTypes:
  - Ingress
  podSelector: {}
  ingress:
  - from:
    - podSelector: {}
</code></pre>

<p>For services that require egress to resources outside of the cluster, for example, a database whitelist the subnet that the network resource is on.</p>

<pre><code>kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: customer-api-allow-web
  namespace: prod
spec:
  policyTypes:
  - Egress
  podSelector:
    matchLabels:
      app: orders
  egress:
  - ports:
    - port: 3306
    to:
    - ipBlock:
        cidr: 172.16.32.0/27
</code></pre>

<h3 id="calico-networkpolicy">Calico NetworkPolicy</h3>

<p>Calico network policy provides a richer set of policy capabilities than Kubernetes including:</p>

<ul>
<li>policy ordering/priority</li>
<li>deny rules</li>
<li>Protocols: TCP, UDP, ICMP, SCTP, UDPlite, ICMPv6, protocol numbers (1-255)</li>
</ul>

<p>Calico network policies apply to endpoints. In Kubernetes, each pod is a Calico endpoint. However, Calico can support other kinds of endpoints. There are two types of Calico endpoints: workload endpoints (such as a Kubernetes pod or OpenStack VM) and host endpoints (an interface or group of interfaces on a host).</p>

<pre><code>kind: NetworkPolicy
apiVersion: projectcalico.org/v3
metadata:
  name: allow-egress-same-namespace
  namespace: default
spec:
  selector: color == 'red'
  ingress:
  - action: Allow
    protocol: TCP
    source:
      selector: color == 'blue'
    destination:
      ports:
        - 80
</code></pre>

<p>In the following example, incoming TCP traffic to any pods with label color: red is denied if it comes from a pod with color: blue.</p>

<pre><code>apiVersion: projectcalico.org/v3
kind: GlobalNetworkPolicy
metadata:
  name: deny-blue
spec:
  selector: color == 'red'
  ingress:
  - action: Deny
    protocol: TCP
    source:
      selector: color == 'blue'
</code></pre>

<p>Apply network policies in specific order:</p>

<pre><code>apiVersion: projectcalico.org/v3
kind: GlobalNetworkPolicy
metadata:
  name: drop-other-ingress
spec:
  order: 20
  ...deny policy rules here...

apiVersion: projectcalico.org/v3
kind: GlobalNetworkPolicy
metadata:
  name: allow-cluster-internal-ingress
spec:
  order: 10
  ...allow policy rules here...
</code></pre>

<p>In the following example, incoming TCP traffic to an application is denied, and each connection attempt is logged to syslog:</p>

<pre><code>apiVersion: projectcalico.org/v3
kind: NetworkPolicy
Metadata:
  name: allow-tcp-6379
  namespace: production
Spec:
  selector: role == 'database'
  types:
  - Ingress
  - Egress
  ingress:
  - action: Log
    protocol: TCP
    source:
      selector: role == 'frontend'
  - action: Deny
    protocol: TCP
    source:
      selector: role == 'frontend'
</code></pre>

<p>It is important to enforce separation of containers. As you can see you can create a <code>NetworkPolicy</code> for a specific namespace. So don&rsquo;t forget to create the default best practice Policies. In the next post will show you how you can automate the creation of the Default Policies for new namespaces.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[K0S The tiny Kubernetes]]></title>
            <link href="https://devopstales.github.io/home/k0s/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/image-security-admission-controller/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller" />
                <link href="https://devopstales.github.io/home/rke2-pod-security-policy/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Pod Security Policy" />
                <link href="https://devopstales.github.io/home/admission-controllers/?utm_source=atom_feed" rel="related" type="text/html" title="Using Admission Controllers" />
                <link href="https://devopstales.github.io/home/rke2-airgap-install/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 The Secure Kubernetes Engine" />
                <link href="https://devopstales.github.io/home/k8s-security/?utm_source=atom_feed" rel="related" type="text/html" title="Best Practices to keeping Kubernetes Clusters Secure" />
            
                <id>https://devopstales.github.io/home/k0s/</id>
            
            
            <published>2020-12-15T00:00:00+00:00</published>
            <updated>2020-12-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>We all know and love K3s, right?  It’s now time to discover a new distribution: <a href="https://k0sproject.io">k0s</a>.</p>

<h3 id="what-s-k0s">What’s k0s ?</h3>

<p>k0s is a brand new Kubernetes distribution. The current release is 0.8.0. It was published in December 2020.</p>

<p>The latest k0s release:</p>

<ul>
<li>Ships a certified and (CIS-benchmarked) Kubernetes 1.19</li>
<li>Uses containerd as the default container runtime</li>
<li>Uses an in-cluster etcd by default and supports SQLite, MySQL (or any compatible), PostgreSQL</li>
<li>Uses the Calico network plugin by default with network policies</li>
<li>Enables the Pod Security Policies admission controller</li>
<li>Uses DNS with CoreDNS</li>
<li>Exposes cluster metrics via Metrics Server</li>
<li>Allows the usage of Horizontal Pod Autoscaling (HPA)</li>
</ul>

<p>A lot of great features will come in future releases, among them:</p>

<ul>
<li>Micro VM runtimes (really looking forward to testing this one)</li>
<li>Zero-downtime cluster upgrades</li>
<li>Cluster backup and restore</li>
<li>Air-Gap install</li>
<li>FIPS 140-2 (coming soon)</li>
</ul>

<p>We’ll now see how to install k0s.</p>

<h3 id="install-singel-master">Install singel master</h3>

<p>k0s as a single binary acts as the process supervisor for all other control plane components. This means there&rsquo;s no container engine or kubelet running on controllers (by default). Which means there is no way for a cluster user to schedule workloads onto controller nodes.</p>

<pre><code>curl -sSLf get.k0s.sh | sudo sh

k0s version

mkdir /etc/k0s
k0s default-config &gt; /etc/k0s/k0s.yaml
</code></pre>

<h3 id="config">Config</h3>

<p>In the config file <code>/etc/k0s/k0s.yaml</code> you can add helm charts thet will be installed at startup, like prometheus for monitoring or nginx ingress controller.</p>

<pre><code>apiVersion: k0s.k0sproject.io/v1beta1
kind: Cluster
metadata:
  name: k0s
spec:
  api:
    address: 192.168.68.106
    sans:
    - my-k0s-control.my-domain.com
  network:
    podCIDR: 10.244.0.0/16
    serviceCIDR: 10.96.0.0/12
extensions:
  helm:
    repositories:
    - name: prometheus-community
      url: https://prometheus-community.github.io/helm-charts
    charts:
    - name: prometheus-stack
      chartname: prometheus-community/prometheus
      version: &quot;11.16.8&quot;
      namespace: default
</code></pre>

<pre><code>cat &lt;&lt;EOF &gt; /etc/systemd/system/k0s.service
[Unit]
Description=&quot;k0s server&quot;
After=network-online.target
Wants=network-online.target
 
[Service]
Type=simple
ExecStart=/usr/bin/k0s server -c /etc/k0s/k0s.yaml --enable-worker
Restart=always
EOF
</code></pre>

<pre><code>systemctl start k0s.service
systemctl enable k0s.service
journalctl -u k0s.service -f
</code></pre>

<pre><code>sudo curl --output /usr/local/sbin/kubectl -L &quot;https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl&quot;
chmod +x /usr/local/sbin/kubectl
mkdir ~/.kube
cp /var/lib/k0s/pki/admin.conf ~/.kube/config

kubectl get node
kubectl get po -A
</code></pre>

<pre><code>kubectl run nginx --image=nginx -n default
kubectl get po -A              
</code></pre>

<h3 id="check-tge-default-psp">Check tge default PSP</h3>

<pre><code>NAME                PRIV    CAPS   SELINUX    RUNASUSER   FSGROUP    SUPGROUP   READONLYROOTFS   VOLUMES
00-k0s-privileged   true    *      RunAsAny   RunAsAny    RunAsAny   RunAsAny   false            *
99-k0s-restricted   false          RunAsAny   RunAsAny    RunAsAny   RunAsAny   false            configMap,downwardAPI,emptyDir,persistentVolumeClaim,projected,secret
</code></pre>

<pre><code>kubectl get psp 99-k0s-restricted -o yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  annotations:
    k0s.k0sproject.io/last-applied-configuration: |
      {&quot;apiVersion&quot;:&quot;policy/v1beta1&quot;,&quot;kind&quot;:&quot;PodSecurityPolicy&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:null,&quot;name&quot;:&quot;99-k0s-restricted&quot;},&quot;spec&quot;:{&quot;allowPrivilegeEscalation&quot;:false,&quot;allowedCapabilities&quot;:[],&quot;fsGroup&quot;:{&quot;rule&quot;:&quot;RunAsAny&quot;},&quot;hostIPC&quot;:false,&quot;hostNetwork&quot;:false,&quot;hostPID&quot;:false,&quot;privileged&quot;:false,&quot;readOnlyRootFilesystem&quot;:false,&quot;runAsUser&quot;:{&quot;rule&quot;:&quot;RunAsAny&quot;},&quot;seLinux&quot;:{&quot;rule&quot;:&quot;RunAsAny&quot;},&quot;supplementalGroups&quot;:{&quot;rule&quot;:&quot;RunAsAny&quot;},&quot;volumes&quot;:[&quot;configMap&quot;,&quot;downwardAPI&quot;,&quot;emptyDir&quot;,&quot;persistentVolumeClaim&quot;,&quot;projected&quot;,&quot;secret&quot;]}}
    k0s.k0sproject.io/stack-checksum: b0c62cb2696c6167d7a8289411b06f69
  creationTimestamp: &quot;2020-12-14T17:39:37Z&quot;
  labels:
    k0s.k0sproject.io/stack: defaultpsp
  managedFields:
  - apiVersion: policy/v1beta1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .: {}
          f:k0s.k0sproject.io/last-applied-configuration: {}
          f:k0s.k0sproject.io/stack-checksum: {}
        f:labels:
          .: {}
          f:k0s.k0sproject.io/stack: {}
      f:spec:
        f:allowPrivilegeEscalation: {}
        f:fsGroup:
          f:rule: {}
        f:runAsUser:
          f:rule: {}
        f:seLinux:
          f:rule: {}
        f:supplementalGroups:
          f:rule: {}
        f:volumes: {}
    manager: k0s
    operation: Update
    time: &quot;2020-12-14T17:39:37Z&quot;
  name: 99-k0s-restricted
  resourceVersion: &quot;245&quot;
  selfLink: /apis/policy/v1beta1/podsecuritypolicies/99-k0s-restricted
  uid: b59e0bfe-57c2-4b8b-a17b-baa9047a6fcb
spec:
  allowPrivilegeEscalation: false
  fsGroup:
    rule: RunAsAny
  runAsUser:
    rule: RunAsAny
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  volumes:
  - configMap
  - downwardAPI
  - emptyDir
  - persistentVolumeClaim
  - projected
  - secret
</code></pre>

<p>If you check the config file <code>/etc/k0s/k0s.yaml</code> you can see it use the 00-k0s-privileged PSP as default and 00-k0s-privileged dose not disable run as root by default. It&rsquo;s sad.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[RKE2 Image security Admission Controller]]></title>
            <link href="https://devopstales.github.io/home/image-security-admission-controller/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/image-security-admission-controller/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Image security Admission Controller" />
                <link href="https://devopstales.github.io/home/rke2-pod-security-policy/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Pod Security Policy" />
                <link href="https://devopstales.github.io/home/admission-controllers/?utm_source=atom_feed" rel="related" type="text/html" title="Using Admission Controllers" />
                <link href="https://devopstales.github.io/home/rke2-airgap-install/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 The Secure Kubernetes Engine" />
                <link href="https://devopstales.github.io/kubernetes/rke2-pod-security-policy/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Pod Security Policy" />
            
                <id>https://devopstales.github.io/home/image-security-admission-controller/</id>
            
            
            <published>2020-12-13T00:00:00+00:00</published>
            <updated>2020-12-13T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In a previous post we talked about <a href="https://devopstales.github.io/home/admission-controllers/">Admission Controllers</a>. In this post I will show you how to use an Admission Controller to test image vulnerabilities.</p>

<h3 id="parst-of-the-k8s-security-series">Parst of the K8S Security series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
<li>Part2: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
<li>Part3: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
<li>Part4: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a>
<!-- RBAC -->
<!-- # Policy --></li>
<li>Part5: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
<li>Part6: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
<li>Part7: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a>
<!-- # Admission Controllers --></li>
<li>Part8: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a>
<!-- ## Image Scan --></li>
<li>Part9a: <a href="../../kubernetes/image-security-admission-controller">Image security Admission Controller</a></li>
<li>Part9b: <a href="../../kubernetes/image-security-admission-controller-v2">Image security Admission Controller V2</a></li>
<li>Part9c: <a href="../../kubernetes/image-security-admission-controller-v3">Image security Admission Controller V3</a>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring --></li>
<li>Part10: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
<li>Part11: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a>
<!-- DAST Exporter?? -->
<!-- CIS Exporter
https://github.com/ibrokethecloud/kube-bench-metrics --></li>
<li>Part12 <a href="../../kubernetes/k8s-connaisseur">Image Signature Verification with Connaisseur</a></li>
<li>Part13 <a href="../../kubernetes/k8s-backup">Backup your Kubernetes Cluster</a>
<!-- # Secrets --></li>
<li>Part14a <a href="../../kubernetes/k8s-vault">Kubernetes and Vault integration</a></li>
<li>Part14b <a href="../../kubernetes/k8s-vault-v2">Kubernetes External Vault integration</a></li>
<li>Part15a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
<li>Part15b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
<li>Part15c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a>
<!-- # Image -->
<!-- keel --></li>
<li>Part16: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a>
<!-- flux image update --></li>
<li>Part17: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
</ul>

<!-- admission-controller ami cseréli az imageben a repot !!!
 -->

<!-- sysdig falco -->

<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
-->

<!-- resource kvota -->

<!-- Backup
https://github.com/pieterlange/kube-backup 
-->

<!--
https://medium.com/@beatrizmrg/network-security-for-microservices-with-ebpf-bis-478b40e7befa

# container runetime
https://www.ianlewis.org/en/container-runtimes-part-1-introduction-container-r
http://pouchcontainer.io/pouch/docs/features/pouch_with_runV.html
-->

<p>There is multiple tools to scan vulnerabilities, but less Admission Controller to use them. I found multiple solution for Anchore Engine so the first step is to deploy with its helm chart. In RKE2 I will use Rancher&rsquo;s <a href="https://devopstales.github.io/cloud/k3s-helm-controller/">Helm controller</a> what is preinstalled.</p>

<p>Then we can Deploy an Admission Controller to us this tool to automaticle scann any image deploy in the cluster and reject if is vulnerable. As I sad before there is multiple solution for this. One is Anchore&rsquo;s own Admission Controller but I will use Banzaicloud&rsquo;s solution because this easier to deploy. Sadly anchore-image-validator run as root so we need to use <a href="https://devopstales.github.io/home/rke2-pod-security-policy/">my predifinde PSP</a> to allow this.</p>

<pre><code class="language-yaml">nano /var/lib/rancher/rke2/server/manifests/10_image-security.yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: securty-system
---
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: anchore-enginn
  namespace: kube-system
spec:
  repo: &quot;https://charts.anchore.io&quot;
  chart: anchore-engine
  targetNamespace: securty-system
  valuesContent: |-
     postgresql:
       postgresPassword: Password1
       persistence:
         size: 10Gi
     anchoreGlobal:
       defaultAdminPassword: Password1
       defaultAdminEmail: devopstales@mydomain.intra
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: psp-rolebinding-securty-system
  namespace: securty-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system-unrestricted-psp-role
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:serviceaccounts
---
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: anchore-policy-validator
  namespace: kube-system
spec:
  repo: &quot;https://kubernetes-charts.banzaicloud.com&quot;
  chart: anchore-policy-validator
  targetNamespace: securty-system
  valuesContent: |-
    externalAnchore:
      anchoreHost: &quot;http://anchore-enginn-anchore-engine-api:8228/v1/&quot;
      anchoreUser: admin
      anchorePass: Password1
    rbac:
      psp:
        enabled: true
    createPolicies: true
</code></pre>

<p>During deploying this chart, it&rsquo;s creating predefined policy bundles and activates <code>AllowAll</code> by default if <code>createPolicies</code> flag is set.</p>

<table>
<thead>
<tr>
<th>Bundle Name</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>Allow all and warn bundle</td>
<td>Allow all images and warn if vulnerabilities are found</td>
</tr>

<tr>
<td>Reject critical bundle</td>
<td>Reject deploying images that contain <code>critical</code> vulnerabiliy</td>
</tr>

<tr>
<td>Reject high bundle</td>
<td>Reject deploying images that contain <code>high</code> vulnerabiliy</td>
</tr>

<tr>
<td>Block root bundle</td>
<td>Block deploying images that using <code>root</code> as effective user</td>
</tr>

<tr>
<td>Deny all images</td>
<td>Deny all imagest to deploy</td>
</tr>
</tbody>
</table>

<h3 id="test-the-admission-controller">Test the Admission Controller</h3>

<pre><code class="language-bash">kubectl run --image=busybox -- sleep 3600
Error from server: admission webhook &quot;pods.anchore-policy-validator.admission.banzaicloud.com&quot; denied the request: Image failed policy check: busybox
</code></pre>

<pre><code class="language-yaml">kubectl describe audits busybox1
Name:         busybox1
Namespace:    
Labels:       fakerelease=true
Annotations:  &lt;none&gt;
API Version:  security.banzaicloud.com/v1alpha1
Kind:         Audit
Metadata:
  Creation Timestamp:  2020-11-29T10:20:41Z
  Generation:          1
  Managed Fields:
    API Version:  security.banzaicloud.com/v1alpha1
    Fields Type:  FieldsV1
    fieldsV1:
      f:metadata:
        f:labels:
          .:
          f:fakerelease:
      f:spec:
        .:
        f:action:
        f:image:
        f:releaseName:
        f:resource:
        f:result:
      f:status:
        .:
        f:state:
    Manager:         anchore-image-validator
    Operation:       Update
    Time:            2020-11-29T10:20:41Z
  Resource Version:  39174
  Self Link:         /apis/security.banzaicloud.com/v1alpha1/audits/busybox1
  UID:               1e90c8b0-fffa-45f6-a986-d9fd269f0a83
Spec:
  Action:  reject
  Image:
    Image Digest:  
    Image Name:    
    Image Tag:     
    Last Updated:  
  Release Name:    busybox1
  Resource:        Pod
  Result:
    Image failed policy check: busybox
Status:
  State:  
Events:   &lt;none&gt;
</code></pre>

<p>The default policy is deny All Image theat failed on policy check but we can white list a specific image or set <code>createPolicies</code> to <code>true</code> in Banzaicloud&rsquo;s Helm chart to create default AllowAll Policy.</p>

<pre><code class="language-bash">kubectl apply -f - &lt;&lt; EOF
apiVersion: security.banzaicloud.com/v1alpha1
kind:  WhiteListItem
metadata:
  name: busybox1
spec:
  reason: testing
  creator: devopstales-sa
EOF
</code></pre>

<pre><code class="language-bash">kubectl run busybox1 --image=busybox -- sleep 3600
pod/busybox1 created
</code></pre>

<pre><code class="language-bash">kubectl get whitelistitems -o wide -o=custom-columns=NAME:.metadata.name,CREATOR:.spec.creator,REASON:.spec.reason
NAME       CREATOR          REASON
busybox1   devopstales-sa   testing

kubectl get audits -o wide -o=custom-columns=NAME:.metadata.name,RELEASE:.spec.releaseName,IMAGES:.spec.image,RESULT:.spec.result
NAME       RELEASE    IMAGES                                                  RESULT
busybox1   busybox1   [map[imageDigest: imageName: imageTag: lastUpdated:]]   [Image failed policy check: busybox]
</code></pre>

<p>You can find the config files in my github repo: <a href="https://github.com/devopstales/k8s_sec_lab">https://github.com/devopstales/k8s_sec_lab</a></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[RKE2 Pod Security Policy]]></title>
            <link href="https://devopstales.github.io/home/rke2-pod-security-policy/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/rke2-airgap-install/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 The Secure Kubernetes Engine" />
                <link href="https://devopstales.github.io/kubernetes/rke2-pod-security-policy/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 Pod Security Policy" />
                <link href="https://devopstales.github.io/home/admission-controllers/?utm_source=atom_feed" rel="related" type="text/html" title="Using Admission Controllers" />
                <link href="https://devopstales.github.io/kubernetes/rke2-airgap-install/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 The Secure Kubernetes Engine" />
                <link href="https://devopstales.github.io/home/k8s-security/?utm_source=atom_feed" rel="related" type="text/html" title="Best Practices to keeping Kubernetes Clusters Secure" />
            
                <id>https://devopstales.github.io/home/rke2-pod-security-policy/</id>
            
            
            <published>2020-12-10T00:00:00+00:00</published>
            <updated>2020-12-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can use Pod Security Policys in RKE2.</p>

<h3 id="parst-of-the-k8s-security-series">Parst of the K8S Security series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
<li>Part2: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
<li>Part3: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
<li>Part4: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a>
<!-- RBAC -->
<!-- # Policy --></li>
<li>Part5: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
<li>Part6: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
<li>Part7: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a>
<!-- # Admission Controllers --></li>
<li>Part8: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a>
<!-- ## Image Scan --></li>
<li>Part9a: <a href="../../kubernetes/image-security-admission-controller">Image security Admission Controller</a></li>
<li>Part9b: <a href="../../kubernetes/image-security-admission-controller-v2">Image security Admission Controller V2</a></li>
<li>Part9c: <a href="../../kubernetes/image-security-admission-controller-v3">Image security Admission Controller V3</a>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring --></li>
<li>Part10: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
<li>Part11: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a>
<!-- DAST Exporter?? -->
<!-- CIS Exporter
https://github.com/ibrokethecloud/kube-bench-metrics --></li>
<li>Part12 <a href="../../kubernetes/k8s-connaisseur">Image Signature Verification with Connaisseur</a></li>
<li>Part13 <a href="../../kubernetes/k8s-backup">Backup your Kubernetes Cluster</a>
<!-- # Secrets --></li>
<li>Part14a <a href="../../kubernetes/k8s-vault">Kubernetes and Vault integration</a></li>
<li>Part14b <a href="../../kubernetes/k8s-vault-v2">Kubernetes External Vault integration</a></li>
<li>Part15a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
<li>Part15b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
<li>Part15c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a>
<!-- # Image -->
<!-- keel --></li>
<li>Part16: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a>
<!-- flux image update --></li>
<li>Part17: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
</ul>

<!-- admission-controller ami cseréli az imageben a repot !!!
 -->

<!-- sysdig falco -->

<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
-->

<!-- resource kvota -->

<!-- Backup
https://github.com/pieterlange/kube-backup 
-->

<!--
https://medium.com/@beatrizmrg/network-security-for-microservices-with-ebpf-bis-478b40e7befa

# container runetime
https://www.ianlewis.org/en/container-runtimes-part-1-introduction-container-r
http://pouchcontainer.io/pouch/docs/features/pouch_with_runV.html
-->

<h3 id="what-is-a-pod-security-policy">What is a Pod Security Policy?</h3>

<p>A Pod Security Policy is a cluster-level resource that controls security sensitive aspects of the pod specification. RBAC Controlls the usable Kubernetes objects for a user but nt the conditions of a specific ofject like allow run as root or not in a container.  PSP objects define a set of conditions that a pod must run with in order to be accepted into the system, as well as defaults for their related fields. PodSecurityPolicy is an optional admission controller that is enabled by default through the API, thus policies can be deployed without the PSP admission plugin enabled.</p>

<h3 id="psp-examples-using-rke2">PSP examples using RKE2</h3>

<p>RKE2 can be ran with or without the <code>profile: cis-1.5</code> configuration parameter. This will cause it to apply different PodSecurityPolicies (PSPs) at start-up. If running with the <code>cis-1.5</code> profile, RKE2 will apply a restrictive policy called <code>global-restricted-psp</code> to all namespaces except <code>kube-system</code>. The <code>kube-system</code> namespace needs a less restrictive policy named <code>system-unrestricted-psp</code> in order to launch critical components.</p>

<p>The policies are outlined below.</p>

<pre><code>apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: global-restricted-psp
spec:
  privileged: false                # CIS - 5.2.1
  allowPrivilegeEscalation: false  # CIS - 5.2.5
  requiredDropCapabilities:        # CIS - 5.2.7/8/9
    - ALL
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  hostNetwork: false               # CIS - 5.2.4
  hostIPC: false                   # CIS - 5.2.3
  hostPID: false                   # CIS - 5.2.2
  runAsUser:
    rule: 'MustRunAsNonRoot'       # CIS - 5.2.6
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'MustRunAs'
    ranges:
      - min: 1
        max: 65535
  fsGroup:
    rule: 'MustRunAs'
    ranges:
      - min: 1
        max: 65535
  readOnlyRootFilesystem: false
</code></pre>

<p>This PSP disables <code>privileged</code> and <code>allowPrivilegeEscalation</code> and force tu run conatiners with UserID and GroupID betwean 1-65535 threat means you cannot run containers wit UserID/GroupID 0 what is root.</p>

<p>The &ldquo;system unrestricted policy&rdquo; is applied. See below.</p>

<pre><code>apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: system-unrestricted-psp
spec:
  privileged: true
  allowPrivilegeEscalation: true
  allowedCapabilities:
  - '*'
  volumes:
  - '*'
  hostNetwork: true
  hostPorts:
  - min: 0
    max: 65535
  hostIPC: true
  hostPID: true
  runAsUser:
    rule: 'RunAsAny'
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'
</code></pre>

<h3 id="test-psp">Test PSP</h3>

<p>Is I try to deploy a Deployment with a container running as root it will fail.</p>

<pre><code>kubectl get deploy,rs,pod
# output
NAME                                READY   UP-TO-DATE   AVAILABLE   AGE
deployment.extensions/alpine-test   0/1     0            0           67s

NAME                                          DESIRED   CURRENT   READY   AGE
replicaset.extensions/alpine-test-85c976cdd   1         0         0       67s
</code></pre>

<p>What happened?</p>

<pre><code>kubectl describe replicaset.extensions/alpine-test-85c976cdd | tail -n3
# output
 Type     Reason        Age                    From                   Message
  ----     ------        ----                   ----                   -------
  Warning  FailedCreate  114s (x16 over 4m38s)  replicaset-controller  Error creating: pods &quot;alpine-test-85c976cdd-&quot; is forbidden: unable to validate against any pod security policy: []
</code></pre>

<h3 id="custom-psp">Custom PSP</h3>

<p>I usually create a restricted rule with allowing the root user in the cobtainer because some operator&rsquo;s container still use it.</p>

<pre><code>apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: allow-root-psp
spec:
  privileged: false                # CIS - 5.2.1
  allowPrivilegeEscalation: false  # CIS - 5.2.5
  requiredDropCapabilities:        # CIS - 5.2.7/8/9
    - ALL
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  hostNetwork: false               # CIS - 5.2.4
  hostIPC: false                   # CIS - 5.2.3
  hostPID: false                   # CIS - 5.2.2
  runAsUser:
    rule: 'MustRunAsNonRoot'       # CIS - 5.2.6
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'
  readOnlyRootFilesystem: false
</code></pre>

<p>To use this PSP we need to create a <code>ClusterRole</code>.</p>

<pre><code>apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: allow-root-psp-role
rules:
- apiGroups:
  - policy
  resourceNames:
  - allow-root-psp
  resources:
  - podsecuritypolicies
  verbs:
  - use
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Using Admission Controllers]]></title>
            <link href="https://devopstales.github.io/home/admission-controllers/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/rke2-airgap-install/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 The Secure Kubernetes Engine" />
                <link href="https://devopstales.github.io/kubernetes/admission-controllers/?utm_source=atom_feed" rel="related" type="text/html" title="Using Admission Controllers" />
                <link href="https://devopstales.github.io/home/k8s-security/?utm_source=atom_feed" rel="related" type="text/html" title="Best Practices to keeping Kubernetes Clusters Secure" />
                <link href="https://devopstales.github.io/kubernetes/rke2-airgap-install/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 The Secure Kubernetes Engine" />
                <link href="https://devopstales.github.io/home/k8s-ipvs/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with IPVS mode" />
            
                <id>https://devopstales.github.io/home/admission-controllers/</id>
            
            
            <published>2020-12-07T00:00:00+00:00</published>
            <updated>2020-12-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can use Admission Controllers.</p>

<h3 id="parst-of-the-k8s-security-series">Parst of the K8S Security series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
<li>Part2: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
<li>Part3: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
<li>Part4: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a>
<!-- RBAC -->
<!-- # Policy --></li>
<li>Part5: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
<li>Part6: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
<li>Part7: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a>
<!-- # Admission Controllers --></li>
<li>Part8: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a>
<!-- ## Image Scan --></li>
<li>Part9a: <a href="../../kubernetes/image-security-admission-controller">Image security Admission Controller</a></li>
<li>Part9b: <a href="../../kubernetes/image-security-admission-controller-v2">Image security Admission Controller V2</a></li>
<li>Part9c: <a href="../../kubernetes/image-security-admission-controller-v3">Image security Admission Controller V3</a>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring --></li>
<li>Part10: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
<li>Part11: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a>
<!-- DAST Exporter?? -->
<!-- CIS Exporter
https://github.com/ibrokethecloud/kube-bench-metrics --></li>
<li>Part12 <a href="../../kubernetes/k8s-connaisseur">Image Signature Verification with Connaisseur</a></li>
<li>Part13 <a href="../../kubernetes/k8s-backup">Backup your Kubernetes Cluster</a>
<!-- # Secrets --></li>
<li>Part14a <a href="../../kubernetes/k8s-vault">Kubernetes and Vault integration</a></li>
<li>Part14b <a href="../../kubernetes/k8s-vault-v2">Kubernetes External Vault integration</a></li>
<li>Part15a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
<li>Part15b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
<li>Part15c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a>
<!-- # Image -->
<!-- keel --></li>
<li>Part16: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a>
<!-- flux image update --></li>
<li>Part17: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
</ul>

<!-- admission-controller ami cseréli az imageben a repot !!!
 -->

<!-- sysdig falco -->

<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
-->

<!-- resource kvota -->

<!-- Backup
https://github.com/pieterlange/kube-backup 
-->

<!--
https://medium.com/@beatrizmrg/network-security-for-microservices-with-ebpf-bis-478b40e7befa

# container runetime
https://www.ianlewis.org/en/container-runtimes-part-1-introduction-container-r
http://pouchcontainer.io/pouch/docs/features/pouch_with_runV.html
-->

<h3 id="what-is-an-admission-controller">What is an Admission Controller</h3>

<p>An admission controller is a piece of code that intercepts requests to the Kubernetes API server prior to persistence of the object, but after the request is authenticated and authorized. […] Admission controllers may be “validating”, “mutating”, or both. Mutating controllers may modify the objects they admit; validating controllers may not. […] If any of the controllers in either phase reject the request, the entire request is rejected immediately and an error is returned to the end-user. (Source: <a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/">Kubernetes Website</a> )</p>

<p>In a nutshell, Kubernetes admission controllers are plugins that govern and enforce how the cluster is used. They can be thought of as a gatekeeper that intercepts (authenticated) API requests and may change the request object or deny the request altogether.</p>

<h3 id="how-do-i-turn-on-an-admission-controller">How do I turn on an admission controller?</h3>

<p>A list of previously implemented controllers comes with Kubernetes, or you can write your own. To do so you must enable them in the <code>kube-apiserver</code> The Kubernetes API server flag enable-admission-plugins takes a comma-delimited list of admission control plugins to invoke prior to modifying objects in the cluster. For example, the following command line enables the NamespaceLifecycle and the LimitRanger admission control plugins:</p>

<pre><code>kube-apiserver --enable-admission-plugins=NamespaceLifecycle,LimitRanger ...
</code></pre>

<h3 id="what-is-an-admission-webhook">What is an admission webhook?</h3>

<p>There are two special admission controllers in the list included in the Kubernetes apiserver: MutatingAdmissionWebhook and ValidatingAdmissionWebhook. These are special admission controllers that send admission requests to external HTTP callbacks and receive admission responses. If these two admission controllers are enabled, a Kubernetes administrator can create and configure an admission webhook in the cluster.</p>

<p><img src="/img/include/admission-controller-phases.png" alt="Example image" /></p>

<p>Validating webhooks can reject a request, but they cannot modify the object they are receiving in the admission request, while mutating webhooks can modify objects by creating a patch that will be sent back in the admission response. If a webhook rejects a request, an error is returned to the end-user.</p>

<h3 id="why-do-i-need-admission-controllers">Why do I need admission controllers?</h3>

<p>Security: Admission controllers can increase security by mandating a reasonable security baseline across an entire namespace or cluster. The built-in PodSecurityPolicy admission controller is perhaps the most prominent example; it can be used for disallowing containers from running as root or making sure the container’s root filesystem is always mounted read-only, for example. Further use cases that can be realized by custom, webhook-based admission controllers include:</p>

<ul>
<li>Allow pulling images only from specific registries known to the enterprise, while denying unknown image registries.</li>
<li>Reject deployments that do not meet security standards. For example, containers using the privileged flag can circumvent a lot of security checks. This risk could be mitigated by a webhook-based admission controller that either rejects such deployments (validating) or overrides the privileged flag, setting it to false.</li>
</ul>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes deprecated Docker? Containderd is the new Docker!!]]></title>
            <link href="https://devopstales.github.io/home/kubernetes-deprecated-docker-containderd-docker/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/kubernetes-deprecated-docker-containderd-docker/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes deprecated Docker? Containderd is the new Docker!!" />
                <link href="https://devopstales.github.io/home/container-runtimes/?utm_source=atom_feed" rel="related" type="text/html" title="Container runtimes" />
                <link href="https://devopstales.github.io/home/rke2-airgap-install/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 The Secure Kubernetes Engine" />
                <link href="https://devopstales.github.io/home/k8s-security/?utm_source=atom_feed" rel="related" type="text/html" title="Best Practices to keeping Kubernetes Clusters Secure" />
                <link href="https://devopstales.github.io/home/k8s-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure K8S vSphere Cloud Provider" />
            
                <id>https://devopstales.github.io/home/kubernetes-deprecated-docker-containderd-docker/</id>
            
            
            <published>2020-12-04T00:00:00+00:00</published>
            <updated>2020-12-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Docker is now deprecated in Kubernetes in the next 1.20 version, but thet dose no mean yo can not run containers wit docker.</p>

<p>&ldquo;Given the impact of this change, we are using an extended deprecation timeline. It will not be removed before Kubernetes 1.22, meaning the earliest release without dockershim would be 1.23 in late 2021.&rdquo; (Source: <a href="https://kubernetes.io/blog/2020/12/02/dockershim-faq/">Kubernetes</a> )</p>

<h3 id="but-why-is-docker-deprecated">But why is Docker deprecated?</h3>

<p>In the beginning Kubernetes supported only Docker as a container runtime but to use other runtime&rsquo;s Kubernetes, Docker, Google, CoreOS, and other vendors created the <a href="https://opencontainers.org/">Open Container Initiative (OCI)</a>. The OCI currently contains two specifications: the Runtime Specification (runtime-spec)  and the Image Specification (image-spec).
For the Runtime Specification they created the CRI (Container runtime Interface) as a standerd interface fro kubernetes to communicate with container runtimes. Before 1.20 Kubernetes used the old dockershim for docker engine not the standerd CRI interface.</p>

<p>To explain the next reason, we have to see the Docker architecture a bit. Here&rsquo;s the diagram.</p>

<p><img src="/img/include/docker_engine.png" alt="Example image" /></p>

<p>Kubernetes needs the tings inside of the red area. Docker has many other features like Docker Network and Volume that Kubernetes not uses.</p>

<h3 id="can-i-use-docker">Can I use Docker??</h3>

<p>Mirantis and Docker have agreed to partner to maintain the shim code standalone outside Kubernetes, as a conformant CRI interface for Docker Engine. &hellip; This means that you can continue to build Kubernetes based on Docker Engine as before, just switching from the built in dockershim to the external one. Docker and Mirantis will work together on making sure it continues to work as well as before and that it passes all the conformance tests and works just like the built in version did. Docker will continue to ship this shim in Docker Desktop as this gives a great developer experience, and Mirantis will be using this in Mirantis Kubernetes Engine. (Source: <a href="https://www.docker.com/blog/what-developers-need-to-know-about-docker-docker-engine-and-kubernetes-v1-20/">Docker Blog</a> )</p>

<h3 id="what-can-i-use-instad-of-docker">What can I use instad of Docker</h3>

<p>You can use containerd or CRI-O instad of docker. In [a previous post]() I showed how you can install Kubernetes with CRI-O, so now I will show you how you can use containerd instad of Docker. If you just want to migrate from Docker, this is the best option as containerd is actually used inside of Docker to do all the &ldquo;runtime&rdquo; jobs as you can see in the diagram above.</p>

<h3 id="install-and-configure-containerd-prerequisites">Install and configure Containerd prerequisites:</h3>

<pre><code>cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

# Setup required sysctl params, these persist across reboots.
cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

# Apply sysctl params without reboot
sudo sysctl --system
</code></pre>

<pre><code>### Install required packages
sudo yum install -y yum-utils device-mapper-persistent-data lvm2

## Add docker repository
sudo yum-config-manager \
    --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo

## Install containerd
sudo yum update -y &amp;&amp; sudo yum install -y containerd.io

## Configure containerd
sudo mkdir -p /etc/containerd
sudo containerd config default &gt; /etc/containerd/config.toml
</code></pre>

<p>To use the <code>systemd</code> cgroup driver in <code>/etc/containerd/config.toml</code> with <code>runc</code>, set</p>

<pre><code>nano /etc/containerd/config.toml
systemd_cgroup = true

# Restart containerd
sudo systemctl restart containerd
</code></pre>

<pre><code>free -h
swapoff -a
swapoff -a
sed -i.bak -r 's/(.+ swap .+)/#\1/' /etc/fstab
free -h
</code></pre>

<pre><code>cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
        https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF


yum install epel-release -y
yum install -y kubeadm kubelet kubectl
</code></pre>

<pre><code>echo &quot;runtime-endpoint: unix:///run/containerd/containerd.sock&quot; &gt; /etc/crictl.yaml
crictl ps
</code></pre>

<pre><code>systemctl enable kubelet.service
kubeadm config images pull
crictl images
ubeadm init --pod-network-cidr=10.244.0.0/16
</code></pre>

<pre><code>mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
</code></pre>

<pre><code>kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
kubectl get node
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Container runtimes]]></title>
            <link href="https://devopstales.github.io/home/container-runtimes/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/container-runtimes/?utm_source=atom_feed" rel="related" type="text/html" title="Container runtimes" />
                <link href="https://devopstales.github.io/home/k3s-crio/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with CRI-O and kadalu" />
                <link href="https://devopstales.github.io/home/k8s-crio/?utm_source=atom_feed" rel="related" type="text/html" title="Install K8S with CRI-O and kadalu" />
                <link href="https://devopstales.github.io/home/rke2-airgap-install/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 The Secure Kubernetes Engine" />
                <link href="https://devopstales.github.io/home/k8s-security/?utm_source=atom_feed" rel="related" type="text/html" title="Best Practices to keeping Kubernetes Clusters Secure" />
            
                <id>https://devopstales.github.io/home/container-runtimes/</id>
            
            
            <published>2020-11-29T00:00:00+00:00</published>
            <updated>2020-11-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>One of the terms you hear a lot when dealing with containers is &ldquo;container runtime&rdquo;. This post will explain what container runtimes are.</p>

<h3 id="oci">OCI</h3>

<p>Docker, Google, CoreOS, and other vendors created the <a href="https://opencontainers.org/">Open Container Initiative (OCI)</a>. The OCI currently contains two specifications: the Runtime Specification (runtime-spec) as a standerd of CRI (Container runtime Interface) and the Image Specification (image-spec). This led to other standards like CNI (Container Network Interface), a Cloud Native Computing Foundation project, or Container Storage Interface (CSI).</p>

<h3 id="what-is-a-container-runtime">What is a container runtime?</h3>

<p>Container runtime is the engine that runs and manages the components required to run containers. Communicating with the kernel to start containerized processes, setting up cgroups, configure mount points and do many things to make your container work.</p>

<h3 id="docker">Docker</h3>

<p>Docker was released in 2013 and solved many of the problems that developers had running containers like LXC or OpenVZ. Before version 1.11, the implementation of Docker was a monolithic daemon. The monolith did everything as one package such as downloading container images, launching container processes, exposing a remote API, and acting as a log collection daemon, all in a centralized process running as root. (Source: <a href="https://coreos.com/rkt/docs/latest/rkt-vs-other-projects.html#rkt-vs-docker">Coreos</a> )</p>

<p>At this time docker was the only runtime that Kubernetes supported, but wit the release of Coreos&rsquo;s rkt Kubernetes needed a standard interface to ease the integration of other container runtimes. This led to the splitting of Docker into different parts.</p>

<p><img src="/img/include/conatinerd.png" alt="Example image" /></p>

<p><img src="/img/include/docker_engine.png" alt="Example image" /></p>

<h3 id="cri-o">CRI-O</h3>

<p>CRI-O is an implementation of the Kubernetes CRI (Container Runtime Interface) to enable using OCI (Open Container Initiative) compatible runtimes. It is a lightweight alternative to using Docker as the runtime for kubernetes. It allows Kubernetes to use any OCI-compliant runtime as the container runtime for running pods. Today it supports runc and Kata Containers as the container runtimes but any OCI-conformant runtime can be plugged in principle.</p>

<p>CRI-O supports OCI container images and can pull from any container registry. It is a lightweight alternative to using Docker, Moby or rkt as the runtime for Kubernetes. (Source: <a href="https://cri-o.io/">CRI-O Website</a> )</p>

<pre><code>crictl ps - list containers
crictl pods - list pods
</code></pre>

<h3 id="pouchcontainer">PouchContainer</h3>

<p>PouchContainer is an open-source project created by Alibaba Group. It provides applications with a lightweight runtime environment with strong isolation and minimal overhead. PouchContainer isolates applications from varying runtime environment, and minimizes operational workload. t includes lots of security features, like hypervisor-based container technology, lxcfs, directory disk quota, patched Linux kernel.  PouchContainer utilizes Dragonfly, a P2P-base distribution system, to achieve lightning-fast container image distribution at enterprise&rsquo;s large scale.</p>

<h3 id="kata-containers">Kata Containers</h3>

<p>Kata Containers is an open source community working to build a secure container runtime with lightweight virtual machines that feel and perform like containers, but provide stronger workload isolation using hardware virtualization technology as a second layer of defense. (Source: <a href="https://katacontainers.io/">Kata Containers Website</a> )</p>

<h3 id="podman">Podman</h3>

<p>Podman is a daemonless, open source, Linux-native tool designed to develop, manage, and run Open Container Initiative (OCI) containers and pods. It has a similar directory structure to Buildah, Skopeo, and CRI-O. Podman doesn’t admin privileges for its commands to work.</p>

<h4 id="containerd">Containerd</h4>

<p><code>containerd</code> is a daemon that controls runC. From containerd website, &ldquo;containerd manages the complete container lifecycle of its host system, from image transfer and storage to container execution and supervision to low-level storage to network attachments and beyond&rdquo;.</p>

<h3 id="what-is-a-low-level-container-runtime">What is a Low-Level Container Runtime?</h3>

<p>Containers are implemented using Linux namespaces and cgroups. Namespaces let you virtualize system resources, like the file system or networking, for each container. Cgroups provide a way to limit the amount of resources like CPU and memory that each container can use. At the lowest level, container runtimes are responsible for setting up these namespaces and cgroups for containers, and then running commands inside those namespaces and cgroups. Low-level runtimes support using these operating system features.</p>

<h4 id="runc">runc</h4>

<p><code>runc</code> is a CLI tool for spawning and running containers according to the OCI specification. Docker donated this library to OCI as a reference implementation of the OCI runtime specification.</p>

<h4 id="crun">crun</h4>

<p><code>crun</code> is a lightweight fully featured OCI runtime and C library for running containers.</p>

<p>&ldquo;While most of the tools used in the Linux containers ecosystem are written in Go, I believe C is a better fit for a lower level tool like a container runtime. runc, the most used implementation of the OCI runtime specs written in Go, re-execs itself and use a module written in C for setting up the environment before the container process starts.&rdquo; (Source: <a href="https://github.com/containers/crun#why-another-implementation">crun GitHub page</a> )</p>

<p><code>crun</code> is faster than runc and has a much lower memory footprint.</p>

<table>
<thead>
<tr>
<th></th>
<th>crun</th>
<th>runc</th>
<th>%</th>
</tr>
</thead>

<tbody>
<tr>
<td>100 /bin/true</td>
<td>0:01.69</td>
<td>0:3.34</td>
<td>-49.4%</td>
</tr>
</tbody>
</table>

<h4 id="runv">runv</h4>

<p><code>runv</code> is a hypervisor-based runtime for OCI. <code>runV</code> supports the following hypervisors:</p>

<ul>
<li>KVM (QEMU 2.1 or later)</li>
<li>KVM (Kvmtool)</li>
<li>Xen (4.5 or later)</li>
<li>QEMU without KVM (NOT RECOMMENDED. QEMU 2.1 or later)</li>
</ul>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[RKE2 The Secure Kubernetes Engine]]></title>
            <link href="https://devopstales.github.io/home/rke2-airgap-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/rke2-airgap-install/?utm_source=atom_feed" rel="related" type="text/html" title="RKE2 The Secure Kubernetes Engine" />
                <link href="https://devopstales.github.io/home/k8s-security/?utm_source=atom_feed" rel="related" type="text/html" title="Best Practices to keeping Kubernetes Clusters Secure" />
                <link href="https://devopstales.github.io/home/k8s-ipvs/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with IPVS mode" />
                <link href="https://devopstales.github.io/home/k8s-calico-ebpf/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with calico&#39;s eBPF mode" />
                <link href="https://devopstales.github.io/home/k8s-install-openebs/?utm_source=atom_feed" rel="related" type="text/html" title="Install OpenEBS for Kubernetes" />
            
                <id>https://devopstales.github.io/home/rke2-airgap-install/</id>
            
            
            <published>2020-11-25T00:00:00+00:00</published>
            <updated>2020-11-25T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can install a secure Kubernetes Engine variant called RKE2 in a Air-Gap environment.</p>

<h3 id="parst-of-the-k8s-security-series">Parst of the K8S Security series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
<li>Part2: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
<li>Part3: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
<li>Part4: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a>
<!-- RBAC -->
<!-- # Policy --></li>
<li>Part5: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
<li>Part6: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
<li>Part7: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a>
<!-- # Admission Controllers --></li>
<li>Part8: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a>
<!-- ## Image Scan --></li>
<li>Part9a: <a href="../../kubernetes/image-security-admission-controller">Image security Admission Controller</a></li>
<li>Part9b: <a href="../../kubernetes/image-security-admission-controller-v2">Image security Admission Controller V2</a></li>
<li>Part9c: <a href="../../kubernetes/image-security-admission-controller-v3">Image security Admission Controller V3</a>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring --></li>
<li>Part10: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
<li>Part11: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a>
<!-- DAST Exporter?? -->
<!-- CIS Exporter
https://github.com/ibrokethecloud/kube-bench-metrics --></li>
<li>Part12 <a href="../../kubernetes/k8s-connaisseur">Image Signature Verification with Connaisseur</a></li>
<li>Part13 <a href="../../kubernetes/k8s-backup">Backup your Kubernetes Cluster</a>
<!-- # Secrets --></li>
<li>Part14a <a href="../../kubernetes/k8s-vault">Kubernetes and Vault integration</a></li>
<li>Part14b <a href="../../kubernetes/k8s-vault-v2">Kubernetes External Vault integration</a></li>
<li>Part15a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
<li>Part15b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
<li>Part15c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a>
<!-- # Image -->
<!-- keel --></li>
<li>Part16: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a>
<!-- flux image update --></li>
<li>Part17: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
</ul>

<!-- admission-controller ami cseréli az imageben a repot !!!
 -->

<!-- sysdig falco -->

<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
-->

<!-- resource kvota -->

<!-- Backup
https://github.com/pieterlange/kube-backup 
-->

<!--
https://medium.com/@beatrizmrg/network-security-for-microservices-with-ebpf-bis-478b40e7befa

# container runetime
https://www.ianlewis.org/en/container-runtimes-part-1-introduction-container-r
http://pouchcontainer.io/pouch/docs/features/pouch_with_runV.html
-->

<h3 id="what-is-rke2">What is RKE2</h3>

<p>RKE2, also known as RKE Government, is Rancher&rsquo;s next-generation Kubernetes distribution. It is a fully conformant Kubernetes distribution that focuses on security and compliance within the U.S. Federal Government sector.</p>

<h3 id="install-rke2-from-rpms">Install RKE2 from rpms</h3>

<p>Not like K3S RKE2 offers an rpm repository. Of course in an Air-Gap environment you need an internal repository to sync the packages.</p>

<pre><code class="language-bash">cat &lt;&lt; EOF &gt; /etc/yum.repos.d/rancher-rke2-1-18-latest.repo
[rancher-rke2-common-latest]
name=Rancher RKE2 Common Latest
baseurl=https://rpm.rancher.io/rke2/latest/common/centos/7/noarch
enabled=1
gpgcheck=1
gpgkey=https://rpm.rancher.io/public.key

[rancher-rke2-1-18-latest]
name=Rancher RKE2 1.18 Latest
baseurl=https://rpm.rancher.io/rke2/latest/1.18/centos/7/x86_64
enabled=1
gpgcheck=1
gpgkey=https://rpm.rancher.io/public.key
EOF

yum -y install rke2-server nano
</code></pre>

<p>In an Air-Gap environment you cannot connect to the public internet so the containerd engine cannot connest to the registry. In this scenario yo have two options. Create an internal registry and upload all images or import images from tarball. In this demo I will use the second option.</p>

<pre><code class="language-bash">mkdir -p /var/lib/rancher/rke2/agent/images/
scp rke2-images.linux-amd64.tar masern01:/var/lib/rancher/rke2/agent/images/
cd /var/lib/rancher/rke2/agent/images/
</code></pre>

<p>For RKE2 you didn&rsquo;t nee docker engine. The rpms will install all the necessary binaris to run a container.</p>

<pre><code class="language-bash">echo 'PATH=$PATH:/usr/local/bin' &gt;&gt; /etc/profile
echo 'PATH=$PATH:/var/lib/rancher/rke2/bin' &gt;&gt; /etc/profile
source /etc/profile
</code></pre>

<pre><code class="language-bash">setenforce 1
getenforce
sed -i 's/=\(disabled\|permissive\)/=enforcing/g' /etc/sysconfig/selinux
systemctl start firewalld
systemctl enable firewalld
</code></pre>

<p>For the demo I will use <code>firewalld</code> to block all outgoing request from the server. This is how I emulate the Air-Gap environment.</p>

<pre><code class="language-bash">firewall-cmd --permanent --direct --add-rule ipv4 filter OUTPUT 0 -p tcp -m tcp --dport=443 -j DROP
firewall-cmd --permanent --direct --add-rule ipv4 filter OUTPUT 0 -p tcp -m tcp --dport=80 -j DROP
firewall-cmd --permanent --direct --add-rule ipv4 filter OUTPUT 1 -j ACCEPT
firewall-cmd --reload
</code></pre>

<p>Enable hardened mode.</p>

<pre><code class="language-bash">mkdir -p /etc/rancher/rke2
cat &lt;&lt; EOF &gt;  /etc/rancher/rke2/config.yaml
write-kubeconfig-mode: &quot;0644&quot;
profile: &quot;cis-1.5&quot;
selinux: true
EOF


sudo cp -f /usr/share/rke2/rke2-cis-sysctl.conf /etc/sysctl.d/60-rke2-cis.conf
sysctl -p /etc/sysctl.d/60-rke2-cis.conf

useradd -r -c &quot;etcd user&quot; -s /sbin/nologin -M etcd
</code></pre>

<p>On my VM there is multiple network interface So I will configure what to use the kubernetes engine.</p>

<pre><code class="language-bash">mkdir -p /var/lib/rancher/rke2/server/manifests/
cat &lt;&lt; EOF &gt; /var/lib/rancher/rke2/server/manifests/rke2-canal-config.yml
apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-canal
  namespace: kube-system
spec:
  valuesContent: |-
    flannel:
      iface: &quot;enp0s8&quot;
EOF

cat &lt;&lt; EOF &gt; /var/lib/rancher/rke2/server/manifests/rke2-ingress-nginx-config.yaml
apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-ingress-nginx
  namespace: kube-system
spec:
  valuesContent: |-
    controller:
      metrics:
        enable: true
        service:
          annotations:
            prometheus.io/scrape: &quot;true&quot;
            prometheus.io/port: &quot;10254&quot;
EOF

cat &lt;&lt; EOF &gt; /var/lib/rancher/rke2/server/manifests/rke2-kube-proxy-config.yaml
apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-kube-proxy
  namespace: kube-system
spec:
  valuesContent: |-
    metricsBindAddress: 0.0.0.0:10249
EOF
</code></pre>

<pre><code class="language-bash">systemctl enable rke2-server.service
systemctl start rke2-server.service
journalctl -u rke2-server -f


mkdir ~/.kube
ln -s /etc/rancher/rke2/rke2.yaml ~/.kube/config
ln -s /var/lib/rancher/rke2/agent/etc/crictl.yaml /etc/crictl.yaml
chmod 600 ~/.kube/config

kubectl get node
crictl ps
crictl images

kubectl edit psp global-restricted-psp
# remove apparmor lines in annotation and save

### Autodeploy folder
/var/lib/rancher/rke2/server/manifests/
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Best Practices to keeping Kubernetes Clusters Secure]]></title>
            <link href="https://devopstales.github.io/home/k8s-security/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-security/?utm_source=atom_feed" rel="related" type="text/html" title="Best Practices to keeping Kubernetes Clusters Secure" />
                <link href="https://devopstales.github.io/home/k8s-calico-ebpf/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with calico&#39;s eBPF mode" />
                <link href="https://devopstales.github.io/home/k8s-ipvs/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with IPVS mode" />
                <link href="https://devopstales.github.io/home/k8s-install-openebs/?utm_source=atom_feed" rel="related" type="text/html" title="Install OpenEBS for Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-kubeadm-ha/?utm_source=atom_feed" rel="related" type="text/html" title="How to install kubernetes with kubeadm in HA mode" />
            
                <id>https://devopstales.github.io/home/k8s-security/</id>
            
            
            <published>2020-11-20T00:00:00+00:00</published>
            <updated>2020-11-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Kubernetes offers rich configuration options, but defaults are usually the least secure. Most sysadmin didn&rsquo;t knows how to secure a kubernetes clyuster. So this is my Best Practice list to keeping Kubernetes Clusters Secure.</p>

<h3 id="parst-of-the-k8s-security-series">Parst of the K8S Security series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/k8s-security/">Best Practices to keeping Kubernetes Clusters Secure</a></li>
<li>Part2: <a href="../../kubernetes/rke2-airgap-install/">RKE2 The Secure Kubernetes Engine</a></li>
<li>Part3: <a href="../../kubernetes/rke2-cilium/">RKE2 Install With cilium</a></li>
<li>Part4: <a href="../../kubernetes/k8s-cert/">Kubernetes Certificate Rotation</a>
<!-- RBAC -->
<!-- # Policy --></li>
<li>Part5: <a href="../../kubernetes/rke2-psp/">RKE2 Pod Security Policy</a></li>
<li>Part6: <a href="../../kubernetes/k8s-networkpolicy/">Kubernetes Network Policy</a></li>
<li>Part7: <a href="../../kubernetes/kubernetes-policy/">Kubernetes Cluster Policy</a>
<!-- # Admission Controllers --></li>
<li>Part8: <a href="../../kubernetes/admission-controllers/">Using Admission Controllers</a>
<!-- ## Image Scan --></li>
<li>Part9a: <a href="../../kubernetes/image-security-admission-controller">Image security Admission Controller</a></li>
<li>Part9b: <a href="../../kubernetes/image-security-admission-controller-v2">Image security Admission Controller V2</a></li>
<li>Part9c: <a href="../../kubernetes/image-security-admission-controller-v3">Image security Admission Controller V3</a>
<!-- https://github.com/banzaicloud/dast-operator -->
<!-- # Scans & Monitoring --></li>
<li>Part10: <a href="../../kubernetes/continuous-image-security/">Continuous Image security</a></li>
<li>Part11: <a href="../../kubernetes/k8s-prometheus-stack/">K8S Logging And Monitoring</a>
<!-- DAST Exporter?? -->
<!-- CIS Exporter
https://github.com/ibrokethecloud/kube-bench-metrics --></li>
<li>Part12 <a href="../../kubernetes/k8s-connaisseur">Image Signature Verification with Connaisseur</a></li>
<li>Part13 <a href="../../kubernetes/k8s-backup">Backup your Kubernetes Cluster</a>
<!-- # Secrets --></li>
<li>Part14a <a href="../../kubernetes/k8s-vault">Kubernetes and Vault integration</a></li>
<li>Part14b <a href="../../kubernetes/k8s-vault-v2">Kubernetes External Vault integration</a></li>
<li>Part15a: <a href="../../kubernetes/argocd-kubeseal/">ArgoCD and kubeseal to encript secrets</a></li>
<li>Part15b: <a href="../../kubernetes/gitops-flux2-kubeseal/">Flux2 and kubeseal to encrypt secrets</a></li>
<li>Part15c: <a href="../../kubernetes/gitops-flux2-sops/">Flux2 and Mozilla SOPS to encrypt secrets</a>
<!-- # Image -->
<!-- keel --></li>
<li>Part16: <a href="../../kubernetes/argocd-image-updater/">ArgoCD auto image updater</a>
<!-- flux image update --></li>
<li>Part17: <a href="../../kubernetes/k3s-gvisor/">Secure k3s with gVisor</a></li>
</ul>

<!-- admission-controller ami cseréli az imageben a repot !!!
 -->

<!-- sysdig falco -->

<!-- Image auto update:
https://keel.sh
https://blog.weareopensource.me/kell-automate-rancher-workloads-update/
flux
-->

<!-- resource kvota -->

<!-- Backup
https://github.com/pieterlange/kube-backup 
-->

<!--
https://medium.com/@beatrizmrg/network-security-for-microservices-with-ebpf-bis-478b40e7befa

# container runetime
https://www.ianlewis.org/en/container-runtimes-part-1-introduction-container-r
http://pouchcontainer.io/pouch/docs/features/pouch_with_runV.html
-->

<h3 id="use-firewalld">Use firewalld</h3>

<p>In most tutorial the first thing in a Kubernets installation is to disable the firewall because is it easier than configure properly.</p>

<pre><code># master
firewall-cmd --permanent --add-port=6443/tcp
firewall-cmd --permanent --add-port=2379-2380/tcp
firewall-cmd --permanent --add-port=10250/tcp
firewall-cmd --permanent --add-port=10251/tcp
firewall-cmd --permanent --add-port=10252/tcp
firewall-cmd --permanent --add-port=10255/tcp
firewall-cmd --permanent --add-port=8472/udp
firewall-cmd --add-masquerade --permanent
firewall-cmd --permanent --add-port=30000-32767/tcp

# worker
firewall-cmd --permanent --add-port=10250/tcp
firewall-cmd --permanent --add-port=10255/tcp
firewall-cmd --permanent --add-port=8472/udp
firewall-cmd --permanent --add-port=30000-32767/tcp
firewall-cmd --add-masquerade --permanent

# frontend
firewall-cmd --permanent --add-port=10250/tcp
firewall-cmd --permanent --add-port=10255/tcp
firewall-cmd --permanent --add-port=8472/udp
firewall-cmd --permanent --add-port=30000-32767/tcp
firewall-cmd --add-masquerade --permanent
firewall-cmd --permanent --zone=public --add-service=http
firewall-cmd --permanent --zone=public --add-service=https
</code></pre>

<h3 id="pod-network-add-on">Pod network add-on</h3>

<p>Several external projects provide Kubernetes Pod networks using CNI, some of which also support Network Policy. Use one of them.</p>

<ul>
<li>Calico</li>
<li>Canal</li>
<li>Weave Net</li>
<li>Contiv</li>
<li>Cilium</li>
</ul>

<p>See the list of available <a href="https://kubernetes.io/docs/concepts/cluster-administration/addons/#networking-and-network-policy">networking and network policy add-ons</a>.</p>

<h3 id="using-rbac-authorization">Using RBAC Authorization</h3>

<p>Role-based access control (RBAC) is a method of regulating access to computer or network resources based on the roles of individual users within your organization. For that you need to create <code>Role</code> or <code>ClusterRole</code> objects then assign that objects to a user wit <code>RoleBinding</code> or <code>ClusterRoleBinding</code>.</p>

<pre><code>---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: deployer
  namespace: $NAMESPACE
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: deployer-access
  namespace: $NAMESPACE
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: deployer-access
  namespace: $NAMESPACE
rules:
- apiGroups: [&quot;&quot;, &quot;extensions&quot;, &quot;apps&quot;]
  resources: [&quot;*&quot;]
  verbs: [&quot;*&quot;]
- apiGroups: [&quot;batch&quot;]
  resources:
  - jobs
  - cronjobs
  verbs: [&quot;*&quot;]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: deployer
  namespace: $NAMESPACE
subjects:
- kind: ServiceAccount
  name: deployer
  namespace: $NAMESPACE
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: deployer-access
</code></pre>

<h3 id="podsecuritypolicy">PodSecurityPolicy</h3>

<p>At default configuration users in docker containers has the same UID and GUID pool than the users on the host system. So if an unprivileged user runs a container as root and mount the host&rsquo;s filesystem to this container it can do what avers it wants on your host. Docker has an option to change the id pool us the users in the containers but kubernetes dose not support it. The RBAC adds access to an apiGroup like create deployments but dose not allow to configure the options you can use in the deployment.</p>

<p>A PodSecurityPolicy is a cluster-level resource for managing security aspects of a pod specification.</p>

<p>PSPs allow you to control:</p>

<ul>
<li>The ability to run privileged containers and control privilege escalation</li>
<li>Access to host filesystems</li>
<li>Usage of volume types</li>
<li>And a few other aspects including SELinux, AppArmor, sysctl, and seccomp profiles</li>
</ul>

<p>Pod Security Policies are implemented as an Admission Controller in Kubernetes. To enable PSPs in your cluster, make sure to include PodSecurityPolicy in the enable-admission-plugins list that is passed as a parameter to your Kubernetes API configuration:</p>

<pre><code>nano /etc/kubernetes/manifests/kube-apiserver.yaml
...
--enable-admission-plugins=...,PodSecurityPolicy
...
</code></pre>

<h4 id="creating-pod-security-policies">Creating Pod Security Policies</h4>

<pre><code>apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: restricted
  annotations:
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default'
    apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default'
    seccomp.security.alpha.kubernetes.io/defaultProfileName:  'runtime/default'
    apparmor.security.beta.kubernetes.io/defaultProfileName:  'runtime/default'
spec:
  privileged: false
  allowPrivilegeEscalation: false
  defaultAllowPrivilegeEscalation: false
  readOnlyRootFilesystem: false
  hostNetwork: false
  hostIPC: false
  hostPID: false
  requiredDropCapabilities:
    - ALL
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  hostPorts:
    - min: 0
      max: 0
  seLinux:
    rule: 'RunAsAny'
  runAsUser:
    rule: 'MustRunAsNonRoot'
  supplementalGroups:
    rule: 'MustRunAs'
    ranges:
      - min: 1
        max: 65535
  fsGroup:
    rule: 'MustRunAs'
    ranges:
      - min: 1
        max: 65535
</code></pre>

<h4 id="assigning-pod-security-policies">Assigning Pod Security Policies</h4>

<pre><code>kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: psp:restricted
rules:
- apiGroups:
  - extensions
  resources:
  - podsecuritypolicies
  resourceNames:
  - restricted # the psp we are giving access to
  verbs:
  - use
---
# This applies psp/restricted to all authenticated users
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: psp:restricted
subjects:
- kind: Group
  name: system:authenticated # All authenticated users
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: psp:restricted # A references to the role above
  apiGroup: rbac.authorization.k8s.io
view raw
</code></pre>

<h3 id="audit-log">Audit Log</h3>

<p>Usually it’s a best practice to enable audits in your cluster. Let’s go ahead and create a basic policy saved in our master.</p>

<pre><code>mkdir -p /etc/kubernetes

cat &gt; /etc/kubernetes/audit-policy.yaml &lt;&lt;EOF
apiVersion: audit.k8s.io/v1beta1
kind: Policy
rules:
# Do not log from kube-system accounts
- level: None
  userGroups:
  - system:serviceaccounts:kube-system
  - system:nodes
- level: None
  users:
  - system:apiserver
  - system:kube-scheduler
  - system:volume-scheduler
  - system:kube-controller-manager
  - system:node
# Don't log these read-only URLs.
- level: None
  nonResourceURLs:
  - /healthz*
  - /version
  - /swagger*
# limit level to Metadata so token is not included in the spec/status
- level: Metadata
  omitStages:
  - RequestReceived
  resources:
  - group: authentication.k8s.io
    resources:
    - tokenreviews
EOF

mkdir -p /var/log/kubernetes/audit

kube-apiserver --audit-log-path=/var/log/kubernetes/apiserver/audit.log \
--audit-policy-file=/etc/kubernetes/audit-policies/policy.yaml
</code></pre>

<h2 id="image-security">Image security</h2>

<p>Doesn&rsquo;t matter how secure is your kubernetes network or infrastructure is if you runs outdated unsecur images. You mast always update your base image, scan for known vulnerabilities. For applications use hardened base images and install as less components as you can. Some application for image scann:</p>

<ul>
<li>Anchore Engine</li>
<li>Clair</li>
<li>trivy</li>
</ul>

<h3 id="find-the-right-baseimage">Find the right baseimage</h3>

<p>I think the best choice for a base image is <code>Distroless</code>, which is set of images made by Google, that were created with intent to be secure. These images contain the bare minimum that&rsquo;s needed for your app.</p>

<pre><code>FROM gcr.io/distroless/python3
COPY --from=build-env /app /app
WORKDIR /app
CMD [&quot;hello.py&quot;, &quot;/etc&quot;]
</code></pre>

<h3 id="least-privileged-user">Least privileged user</h3>

<p>Create a dedicated user and group on the image, with minimal permissions to run the application; use the same user to run this process. For example, Node.js image which has a built-in node generic user:</p>

<pre><code>USER node
CMD node index.js
</code></pre>

<h3 id="store-secret-in-etcd-encripted">Store secret in etcd encripted.</h3>

<p>The Kubernetes&rsquo;s base secret store is not so secure because it stores the data as base64 encoded plain text in the etcd.</p>

<p>The <code>kube-apiserver</code> process accepts an argument <code>--encryption-provider-config</code> that controls how API data is encrypted in etcd. An example configuration is provided below.</p>

<pre><code>mkdir /etc/kubernetes/etcd-enc/

head -c 32 /dev/urandom | base64

nano /etc/kubernetes/etcd-enc/etcd-encription.yaml
---
apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
    - secrets
    providers:
    - identity: {}
    - aesgcm:
        keys:
        - name: key1
          secret: &lt;BASE 64 ENCODED SECRET&gt;
</code></pre>

<p>In this example key1 is the secret contains the encryption/decryption key.</p>

<pre><code>nano kube-apiserver.yaml
...
    - --encryption-provider-config=/etc/kubernetes/etcd-enc/etcd-encription.yaml
...
    volumeMounts:
...
    - mountPath: /etc/kubernetes/etcd-enc
      name: etc-kubernetes-etcd-enc
      readOnly: true
  hostNetwork: true
...
  - hostPath:
      path: /etc/kubernetes/etcd-enc
      type: DirectoryOrCreate
    name: etc-kubernetes-etcd-enc
status: {}
</code></pre>

<h3 id="the-cis-kubernetes-benchmark">The CIS Kubernetes Benchmark</h3>

<p>The Center for Internet Security (CIS) Kubernetes Benchmark is a reference document that can be used by system administrators, security and audit professionals and other IT roles to establish a secure configuration baseline for Kubernetes.</p>

<p>Create kube-bench job</p>

<pre><code>kubectl apply -f https://raw.githubusercontent.com/aquasecurity/kube-bench/master/job.yaml
kubectl get jobs --watch
</code></pre>

<p>Get job output from logs</p>

<pre><code>kubectl logs $(kubectl get pods -l app=kube-bench -o name)
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Azure Conainer Registry integration for AKS]]></title>
            <link href="https://devopstales.github.io/home/aks-registry/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/aks-ingress-controller/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ingress to AKS" />
                <link href="https://devopstales.github.io/home/aks/?utm_source=atom_feed" rel="related" type="text/html" title="Install AKS Cluster" />
                <link href="https://devopstales.github.io/cloud/aks-registry/?utm_source=atom_feed" rel="related" type="text/html" title="Azure Conainer Registry integration for AKS" />
                <link href="https://devopstales.github.io/cloud/aks-ingress-controller/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ingress to AKS" />
                <link href="https://devopstales.github.io/cloud/aks/?utm_source=atom_feed" rel="related" type="text/html" title="Install AKS Cluster" />
            
                <id>https://devopstales.github.io/home/aks-registry/</id>
            
            
            <published>2020-11-18T00:00:00+00:00</published>
            <updated>2020-11-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this pos I will show you how you can integrate Azure Container Registry to AKS (Azure Kubernetes Service) Cluster.</p>

<h3 id="parst-of-the-aks-series">Parst of the AKS series</h3>

<ul>
<li>Part1: <a href="../../cloud/AKS/">Install AKS Cluster</a></li>
<li>Part2: <a href="../../cloud/aks-registry/">Integrate AKS with Registry</a></li>
<li>Part3: <a href="../../cloud/aks-ingress-controller/">Install Ingresscontreoller To AKS</a></li>
</ul>

<h3 id="set-the-subscription">Set the subscription</h3>

<pre><code>az login
az account list
az account set --subscription &lt;SUBSCRIPTION_ID&gt;
</code></pre>

<h3 id="creating-an-azure-resource-group">Creating an Azure Resource Group</h3>

<pre><code>az group create --location &lt;REGION_NAME&gt; --name &lt;RESOURCE_GROUP_NAME&gt;
</code></pre>

<h3 id="provisioning-an-azure-container-registry">Provisioning an Azure Container Registry</h3>

<pre><code>az acr create --name &lt;REGISTRY_NAME&gt; \
--resource-group &lt;RESOURCE_GROUP_NAME&gt; \
--sku Basic


az ad sp create-for-rbac \
  --scopes /subscriptions/&lt;SUBSCRIPTION_ID&gt;/resourcegroups/&lt;RESOURCE_GROUP_NAME&gt;/providers/Microsoft.ContainerRegistry/registries/&lt;REGISTRY_NAME&gt; \
  --role Contributor \
  --name &lt;SERVICE_PRINCIPAL_NAME&gt;

docker login &lt;REGISTRY_NAME&gt; -u &lt;CLIENT_ID&gt;
</code></pre>

<h3 id="create-a-new-aks-cluster-with-acr-integration">Create a new AKS cluster with ACR integration</h3>

<pre><code>az aks create -n &lt;KUBERNETS_CLUSTER_NAME&gt; \
-g &lt;RESOURCE_GROUP_NAME&gt; \
--generate-ssh-keys \
--attach-acr &lt;REGISTRY_NAME&gt;
</code></pre>

<h3 id="configure-acr-integration-for-existing-aks-clusters">Configure ACR integration for existing AKS clusters</h3>

<pre><code>az aks update -n &lt;KUBERNETS_CLUSTER_NAME&gt; \
-g &lt;RESOURCE_GROUP_NAME&gt; \
--attach-acr &lt;REGISTRY_NAME&gt;
</code></pre>

<h3 id="use-kubernetes-secret-for-registry-integration">Use Kubernetes Secret for registry integration</h3>

<pre><code>ACR_NAME=&lt;REGISTRY_NAME&gt;
ACR_UNAME=$(az acr credential show -n $ACR_NAME --query=&quot;username&quot; -o tsv)
ACR_PASSWD=$(az acr credential show -n $ACR_NAME --query=&quot;passwords[0].value&quot; -o tsv)

kubectl create secret docker-registry acr-secret \
  --docker-server=$ACR_NAME \
  --docker-username=$ACR_UNAME \
  --docker-password=$ACR_PASSWD \
  --docker-email=ignorethis@email.com
</code></pre>

<p>Use secret in Kubernetes</p>

<pre><code>---
apiVersion: v1
kind: Pod
metadata:
  name: sample-pod
spec:
  containers:
  - name: sample-pod-container
    image: youruniquename.azurecr.io/sample-container:0.0.1
  imagePullSecrets:
  - name: acr-secret
</code></pre>

<h3 id="use-service-account-for-authentication">Use Service Account For authentication</h3>

<pre><code>ACR_NAME=&lt;REGISTRY_NAME&gt;
ACR_UNAME=$()
ACR_PASSWD=$()

kubectl create secret docker-registry acr-secret \
  --docker-server=$ACR_NAME \
  --docker-username=$ACR_UNAME \
  --docker-password=$ACR_PASSWD \
  --docker-email=ignorethis@email.com
</code></pre>

<p>Use ServiceAccount in Kubernetes</p>

<pre><code>--docker-password=$ACR_PASSWD \
--docker-email=ignorethis@email.com

---
apiVersion: v1
kind: ServiceAccount
metadata:
name: SampleAccount
namespace: default
imagePullSecrets:
- name: acr-secret
---
apiVersion: v1
kind: Pod
metadata:
name: sample-pod
spec:
containers:
- name: sample-pod-container
  image: youracrname.azurecr.io/sample-container:0.0.1
serviceAccountName: SampleAccount
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Ingress to AKS]]></title>
            <link href="https://devopstales.github.io/home/aks-ingress-controller/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/aks/?utm_source=atom_feed" rel="related" type="text/html" title="Install AKS Cluster" />
                <link href="https://devopstales.github.io/cloud/aks-ingress-controller/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ingress to AKS" />
                <link href="https://devopstales.github.io/cloud/aks/?utm_source=atom_feed" rel="related" type="text/html" title="Install AKS Cluster" />
                <link href="https://devopstales.github.io/home/jitsi-meet/?utm_source=atom_feed" rel="related" type="text/html" title="Install Jitsi meet" />
                <link href="https://devopstales.github.io/home/jitsi-jibri/?utm_source=atom_feed" rel="related" type="text/html" title="Install recording fot Jitsi" />
            
                <id>https://devopstales.github.io/home/aks-ingress-controller/</id>
            
            
            <published>2020-11-15T00:00:00+00:00</published>
            <updated>2020-11-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this pos I will show you how you can Install Ingress to AKS (Azure Kubernetes Service) Cluster.</p>

<h3 id="parst-of-the-aks-series">Parst of the AKS series</h3>

<ul>
<li>Part1: <a href="../../cloud/AKS/">Install AKS Cluster</a></li>
<li>Part2: <a href="../../cloud/aks-registry/">Integrate AKS with Registry</a></li>
<li>Part3: <a href="../../cloud/aks-ingress-controller/">Install Ingresscontreoller To AKS</a></li>
</ul>

<h3 id="get-aks-credentials">Get AKS credentials</h3>

<pre><code>az login
az aks get-credentials --resource-group test-cluster --name test-cluster
kubectl get nodes
</code></pre>

<h3 id="create-ingress-with-static-public-ip">Create ingress with static public ip</h3>

<pre><code>az network public-ip create \
--location &lt;REGION_NAME&gt; \
--resource-group &lt;RESOURCE_GROUP_NAME&gt; \
--name &lt;IP_NAME&gt; --sku Standard \
--allocation-method static \
--query publicIp.ipAddress -o tsv

# 51.105.230.165
</code></pre>

<h3 id="deploy-nginx-ingress-controller-with-helm">Deploy nginx ingress controller with helm</h3>

<pre><code>kubectl create namespace ingress

helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx

helm install &lt;INGERSS_NAME&gt; \
ingress-nginx/ingress-nginx \
--namespace ingress \
--set controller.replicaCount=2 \
--set controller.nodeSelector.&quot;beta\.kubernetes\.io/os&quot;=linux \
--set defaultBackend.nodeSelector.&quot;beta\.kubernetes\.io/os&quot;=linux \
--set controller.service.loadBalancerIP=&quot;51.105.230.165&quot;
</code></pre>

<pre><code>kubectl --namespace ingress get services -o wide -w nginx-ingress-controller
kubectl get service -l app=nginx-ingress --namespace ingress
</code></pre>

<h3 id="create-an-ingress-controller-to-an-internal-virtual-network-in">Create an ingress controller to an internal virtual network in</h3>

<p>By default, an NGINX ingress controller is created with a dynamic public IP address assignment. A common configuration requirement is to use an internal, private network and IP address. This approach allows you to restrict access to your services to internal users, with no external access. This example assigns 10.240.0.42 to the loadBalancerIP resource.</p>

<pre><code>nano internal-ingress.yaml
---
controller:
  service:
    loadBalancerIP: 10.240.0.42
    annotations:
      service.beta.kubernetes.io/azure-load-balancer-internal: &quot;true&quot;
</code></pre>

<pre><code>helm install nginx-ingress \
ingress-nginx/ingress-nginx \
--namespace ingress \
-f internal-ingress.yaml \
--set controller.ingressClass=internal \
--set controller.replicaCount=2 \
--set controller.nodeSelector.&quot;beta\.kubernetes\.io/os&quot;=linux \
--set defaultBackend.nodeSelector.&quot;beta\.kubernetes\.io/os&quot;=linux
</code></pre>

<p>Create an ingress route</p>

<pre><code>apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: hello-world-ingress
  namespace: ingress-basic
  annotations:
    kubernetes.io/ingress.class: internal
...
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install AKS Cluster]]></title>
            <link href="https://devopstales.github.io/home/aks/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/aks/?utm_source=atom_feed" rel="related" type="text/html" title="Install AKS Cluster" />
                <link href="https://devopstales.github.io/home/jitsi-meet/?utm_source=atom_feed" rel="related" type="text/html" title="Install Jitsi meet" />
                <link href="https://devopstales.github.io/home/jitsi-jibri/?utm_source=atom_feed" rel="related" type="text/html" title="Install recording fot Jitsi" />
                <link href="https://devopstales.github.io/home/k8s-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure K8S vSphere Cloud Provider" />
                <link href="https://devopstales.github.io/home/k8s-ipvs/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with IPVS mode" />
            
                <id>https://devopstales.github.io/home/aks/</id>
            
            
            <published>2020-11-07T00:00:00+00:00</published>
            <updated>2020-11-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this pos I will show you how you can create an AKS (Azure Kubernetes Service) Cluster.</p>

<h3 id="parst-of-the-aks-series">Parst of the AKS series</h3>

<ul>
<li>Part1: <a href="../../cloud/AKS/">Install AKS Cluster</a></li>
<li>Part2: <a href="../../cloud/aks-registry/">Integrate AKS with Registry</a></li>
<li>Part3: <a href="../../cloud/aks-ingress-controller/">Install Ingresscontreoller To AKS</a></li>
</ul>

<h3 id="what-is-azure-kubernetes-service">What is Azure Kubernetes Service?</h3>

<p>Azure Kubernetes Service (AKS) is a managed Kubernetes service that lets you quickly deploy and manage clusters.</p>

<p>A Kubernetes cluster is divided into two components:</p>

<ul>
<li>Control plane nodes provide the core Kubernetes services and orchestration of application workloads.</li>
<li>Nodes run your application workloads.</li>
</ul>

<p><img src="/img/include/aks-control-plane-and-nodes.png" alt="Example image" /></p>

<p>When you create an AKS cluster, a control plane is automatically created and configured. This control plane is provided as a managed Azure resource abstracted from the user. There&rsquo;s no cost for the control plane, only the nodes that are part of the AKS cluster. The control plane and its resources reside only on the region where you created the cluster.</p>

<h3 id="how-to-create-azure-kubernetes-cluster">How To Create Azure Kubernetes Cluster</h3>

<p>There are 2 ways to deploy an Azure Kubernetes Cluster, which are using:</p>

<ul>
<li>Azure Portal</li>
<li>Azure CLI</li>
</ul>

<p>I prefer to use the cli because it is easier to reproduce.</p>

<h4 id="install-azure-cli">Install Azure CLI</h4>

<pre><code># OSX
brew update &amp;&amp; brew install azure-cli

# Yum
sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc

sudo sh -c 'echo -e &quot;[azure-cli]
name=Azure CLI
baseurl=https://packages.microsoft.com/yumrepos/azure-cli
enabled=1
gpgcheck=1
gpgkey=https://packages.microsoft.com/keys/microsoft.asc&quot; &gt; /etc/yum.repos.d/azure-cli.repo'

sudo yum install azure-cli

# apt
curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

sudo apt-get update
sudo apt-get install ca-certificates curl apt-transport-https lsb-release gnupg

curl -sL https://packages.microsoft.com/keys/microsoft.asc |
    gpg --dearmor |
    sudo tee /etc/apt/trusted.gpg.d/microsoft.gpg &gt; /dev/null

AZ_REPO=$(lsb_release -cs)
echo &quot;deb [arch=amd64] https://packages.microsoft.com/repos/azure-cli/ $AZ_REPO main&quot; |
    sudo tee /etc/apt/sources.list.d/azure-cli.list

sudo apt-get update
sudo apt-get install azure-cli
</code></pre>

<h4 id="set-the-subscription">Set the subscription</h4>

<pre><code>az login
az account list
az account set --subscription &lt;SUBSCRIPTION_ID&gt;
</code></pre>

<h4 id="creating-an-azure-resource-group">Creating an Azure Resource Group</h4>

<pre><code>az group create --location &lt;REGION_NAME&gt; --name &lt;RESOURCE_GROUP_NAME&gt;
</code></pre>

<h4 id="create-aks-cluster">Create AKS Cluster</h4>

<pre><code># get available kubernetes versions
az aks get-versions --location &lt;REGION_NAME&gt;

az aks create --resource-group &lt;RESOURCE_GROUP_NAME&gt; \
--name &lt;AKS_CLUSTER_NAME&gt; \
--node-count 3 \
--generate-ssh-keys \
--kubernetes-version 1.17.0
</code></pre>

<h3 id="uses-availability-zones-for-aks-cluster">Uses availability zones for AKS Cluster</h3>

<p>AKS clusters that are deployed using availability zones can distribute nodes across multiple zones within a single region. For example, a cluster in the East US 2 region can create nodes in all three availability zones in East US 2. This distribution of AKS cluster resources improves cluster availability as they&rsquo;re resilient to failure of a specific zone.</p>

<p>KÉP: <a href="https://docs.microsoft.com/en-us/azure/aks/media/availability-zones/aks-availability-zones.png">https://docs.microsoft.com/en-us/azure/aks/media/availability-zones/aks-availability-zones.png</a></p>

<pre><code># get available kubernetes versions
az aks get-versions --location &lt;REGION_NAME&gt;

az aks create --resource-group &lt;RESOURCE_GROUP_NAME&gt; \
--name &lt;AKS_CLUSTER_NAME&gt; \
--node-count 3 \
--zones 1 2 3 \
--vm-set-type VirtualMachineScaleSets \
--load-balancer-sku standard \
--generate-ssh-keys \
--kubernetes-version 1.17.0
</code></pre>

<h4 id="get-kubeconfig-congig-of-the-cluster">Get Kubeconfig Congig of the Cluster</h4>

<pre><code>az aks get-credentials --name &lt;AKS_CLUSTER_NAME&gt; \
 --resource-group &lt;RESOURCE_GROUP_NAME&gt;

kubectl get nodes
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install recording fot Jitsi]]></title>
            <link href="https://devopstales.github.io/home/jitsi-jibri/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/jitsi-meet/?utm_source=atom_feed" rel="related" type="text/html" title="Install Jitsi meet" />
                <link href="https://devopstales.github.io/home/k8s-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure K8S vSphere Cloud Provider" />
                <link href="https://devopstales.github.io/home/k8s-ipvs/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with IPVS mode" />
                <link href="https://devopstales.github.io/home/k8s-calico-ebpf/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with calico&#39;s eBPF mode" />
                <link href="https://devopstales.github.io/home/install-unifi-controller/?utm_source=atom_feed" rel="related" type="text/html" title="How to setup Unifi Controller on Debian 10" />
            
                <id>https://devopstales.github.io/home/jitsi-jibri/</id>
            
            
            <published>2020-10-21T00:00:00+00:00</published>
            <updated>2020-10-21T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you my productivity tips with kubectlyo can install Jibri the recording component of Jitsi.</p>

<h3 id="install-requirements">Install requirements</h3>

<pre><code>sudo apt install ffmpeg curl unzip software-properties-common
</code></pre>

<p>Enable the ALSA loopback module to start on boot,</p>

<pre><code>echo &quot;snd_aloop&quot; &gt;&gt; /etc/modules
modprobe snd_aloop
</code></pre>

<h3 id="install-google-chrome">Install Google Chrome:</h3>

<pre><code>curl -o - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add
echo &quot;deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main&quot; &gt; /etc/apt/sources.list.d/google-chrome.list
apt update
apt install google-chrome-stable

# install headless browser driver
CHROME_DRIVER_VERSION=`curl -sS chromedriver.storage.googleapis.com/LATEST_RELEASE`
wget -N http://chromedriver.storage.googleapis.com/$CHROME_DRIVER_VERSION/chromedriver_linux64.zip -P ~/
unzip ~/chromedriver_linux64.zip -d ~/
rm ~/chromedriver_linux64.zip
sudo mv -f ~/chromedriver /usr/local/bin/chromedriver
sudo chmod 0755 /usr/local/bin/chromedriver

# disable warnings
mkdir -p /etc/opt/chrome/policies/managed
echo '{ &quot;CommandLineFlagSecurityWarningsEnabled&quot;: false }' &gt;&gt;/etc/opt/chrome/policies/managed/managed_policies.json
</code></pre>

<h3 id="install-jibri">Install jibri</h3>

<pre><code>apt install jibri
usermod -aG adm,audio,video,plugdev jibri

nano /etc/prosody/prosody.cfg.lua
...
Component &quot;conference.jitsi.mydomain.intra&quot; &quot;muc&quot;
modules_enabled = { &quot;muc_mam&quot; }
...
Component &quot;internal.auth.jitsi.mydomain.intra&quot; &quot;muc&quot;
modules_enabled = {
&quot;ping&quot;;
}
storage = &quot;internal&quot;
muc_room_cache_size = 1000
...
VirtualHost &quot;recorder.jitsi.mydomain.intra&quot;
modules_enabled = {
&quot;ping&quot;;
}
authentication = &quot;internal_plain&quot;
</code></pre>

<p>Create accounts:</p>

<pre><code>prosodyctl register jibri auth.jitsi.mydomain.intra Password1
prosodyctl register recorder recorder.jitsi.mydomain.intra Password1
</code></pre>

<pre><code>sudo nano /etc/jitsi/jicofo/sip-communicator.properties
...
org.jitsi.jicofo.jibri.BREWERY=JibriBrewery@internal.auth.jitsi.mydomain.intra
org.jitsi.jicofo.jibri.PENDING_TIMEOUT=90

sudo nano /etc/jitsi/meet/jitsi.mydomain.intra-config.js
...
fileRecordingsEnabled: true,
liveStreamingEnabled: true,
hiddenDomain: 'recorder.jitsi.mydomain.intra',
</code></pre>

<p>Configure storage:</p>

<pre><code>mkdir /recordings
chown jibri:jibri /recordings

sudo nano /etc/jitsi/jibri/config.json
&quot;recording_directory&quot;: &quot;/recordings&quot;,
&quot;finalize_recording_script_path&quot;: &quot;&quot;,
&quot;xmpp_server_hosts&quot;: [
&quot;jitsi.mydomain.intra&quot;
],
&quot;xmpp_domain&quot;: &quot;jitsi.mydomain.intra&quot;,
&quot;control_login&quot;: {
&quot;domain&quot;: &quot;auth.jitsi.mydomain.intra&quot;,
&quot;username&quot;: &quot;jibri&quot;,
&quot;password&quot;: &quot;Password1d&quot;
},
&quot;control_muc&quot;: {
&quot;domain&quot;: &quot;internal.auth.jitsi.mydomain.intra&quot;,
&quot;room_name&quot;: &quot;JibriBrewery&quot;,
&quot;nickname&quot;: &quot;jibri&quot;
},
&quot;call_login&quot;: {
&quot;domain&quot;: &quot;recorder.jitsi.mydomain.intra&quot;,
&quot;username&quot;: &quot;recorder&quot;,
&quot;password&quot;: &quot;Password1&quot;
},
</code></pre>

<h3 id="install-java-8">Install Java 8</h3>

<p>Jibri only works with java 1.8 so we need to install and configure Jibri to use it:</p>

<pre><code>wget -O - https://adoptopenjdk.jfrog.io/adoptopenjdk/api/gpg/key/public | sudo apt-key add -
add-apt-repository https://adoptopenjdk.jfrog.io/adoptopenjdk/deb/
apt update
apt install adoptopenjdk-8-hotspot

# the systemd sript use this script so we edit it
nano /opt/jitsi/jibri/launch.sh
# change this:
# exec java ...
# tho this:
exec /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64/bin/java ...
</code></pre>

<pre><code>systemctl restart prosody jicofo jitsi-videobridge2 jibri
systemctl enable jibri
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Jitsi meet]]></title>
            <link href="https://devopstales.github.io/home/jitsi-meet/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/jitsi-meet/?utm_source=atom_feed" rel="related" type="text/html" title="Install Jitsi meet" />
                <link href="https://devopstales.github.io/home/k8s-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure K8S vSphere Cloud Provider" />
                <link href="https://devopstales.github.io/home/k8s-ipvs/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with IPVS mode" />
                <link href="https://devopstales.github.io/home/k8s-calico-ebpf/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with calico&#39;s eBPF mode" />
                <link href="https://devopstales.github.io/home/install-unifi-controller/?utm_source=atom_feed" rel="related" type="text/html" title="How to setup Unifi Controller on Debian 10" />
            
                <id>https://devopstales.github.io/home/jitsi-meet/</id>
            
            
            <published>2020-10-20T00:00:00+00:00</published>
            <updated>2020-10-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can install Jitsi meet on your server.</p>

<p>Jitsi is state-of-the art video conferencing software that you can self-host or simply use at meet.jit.si.</p>

<h3 id="configure-hostname">Configure hostname</h3>

<p>In this step, you will change the system’s hostname to match the domain name that you intend to use for your Jitsi Meet instance.</p>

<pre><code>sudo hostnamectl set-hostname jitsi.mydomain.intra
sudo nano nano /etc/hosts
127.0.0.1 jitsi.mydomain.intra
</code></pre>

<h3 id="install-jitsi-meet">Install Jitsi Meet</h3>

<pre><code>curl https://download.jitsi.org/jitsi-key.gpg.key | \
sudo sh -c 'gpg --dearmor &gt; /usr/share/keyrings/jitsi-keyring.gpg'

echo 'deb [signed-by=/usr/share/keyrings/jitsi-keyring.gpg] https://download.jitsi.org stable/' | \
sudo tee /etc/apt/sources.list.d/jitsi-stable.list &gt; /dev/null
</code></pre>

<pre><code>sudo apt install apt-transport-https
sudo apt update
sudo apt install jitsi-meet
</code></pre>

<p>During the installation of <code>jitsi-meet</code> you will be prompted to enter the domain name. Then you will be shown a new dialog box that asks if you want Jitsi to create and use a self-signed TLS certificate or use an existing one. For this demo I will use the self-signed option.</p>

<p>If you want to use letsencrypt select self-signed the install certboot and use jitsi&rsquo;s script like this:</p>

<pre><code>sudo add-apt-repository ppa:certbot/certbot
sudo apt install certbot
sudo /usr/share/jitsi-meet/scripts/install-letsencrypt-cert.sh
</code></pre>

<h3 id="nat-configuration">NAT Configuration</h3>

<p>If the installation is behind NAT jitsi-videobridge should configure jitsi-videobridge  in order for it to be accessible from outside.</p>

<pre><code>nano /etc/jitsi/videobridge/sip-communicator.properties
org.ice4j.ice.harvest.NAT_HARVESTER_LOCAL_ADDRESS=&lt;Local.IP.Address&gt;
org.ice4j.ice.harvest.NAT_HARVESTER_PUBLIC_ADDRESS=&lt;Public.IP.Address&gt;
</code></pre>

<p>The you need to NAT the ports 443 4443 10000 from externat ip to the server.</p>

<h3 id="configure-jitsi">Configure Jitsi</h3>

<p>The default config allow any user to start meetings without authentication. For a publicly accessible server this is not what we want.</p>

<p>With this configuration all users need to authenticate to use Jitsi.</p>

<pre><code>sudo nano /etc/prosody/conf.avail/jitsi.mydomain.intra.cfg.lua
...
VirtualHost &quot;jitsi.mydomain.intra&quot;
# change this:
#       authentication = &quot;anonymous&quot;
# tho this:
        authentication = &quot;internal_plain&quot;

sudo nano /etc/jitsi/jicofo/sip-communicator.properties
...
org.jitsi.jicofo.auth.URL=XMPP:jitsi.mydomain.intra
</code></pre>

<p>Create users manually</p>

<pre><code>prosodyctl register &lt;username&gt; jitsi.mydomain.intra &lt;Password&gt;
</code></pre>

<p>For a few user is is ok to create them manually but for many users you need something like ldap:</p>

<pre><code>sudo apt install sasl2-bin libsasl2-modules-ldap lua-cyrussasl
sudo nano /etc/prosody/conf.avail/ldap.cfg.lua
VirtualHost &quot;jitsi.mydomain.intra&quot;
# change this:
#       authentication = &quot;anonymous&quot;
# tho this:
        authentication = &quot;cyrus&quot;
        cyrus_application_name = &quot;xmpp&quot;
...
        modules_enabled = {
...
            &quot;auth_cyrus&quot;; -- Add this line
        }

        c2s_require_encryption = false
...
</code></pre>

<pre><code>sudo nano /etc/sasl/xmpp.conf
pwcheck_method: saslauthd
mech_list: PLAIN

sudo nano /etc/saslauthd.conf
ldap_servers: ldap://10.0.0.1
ldap_search_base: dc=my,dc=search,dc=base
ldap_bind_dn: cn=Administrator,cn=Users,dc=foo,dc=bar
ldap_bind_pw: PassW0rd
ldap_filter: (samaccountname=%u)
ldap_version: 3
ldap_auth_method: bind
# for tls change ldap_servers to ldaps://
# the add uncomment and configure this:
#ldap_tls_key: /config/certs/meet.jit.si.key
#ldap_tls_cert: /config/certs/meet.jit.si.crt

#ldap_tls_check_peer: yes
#ldap_tls_cacert_file: /etc/ssl/certs/ca-certificates.crt
#ldap_tls_cacert_dir: /etc/ssl/certs
</code></pre>

<p>The next configuration allows anonymous users to join conference rooms that were created by an authenticated user.</p>

<pre><code>sudo nano /etc/prosody/conf.avail/jitsi.mydomain.intra.cfg.lua
...
# to the end
VirtualHost &quot;guest.jitsi.mydomain.intra&quot;
    authentication = &quot;anonymous&quot;
    c2s_require_encryption = false

</code></pre>

<p>The <code>guest.</code> hostname is only used internally by Jitsi Meet. You will never enter it into a browser or need to create a DNS record for it.</p>

<pre><code>sudo nano /etc/jitsi/meet/jitsi.mydomain.intra-config.js
var config = {
    hosts: {
        domain: &quot;jitsi.mydomain.intra&quot;,
        anonymousdomain: 'guest.jitsi.mydomain.intra',
</code></pre>

<p>After a config change yo need to restart the services:</p>

<pre><code>systemctl restart prosody jicofo jitsi-videobridge2
</code></pre>

<h2 id="more-info">More info:</h2>

<ul>
<li><a href="https://github.com/jitsi/jitsi-meet/wiki/LDAP-Authentication#configure-saslauthd">https://github.com/jitsi/jitsi-meet/wiki/LDAP-Authentication#configure-saslauthd</a></li>
</ul>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configure K8S vSphere Cloud Provider]]></title>
            <link href="https://devopstales.github.io/home/k8s-vmware/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-ipvs/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with IPVS mode" />
                <link href="https://devopstales.github.io/home/k8s-calico-ebpf/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with calico&#39;s eBPF mode" />
                <link href="https://devopstales.github.io/home/openshift-rbd-fsck/?utm_source=atom_feed" rel="related" type="text/html" title="How to fixing filesystem corruption on a Kubernetes Ceph RBD PersistentVolume" />
                <link href="https://devopstales.github.io/home/k3s-helm-controller/?utm_source=atom_feed" rel="related" type="text/html" title="Install and use rancher helm-controller" />
                <link href="https://devopstales.github.io/home/k3s-crio/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with CRI-O and kadalu" />
            
                <id>https://devopstales.github.io/home/k8s-vmware/</id>
            
            
            <published>2020-10-14T00:00:00+00:00</published>
            <updated>2020-10-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how can you use vmware for persistent storagi on K8S.</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
<li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-contour-ingress --></li>
<li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
<li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
<li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... --></li>
<li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
<li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
<li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<h3 id="vsphere-configuration">vSphere Configuration</h3>

<ul>
<li>Create a folder for all the VMs in vCenter

<ul>
<li>In the navigator, select the data center</li>
<li>Right-click and select the menu option to create the folder.</li>
<li>Select: All vCenter Actions &gt; New VM and Template Folder.</li>
<li>Move K8S vms to this folder</li>
</ul></li>
<li>The name of the virtual machine must match the name of the nodes for the K8S cluster.</li>
</ul>

<p><img src="/img/include/k8s-vmware.png" alt="Example image" /></p>

<h3 id="set-up-the-govc-environment">Set up the GOVC environment:</h3>

<pre><code class="language-bash"># on deployer
curl -LO https://github.com/vmware/govmomi/releases/download/v0.20.0/govc_linux_amd64.gz
gunzip govc_linux_amd64.gz
chmod +x govc_linux_amd64
cp govc_linux_amd64 /usr/bin/govc
echo &quot;export GOVC_URL='vCenter IP OR FQDN'&quot; &gt;&gt; /etc/profile
echo &quot;export GOVC_USERNAME='vCenter User'&quot; &gt;&gt; /etc/profile
echo &quot;export GOVC_PASSWORD='vCenter Password'&quot; &gt;&gt; /etc/profile
echo &quot;export GOVC_INSECURE=1&quot; &gt;&gt; /etc/profile
source /etc/profile
</code></pre>

<p>Add <code>disk.enableUUID=1</code> for all VM:</p>

<pre><code class="language-bash">govc vm.info &lt;vm&gt;
govc ls /Datacenter/kubernetes/&lt;vm-folder-name&gt;
# example:
govc ls /Datacenter/kubernetes/k8s-01

govc vm.change -e=&quot;disk.enableUUID=1&quot; -vm='VM Path'
# example:
govc vm.change -e=&quot;disk.enableUUID=1&quot; -vm='/datacenter/kubernetes/k8s-01/k8s-m01'
</code></pre>

<p>VM Hardware should be at version 15 or higher. Upgrade if needed:</p>

<pre><code class="language-bash">govc vm.option.info '/datacenter/kubernetes/k8s-01/k8s-m01' | grep HwVersion

govc vm.upgrade -version=15 -vm '/datacenter/kubernetes/k8s-01/k8s-m01'
</code></pre>

<h3 id="create-the-required-roles">Create the required Roles</h3>

<ul>
<li>Navigate in the vSphere Client - Menu &gt; Administration &gt; Roles</li>
<li>Add a new Role and select the permissions required. Repeat for each role.</li>
</ul>

<table>
<thead>
<tr>
<th align="center">Roles</th>
<th align="center">Privileges</th>
<th align="center">Entities</th>
<th align="center">Propagate to Children</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">vcp-manage-k8s-node-vms</td>
<td align="center">Resource.AssignVMToPoolVirtualMachine.Config.AddExistingDisk, VirtualMachine.Config.AddNewDisk, VirtualMachine.Config.AddRemoveDevice, VirtualMachine.Config.RemoveDisk, VirtualMachine.Config.SettingsVirtualMachine.Inventory.Create, VirtualMachine.Inventory.Delete</td>
<td align="center">Cluster, Hosts, VM Folder</td>
<td align="center">Yes</td>
</tr>

<tr>
<td align="center">vcp-manage-k8s-volumes</td>
<td align="center">Datastore.AllocateSpace, Datastore.FileManagement (Low level file operations)</td>
<td align="center">Datastore</td>
<td align="center">No</td>
</tr>

<tr>
<td align="center">vcp-view-k8s-spbm-profile</td>
<td align="center">StorageProfile.View (Profile-driven storage view)</td>
<td align="center">vCenter</td>
<td align="center">No</td>
</tr>

<tr>
<td align="center">Read-only (pre-existing default role)</td>
<td align="center">System.Anonymous, System.Read, System.View</td>
<td align="center">Datacenter, Datastore Cluster, Datastore Storage Folder</td>
<td align="center">No</td>
</tr>
</tbody>
</table>

<h3 id="create-a-service-account">Create a service account</h3>

<ul>
<li>Create a vsphere user, or add a domain user, to provide access and assign the new roles to.</li>
</ul>

<h3 id="create-vsphere-conf">Create vsphere.conf</h3>

<p>Create the vSphere configuration file in /etc/kubernetes/vcp/vsphere.conf - you’ll need to create the folder.</p>

<pre><code class="language-bash">nano /etc/kubernetes/vcp/vsphere.conf
[Global]
user = &quot;k8s-user@vsphere.local&quot;
password = &quot;password for k8s-user&quot;
port = &quot;443&quot;
insecure-flag = &quot;1&quot;

[VirtualCenter &quot;10.0.1.200&quot;]
datacenters = &quot;DC-1&quot;

[Workspace]
server = &quot;10.0.1.200&quot;
datacenter = &quot;DC-1&quot;
default-datastore = &quot;vsanDatastore&quot;
resourcepool-path = &quot;ClusterNameHere/Resources&quot;
folder = &quot;kubernetes&quot;

[Disk]
scsicontrollertype = pvscsi
</code></pre>

<h3 id="modify-the-kubelet-service">Modify the kubelet service</h3>

<p>On master:</p>

<pre><code class="language-bash">nano /etc/systemd/system/kubelet.service
[Service]
...
ExecStart=/usr/bin/docker run \
...
        /hyperkube kubelet \
...
--cloud-provider=vsphere --cloud-config=/etc/kubernetes/vsphere.conf    
</code></pre>

<p>On worker:</p>

<pre><code class="language-bash">nano /etc/systemd/system/kubelet.service
[Service]
...
ExecStart=/usr/bin/docker run \
...
        /hyperkube kubelet \
...
--cloud-provider=vsphere  
</code></pre>

<h3 id="modify-container-manifests">Modify container manifests</h3>

<p>Add following flags to the kubelet service configuration (usually in the systemd config file), as well as the controller-manager and api-server container manifest files on the master node (usually in /etc/kubernetes/manifests).</p>

<pre><code class="language-bash">nano /etc/kubernetes/manifests/kube-apiserver.yaml
...
spec:
  containers:
  - command:
    - kube-apiserver
...
    - --cloud-provider=vsphere
    - --cloud-config=/etc/kubernetes/vsphere.conf
    volumeMounts:
    - mountPath: /etc/kubernetes/vcp
      name: vcp
      readOnly: true
...
  volumes:
  - hostPath:
      path: /etc/kubernetes/vcp
      type: DirectoryOrCreate
    name: vcp
</code></pre>

<p>Restart the services.</p>

<pre><code class="language-bash">systemctl restart kubelet docker
</code></pre>

<h3 id="add-providerid">Add providerID</h3>

<pre><code class="language-bash">kubectl get nodes -o json | jq '.items[]|[.metadata.name, .spec.providerID, .status.nodeInfo.systemUUID]'

nano k8s-vmware-pacher.sh
DATACENTER='&lt;Datacenter&gt;'
FOLDER='&lt;vm-folder-name&gt;'
for vm in $(govc ls /$DATACENTER/vm/$FOLDER ); do
  MACHINE_INFO=$(govc vm.info -json -dc=$DATACENTER -vm.ipath=&quot;$vm&quot; -e=true)
  # My VMs are created on vmware with upper case names, so I need to edit the names with awk
  VM_NAME=$(jq -r ' .VirtualMachines[] | .Name' &lt;&lt;&lt; $MACHINE_INFO | awk '{print tolower($0)}')
  # UUIDs come in lowercase, upper case then
  VM_UUID=$( jq -r ' .VirtualMachines[] | .Config.Uuid' &lt;&lt;&lt; $MACHINE_INFO | awk '{print toupper($0)}')
  echo &quot;Patching $VM_NAME with UUID:$VM_UUID&quot;
  # This is done using dry-run to avoid possible mistakes, remove when you are confident you got everything right.
  kubectl patch node $VM_NAME -p &quot;{\&quot;spec\&quot;:{\&quot;providerID\&quot;:\&quot;vsphere://$VM_UUID\&quot;}}&quot;
done

chmod +x openshift-vmware-pacher.sh
./openshift-vmware-pacher.sh
</code></pre>

<h3 id="create-vsphere-storage-class">Create vSphere storage-class</h3>

<pre><code class="language-bash">nano vmware-sc.yml
---
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: &quot;true&quot;
  name: &quot;vsphere-standard&quot;
provisioner: kubernetes.io/vsphere-volume
parameters:
    diskformat: zeroedthick
    datastore: &quot;NFS&quot;
reclaimPolicy: Delete

kubectl aplay -f vmware-sc.yml
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install k8s with calico's eBPF mode]]></title>
            <link href="https://devopstales.github.io/home/k8s-calico-ebpf/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-ipvs/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with IPVS mode" />
                <link href="https://devopstales.github.io/kubernetes/k8s-calico-ebpf/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with calico&#39;s eBPF mode" />
                <link href="https://devopstales.github.io/home/k8s-install-openebs/?utm_source=atom_feed" rel="related" type="text/html" title="Install OpenEBS for Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-kubeadm-ha/?utm_source=atom_feed" rel="related" type="text/html" title="How to install kubernetes with kubeadm in HA mode" />
                <link href="https://devopstales.github.io/home/k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install kubernetes with kubeadm" />
            
                <id>https://devopstales.github.io/home/k8s-calico-ebpf/</id>
            
            
            <published>2020-10-13T00:00:00+00:00</published>
            <updated>2020-10-13T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I&rsquo;will show you how to install kubernetes Without kube-proxy using calico&rsquo;s eBPF mode.</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
<li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-contour-ingress --></li>
<li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
<li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
<li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... --></li>
<li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
<li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
<li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<h3 id="wthat-is-kube-proxy">Wthat is kube-proxy</h3>

<p>kube-proxy is a key component of any Kubernetes deployment.  Its role is to load-balance traffic to the pods. It listens to all the service requests coming through from kubernetes and creates entries in iptables for each of these service IPs to achieve proper routing to the pod. So kube-proxy adds iptables ruleset for each new service defined. As the number of services grow, this list is going to be huge. This potentially impact the performance because the iptables processing is sequential and wit every new line the list goes longer and longer. Kubernetes&rsquo;s solution for this problem was IPVS.</p>

<h3 id="what-is-ipvs">What is IPVS?</h3>

<p>IPVS (IP Virtual Server) is built on top of the Netfilter and implements transport-layer load balancing as part of the Linux kernel. It runs on a host and acts as a load balancer in front of a cluster of real servers. IPVS can direct requests for TCP- and UDP-based services to the real servers, and make services of the real servers appear as virtual services on a single IP address. Therefore, IPVS naturally supports Kubernetes Service. IPVS mode provides greater scale and performance vs iptables mode. However, it comes with some limitations. There is no option to install IPVS mode in kubeadm. <a href="../../cloud/k8s-ipvs/">In the previous post you can see ho you can do install.</a> The other solution is eBPF.</p>

<h3 id="what-is-ebpf">What is eBPF ?</h3>

<p>eBPF is a virtual machine embedded within the Linux kernel. It allows small programs to be loaded into the kernel, and attached to hooks, which are triggered when some event occurs. For example, when a network interface emits a packet.</p>

<h3 id="requirements">Requirements</h3>

<p>First you need a supported Linuy Distrubution:
* Ubuntu 20.04.
* Red Hat v8.2 with Linux kernel v4.18.0-193 or above (Red Hat have backported the required features to that build on CentOS 8 but not on 7)
* Pre installed K8s cluster</p>

<p>Verify that your cluster is ready for eBPF mode</p>

<pre><code>mount | grep &quot;/sys/fs/bpf&quot;
bpf on /sys/fs/bpf type bpf (rw,nosuid,nodev,noexec,relatime,mode=700)
</code></pre>

<h3 id="install-calico-with-operator">Install Calico With Operator</h3>

<p>We need to change the <code>cidr</code> in <code>custom-resources.yaml</code> to match to our clusters <code>cdir</code>.</p>

<pre><code>kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml

wget https://docs.projectcalico.org/manifests/custom-resources.yaml
nano custom-resources.yaml
...
cidr: 10.244.0.0/16


kubectl create -f custom-resources.yaml
watch kubectl get pods -n calico-system
</code></pre>

<h3 id="configure-calicoctl">Configure calicoctl</h3>

<pre><code>export CALICO_DATASTORE_TYPE=kubernetes
export CALICO_KUBECONFIG=~/.kube/config
calicoctl get workloadendpoints
calicoctl get nodes
</code></pre>

<p>Configure tigera-operator to communicate with kubernetes&rsquo;s api.</p>

<pre><code>nano kubernetes-services-endpoint.yaml
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kubernetes-services-endpoint
  namespace: tigera-operator
data:
  KUBERNETES_SERVICE_HOST: &quot;172.17.9.10&quot;
  KUBERNETES_SERVICE_PORT: &quot;6443&quot;

kubectl apply -f  kubernetes-services-endpoint.yaml
kubectl delete pod -n tigera-operator -l k8s-app=tigera-operator
watch kubectl get pods -n calico-system
</code></pre>

<h3 id="change-to-ebpf-mode">Change to eBPF mode</h3>

<p>First we change kube-proxy&rsquo;s <code>nodeSelector</code> to a none existing node to disable <code>kube-proxy</code>, the patch calico to run in eBPF mode.</p>

<pre><code>kubectl patch ds -n kube-system kube-proxy -p '{&quot;spec&quot;:{&quot;template&quot;:{&quot;spec&quot;:{&quot;nodeSelector&quot;:{&quot;non-calico&quot;: &quot;true&quot;}}}}}'
calicoctl patch felixconfiguration default --patch='{&quot;spec&quot;: {&quot;bpfKubeProxyIptablesCleanupEnabled&quot;: false}}'
calicoctl patch felixconfiguration default --patch='{&quot;spec&quot;: {&quot;bpfEnabled&quot;: true}}'
calicoctl patch felixconfiguration default --patch='{&quot;spec&quot;: {&quot;bpfExternalServiceMode&quot;: &quot;DSR&quot;}}'
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install k8s with IPVS mode]]></title>
            <link href="https://devopstales.github.io/home/k8s-ipvs/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-calico-ebpf/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with calico&#39;s eBPF mode" />
                <link href="https://devopstales.github.io/kubernetes/k8s-ipvs/?utm_source=atom_feed" rel="related" type="text/html" title="Install k8s with IPVS mode" />
                <link href="https://devopstales.github.io/home/k8s-install-openebs/?utm_source=atom_feed" rel="related" type="text/html" title="Install OpenEBS for Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-kubeadm-ha/?utm_source=atom_feed" rel="related" type="text/html" title="How to install kubernetes with kubeadm in HA mode" />
                <link href="https://devopstales.github.io/home/k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install kubernetes with kubeadm" />
            
                <id>https://devopstales.github.io/home/k8s-ipvs/</id>
            
            
            <published>2020-10-13T00:00:00+00:00</published>
            <updated>2020-10-13T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I&rsquo;will show you how to install kubernetes with kube-proxy IPVS mode.</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
<li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-contour-ingress --></li>
<li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
<li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
<li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... --></li>
<li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
<li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
<li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<h3 id="wthat-is-kube-proxy">Wthat is kube-proxy</h3>

<p>kube-proxy is a key component of any Kubernetes deployment.  Its role is to load-balance traffic to the pods. It listens to all the service requests coming through from kubernetes and creates entries in iptables for each of these service IPs to achieve proper routing to the pod. So kube-proxy adds iptables ruleset for each new service defined. As the number of services grow, this list is going to be huge. This potentially impact the performance because the iptables processing is sequential and wit every new line the list goes longer and longer. Kubernetes&rsquo;s solution for this problem was IPVS.</p>

<h3 id="what-is-ipvs">What is IPVS?</h3>

<p>IPVS (IP Virtual Server) is built on top of the Netfilter and implements transport-layer load balancing as part of the Linux kernel. It runs on a host and acts as a load balancer in front of a cluster of real servers. IPVS can direct requests for TCP- and UDP-based services to the real servers, and make services of the real servers appear as virtual services on a single IP address. Therefore, IPVS naturally supports Kubernetes Service. IPVS mode provides greater scale and performance vs iptables mode.</p>

<p>Installing Kubernetes with IPVS kube-proxy mode is a little bit hard because there in no built in option for theat in kubeadm. So we have two option. Createt a custom kubeadm.yaml or edit an installed cluster.</p>

<h3 id="install-requirements">Install Requirements</h3>

<pre><code>yum install ipset ipvsadm -y

cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF
chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4
</code></pre>

<h3 id="createt-a-custom-kubeadm-yaml">Createt a custom kubeadm.yaml</h3>

<pre><code>kubeadm config print init-defaults &gt; kubeadm.yaml

nano kubeadm.yaml
...
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: ipvs
</code></pre>

<h3 id="edit-running-cluster">Edit running cluster</h3>

<pre><code>kubectl edit configmap kube-proxy -n kube-system
...
mode: ipvs
</code></pre>

<pre><code>kubectl get po -n kube-system
kubectl delete po -n kube-system &lt;pod-name&gt;
</code></pre>

<pre><code>kubectl logs [kube-proxy pod] | grep &quot;Using ipvs Proxier&quot;
</code></pre>

<h3 id="test-ipvs-mode-is-running">Test IPVS mode is running</h3>

<pre><code>ipvsadm -Ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.96.0.1:443 rr
  -&gt; 1.1.1.101:6443               Masq    1      0          0
TCP  10.96.0.10:53 rr
  -&gt; 10.244.0.2:53                Masq    1      0          0
  -&gt; 10.244.2.8:53                Masq    1      0          0
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to setup Unifi Controller on Debian 10]]></title>
            <link href="https://devopstales.github.io/home/install-unifi-controller/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/install-unifi-controller/?utm_source=atom_feed" rel="related" type="text/html" title="How to setup Unifi Controller on Debian 10" />
                <link href="https://devopstales.github.io/home/pfsense-usg/?utm_source=atom_feed" rel="related" type="text/html" title="Pfsese USG S2S VPN" />
                <link href="https://devopstales.github.io/home/backup-and-retore-prometheus/?utm_source=atom_feed" rel="related" type="text/html" title="How to backup and restore Prometheus?" />
                <link href="https://devopstales.github.io/home/openshift-rbd-fsck/?utm_source=atom_feed" rel="related" type="text/html" title="How to fixing filesystem corruption on a Kubernetes Ceph RBD PersistentVolume" />
                <link href="https://devopstales.github.io/home/being-productive-with-kubectl/?utm_source=atom_feed" rel="related" type="text/html" title="Being Productive with K8S" />
            
                <id>https://devopstales.github.io/home/install-unifi-controller/</id>
            
            
            <published>2020-10-12T00:00:00+00:00</published>
            <updated>2020-10-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how to install Unifi Controller on Debian 10 Buster.</p>

<h3 id="install-requiremens">Install requiremens</h3>

<pre><code>apt install apt-transport-https ca-certificates wget dirmngr gnupg gnupg2 software-properties-common multiarch-support
</code></pre>

<h3 id="install-mongodb">Install MongoDB</h3>

<p>Unifi Controller requires a MongoDB from 2.4 to 4.0. The stander version of MongoDB to Debian 10 is 4.4 and 4.0-s requiremens conflicts with the Debians main libs. So we need to install MongoDB 3.4.</p>

<pre><code>wget -qO - https://www.mongodb.org/static/pgp/server-3.4.asc |  apt-key add -
echo &quot;deb http://repo.mongodb.org/apt/debian jessie/mongodb-org/3.4 main&quot; | tee /etc/apt/sources.list.d/mongodb-org-3.4.list
</code></pre>

<p>We need an old version of libssl to run MongoDB 3.4:</p>

<pre><code>wget http://security.debian.org/debian-security/pool/updates/main/o/openssl/libssl1.0.0_1.0.1t-1+deb8u12_amd64.deb
dpkg -i libssl1.0.0_1.0.1t-1+deb8u12_amd64.deb
</code></pre>

<h3 id="install-java-8">Install Java 8</h3>

<pre><code>wget -qO - https://adoptopenjdk.jfrog.io/adoptopenjdk/api/gpg/key/public | sudo apt-key add -
sudo add-apt-repository --yes https://adoptopenjdk.jfrog.io/adoptopenjdk/deb/

apt update
apt install adoptopenjdk-8-hotspot

java -version

nano /etc/profile
export JAVA_HOME=&quot;/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64&quot;
source /etc/profile

echo $JAVA_HOME
</code></pre>

<h3 id="install-unifi-controller">Install Unifi Controller</h3>

<pre><code>apt-key adv --keyserver keyserver.ubuntu.com --recv 06E85760C0A52C50
echo 'deb https://www.ui.com/downloads/unifi/debian stable ubiquiti' | tee /etc/apt/sources.list.d/100-ubnt-unifi.list
apt update &amp;&amp; apt install unifi
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to backup and restore Prometheus?]]></title>
            <link href="https://devopstales.github.io/home/backup-and-retore-prometheus/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/backup-and-retore-prometheus/?utm_source=atom_feed" rel="related" type="text/html" title="How to backup and restore Prometheus?" />
                <link href="https://devopstales.github.io/home/openshift-rbd-fsck/?utm_source=atom_feed" rel="related" type="text/html" title="How to fixing filesystem corruption on a Kubernetes Ceph RBD PersistentVolume" />
                <link href="https://devopstales.github.io/home/being-productive-with-kubectl/?utm_source=atom_feed" rel="related" type="text/html" title="Being Productive with K8S" />
                <link href="https://devopstales.github.io/home/k3s-helm-controller/?utm_source=atom_feed" rel="related" type="text/html" title="Install and use rancher helm-controller" />
                <link href="https://devopstales.github.io/home/k3s-crio/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with CRI-O and kadalu" />
            
                <id>https://devopstales.github.io/home/backup-and-retore-prometheus/</id>
            
            
            <published>2020-10-10T00:00:00+00:00</published>
            <updated>2020-10-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how to take a backup from a running Prometheus and restore it.</p>

<p>I will assumes that you have a running Prometheus deployed with <code>prometheus-operator</code> in the <code>monitoring</code> namespace.</p>

<h2 id="enable-admin-api">Enable Admin Api</h2>

<p>First we need to enable the Prometheus&rsquo;s admin api</p>

<pre><code>kubectl -n monitoring patch prometheus prometheus-operator-prometheus \
  --type merge --patch '{&quot;spec&quot;:{&quot;enableAdminAPI&quot;:true}}'
</code></pre>

<p>In <code>tmux</code> or a separate window open a port forward to the admin api.</p>

<pre><code>ubectl -n monitoring port-forward svc/prometheus-operator-prometheus 9090
</code></pre>

<h3 id="backup-prometheus-data">Backup Prometheus data</h3>

<p>Run following command to create a snapshot:</p>

<pre><code>curl -XPOST http://localhost:9090/api/v2/admin/tsdb/snapshot
{&quot;name&quot;:&quot;20200731T123913Z-6e661e92759805f5&quot;}
</code></pre>

<p>Find the snapshot and copy it to locally. The default folder is <code>/prometheus/snapshots/</code> but you can find the data folder by finding the <code>--storage.tsdb.path</code> config in your deployment.</p>

<pre><code>kubectl -n monitoring exec -it prometheus-prometheus-operator-prometheus-0 \
  -c prometheus -- /bin/sh -c \
  &quot;ls /prometheus/snapshots/20200731T123913Z-6e661e92759805f5&quot;
01EE25G1ZTKBFBBHFPHNBF99KJ  01EEFF7TE5ENDAGDR5K7ERW3BX
...

kubectl cp -n monitoring \
  prometheus-prometheus-operator-prometheus-0:/prometheus/snapshots/20200731T123913Z-6e661e92759805f5 \
  -c prometheus .
</code></pre>

<h3 id="restore-prometheus-data">Restore Prometheus data</h3>

<p>In a new Prometheus instance delete the data folder and copy the content of the snapshot:</p>

<pre><code>kubectl -n newprom exec -it prometheus -- /bin/sh -c &quot;rm -rf /prometheus/*&quot;

kubectl -n newprom cp ./* prometheus:/prometheus/
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to fixing filesystem corruption on a Kubernetes Ceph RBD PersistentVolume]]></title>
            <link href="https://devopstales.github.io/home/openshift-rbd-fsck/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/openshift-rbd-fsck/?utm_source=atom_feed" rel="related" type="text/html" title="How to fixing filesystem corruption on a Kubernetes Ceph RBD PersistentVolume" />
                <link href="https://devopstales.github.io/home/k3s-helm-controller/?utm_source=atom_feed" rel="related" type="text/html" title="Install and use rancher helm-controller" />
                <link href="https://devopstales.github.io/home/k3s-crio/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with CRI-O and kadalu" />
                <link href="https://devopstales.github.io/home/k8s-crio/?utm_source=atom_feed" rel="related" type="text/html" title="Install K8S with CRI-O and kadalu" />
                <link href="https://devopstales.github.io/home/k8s-metallb-bgp-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Self-hosted Load Balancer for bare metal Kubernetes" />
            
                <id>https://devopstales.github.io/home/openshift-rbd-fsck/</id>
            
            
            <published>2020-10-10T00:00:00+00:00</published>
            <updated>2020-10-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this tutorial I will show you how to fix a corruptid filesystem on Ceph RBD PersistentVolume uyed by Kubernetes.</p>

<pre><code>oc describe po gitlab-ce-1-wl9wf
...
Events:
  Type     Reason                  Age               From                                           Message
  ----     ------                  ----              ----                                           -------
  Normal   Scheduled               27s               default-scheduler                              Successfully assigned gitlab-prod/gitlab-ce-1-j7lph to k8sw09
  Normal   SuccessfulAttachVolume  27s               attachdetach-controller                        AttachVolume.Attach succeeded for volume &quot;pvc-e27f498e-85cf-11e9-af1a-66934f1af826&quot;
  Warning  FailedMount             2s (x6 over 19s)  kubelet, k8sw09  MountVolume.MountDevice failed for volume &quot;pvc-e27f498e-85cf-11e9-af1a-66934f1af826&quot; : rbd: failed to mount device /dev/rbd3 at /var/lib/origin/openshift.local.volumes/plugins/kubernetes.io/rbd/mounts/k8s-rbd-image-kubernetes-dynamic-pvc-e3042618-85cf-11e9-8762-aa9884bf6706 (fstype: ), error 'fsck' found errors on device /dev/rbd3 but could not correct them: fsck from util-linux 2.23.2
/dev/rbd3: Superblock needs_recovery flag is clear, but journal has data.
/dev/rbd3: Run journal anyway

/dev/rbd3: UNEXPECTED INCONSISTENCY; RUN fsck MANUALLY.
  (i.e., without -a or -p options)
</code></pre>

<p>Check the log on the worker. In my case this is k8sw09.</p>

<pre><code>journalctl -u kubelet


jan 08 15:44:58 k8sw09 origin-node[14927]: I0108 15:44:58.251201   14927 reconciler.go:252] operationExecutor.MountVolume started for volume &quot;pvc-e27f498e-85cf-11e9-af1a-66934f1af826&quot; (UniqueName: &quot;kubernetes.io/rbd/k8s-rbd:kubernetes-dynamic-pvc-e3042618-85cf-11e9-8762-aa9884bf6706&quot;) pod &quot;gitlab-ce-1-j7lph&quot; (UID: &quot;69151c2c-3223-11ea-9bcf-aa9884bf6706&quot;)
jan 08 15:44:58 k8sw09 origin-node[14927]: I0108 15:44:58.251299   14927 operation_generator.go:489] MountVolume.WaitForAttach entering for volume &quot;pvc-e27f498e-85cf-11e9-af1a-66934f1af826&quot; (UniqueName: &quot;kubernetes.io/rbd/k8s-rbd:kubernetes-dynamic-pvc-e3042618-85cf-11e9-8762-aa9884bf6706&quot;) pod &quot;gitlab-ce-1-j7lph&quot; (UID: &quot;69151c2c-3223-11ea-9bcf-aa9884bf6706&quot;) DevicePath &quot;&quot;
jan 08 15:44:58 k8sw09 origin-node[14927]: I0108 15:44:58.451965   14927 operation_generator.go:498] MountVolume.WaitForAttach succeeded for volume &quot;pvc-e27f498e-85cf-11e9-af1a-66934f1af826&quot; (UniqueName: &quot;kubernetes.io/rbd/k8s-rbd:kubernetes-dynamic-pvc-e3042618-85cf-11e9-8762-aa9884bf6706&quot;) pod &quot;gitlab-ce-1-j7lph&quot; (UID: &quot;69151c2c-3223-11ea-9bcf-aa9884bf6706&quot;) DevicePath &quot;/dev/rbd3&quot;
jan 08 15:44:58 k8sw09 origin-node[14927]: E0108 15:44:58.498052   14927 nestedpendingoperations.go:267] Operation for &quot;\&quot;kubernetes.io/rbd/k8s-rbd:kubernetes-dynamic-pvc-e3042618-85cf-11e9-8762-aa9884bf6706&quot;&quot; failed. No retries permitted until 2020-01-08 15:47:00.498014981 +0100 CET m=+619493.508747496 (durationBeforeRetry 2m2s). Error: &quot;MountVolume.MountDevice failed for volume \&quot;pvc-e27f498e-85cf-11e9-af1a-66934f1af826\&quot; (UniqueName: \&quot;kubernetes.io/rbd/k8s-rbd:kubernetes-dynamic-pvc-e3042618-85cf-11e9-8762-aa9884bf6706\&quot;) pod \&quot;gitlab-ce-1-j7lph\&quot; (UID: \&quot;69151c2c-3223-11ea-9bcf-aa9884bf6706\&quot;) : rbd: failed to mount device /dev/rbd3 at /var/lib/origin/openshift.local.volumes/plugins/kubernetes.io/rbd/mounts/k8s-rbd-image-kubernetes-dynamic-pvc-e3042618-85cf-11e9-8762-aa9884bf6706 (fstype: ), error 'fsck' found errors on device /dev/rbd3 but could not correct them: fsck from util-linux 2.23.2\n/dev/rbd3: Superblock needs_recovery flag is clear, but journal has data.\n/dev/rbd3: Run journal anyway\n\n/dev/rbd3: UNEXPECTED INCONSISTENCY; RUN fsck MANUALLY.\n\t(i.e., without -a or -p options)\n.&quot;
</code></pre>

<p>We can see the problem is with <code>/dev/rbd3</code>. First check thi is the block device user for <code>pvc-e3042618-85cf-11e9-8762-aa9884bf6706</code> PersistenVolume.</p>

<pre><code>sudo rbd showmapped | grep pvc-e3042618-85cf-11e9-8762-aa9884bf6706
3  k8s-rbd kubernetes-dynamic-pvc-e3042618-85cf-11e9-8762-aa9884bf6706 -    /dev/rbd3
</code></pre>

<p>So let&rsquo;s try to use <code>fsck</code> on this disk.</p>

<pre><code>sudo rbd unmap /dev/rbd3


sudo fsck -fv /dev/rbd3
fsck from util-linux 2.27.1
e2fsck 1.42.13 (17-May-2015)
Pass 1: Checking inodes, blocks, and sizes
Pass 2: Checking directory structure
Pass 3: Checking directory connectivity
Pass 4: Checking reference counts
Unattached inode 303
Connect to /lost+found&lt;y&gt;? yes
Inode 303 ref count is 2, should be 1.  Fix&lt;y&gt;? yes
Pass 5: Checking group summary information
Block bitmap differences:  -(71680--73727) -(94208--95231)
Fix&lt;y&gt;? yes

/dev/rbd3: ***** FILE SYSTEM WAS MODIFIED *****

         326 inodes used (0.50%, out of 65536)
          35 non-contiguous files (10.7%)
           0 non-contiguous directories (0.0%)
             # of inodes with ind/dind/tind blocks: 0/0/0
             Extent depth histogram: 311/7
       63642 blocks used (24.28%, out of 262144)
           0 bad blocks
           1 large file

         308 regular files
           9 directories
           0 character device files
           0 block device files
           0 fifos
           1 link
           0 symbolic links (0 fast symbolic links)
           0 sockets
------------
         317 files
</code></pre>

<p>Then our pod is running again!</p>

<pre><code>oc get po
NAME                        READY     STATUS    RESTARTS   AGE
gitlab-ce-1-j7lph           1/1       Running   0          28m
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Being Productive with K8S]]></title>
            <link href="https://devopstales.github.io/home/being-productive-with-kubectl/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/being-productive-with-kubectl/?utm_source=atom_feed" rel="related" type="text/html" title="Being Productive with K8S" />
                <link href="https://devopstales.github.io/home/k3s-helm-controller/?utm_source=atom_feed" rel="related" type="text/html" title="Install and use rancher helm-controller" />
                <link href="https://devopstales.github.io/home/k8s-metallb-bgp-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Self-hosted Load Balancer for bare metal Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-longhorn/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes project longhorn" />
                <link href="https://devopstales.github.io/home/k8s-gangway/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak" />
            
                <id>https://devopstales.github.io/home/being-productive-with-kubectl/</id>
            
            
            <published>2020-10-09T00:00:00+00:00</published>
            <updated>2020-10-09T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you my productivity tips with kubectl.</p>

<h3 id="bash-aliases-for-kubectl">Bash aliases for kubectl</h3>

<p>I have above aliases setup in the <code>~/.bashrc</code> file.</p>

<pre><code>alias k=kubectl
alias kg='kubectl get'
alias kd='kubectl describe'
alias kdel='kubectl delete'
alias kdelf='kubectl delete -f'
alias kaf='kubectl apply -f'
alias keti='kubectl exec -ti'
alias kgds='kubectl get DaemonSet'
alias kgdsy='kubectl get DaemonSet -oyaml'
alias kdds='kubectl describe DaemonSet'
alias kdelds='kubectl delete DaemonSet'
alias kgp='kubectl get pods'
alias kgpy='kubectl get pods -oyaml'
alias kgpa='kubectl get pods --all-namespaces'
alias kgpw='kgp --watch'
alias kgpwide='kgp -o wide'
alias kdp='kubectl describe pods'
alias kdelp='kubectl delete pods'
# get pod by label: kgpl &quot;app=myapp&quot; -n myns
alias kgpl='kgp -l'
# get pod by namespace: kgpn kube-system&quot;
alias kgpn='kgp -n'
alias kgs='kubectl get svc'
alias kgsa='kubectl get svc --all-namespaces'
alias kgsw='kgs --watch'
alias kgswide='kgs -o wide'
alias kes='kubectl edit svc'
alias kds='kubectl describe svc'
alias kdels='kubectl delete svc'
alias kgi='kubectl get ingress'
alias kgia='kubectl get ingress --all-namespaces'
alias kei='kubectl edit ingress'
alias kdi='kubectl describe ingress'
alias kdeli='kubectl delete ingress'
alias kgns='kubectl get namespaces'
alias kens='kubectl edit namespace'
alias kdns='kubectl describe namespace'
alias kdelns='kubectl delete namespace'
alias kcn='kubectl config set-context $(kubectl config current-context) --namespace'
alias kgcm='kubectl get configmaps'
alias kgcmy='kubectl get configmaps -oyaml'
alias kgcma='kubectl get configmaps --all-namespaces'
alias kecm='kubectl edit configmap'
alias kdcm='kubectl describe configmap'
alias kdelcm='kubectl delete configmap'
alias kgsec='kubectl get secret'
alias kgseca='kubectl get secret --all-namespaces'
alias kdsec='kubectl describe secret'
alias kdelsec='kubectl delete secret'
alias kgss='kubectl get statefulset'
alias kgssa='kubectl get statefulset --all-namespaces'
alias kgssw='kgss --watch'
alias kgsswide='kgss -o wide'
alias kess='kubectl edit statefulset'
alias kdss='kubectl describe statefulset'
alias kdelss='kubectl delete statefulset'
alias ksss='kubectl scale statefulset'
alias krsss='kubectl rollout status statefulset'
alias kga='kubectl get all'
alias kgaa='kubectl get all --all-namespaces'
alias kl='kubectl logs'
alias kl1h='kubectl logs --since 1h'
alias kl1m='kubectl logs --since 1m'
alias kl1s='kubectl logs --since 1s'
alias klf='kubectl logs -f'
alias klf1h='kubectl logs --since 1h -f'
alias klf1m='kubectl logs --since 1m -f'
alias klf1s='kubectl logs --since 1s -f'
alias kcp='kubectl cp'
alias kgno='kubectl get nodes'
alias keno='kubectl edit node'
alias kdno='kubectl describe node'
alias kdelno='kubectl delete node'
alias kgpvc='kubectl get pvc'
alias kgpvca='kubectl get pvc --all-namespaces'
alias kgpvcw='kgpvc --watch'
alias kepvc='kubectl edit pvc'
alias kdpvc='kubectl describe pvc'
alias kdelpvc='kubectl delete pvc'
alias kgpv='kubectl get pv'
alias kgsc='kubectl get storageclass'
# custom resource
alias kgir='kubectl get IngressRoute'
alias kgira='kubectl get IngressRoute --all-namespaces'
alias keir='kubectl edit IngressRoute'
alias kdir='kubectl describe IngressRoute'
alias kdelir='kubectl delete IngressRoute'
</code></pre>

<h3 id="kubens-kubectx">kubens kubectx</h3>

<pre><code>sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx
sudo ln -s /opt/kubectx/kubens /usr/local/bin/kubens
echo &quot;PATH=$PATH:/usr/local/bin/&quot; &gt;&gt; /etc/profile
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install and use rancher helm-controller]]></title>
            <link href="https://devopstales.github.io/home/k3s-helm-controller/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k3s-helm-controller/?utm_source=atom_feed" rel="related" type="text/html" title="Install and use rancher helm-controller" />
                <link href="https://devopstales.github.io/home/k3s-crio/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with CRI-O and kadalu" />
                <link href="https://devopstales.github.io/home/k3s-fcos/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S on Fedora CoreOS" />
                <link href="https://devopstales.github.io/home/k8s-metallb-bgp-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Self-hosted Load Balancer for bare metal Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-longhorn/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes project longhorn" />
            
                <id>https://devopstales.github.io/home/k3s-helm-controller/</id>
            
            
            <published>2020-09-20T00:00:00+00:00</published>
            <updated>2020-09-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>K3S comes with a Helm operator called Helm Controller. Helm Controller defines a new HelmChart custom resource definition, or CRD, for managing Helm charts.</p>

<h3 id="parst-of-the-k3s-series">Parst of the K3S series</h3>

<ul>
<li>Part1a: <a href="../../kubernetes/k3s-etcd-kube-vip/">Install K3S with k3sup and kube-vip</a></li>
<li>Part1b: <a href="../../kubernetes/k3s-crio/">Install K3S with CRI-O</a></li>
<li>Part1c: <a href="../../kubernetes/k3s-fcos/">Install K3S on Fedora CoreOS</a>
<!-- * K3S falnnel wiregard - https://medium.com/@bestpractices/k3s-with-flannel-wireguard-backend-fa433065a401 --></li>
<li>Part2b: <a href="../../kubernetes/k3s-calico/">Install K3S with k3sup and Calico</a></li>
<li>Part2c: <a href="../../kubernetes/k3s-cilium/">Install K3S with k3sup and Cilium</a></li>
<li>Part3: <a href="../../kubernetes/k3s-helm-controller/">K3S helm CR</a>
<!-- * K3D --></li>
<li>Part5: <a href="../../kk3s-gvisor/">Secure k3s with gVisor</a></li>
<li>Part6: <a href="../../kk3s-cert/">Kubernetes Certificate Rotation</a></li>
</ul>

<h3 id="install-helm-controller-k8s">Install helm-controller K8S</h3>

<p>Thanks to Francisco Bobadilla who created RBAC for Rancher&rsquo;s helm-controller we can deploy this solution to a stander K8S cluster.</p>

<pre><code>kubectl apply -f https://raw.githubusercontent.com/iotops/helm-controller/master/manifests/deploy-namespaced.yaml
</code></pre>

<pre><code>kubectl api-resources --api-group=helm.cattle.io
NAME         SHORTNAMES   APIGROUP         NAMESPACED   KIND
helmcharts                helm.cattle.io   true         HelmChart
</code></pre>

<h3 id="deploy-nginx-ingress-controller">Deploy Nginx ingress Controller</h3>

<pre><code>nano ginx-ingress.yaml
---
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: nginx-ingress
  namespace: helm-controller
spec:
  chart: stable/nginx-ingress
  targetNamespace: nginx-ingress
  valuesContent: |-
    rbac:
      create: &quot;true&quot;
    controller:

      kind: DaemonSet
      hostNetwork: &quot;true&quot;
      daemonset:
        useHostPort: &quot;true&quot;
      service:
        type: &quot;NodePort&quot;
</code></pre>

<p>On a K3S cluster the <code>namespace:</code> must be <code>kube-system</code> because k3S deploys the helm-controller to that namespace.</p>

<h3 id="auto-deploying-manifests">Auto-Deploying Manifests</h3>

<p>Any file found in <code>/var/lib/rancher/k3s/server/manifests</code> will automatically be deployed to Kubernetes in a manner similar to <code>kubectl apply</code>.</p>

<pre><code>cp nginx-ingress.yaml /var/lib/rancher/k3s/server/manifests/
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install K3S with CRI-O and kadalu]]></title>
            <link href="https://devopstales.github.io/home/k3s-crio/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k3s-crio/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S with CRI-O and kadalu" />
                <link href="https://devopstales.github.io/home/k8s-crio/?utm_source=atom_feed" rel="related" type="text/html" title="Install K8S with CRI-O and kadalu" />
                <link href="https://devopstales.github.io/home/k3s-fcos/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S on Fedora CoreOS" />
                <link href="https://devopstales.github.io/kubernetes/k8s-crio/?utm_source=atom_feed" rel="related" type="text/html" title="Install K8S with CRI-O and kadalu" />
                <link href="https://devopstales.github.io/home/k8s-metallb-bgp-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Self-hosted Load Balancer for bare metal Kubernetes" />
            
                <id>https://devopstales.github.io/home/k3s-crio/</id>
            
            
            <published>2020-09-10T00:00:00+00:00</published>
            <updated>2020-09-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how to install cri-o container runtime and initialize a Kubernetes.</p>

<h3 id="parst-of-the-k3s-series">Parst of the K3S series</h3>

<ul>
<li>Part1a: <a href="../../kubernetes/k3s-etcd-kube-vip/">Install K3S with k3sup and kube-vip</a></li>
<li>Part1b: <a href="../../kubernetes/k3s-crio/">Install K3S with CRI-O</a></li>
<li>Part1c: <a href="../../kubernetes/k3s-fcos/">Install K3S on Fedora CoreOS</a>
<!-- * K3S falnnel wiregard - https://medium.com/@bestpractices/k3s-with-flannel-wireguard-backend-fa433065a401 --></li>
<li>Part2b: <a href="../../kubernetes/k3s-calico/">Install K3S with k3sup and Calico</a></li>
<li>Part2c: <a href="../../kubernetes/k3s-cilium/">Install K3S with k3sup and Cilium</a></li>
<li>Part3: <a href="../../kubernetes/k3s-helm-controller/">K3S helm CR</a>
<!-- * K3D --></li>
<li>Part5: <a href="../../kk3s-gvisor/">Secure k3s with gVisor</a></li>
<li>Part6: <a href="../../kk3s-cert/">Kubernetes Certificate Rotation</a></li>
</ul>

<h3 id="install-cri-o-instad-of-docker">Install CRI-O instad of Docker</h3>

<pre><code>VERSION=1.18
sudo curl -L -o /etc/yum.repos.d/devel_kubic_libcontainers_stable.repo https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable/CentOS_7/devel:kubic:libcontainers:stable.repo
sudo curl -L -o /etc/yum.repos.d/devel_kubic_libcontainers_stable_cri-o_${VERSION}.repo https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:${VERSION}/CentOS_7/devel:kubic:libcontainers:stable:cri-o:${VERSION}.repo

yum install cri-o
</code></pre>

<h3 id="configure">Configure</h3>

<pre><code>modprobe overlay
modprobe br_netfilter

cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

sysctl --system
</code></pre>

<pre><code>free -h
swapoff -a
swapoff -a
sed -i.bak -r 's/(.+ swap .+)/#\1/' /etc/fstab
free -h
</code></pre>

<p>You nee the same cgroup manager in cri-o and kubeadm. The default for kubeadm is cgroupfs and for cri-o the default is systemd. In this example I configured cri-o for cgroupfs.</p>

<pre><code>nano /etc/crio/crio.conf
cgroup_manager = &quot;cgroupfs&quot;
...
registries = [
  &quot;quay.io&quot;,
  &quot;docker.io&quot;
]
</code></pre>

<p>Disable ipv6 and configure cri-o CNI confg for flanel&rsquo;s network:</p>

<pre><code>echo &quot;net.ipv6.conf.all.disable_ipv6 = 1&quot; &gt;&gt; /etc/sysctl.conf
echo &quot;net.ipv6.conf.default.disable_ipv6 = 1&quot; &gt;&gt; /etc/sysctl.conf
sysctl -p

sed -i &quot;s|::1|#::1|&quot; /etc/hosts

nano /etc/cni/net.d/100-crio-bridge.conf
{
...
&quot;ipam&quot;: {
    &quot;type&quot;: &quot;host-local&quot;,
    &quot;routes&quot;: [
        { &quot;dst&quot;: &quot;0.0.0.0/0&quot; }
    ],

    &quot;ranges&quot;: [
        [{ &quot;subnet&quot;: &quot;10.244.0.0/16&quot; }]
    ]
}
}
</code></pre>

<pre><code>systemctl enable --now cri-o

echo &quot;export PATH=$PATH:/usr/local/bin/&quot; &gt;&gt; /etc/profile
echo &quot;export KUBECONFIG=/etc/rancher/k3s/k3s.yaml&quot; &gt;&gt; /etc/profile
source /etc/profile

yum install -y container-selinux selinux-policy-base
rpm -i https://rpm.rancher.io/k3s-selinux-0.1.1-rc1.el7.noarch.rpm
</code></pre>

<pre><code>export K3S_KUBECONFIG_MODE=&quot;644&quot;
export INSTALL_K3S_EXEC=&quot; --container-runtime-endpoint /var/run/crio/crio.sock --no-deploy servicelb --no-deploy traefik&quot;

curl -sfL https://get.k3s.io | sh -
</code></pre>

<pre><code>systemctl status k3s

crictl info
crictl ps
kubectl get node -o wide
kubectl get pods -A -o wide
</code></pre>

<h3 id="install-tools">Install tools</h3>

<pre><code>yum install git -y

sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubectx /usr/local/sbin/kubectx
sudo ln -s /opt/kubectx/kubens /usr/local/sbin/kubens
COMPDIR=$(pkg-config --variable=completionsdir bash-completion)
ln -sf /opt/kubectx/completion/kubens.bash $COMPDIR/kubens
ln -sf /opt/kubectx/completion/kubectx.bash $COMPDIR/kubectx
</code></pre>

<h3 id="deploy-kadalu-storage">Deploy kadalu storage</h3>

<pre><code>sudo wipefs -a -t dos -f /dev/sdb
sudo mkfs.xfs /dev/sdb

yum install python3-pip -y
sudo pip3 install kubectl-kadalu

echo &quot;export PATH=$PATH:/usr/local/bin/&quot; &gt;&gt; /etc/profile
source /etc/profile

kubectl kadalu install

# k8s.mydomain.intra is the nod name in Kubernetes
# /dev/sdb is the disk

kubectl kadalu storage-add storage-pool-1 \
    --device k8s.mydomain.intra:/dev/sdb

# to delete object if you misconfigured kadalu
kubectl delete kadalustorages.kadalu-operator.storage storage-pool-1


kubectl get pods -n kadalu

kubectl patch storageclass kadalu.replica1 -p '{&quot;metadata&quot;: {&quot;annotations&quot;:{&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;true&quot;}}}'
</code></pre>

<pre><code>nano test-pvc.yaml
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pv1
spec:
  storageClassName: kadalu.replica1
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install K3S on Fedora CoreOS]]></title>
            <link href="https://devopstales.github.io/home/k3s-fcos/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k3s-fcos/?utm_source=atom_feed" rel="related" type="text/html" title="Install K3S on Fedora CoreOS" />
                <link href="https://devopstales.github.io/home/fcos-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Fedora CoreOS as a VM" />
                <link href="https://devopstales.github.io/home/k8s-longhorn/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes project longhorn" />
                <link href="https://devopstales.github.io/home/k8s-crio/?utm_source=atom_feed" rel="related" type="text/html" title="Install K8S with CRI-O and kadalu" />
                <link href="https://devopstales.github.io/cloud/fcos-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Fedora CoreOS as a VM" />
            
                <id>https://devopstales.github.io/home/k3s-fcos/</id>
            
            
            <published>2020-09-05T00:00:00+00:00</published>
            <updated>2020-09-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can Install K3S on Fedora CoreOS(FCOS) in virtualization environment.</p>

<h3 id="parst-of-the-k3s-series">Parst of the K3S series</h3>

<ul>
<li>Part1a: <a href="../../kubernetes/k3s-etcd-kube-vip/">Install K3S with k3sup and kube-vip</a></li>
<li>Part1b: <a href="../../kubernetes/k3s-crio/">Install K3S with CRI-O</a></li>
<li>Part1c: <a href="../../kubernetes/k3s-fcos/">Install K3S on Fedora CoreOS</a>
<!-- * K3S falnnel wiregard - https://medium.com/@bestpractices/k3s-with-flannel-wireguard-backend-fa433065a401 --></li>
<li>Part2b: <a href="../../kubernetes/k3s-calico/">Install K3S with k3sup and Calico</a></li>
<li>Part2c: <a href="../../kubernetes/k3s-cilium/">Install K3S with k3sup and Cilium</a></li>
<li>Part3: <a href="../../kubernetes/k3s-helm-controller/">K3S helm CR</a>
<!-- * K3D --></li>
<li>Part5: <a href="../../kk3s-gvisor/">Secure k3s with gVisor</a></li>
<li>Part6: <a href="../../kk3s-cert/">Kubernetes Certificate Rotation</a></li>
</ul>

<h3 id="what-is-k3s">What is K3S</h3>

<p>K3S is a lightweight certified kubernetes distribution. Designed to be a single binary of less than 40MB. It didn&rsquo;t use etcd instad stora it&rsquo;s data in sqlite.</p>

<h3 id="install-requirements">Install requirements</h3>

<pre><code>sudo -i
rpm-ostree install https://rpm.rancher.io/k3s-selinux-0.1.1-rc1.el7.noarch.rpm
systemctl reboot
</code></pre>

<h3 id="install-k3s">Install K3S</h3>

<pre><code>sudo -i
export K3S_KUBECONFIG_MODE=&quot;644&quot;
export INSTALL_K3S_EXEC=&quot; --no-deploy servicelb --no-deploy traefik&quot;

curl -sfL https://get.k3s.io | sh -

systemctl status k3s
kubectl get nodes -o wide
kubectl get pods -A -o wide
</code></pre>

<h3 id="join-other-nodes">Join other nodes</h3>

<p>First we need the join token:</p>

<pre><code>sudo cat /var/lib/rancher/k3s/server/node-token
K1042e2f8e353b9409472c1e0cca8457abe184dc7be3f0805109e92c50c193ceb42::node:c83acbf89a7de7026d6f6928dc270028
</code></pre>

<p>The join the worker nodes:</p>

<pre><code>export K3S_KUBECONFIG_MODE=&quot;644&quot;
export K3S_URL=&quot;https://k3s-master:6443&quot;
export K3S_TOKEN=&quot;K1042e2f8e353b9409472c1e0cca8457abe184dc7be3f0805109e92c50c193ceb42::node:c83acbf89a7de7026d6f6928dc270028&quot;

curl -sfL https://get.k3s.io | sh -
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install K8S with CRI-O and kadalu]]></title>
            <link href="https://devopstales.github.io/home/k8s-crio/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-crio/?utm_source=atom_feed" rel="related" type="text/html" title="Install K8S with CRI-O and kadalu" />
                <link href="https://devopstales.github.io/home/k8s-metallb-bgp-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Self-hosted Load Balancer for bare metal Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-install-openebs/?utm_source=atom_feed" rel="related" type="text/html" title="Install OpenEBS for Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-kubeadm-ha/?utm_source=atom_feed" rel="related" type="text/html" title="How to install kubernetes with kubeadm in HA mode" />
                <link href="https://devopstales.github.io/home/k8s-local-pv/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
            
                <id>https://devopstales.github.io/home/k8s-crio/</id>
            
            
            <published>2020-09-04T00:00:00+00:00</published>
            <updated>2020-09-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how to install cri-o container runtime and initialize a Kubernetes.</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
<li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-contour-ingress --></li>
<li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
<li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
<li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... --></li>
<li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
<li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
<li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<h3 id="what-is-cri-o">What is CRI-O?</h3>

<p>The Kubernetes project has defined a number of standards. One of them is cri. The Container Runtime Interface. This interface defines how Kubernetes talks with a high-level container runtime. CRI-O is an implementation of the Kubernetes CRI to enable using OCI (Open Container Initiative) compatible runtimes. It is a lightweight alternative of Docker as the runtime for kubernetes. t allows Kubernetes to use any OCI-compliant runtime as the container runtime for running pods. Today it supports runc and Kata Containers as the container runtimes but any OCI-conformant runtime can be plugged in principle.</p>

<h3 id="install-cri-o-instad-of-docker">Install CRI-O instad of Docker</h3>

<pre><code>VERSION=1.18
sudo curl -L -o /etc/yum.repos.d/devel_kubic_libcontainers_stable.repo https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable/CentOS_7/devel:kubic:libcontainers:stable.repo
sudo curl -L -o /etc/yum.repos.d/devel_kubic_libcontainers_stable_cri-o_${VERSION}.repo https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:${VERSION}/CentOS_7/devel:kubic:libcontainers:stable:cri-o:${VERSION}.repo

yum install cri-o
</code></pre>

<h3 id="configure">Configure</h3>

<pre><code>modprobe overlay
modprobe br_netfilter

cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

sysctl --system
</code></pre>

<pre><code>free -h
swapoff -a
swapoff -a
sed -i.bak -r 's/(.+ swap .+)/#\1/' /etc/fstab
free -h
</code></pre>

<p>You nee the same cgroup manager in cri-o and kubeadm. The default for kubeadm is cgroupfs and for cri-o the default is systemd. In this example I configured cri-o for cgroupfs.</p>

<pre><code>nano /etc/crio/crio.conf
cgroup_manager = &quot;cgroupfs&quot;

nano /etc/containers/registries.conf
registries = [
  &quot;quay.io&quot;,
  &quot;docker.io&quot;
]
</code></pre>

<p>If yo want to use systemd:</p>

<pre><code>echo &quot;KUBELET_EXTRA_ARGS=--cgroup-driver=systemd&quot; | tee /etc/sysconfig/kubelet
</code></pre>

<h3 id="install-kubernets">Install kubernets</h3>

<pre><code>cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

CRIP_VERSION=$(crio --version | awk '{print $3}')
yum install kubelet-$CRIP_VERSION kubeadm-$CRIP_VERSION kubectl-$CRIP_VERSION -y
</code></pre>

<pre><code>IP=172.17.9.10
# for multi interface configuration
echo 'KUBELET_EXTRA_ARGS=&quot;--node-ip='$IP' --cgroup-driver=systemd&quot;' &gt; /etc/sysconfig/kubelet

systemctl enable kubelet.service
systemctl enable --now cri-o
kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=$IP --cri-socket=unix:///var/run/crio/crio.sock

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config


crictl info
kubectl get node -o wide
kubectl get po --all-namespaces
</code></pre>

<h3 id="inincialize-network">Inincialize network</h3>

<pre><code>wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
kubectl aplly -f kube-flannel.yml
</code></pre>

<pre><code>kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml
kubectl create -f /vagrant/scripts/custom-resources.yaml
</code></pre>

<h3 id="install-tools">Install tools</h3>

<pre><code>yum install git -y

sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubectx /usr/local/sbin/kubectx
sudo ln -s /opt/kubectx/kubens /usr/local/sbin/kubens
</code></pre>

<h3 id="deploy-kadalu-storage">Deploy kadalu storage</h3>

<pre><code>sudo wipefs -a -t dos -f /dev/sdb
sudo mkfs.xfs /dev/sdb

yum install python3-pip -y
sudo pip3 install kubectl-kadalu

echo &quot;export PATH=$PATH:/usr/local/bin/&quot; &gt;&gt; /etc/profile
source /etc/profile

kubectl kadalu install

# k8s.mydomain.intra is the nod name in Kubernetes
# /dev/sdb is the disk

kubectl kadalu storage-add storage-pool-1 \
    --device k8s.mydomain.intra:/dev/sdb

# to delete object if you misconfigured kadalu
kubectl delete kadalustorages.kadalu-operator.storage storage-pool-1


kubectl get pods -n kadalu

kubectl patch storageclass kadalu.replica1 -p '{&quot;metadata&quot;: {&quot;annotations&quot;:{&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;true&quot;}}}'
</code></pre>

<pre><code>nano test-pvc.yaml
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pv1
spec:
  storageClassName: kadalu.replica1
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Fedora CoreOS as a VM]]></title>
            <link href="https://devopstales.github.io/home/fcos-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/fcos-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Fedora CoreOS as a VM" />
                <link href="https://devopstales.github.io/home/k8s-metallb-bgp-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Self-hosted Load Balancer for bare metal Kubernetes" />
                <link href="https://devopstales.github.io/home/how-to-fix-ansible-service-broker-in-openshift-3-11/?utm_source=atom_feed" rel="related" type="text/html" title="How to fix Ansible Service Broker in OpenShift 3.11" />
                <link href="https://devopstales.github.io/home/how-to-fix-cluster-monitoring-operator-in-openshift-3-11/?utm_source=atom_feed" rel="related" type="text/html" title="How to fix cluster-monitoring-operator in OpenShift 3.11" />
                <link href="https://devopstales.github.io/home/how-to-fix-registry-console-ui-in-openshift-3-11/?utm_source=atom_feed" rel="related" type="text/html" title="How to fix registry console UI in OpenShift 3.11" />
            
                <id>https://devopstales.github.io/home/fcos-install/</id>
            
            
            <published>2020-08-30T00:00:00+00:00</published>
            <updated>2020-08-30T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how you can Install Fedora CoreOS(FCOS) in virtualization environment.</p>

<ul>
<li>First you need Fedora CoreOS Config (FCC) - This is a YAML file that specifies the configuration of a machine.</li>
<li>Fedora CoreOS Config Transpiler to validate your FCC and convert it to an Ignition config.</li>
<li>Finally launch a Fedora CoreOS machine and use Ignition config to perform the installation.</li>
</ul>

<h3 id="create-fcc">Create FCC</h3>

<p>Create password hash for default user</p>

<pre><code>$ mkpasswd --method=yescrypt
Password:
$y$j9T$A0Y3wwVOKP69S.1K/zYGN.$S596l11UGH3XjN...
</code></pre>

<pre><code>nano fcos01.fcc
---
variant: fcos
version: 1.0.0
passwd:
  users:
    - name: core
      password_hash: &quot;$y$j9T$A0Y3wwVOKP69S.1K/zYGN.$S596l11UGH3XjN...&quot;
      groups:
        - docker
systemd:
  units:
    - name: install-rpms.service
      enabled: true
      contents: |
        [Unit]
        Description=Install packages
        ConditionFirstBoot=yes
        Wants=network-online.target
        After=network-online.target
        After=multi-user.target
        [Service]
        Type=oneshot
        ExecStart=rpm-ostree install nano git docker-compose htop --reboot
        [Install]
        WantedBy=multi-user.target
storage:
  files:
    - path: /etc/ssh/sshd_config.d/20-enable-passwords.conf
      mode: 0644
      contents:
        inline: |
          # Fedora CoreOS disables SSH password login by default.
          # Enable it.
          # This file must sort before 40-disable-passwords.conf.
          PasswordAuthentication yes
    - path: /etc/hostname
      mode: 0644
      contents:
        inline: fcos01.mydomain.intra
</code></pre>

<h3 id="convert-fcc-to-ignition">Convert FCC to Ignition</h3>

<pre><code>docker run -i --rm quay.io/coreos/fcct --pretty --strict &lt;fcos01.fcc &gt; fcos01.ign

# validate config
docker run --rm -i quay.io/coreos/ignition-validate - &lt; fcos01.ign
</code></pre>

<h3 id="install-fedora-coreos">Install Fedora CoreOS</h3>

<p>At the install stap you need to boot from the Fedora CoreOS ISO and use the Ignition config to install.  So you need a solution to share this files with the running LiveOS. You can use an usb pendrive or a web-server for that. I will use a webserver for thet now.</p>

<pre><code>apt install nginx

mkdir /var/www/html/fcos
cp fcos01.ign /var/www/html/fcos
cd /var/www/html/fcos

systemct start nginx
</code></pre>

<pre><code>cd ~/
wget https://builds.coreos.fedoraproject.org/prod/streams/stable/builds/32.20200726.3.1/x86_64/fedora-coreos-32.20200726.3.1-live.x86_64.iso
</code></pre>

<h3 id="3-2-1-ignition">3 2 1 &hellip; Ignition</h3>

<p>After the live OS bootid star Fedora CoreOS install:</p>

<pre><code>sudo su -
coreos-installer install /dev/sda \
--ignition-url http://example.com/fcos/fcos01.ign \
--insecure-ignition

init 6
</code></pre>

<p>You need <code>--insecure-ignition</code> for insecure http connection.</p>

<h3 id="set-static-ip">Set static IP</h3>

<pre><code>nmcli connection show
NAME              UUID                                  TYPE      DEVICE
Wired Connection  f36f48e0-f75f-4925-ac78-de6119a2fcbb  ethernet  enp0s3

nmcli connection mod 'Wired Connection' \
  ipv4.method manual \
  ipv4.addresses 192.168.0.16/24 \
  ipv4.gateway 192.168.0.0 \
  ipv4.dns 8.8.8.8 \
  +ipv4.dns 8.8.4.4 \
  connection.autoconnect yes

nmcli connection show 'Wired Connection'
systemctl restart NetworkManager
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Self-hosted Load Balancer for bare metal Kubernetes]]></title>
            <link href="https://devopstales.github.io/home/k8s-metallb-bgp-pfsense/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-metallb-bgp-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Self-hosted Load Balancer for bare metal Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-metallb/?utm_source=atom_feed" rel="related" type="text/html" title="Install MetalLB load balancer for Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-longhorn/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes project longhorn" />
                <link href="https://devopstales.github.io/home/k8s-nginx-ingress/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
                <link href="https://devopstales.github.io/home/k8s-install-openebs/?utm_source=atom_feed" rel="related" type="text/html" title="Install OpenEBS for Kubernetes" />
            
                <id>https://devopstales.github.io/home/k8s-metallb-bgp-pfsense/</id>
            
            
            <published>2020-08-18T00:00:00+00:00</published>
            <updated>2020-08-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this tutorial I will show you how to install  Metal LB load balancer in BGP mode for Kubernetes.</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
<li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-contour-ingress --></li>
<li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
<li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
<li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... --></li>
<li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
<li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
<li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<h3 id="metallb">Metallb</h3>

<p>Metallb is a fantastic bare metal-targeted operator for powering LoadBalancer types of services. It can work in two modes: Layer 2 and Border Gateway Protocol (BGP) mode. In layer 2 mode, one of the nodes advertises the load balanced IP (VIP) via ARP. This mode has two limitations: all the traffic goes through a single node VIP potentially limiting the bandwidth. The second limitation is a very slow failover. Detecting unhealthy nodes is a notoriously slow operation in Kubernetes which can take several minutes (5-10 minutes, which can be decreased with the node-problem-detector DaemonSet).</p>

<p>In BGP mode, Metallb advertises the VIP through BGP. It requires a BGP compatible router ath the network wher the kubernetes cluster is created.</p>

<p>I found that the layer 2 mode of Metallb is not a practical solution for production scenarios as it is typically not acceptable to have failover-induced downtimes in the order of minutes.</p>

<h3 id="what-does-the-full-setup-look-like">What does the full setup look like?</h3>

<p>For this Demo I will use a pfsense in virtualbox and tree vm for kubernetes in the same host-only network.</p>

<table>
<thead>
<tr>
<th>vm</th>
<th>nic</th>
<th>ip</th>
<th>mode</th>
</tr>
</thead>

<tbody>
<tr>
<td>pfsense01</td>
<td>em1</td>
<td>192.168.0.200</td>
<td>bridged</td>
</tr>

<tr>
<td>pfsense01</td>
<td>em2</td>
<td>172.17.9.200</td>
<td>host-only</td>
</tr>

<tr>
<td>k8sm01</td>
<td>enp0s8</td>
<td>172.17.9.10</td>
<td>host-only</td>
</tr>

<tr>
<td>k8sm02</td>
<td>enp0s8</td>
<td>172.17.9.11</td>
<td>host-only</td>
</tr>

<tr>
<td>k8sm02</td>
<td>enp0s8</td>
<td>172.17.9.12</td>
<td>host-only</td>
</tr>
</tbody>
</table>

<h3 id="issues-with-calico">Issues with Calico</h3>

<p>Simple BGP config with Calico don’t require anything special. However, if you are using Calico’s <a href="https://docs.projectcalico.org/networking/bgp">external BGP peering capability</a> to advertise your cluster prefixes over BGP, and also want to use BGP in MetalLB, you will need to jump through some hoops.</p>

<h3 id="configuring-pfsense-and-openbgpd">Configuring pfSense and OpenBGPD</h3>

<p>First we need to install OpenBGPD pcakage on pfSense. Go to <code>System &gt; Package Manager &gt; Available Packages</code> Then select <code>OpenBGPD</code> and Install it.</p>

<p>There is otger BGP compatible pckages in pfSense so make  sure you DO NOT have the Quagga_OSPF or FRR packages installed. They directly conflict with each other.</p>

<p>No we need to configure BGP. Ther is a nice UI but I will use the Raw config for simplicity. Go to <code>Services &gt; OpenBGPD &gt; Raw config</code></p>

<pre><code># This file was created by the package manager. Do not edit!

AS 64512
fib-update yes
listen on 172.17.9.200
router-id 172.17.9.200
network 10.25.0.0/22

neighbor 172.17.9.10 {
	remote-as 64513
    announce all
	descr &quot;k8sm01&quot;
}

neighbor 172.17.9.11 {
	remote-as 64513
    announce all
	descr &quot;k8sm02&quot;
}

neighbor 172.17.9.12 {
	remote-as 64513
    announce all
	descr &quot;k8sm03&quot;
}
</code></pre>

<ul>
<li><code>AS 64512</code> - Thi is the Autonomous System Number of pfsense</li>
<li><code>listen on</code> -   This is the address that OpenBGPD should listen to BGP requests on. I highly recommend setting this to the same as the <code>router-id</code> IP address.</li>
<li><code>network</code> - This is the network what you will use for advertise Load Balanced services.</li>
<li><code>neighbor $(kubernetes_worker_node_ip)</code> - The ip of the Kubernetes host</li>
<li><code>remote-as 64513</code> - Thi is the Autonomous System Number of the neighbors. Same for all Kubernetes Node.</li>
<li><code>announce all</code> - We need our nodes to be able to announce to the router their service IP addresses.</li>
</ul>

<h3 id="configuring-pfsense-and-quagga-ospf">Configuring pfSense and Quagga_OSPF</h3>

<p>If you prefer to use <code>Quagga_OSPF</code> Go to <code>System &gt; Package Manager &gt; Available Packages</code> Then select <code>Quagga_OSPF</code> and Install it.</p>

<p>To configure BGP with <code>Quagga_OSPF</code>. Go to <code>Services &gt; Quagga OSPFd &gt; Raw config</code> Edit <code>SAVED bgpd.conf</code> the <code>save</code>.</p>

<pre><code>router bgp 64512
 bgp router-id 172.17.9.200
 neighbor 172.17.9.10 remote-as 64513
 neighbor 172.17.9.11 remote-as 64513
 neighbor 172.17.9.12 remote-as 64513
  network 10.25.0.0/22
</code></pre>

<h3 id="deploy-metallb">Deploy MetalLB</h3>

<pre><code>kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/namespace.yaml
kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/metallb.yaml
# On first install only
kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey=&quot;$(openssl rand -base64 128)&quot;
</code></pre>

<p>Make a BGO config for MetalLB</p>

<pre><code>nano bgpconfig.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    peers:
    - peer-address: 172.17.9.200
      peer-asn: 64512
      my-asn: 64513
    address-pools:
    - name: default
      protocol: bgp
      addresses:
      - 10.25.0.10-10.25.3.250
</code></pre>

<h3 id="demo-time">Demo Time</h3>

<p>Let’s create a demo application for testing.</p>

<pre><code>nano test.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-nginx
  namespace: default
spec:
  selector:
    matchLabels:
      run: test-nginx
  replicas: 3
  template:
    metadata:
      labels:
        run: test-nginx
    spec:
      containers:
      - name: test-nginx
        image: nginx
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: test-nginx
  namespace: default
  labels:
    run: test-nginx
spec:
  type: LoadBalancer
  ports:
  - port: 80
    protocol: TCP
  selector:
    run: test-nginx
</code></pre>

<pre><code>kubectl apply -f bgpconfig.yaml
kubectl apply -f test.yaml
</code></pre>

<p>After a few moments, you can run this command to get the IP address:</p>

<pre><code>$ kubectl describe service test-nginx | grep &quot;LoadBalancer Ingress&quot;
LoadBalancer Ingress:     10.25.0.11
</code></pre>

<p>Let&rsquo;s check the address in a browser. If pfSense is you default gateway it will work perfectly, but in my demo enviroment I need to create a route to pfSense for this network on my host machine:</p>

<pre><code>sudo route add -net 10.25.0.0/22 gw 172.17.9.200
</code></pre>

<pre><code>route -n                                                                                                                     &lt;k8s:kubernetes-admin@kubernetes&gt;
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         192.168.0.1     0.0.0.0         UG    600    0        0 wlan0
10.25.0.0       172.17.9.200    255.255.252.0   UG    0      0        0 vboxnet7
172.17.9.0      0.0.0.0         255.255.255.0   U     0      0        0 vboxnet7
</code></pre>

<p><img src="/img/include/pfsense-bgp-kubernetes.png" alt="Example image" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to fix Ansible Service Broker in OpenShift 3.11]]></title>
            <link href="https://devopstales.github.io/home/how-to-fix-ansible-service-broker-in-openshift-3-11/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/how-to-fix-cluster-monitoring-operator-in-openshift-3-11/?utm_source=atom_feed" rel="related" type="text/html" title="How to fix cluster-monitoring-operator in OpenShift 3.11" />
                <link href="https://devopstales.github.io/home/how-to-fix-registry-console-ui-in-openshift-3-11/?utm_source=atom_feed" rel="related" type="text/html" title="How to fix registry console UI in OpenShift 3.11" />
                <link href="https://devopstales.github.io/home/openshift-cert-manager/?utm_source=atom_feed" rel="related" type="text/html" title="Install cert-manager to Openshift" />
                <link href="https://devopstales.github.io/home/openshift-auto-approval-csr/?utm_source=atom_feed" rel="related" type="text/html" title="How to Enable Auto Approval of CSR in Openshift v3.11" />
                <link href="https://devopstales.github.io/home/openshift-secondary-router/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift secondary route" />
            
                <id>https://devopstales.github.io/home/how-to-fix-ansible-service-broker-in-openshift-3-11/</id>
            
            
            <published>2020-07-10T00:00:00+00:00</published>
            <updated>2020-07-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Ansible Service Broker in OpenShift 3.11 is broken as it uses wrong docker tag.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<h3 id="export-running-deployment-to-yaml">Export running deployment to yaml</h3>

<pre><code>oc project openshift-ansible-service-broker
oc get --export dc/asb -o yaml &gt; asb.yaml
</code></pre>

<h3 id="patch-and-deploy-the-yaml">Patch and deploy the yaml</h3>

<pre><code>replate docker.io/ansibleplaybookbundle/origin-ansible-service-broker:latest to
docker.io/ansibleplaybookbundle/origin-ansible-service-broker:ansible-service-broker-1.3.23-1
oc apply -f asb.yaml


curl -k -H &quot;Authorization: Bearer `oc serviceaccounts get-token asb-client`&quot; https://`oc get routes -n openshift-ansible-service-broker --no-headers | awk '{print $2}'`/osb/v2/catalog
</code></pre>

<h3 id="config-for-new-install">Config for new install</h3>

<pre><code>ansible_service_broker_image=docker.io/ansibleplaybookbundle/origin-ansible-service-broker:ansible-service-broker-1.3.23-1
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to fix cluster-monitoring-operator in OpenShift 3.11]]></title>
            <link href="https://devopstales.github.io/home/how-to-fix-cluster-monitoring-operator-in-openshift-3-11/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/how-to-fix-ansible-service-broker-in-openshift-3-11/?utm_source=atom_feed" rel="related" type="text/html" title="How to fix Ansible Service Broker in OpenShift 3.11" />
                <link href="https://devopstales.github.io/home/how-to-fix-registry-console-ui-in-openshift-3-11/?utm_source=atom_feed" rel="related" type="text/html" title="How to fix registry console UI in OpenShift 3.11" />
                <link href="https://devopstales.github.io/home/openshift-cert-manager/?utm_source=atom_feed" rel="related" type="text/html" title="Install cert-manager to Openshift" />
                <link href="https://devopstales.github.io/home/openshift-auto-approval-csr/?utm_source=atom_feed" rel="related" type="text/html" title="How to Enable Auto Approval of CSR in Openshift v3.11" />
                <link href="https://devopstales.github.io/home/openshift-secondary-router/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift secondary route" />
            
                <id>https://devopstales.github.io/home/how-to-fix-cluster-monitoring-operator-in-openshift-3-11/</id>
            
            
            <published>2020-07-10T00:00:00+00:00</published>
            <updated>2020-07-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Default install use an old image for cluster-monitoring-operator with imagestream false latanci alert  problem.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<h3 id="export-running-deployment-to-yaml">Export running deployment to yaml</h3>

<pre><code>oc project openshift-monitoring
oc get --export deployment/cluster-monitoring-operator -o yaml &gt; cluster-monitoring-operator.yaml
</code></pre>

<h3 id="patch-and-deploy-the-yaml">Patch and deploy the yaml</h3>

<pre><code>sed -i -e &quot;s|image:.*|image: quay.io/openshift/origin-cluster-monitoring-operator:v3.11
|&quot; \
&gt; cluster-monitoring-operator.yaml

oc apply -f cluster-monitoring-operator.yaml


curl -k -H &quot;Authorization: Bearer `oc serviceaccounts get-token asb-client`&quot; https://`oc get routes -n openshift-ansible-service-broker --no-headers | awk '{print $2}'`/osb/v2/catalog

oc delete deployment/prometheus-operator
oc delete statefulset/alertmanager-main
oc delete statefulset/prometheus-k8s
</code></pre>

<h3 id="config-for-new-install">Config for new install</h3>

<pre><code>openshift_cluster_monitoring_operator_image=quay.io/openshift/origin-cluster-monitoring-operator:v3.11
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to fix registry console UI in OpenShift 3.11]]></title>
            <link href="https://devopstales.github.io/home/how-to-fix-registry-console-ui-in-openshift-3-11/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/how-to-fix-ansible-service-broker-in-openshift-3-11/?utm_source=atom_feed" rel="related" type="text/html" title="How to fix Ansible Service Broker in OpenShift 3.11" />
                <link href="https://devopstales.github.io/home/how-to-fix-cluster-monitoring-operator-in-openshift-3-11/?utm_source=atom_feed" rel="related" type="text/html" title="How to fix cluster-monitoring-operator in OpenShift 3.11" />
                <link href="https://devopstales.github.io/home/openshift-cert-manager/?utm_source=atom_feed" rel="related" type="text/html" title="Install cert-manager to Openshift" />
                <link href="https://devopstales.github.io/home/openshift-auto-approval-csr/?utm_source=atom_feed" rel="related" type="text/html" title="How to Enable Auto Approval of CSR in Openshift v3.11" />
                <link href="https://devopstales.github.io/home/openshift-secondary-router/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift secondary route" />
            
                <id>https://devopstales.github.io/home/how-to-fix-registry-console-ui-in-openshift-3-11/</id>
            
            
            <published>2020-07-10T00:00:00+00:00</published>
            <updated>2020-07-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Registry console UI in OpenShift 3.11 is broken on CentOS as it is not available on Docker Hub.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<h3 id="export-running-deployment-to-yaml">Export running deployment to yaml</h3>

<pre><code>oc project default
oc get --export   dc/registry-console -o yaml &gt; registry_console.yaml
</code></pre>

<h3 id="patch-and-deploy-the-yaml">Patch and deploy the yaml</h3>

<pre><code>sed -i -e &quot;s|image:.*|image: docker.io/timbordemann/cockpit-kubernetes:latest|&quot; registry_console.yaml
oc apply -f registry_console.yaml
</code></pre>

<h3 id="config-for-new-install">Config for new install</h3>

<pre><code>openshift_cockpit_deployer_image=docker.io/timbordemann/cockpit-kubernetes:latest
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install cert-manager to Openshift]]></title>
            <link href="https://devopstales.github.io/home/openshift-cert-manager/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-auto-approval-csr/?utm_source=atom_feed" rel="related" type="text/html" title="How to Enable Auto Approval of CSR in Openshift v3.11" />
                <link href="https://devopstales.github.io/home/openshift-secondary-router/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift secondary route" />
                <link href="https://devopstales.github.io/home/openshift-gitlab-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Installing GitLab on OpenShift" />
                <link href="https://devopstales.github.io/home/openshift-kompose/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Run docker-compoe in Openshift" />
                <link href="https://devopstales.github.io/home/openshift-add-node/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Add Nodes to a Cluster" />
            
                <id>https://devopstales.github.io/home/openshift-cert-manager/</id>
            
            
            <published>2020-06-10T00:00:00+00:00</published>
            <updated>2020-06-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p><code>cert-manager</code> is a service that automatically creates certificate requests and sign certificate based on annotations. The created certificate will be stored in a secret.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<p>Normally in kubernetes you can use a secret for TLS in an ingress cinfiguration but in Openshift there is no way to get the certificate from a secret for a route. So we will use <code>cert-utils-operator</code> for recreating routs with the propriety certificate based on annotations.</p>

<h3 id="install-cert-managger">Install cert-managger</h3>

<pre><code>oc create namespace cert-manager
oc apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.15.1/cert-manager-legacy.yaml
</code></pre>

<p>Create <code>ClusterIssuer</code> to create certs. For this demo I will use a Self-signed root CA, what is trustin in my browser. <code>cert-manager</code> can handle Let&rsquo;s encrypt as an issuer both with http and dns challenges so yu can use Let&rsquo;s encrypt certs in a private network without publication your route.</p>

<pre><code>nano issuer.yaml
---
apiVersion: v1
data:
  tls.crt: LS0tLS1C...
  tls.key: LS0tLSGF...
kind: Secret
metadata:
  name: ca-key-pair
  namespace: cert-manager
---
apiVersion: cert-manager.io/v1alpha2
kind: ClusterIssuer
metadata:
  name: ca-issuer
  namespace: cert-manager
spec:
  ca:
    secretName: ca-key-pair
</code></pre>

<h3 id="install-cert-utils-operator">Install cert-utils-operator</h3>

<p>I usethe v0.1.0 and not the latest one (at the moment v0.1.1) besause athe v0.1.1 has a bug on OKD 3.11:
- <a href="https://github.com/redhat-cop/cert-utils-operator/issues/51">https://github.com/redhat-cop/cert-utils-operator/issues/51</a></p>

<pre><code>helm repo add cert-utils-operator https://redhat-cop.github.io/cert-utils-operator
helm update
# export CERT_UTILS_CHART_VERSION=$(helm search cert-utils-operator/cert-utils-operator | grep cert-utils-operator/cert-utils-operator | awk '{print $2}')

helm fetch cert-utils-operator/cert-utils-operator --version v0.1.0
helm template cert-utils-operator-v0.1.0.tgz --namespace cert-manager | oc apply -f - -n cert-manager
</code></pre>

<pre><code>apiVersion: apps.openshift.io/v1
kind: DeploymentConfig
metadata:
  annotations:
    openshift.io/generated-by: OpenShiftWebConsole
  labels:
    app: nginx2
  name: nginx2
spec:
  replicas: 1
  selector:
    app: nginx2
    deploymentconfig: nginx2
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx2
        deploymentconfig: nginx2
    spec:
      containers:
        - image: &gt;-
            bitnami/nginx@sha256:2bff7d085671a8b0f9ec296cf57fba995d06c1b5fb350575dd429c361520f0a4
          imagePullPolicy: Always
          name: nginx2
          ports:
            - containerPort: 8080
              protocol: TCP
            - containerPort: 8443
              protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
  test: false
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    openshift.io/generated-by: OpenShiftWebConsole
  labels:
    app: nginx2
  name: nginx2
spec:
  clusterIP: 172.30.17.64
  ports:
    - name: 8080-tcp
      port: 8080
      protocol: TCP
      targetPort: 8080
    - name: 8443-tcp
      port: 8443
      protocol: TCP
      targetPort: 8443
  selector:
    deploymentconfig: nginx2
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
---
apiVersion: cert-manager.io/v1alpha2
kind: Certificate
metadata:
  name: nginx2-route-tls
  namespace: default
spec:
  secretName: nginx2-route-tls
  duration: 24h
  renewBefore: 12h
  commonName: nginx.openshift.mydomain.intra
  dnsNames:
  - nginx.openshift.mydomain.intra
  issuerRef:
    name: ca-issuer
    kind: ClusterIssuer
    group: cert-manager.io
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    cert-utils-operator.redhat-cop.io/certs-from-secret=nginx2-route-tls
  labels:
    app: nginx2
  name: nginx2
spec:
  host: nginx.openshift.mydomain.intra
  port:
    targetPort: 8080-tcp
  tls:
    insecureEdgeTerminationPolicy: Redirect
    termination: edge
  to:
    kind: Service
    name: nginx2
    weight: 100
  wildcardPolicy: None

---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    cert-utils-operator.redhat-cop.io/certs-from-secret: nginx2-route-tls
  labels:
    app: nginx2
  name: nginx2
spec:
  host: nginx.openshift.mydomain.intra
  port:
    targetPort: 8080-tcp
  tls:
    insecureEdgeTerminationPolicy: Redirect
    termination: edge
  to:
    kind: Service
    name: nginx2
    weight: 100
  wildcardPolicy: None
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to Enable Auto Approval of CSR in Openshift v3.11]]></title>
            <link href="https://devopstales.github.io/home/openshift-auto-approval-csr/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-secondary-router/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift secondary route" />
                <link href="https://devopstales.github.io/home/openshift-gitlab-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Installing GitLab on OpenShift" />
                <link href="https://devopstales.github.io/home/openshift-kompose/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Run docker-compoe in Openshift" />
                <link href="https://devopstales.github.io/home/openshift-add-node/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Add Nodes to a Cluster" />
                <link href="https://devopstales.github.io/home/openshift-letsencrypt/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Letsencrypt certificates" />
            
                <id>https://devopstales.github.io/home/openshift-auto-approval-csr/</id>
            
            
            <published>2020-05-27T00:00:00+00:00</published>
            <updated>2020-05-27T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Nodes certificates are not Completely redeployed through playbook but through a different mechanism.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<p>SSL Certificates will be valid for the period of 1 year and at 85% of the certificate the node will trigger a CSR that would have to be approved for the certificate to be redeployed.</p>

<p>The Only certificates that are renewed/redeployed through CSR’s mechanism are the kubelet/nodes certificates. Any other certificates e.g, router, master, api certs, etcd, docker-registry, etc are still redeployed through the usual playbooks.</p>

<p>If triggered CSR is not approved either manually or in automated way then after one year all nodes will go to NotReady State.</p>

<h3 id="check-and-approve-csr-s-manually">Check and approve csr&rsquo;s manually</h3>

<pre><code>oc get csr
oc describe csr &lt;csr_name&gt;
oc adm certificate &lt;approve csr_name&gt;

oc get csr -o name | xargs oc adm certificate approve
</code></pre>

<h3 id="approve-csr-s-automaticle">Approve csr&rsquo;s automaticle</h3>

<p>At install time you can add this option to your ansible hosts fiel:</p>

<pre><code>openshift_master_bootstrap_auto_approve=true
</code></pre>

<p>If you installed the cluster and want to change this option run this playbook:</p>

<pre><code>ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/openshift-master/enable_bootstrap.yml \
-e openshift_master_bootstrap_auto_approve=true
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install OpenEBS for Kubernetes]]></title>
            <link href="https://devopstales.github.io/home/k8s-install-openebs/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-kubeadm-ha/?utm_source=atom_feed" rel="related" type="text/html" title="How to install kubernetes with kubeadm in HA mode" />
                <link href="https://devopstales.github.io/home/k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install kubernetes with kubeadm" />
                <link href="https://devopstales.github.io/kubernetes/k8s-install-openebs/?utm_source=atom_feed" rel="related" type="text/html" title="Install OpenEBS for Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-local-pv/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
                <link href="https://devopstales.github.io/home/k8s-velero-backup/?utm_source=atom_feed" rel="related" type="text/html" title="Backup your Kubernetes Cluster with Velero" />
            
                <id>https://devopstales.github.io/home/k8s-install-openebs/</id>
            
            
            <published>2020-05-20T00:00:00+00:00</published>
            <updated>2020-05-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>OpenEBS is an open-source project for container-attached and container-native storage on Kubernetes.</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
<li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-contour-ingress --></li>
<li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
<li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
<li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... --></li>
<li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
<li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
<li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<p>On all host we have an unused unpartitioned disk called sdb.</p>

<pre><code>lsblk

NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda               8:0    0   80G  0 disk
├─sda1            8:1    0    1G  0 part /boot
└─sda2            8:2    0   79G  0 part
  ├─centos-root 253:0    0   50G  0 lvm  /
  ├─centos-swap 253:1    0    1G  0 lvm
  └─centos-home 253:3    0   28G  0 lvm  /home
sdb               8:16   0  100G  0 disk
</code></pre>

<h3 id="install-requirements">Install requirements</h3>

<p>OpenEBS use iscsi for persisten volume sharing so we need  <code>iscsid</code>.</p>

<h4 id="debian-ubuntu">Debian / Ubuntu</h4>

<pre><code>apt-get install open-iscsi
service open-iscsi enable
service open-iscsi restart

</code></pre>

<h4 id="centos">CentOS</h4>

<pre><code>yum install iscsi-initiator-utils -y
systemctl enable iscsid
systemctl start iscsid
</code></pre>

<h3 id="deploy-openebs-with-helm">Deploy OpenEBS with helm</h3>

<p>OpenEBS is in the stable repo so we didn&rsquo;t need to add a separate helm repository for installing it. In my case I will use teh <code>openebs-system</code> namespace for the install.</p>

<pre><code>kubectl create ns openebs-system
helm upgrade --install openebs stable/openebs --version 1.7.0 --namespace=openebs-system
</code></pre>

<p>Wait for all the  pods are started.</p>

<pre><code>kubectl get pods -n openebs-system

NAME                                           READY     STATUS    RESTARTS   AGE
openebs-admission-server-7b4859ccd5-bz4zt      1/1       Running   0          14m
openebs-apiserver-556ffff45c-nk9x9             1/1       Running   5          15m
openebs-localpv-provisioner-76b466d4b8-5tj4w   1/1       Running   0          15m
openebs-ndm-f6cqz                              1/1       Running   0          15m
openebs-ndm-operator-5f6c5497d7-chf6t          1/1       Running   1          15m
openebs-ndm-qrmp9                              1/1       Running   0          15m
openebs-ndm-stgml                              1/1       Running   0          15m
openebs-provisioner-c9c7f9ff8-hn4bl            1/1       Running   0          15m
openebs-snapshot-operator-6578d74b7-2wc97      2/2       Running   0          15m
</code></pre>

<p>Now verify if OpenEBS is installed successfully.</p>

<pre><code>kubectl get blockdevice -n openebs-system

NAME                                           NODENAME    SIZE         CLAIMSTATE   STATUS    AGE
blockdevice-0c4e03f9e39a4092108215f19eca9da8   k8s-node1   1048576000   Unclaimed    Active    16m
blockdevice-1aaa1142a7b9c65dfa32dec88fe1749b   k8s-node2   1048576000   Unclaimed    Active    16m
blockdevice-5f728d1068c72337609fc1f88855b9bb   k8s-node3   1048576000   Unclaimed    Active    16m
</code></pre>

<pre><code>kubectl describe blockdevice blockdevice-0c4e03f9e39a4092108215f19eca9da8
...
  Devlinks:
    Kind:  by-id
    Links:
      /dev/disk/by-id/ata-VBOX_HARDDISK_VBd4679835-eb798f2c
      /dev/disk/by-id/lvm-pv-uuid-PWnLFv-b0jS-7CLZ-Cmym-0dia-RQkI-w0Hkam
    Kind:  by-path
    Links:
      /dev/disk/by-path/pci-0000:00:01.1-ata-2.0
  Filesystem:
    Fs Type:  LVM2_member
  Node Attributes:
    Node Name:  k8s-node1
  Partitioned:  No
  Path:         /dev/sdb
...
</code></pre>

<h4 id="verify-storageclasses">Verify StorageClasses:</h4>

<pre><code>kubectl get sc

NAME                        PROVISIONER                                                AGE
openebs-device              openebs.io/local                                           64s
openebs-hostpath            openebs.io/local                                           64s
openebs-jiva-default        openebs.io/provisioner-iscsi                               64s
openebs-snapshot-promoter   volumesnapshot.external-storage.k8s.io/snapshot-promoter   64s
</code></pre>

<h3 id="storage-engines">Storage engines</h3>

<p>OpenEBS offers three storage engines:
* Jiva
* cStor
* LocalPV</p>

<p>Jiva is a light weight storage engine that is recommended to use for low capacity workloads. It is actually based on the same technology that powers Longhorn.</p>

<pre><code>---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: demo-vol1-claim
spec:
  storageClassName: openebs-jiva-default
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 4G
</code></pre>

<p>cStor requires raw disks. The snapshot and storage management features of the other cStor engine are more advanced than Jiva. For provisioning a cStor Volume, it requires a cStor Storage Pool and a StorageClass. cStor provides iSCSI targets, which are appropriate for RWO (ReadWriteOnce) access mode and is suitable for all types of databases. cStor supports thin provisioning by default.</p>

<pre><code>cat stor-pool1-config.yaml
---
apiVersion: openebs.io/v1alpha1
kind: StoragePoolClaim
metadata:
  name: cstor-disk-pool
  annotations:
    cas.openebs.io/config: |
      - name: PoolResourceRequests
        value: |-
            memory: 500Mb
      - name: PoolResourceLimits
        value: |-
            memory: 500Mb
spec:
  name: cstor-disk-pool
  type: disk
  poolSpec:
    poolType: striped
  blockDevices:
    blockDeviceList:
    - blockdevice-0c4e03f9e39a4092108215f19eca9da8
    - blockdevice-1aaa1142a7b9c65dfa32dec88fe1749b
    - blockdevice-5f728d1068c72337609fc1f88855b9bb
</code></pre>

<pre><code>kubectl apply -f stor-pool1-config.yaml

kubectl get spc

NAME              AGE
cstor-disk-pool   20s

kubectl get csp

NAME                   ALLOCATED   FREE      CAPACITY   STATUS    TYPE      AGE
cstor-disk-pool-cxm8   294K        100G      100G       Healthy   striped   27m
cstor-disk-pool-r1hl   270K        100G      100G       Healthy   striped   27m
cstor-disk-pool-t05z   92K         100G      100G       Healthy   striped   27m
</code></pre>

<pre><code>cat openebs-sc-rep3.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: openebs-cstore-default
  annotations:
    openebs.io/cas-type: cstor
    cas.openebs.io/config: |
      - name: StoragePoolClaim
        value: &quot;cstor-disk-pool&quot;
      - name: ReplicaCount
        value: &quot;3&quot;
    storageclass.kubernetes.io/is-default-class: &quot;true&quot;
provisioner: openebs.io/provisioner-iscsi
</code></pre>

<pre><code>cat test-cs-pcv.yaml
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: test-cs-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3G
</code></pre>

<pre><code>kubectl apply -f openebs-sc-rep3.yaml
kubectl apply -f test-cs-pcv.yaml
</code></pre>

<p>Local PV is based on Kubernetes local persistent volumes but it has a dynamic provisioner. It can store data either in a directory, or use disks; in the first case the hostpath can be shared by multiple persistent volumes, while when using disks each persistent volume requires a separate device. Local PV offers extremely high performance close to what you get by reading from and writing to the disk directly, but it doesn’t offer features such as replication, which are built in Jiva and cStor.</p>

<pre><code>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    cas.openebs.io/config: |
      - name: StorageType
        value: &quot;hostpath&quot;
      - name: BasePath
        value: &quot;/mnt/openebs&quot;
    openebs.io/cas-type: local
  name: openebs-hostpath-mount
provisioner: openebs.io/local
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
</code></pre>

<h2 id="sources">Sources:</h2>

<ul>
<li><a href="https://vitobotta.com/2019/07/03/openebs-tips/">https://vitobotta.com/2019/07/03/openebs-tips/</a></li>
</ul>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Foreman openidc SSO with keycloak]]></title>
            <link href="https://devopstales.github.io/home/foreman-sso/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/sso/foreman-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Foreman openidc SSO with keycloak" />
                <link href="https://devopstales.github.io/home/mattermost-keycloak-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Free sso for Mattermost Teams Edition" />
                <link href="https://devopstales.github.io/home/k8s-kuberos/?utm_source=atom_feed" rel="related" type="text/html" title="Kubectl authentication with Kuberos" />
                <link href="https://devopstales.github.io/home/k8s-gangway/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak" />
                <link href="https://devopstales.github.io/home/k8s-dasboard-auth/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak" />
            
                <id>https://devopstales.github.io/home/foreman-sso/</id>
            
            
            <published>2020-05-15T00:00:00+00:00</published>
            <updated>2020-05-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I this post I will show you how you can configure Foreman to use Keycloak asz an OIDC SSO authentication provider.</p>

<p>I use a Self-signed certificate for keycloak so my first step is to add the root CA of this certificate as a trusted certificate on the foreman server:</p>

<pre><code>cp rootca.crt /etc/pki/ca-trust/source/anchors/
update-ca-trust
</code></pre>

<h3 id="install-requirements">install requirements</h3>

<pre><code>yum install mod_auth_openidc keycloak-httpd-client-install
</code></pre>

<p>The <code>keycloak-httpd-client-install</code> is a commandline tool thet helps to configure the apache2&rsquo;s <code>mod_auth_openidc</code> plugin with Keycloak. This feature is not yet supported by foreman-installer. As a result, re-running the foreman-installer command can purge the changes in Apache files added by the keycloak-httpd-client-install.</p>

<pre><code>keycloak-httpd-client-install \
--app-name foreman-openidc \
--keycloak-server-url &quot;https://keycloak.mydomain.intra&quot; \
--keycloak-admin-usernam &quot;admin&quot; \
--keycloak-realm &quot;ssl-realm&quot; \
--keycloak-admin-realm master \
--keycloak-auth-role root-admin \
-t openidc -l /users/extlogin

systemct restart httpd
</code></pre>

<h3 id="configure-keycloak">Configure Keycloak</h3>

<p>We need to create two mappers for the new client in keycloak:</p>

<p><img src="/img/include/foreman-sso-1.png" alt="Example image" /><br/>
<img src="/img/include/foreman-sso-2.png" alt="Example image" /><br/></p>

<p>Then in Keycloak create a group called <code>foreman-admin</code> and add the test user for it.</p>

<h3 id="configure-foreman">Configure Foreman:</h3>

<pre><code>Administer / Settings / Authentication
Authorize login delegation = yes
Authorize login delegation auth source user autocreate = External
OIDC Algorithm = RS256
# the client id from keycloak
OIDC Audience = foreman.mydomain.intra-foreman-openidc
OIDC Issuer = https://keycloak.mydomain.intra/auth/realms/ssl-realm
OIDC JWKs URL = https://keycloak.mydomain.intra/auth/realms/ssl-realm/protocol/openid-connect/certs
</code></pre>

<p>Create an user groupe in foreman called <code>foreman-admin</code> and map with the external group called <code>foreman-admin</code>.
<img src="/img/include/foreman-sso-3.png" alt="Example image" /><br/>
<img src="/img/include/foreman-sso-4.png" alt="Example image" /><br/>
<img src="/img/include/foreman-sso-5.png" alt="Example image" /><br/></p>

<p>The log out and go the he url of the foreman. It will redirect to Keycloak to login. Add the test users credentials the login wit it.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to create your first Chef Cookbook]]></title>
            <link href="https://devopstales.github.io/home/chef-first-cookbook/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/chef-server-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install  Chef server" />
                <link href="https://devopstales.github.io/home/ansible-k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Kubernetes" />
                <link href="https://devopstales.github.io/home/ansible-operator-overview/?utm_source=atom_feed" rel="related" type="text/html" title="Ansible Operator Overview" />
                <link href="https://devopstales.github.io/home/awx-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install AWX" />
                <link href="https://devopstales.github.io/home/awx-docker/?utm_source=atom_feed" rel="related" type="text/html" title="Install AWX in docker" />
            
                <id>https://devopstales.github.io/home/chef-first-cookbook/</id>
            
            
            <published>2020-05-07T00:00:00+00:00</published>
            <updated>2020-05-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I this post I will show you how you can create a basic chef structure.</p>

<h3 id="create-environments">Create environments</h3>

<pre><code>cd ~/chef-repo

mkdir environments
nano environments/production.json
{
  &quot;name&quot;: &quot;Production&quot;,
  &quot;description&quot;: &quot;&quot;,
  &quot;cookbook_versions&quot;: {

  },
  &quot;json_class&quot;: &quot;Chef::Environment&quot;,
  &quot;chef_type&quot;: &quot;environment&quot;,
  &quot;default_attributes&quot;: {

  },
  &quot;override_attributes&quot;: {

  }
}

nano environments/development.json
{
  &quot;name&quot;: &quot;Development&quot;,
  &quot;description&quot;: &quot;&quot;,
  &quot;cookbook_versions&quot;: {

  },
  &quot;json_class&quot;: &quot;Chef::Environment&quot;,
  &quot;chef_type&quot;: &quot;environment&quot;,
  &quot;default_attributes&quot;: {

  },
  &quot;override_attributes&quot;: {

  }
}
</code></pre>

<pre><code>knife environment from file environments/production.json
knife environment from file environments/development.json
</code></pre>

<h3 id="create-roles">Create roles</h3>

<pre><code>mkdir roles
nano roles/base.json
{

  &quot;name&quot;: &quot;base&quot;,
  &quot;description&quot;: &quot;Default operation role&quot;,
  &quot;json_class&quot;: &quot;Chef::Role&quot;,

  &quot;override_attributes&quot;: {
    &quot;chef_client&quot;: {
      &quot;config&quot;: {
        &quot;interval&quot;: 900,
        &quot;splay&quot;: 30
      }
    }
  },

  &quot;chef_type&quot;: &quot;role&quot;,

  &quot;run_list&quot;: [
    &quot;recipe[operation]&quot;
  ],

  &quot;env_run_lists&quot;: {
  }

}
</code></pre>

<pre><code>knife role from file roles/base.json
</code></pre>

<h3 id="create-node-config">Create node config</h3>

<pre><code>mkdir nodes
nano nodes/test.mydomain.intra.json
{
  &quot;name&quot;: &quot;test.mydomain.intra&quot;,
  &quot;chef_environment&quot;: &quot;Production&quot;,
  &quot;normal&quot;: {
  	&quot;tags&quot;: [

    ]
  },
  &quot;run_list&quot;: [
	&quot;role[base]&quot;
]

}
</code></pre>

<pre><code>knife node from file nodes/test.mydomain.intra.json
</code></pre>

<h3 id="create-cookbook">Create cookbook</h3>

<pre><code>cd chef-repo/cookbooks
chef generate cookbook operation
nano operation/recipes/default.rb
include_recipe 'operation::packages'

nano operation/recipes/packages.rb
package &quot;nano&quot; do
  action :install
end
</code></pre>

<pre><code>cd ..
knife cookbook upload operation
knife cookbook list
</code></pre>

<h3 id="test-cookbook">Test cookbook</h3>

<pre><code>knife ssh 'name:test.mydomain.intra' 'sudo chef-client' -x root
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install  Chef server]]></title>
            <link href="https://devopstales.github.io/home/chef-server-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/ansible-k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Kubernetes" />
                <link href="https://devopstales.github.io/home/ansible-operator-overview/?utm_source=atom_feed" rel="related" type="text/html" title="Ansible Operator Overview" />
                <link href="https://devopstales.github.io/home/awx-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install AWX" />
                <link href="https://devopstales.github.io/home/awx-docker/?utm_source=atom_feed" rel="related" type="text/html" title="Install AWX in docker" />
                <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
            
                <id>https://devopstales.github.io/home/chef-server-install/</id>
            
            
            <published>2020-04-30T00:00:00+00:00</published>
            <updated>2020-04-30T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Chef is a powerful configuration management utility writy in ruby. This post will help you to setup a chef 13 on CentOS 7</p>

<ul>
<li>Chef Server: This is the central hub server that stores the cookbooks and recipes uploaded from workstations.</li>
<li>Chef Workstations: This where recipes, cookbooks, and other chef configuration details are created or edited.</li>
<li>Chef Client: This the target node where the configurations are deployed by the chef-client.</li>
</ul>

<h2 id="chef-server-install">Chef Server Install:</h2>

<pre><code>cd /opt
wget https://packages.chef.io/files/stable/chef-server/13.2.0/el/7/chef-server-core-13.2.0-1.el7.x86_64.rpm
yum install chef-server-core-13.2.0-1.el7.x86_64.rpm -y

chef-server-ctl reconfigure
chef-server-ctl status
</code></pre>

<p>Create admin user for chef server:</p>

<pre><code># chef-server-ctl user-create USER_NAME FIRST_NAME LAST_NAME EMAIL 'PASSWORD' -f PATH_FILE_NAME
chef-server-ctl user-create admin admin admin admin@devopstales.intra Password1 -f /etc/chef/admin.pem
</code></pre>

<p>Now create an organization to hold the chef configurations.</p>

<pre><code># chef-server-ctl org-create short_name 'full_organization_name' --association_user user_name --filename ORGANIZATION-validator.pem

chef-server-ctl org-create devopstales &quot;DevOpsTales, Inc&quot; --association_user admin -f /etc/chef/devopstales-validator.pem
</code></pre>

<h2 id="install-chef-workstation">Install Chef workstation:</h2>

<p>Fot this demo I will install the workstation on the same server as the Chef server, but in a pruduction enviroment it is your laptop or pc.</p>

<pre><code>wget https://packages.chef.io/files/stable/chefdk/4.7.73/el/7/chefdk-4.7.73-1.el7.x86_64.rpm
yum install -y chefdk-4.7.73-1.el7.x86_64.rpm
chef verify
</code></pre>

<pre><code>which ruby
echo 'eval &quot;$(chef shell-init bash)&quot;' &gt;&gt; ~/.bash_profile
. ~/.bash_profile
which ruby
</code></pre>

<pre><code>cd ~
chef generate repo chef-repo
mkdir -p ~/chef-repo/.chef
cp /etc/chef/admin.pem ~/chef-repo/.chef/
cp /etc/chef/devopstales-validator.pem ~/chef-repo/.chef/
</code></pre>

<pre><code>nano ~/chef-repo/.chef/knife.rb
current_dir = File.dirname(__FILE__)
log_level                :info
log_location             STDOUT
node_name                &quot;admin&quot;
client_key               &quot;#{current_dir}/admin.pem&quot;
validation_client_name   &quot;devopstzales-validator&quot;
validation_key           &quot;#{current_dir}/itzgeek-validator.pem&quot;
chef_server_url          &quot;https://cchef.mydomain.intra/organizations/devopstales&quot;
syntax_check_cache_path  &quot;#{ENV['HOME']}/.chef/syntaxcache&quot;
cookbook_path            [&quot;#{current_dir}/../cookbooks&quot;]
</code></pre>

<p>test kinife client:</p>

<pre><code>cd ~/chef-repo/
knife ssl fetch
knife client list
</code></pre>

<h2 id="install-chef-client">Install chef client:</h2>

<p>Before we can bootstrap a chef client on a server we need valid DNS resolution for both.</p>

<pre><code>knife bootstrap -N test.mydomain.intra test.mydomain.intra -y root -P vagrant
</code></pre>

<hr />

<ul>
<li><a href="https://downloads.chef.io/chef-server/stable/">https://downloads.chef.io/chef-server/stable/</a></li>
<li><a href="https://downloads.chef.io/chefdk/stable/">https://downloads.chef.io/chefdk/stable/</a></li>
</ul>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to install kubernetes with kubeadm in HA mode]]></title>
            <link href="https://devopstales.github.io/home/k8s-kubeadm-ha/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install kubernetes with kubeadm" />
                <link href="https://devopstales.github.io/kubernetes/k8s-kubeadm-ha/?utm_source=atom_feed" rel="related" type="text/html" title="How to install kubernetes with kubeadm in HA mode" />
                <link href="https://devopstales.github.io/home/k8s-local-pv/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
                <link href="https://devopstales.github.io/home/k8s-velero-backup/?utm_source=atom_feed" rel="related" type="text/html" title="Backup your Kubernetes Cluster with Velero" />
                <link href="https://devopstales.github.io/home/k8s-cert-manager/?utm_source=atom_feed" rel="related" type="text/html" title="Install cert-manager for Kubernetes" />
            
                <id>https://devopstales.github.io/home/k8s-kubeadm-ha/</id>
            
            
            <published>2020-04-02T00:00:00+00:00</published>
            <updated>2020-04-02T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I&rsquo;will show you how to install kubernetes in HA mode with kubeadm, keepaliwed and envoyproxy.</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
<li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-contour-ingress --></li>
<li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
<li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
<li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... --></li>
<li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
<li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
<li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<pre><code>172.17.8.100  # kubernetes cluster ip
172.17.8.101  master01 # master node
172.17.8.102  master02 # frontend node
172.17.8.103  master03 # worker node

# hardware requirement
2 CPU
4G RAM
</code></pre>

<h3 id="install-docker">Install Docker</h3>

<pre><code>yum install -y -q yum-utils device-mapper-persistent-data lvm2
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum install -y -q docker-ce docker-compose

mkdir /etc/docker
echo '{
  &quot;log-driver&quot;: &quot;json-file&quot;,
  &quot;log-opts&quot;: {
    &quot;max-size&quot;: &quot;100m&quot;
  },
  &quot;storage-driver&quot;: &quot;overlay2&quot;,
  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],
  &quot;storage-opts&quot;: [
    &quot;overlay2.override_kernel_check=true&quot;
  ]
}' &gt; /etc/docker/daemon.json

systemctl enable docker
systemctl start docker
</code></pre>

<h3 id="disable-swap">Disable swap</h3>

<pre><code>free -h
swapoff -a
swapoff -a
sed -i.bak -r 's/(.+ swap .+)/#\1/' /etc/fstab
free -h
</code></pre>

<h3 id="configuuration">Configuuration</h3>

<pre><code>cat &gt;&gt;/etc/sysctl.d/kubernetes.conf&lt;&lt;EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.ipv6.conf.all.disable_ipv6      = 1
net.ipv6.conf.default.disable_ipv6  = 1
EOF
cat &gt;&gt;/etc/sysctl.d/ipv6.conf&lt;&lt;EOF
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1
net.ipv6.conf.eth0.disable_ipv6 = 1
EOF
sysctl --system
</code></pre>

<h3 id="install-kubeadm">Install kubeadm</h3>

<pre><code>cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
        https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF


yum install epel-release -y
yum install -y kubeadm kubelet kubectl keepalived
</code></pre>

<h3 id="configure-keepalived-on-first-master">Configure keepalived on first master</h3>

<pre><code>touch /etc/keepalived/check_apiserver.sh
chmod +x /etc/keepalived/check_apiserver.sh

cat &lt;&lt;EOF &gt; /etc/keepalived/keepalived.conf
! Configuration File for keepalived
global_defs {
    router_id k8s-node1
    enable_script_security
    script_user root
}
vrrp_script check_apiserver {
    script &quot;/etc/keepalived/check_apiserver.sh&quot;
    interval 5
    weight -10
    fall 2
    rise 2
}
vrrp_instance VI_1 {
    state BACKUP
    interface enp0s8
    mcast_src_ip 172.17.8.101
    virtual_router_id 51
    priority 150
    advert_int 5
    authentication {
        auth_type PASS
        auth_pass Password1
    }
    virtual_ipaddress {
        172.17.8.100/24 brd 172.17.8.255 dev enp0s8
    }
    track_script {
       check_apiserver
    }
}
EOF

systemctl start keepalived
systemctl enable keepalived
</code></pre>

<h3 id="configure-envoy-on-first-master">Configure envoy on first master</h3>

<pre><code>cat &lt;&lt;EOF &gt; /etc/kubernetes/envoy.yaml
static_resources:
  listeners:
  - name: main
    address:
      socket_address:
        address: 0.0.0.0
        port_value: 16443
    filter_chains:
    - filters:
      - name: envoy.tcp_proxy
        config:
          stat_prefix: ingress_tcp
          cluster: k8s

  clusters:
  - name: k8s
    connect_timeout: 0.25s
    type: strict_dns
    lb_policy: round_robin
    hosts:
    - socket_address:
        address: 172.17.8.101
        port_value: 6443
    - socket_address:
        address: 172.17.8.102
        port_value: 6443
    - socket_address:
        address: 172.17.8.103
        port_value: 6443
    health_checks:
    - timeout: 1s
      interval: 5s
      unhealthy_threshold: 1
      healthy_threshold: 1
      http_health_check:
        path: &quot;/healthz&quot;

admin:
  access_log_path: &quot;/dev/null&quot;
  address:
    socket_address:
      address: 0.0.0.0
      port_value: 8001
EOF
</code></pre>

<h3 id="create-loadbalancer-on-all-masters">Create loadbalancer on all masters</h3>

<pre><code>
cat&lt;&lt;EOF &gt; /etc/kubernetes/envoy.yaml
static_resources:
  listeners:
  - name: main
    address:
      socket_address:
        address: 0.0.0.0
        port_value: 16443
    filter_chains:
    - filters:
      - name: envoy.tcp_proxy
        config:
          stat_prefix: ingress_tcp
          cluster: k8s

  clusters:
  - name: k8s
    connect_timeout: 0.25s
    type: strict_dns # static
    lb_policy: round_robin
    hosts:
    - socket_address:
        address: 172.17.8.101
        port_value: 6443
    - socket_address:
        address: 172.17.8.102
        port_value: 6443
    - socket_address:
        address: 172.17.8.103
        port_value: 6443
    health_checks:
    - timeout: 1s
      interval: 5s
      unhealthy_threshold: 1
      healthy_threshold: 1
      http_health_check:
        path: &quot;/healthz&quot;

admin:
  access_log_path: &quot;/dev/null&quot;
  address:
    socket_address:
      address: 0.0.0.0
      port_value: 8001
EOF

cat&lt;&lt;EOF &gt; /etc/kubernetes/docker-compose.yaml
version: '3'
services:
    api-lb:
        image: envoyproxy/envoy:latest
        restart: always
        network_mode: &quot;host&quot;
        ports:
            - 16443:16443
            - 8001:8001
        volumes:
            - /etc/kubernetes/envoy.yaml:/etc/envoy/envoy.yaml
EOF

cd /etc/kubernetes/
docker-compose pull
docker-compose up -d
docker-compose ps

netstat -tulpn | grep 6443
</code></pre>

<h3 id="initialize-kubernetes-in-the-first-master">Initialize kubernetes in the first master</h3>

<p>I have multiple interfaces in my masters so to use the correct one I need to add <code>--apiserver-advertise-address &quot;172.17.8.101&quot;</code> to my kubeadm commands and add <code>KUBELET_EXTRA_ARGS</code> for kubelet config.</p>

<pre><code>echo 'KUBELET_EXTRA_ARGS=&quot;--node-ip=172.17.8.101&quot;' &gt; /etc/sysconfig/kubelet

kubeadm config images pull --kubernetes-version 1.16.8
kubeadm init --control-plane-endpoint &quot;172.17.8.100:16443&quot; --apiserver-advertise-address &quot;172.17.8.101&quot; --upload-certs --kubernetes-version 1.16.8 --pod-network-cidr=10.244.0.0/16

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

kubectl get no

kubectl apply -f https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml
</code></pre>

<h3 id="join-other-masters">Join other masters</h3>

<pre><code>
kubeadm config images pull --kubernetes-version 1.16.8

echo 'KUBELET_EXTRA_ARGS=&quot;--node-ip=172.17.8.102&quot;' &gt; /etc/sysconfig/kubelet

kubeadm join 172.17.8.100:16443 --token 3vqtop.z2kbok4o0wchu4ed \
  --discovery-token-ca-cert-hash sha256:5840ee4de07bb296e2639669c17df7e3240271a1880115336ebc5b91fb8a3555 \
  --control-plane --certificate-key dc99dc10a0269d1a3edfc2e318a78c6bbebdee8081b460535f699d210cec5dcb \
  --apiserver-advertise-address &quot;172.17.8.102&quot;

echo 'KUBELET_EXTRA_ARGS=&quot;--node-ip=172.17.8.103&quot;' &gt; /etc/sysconfig/kubelet

kubeadm join 172.17.8.100:16443 --token 3vqtop.z2kbok4o0wchu4ed \
  --discovery-token-ca-cert-hash sha256:5840ee4de07bb296e2639669c17df7e3240271a1880115336ebc5b91fb8a3555 \
  --control-plane --certificate-key dc99dc10a0269d1a3edfc2e318a78c6bbebdee8081b460535f699d210cec5dcb \
  --apiserver-advertise-address &quot;172.17.8.103&quot;
</code></pre>

<h3 id="fix-keepalibed-check-script-on-first-master">Fix keepalibed check script on first master</h3>

<pre><code>echo '#!/bin/bash

# if check error then repeat check for 12 times, else exit
err=0
for k in $(seq 1 12)
do
    check_code=$(curl -sk https://localhost:16443)
    if [[ $check_code == &quot;&quot; ]]; then
        err=$(expr $err + 1)
        sleep 5
        continue
    else
        err=0
        break
    fi
done

if [[ $err != &quot;0&quot; ]]; then
    # if apiserver is down send SIG=1
    echo 'apiserver error!'
    exit 1
else
    # if apiserver is up send SIG=0
    echo 'apiserver normal!'
    exit 0
fi' &gt; /etc/keepalived/check_apiserver.sh
</code></pre>

<h3 id="configure-keepalived-on-other-masters">Configure keepalived on other masters</h3>

<pre><code>echo '#!/bin/bash

# if check error then repeat check for 12 times, else exit
err=0
for k in $(seq 1 12)
do
    check_code=$(curl -sk https://localhost:16443)
    if [[ $check_code == &quot;&quot; ]]; then
        err=$(expr $err + 1)
        sleep 5
        continue
    else
        err=0
        break
    fi
done

if [[ $err != &quot;0&quot; ]]; then
    # if apiserver is down send SIG=1
    echo 'apiserver error!'
    exit 1
else
    # if apiserver is up send SIG=0
    echo 'apiserver normal!'
    exit 0
fi' &gt; /etc/keepalived/check_apiserver.sh

chmod +x /etc/keepalived/check_apiserver.sh
</code></pre>

<pre><code>cat &lt;&lt;EOF &gt; /etc/keepalived/keepalived.conf
! Configuration File for keepalived
global_defs {
    router_id k8s-node2
    enable_script_security
    script_user root
}
vrrp_script check_apiserver {
    script &quot;/etc/keepalived/check_apiserver.sh&quot;
    interval 5
    weight -10
    fall 2
    rise 2
}
vrrp_instance VI_1 {
    state BACKUP
    interface enp0s8
    mcast_src_ip 172.17.8.102
    virtual_router_id 51
    priority 100
    advert_int 5
    authentication {
        auth_type PASS
        auth_pass Password1
    }
    virtual_ipaddress {
        172.17.8.100/24 brd 172.17.8.255 dev enp0s8
    }
    track_script {
       check_apiserver
    }
}
EOF

systemctl start keepalived
systemctl enable keepalived
</code></pre>

<pre><code>cat &lt;&lt;EOF &gt; /etc/keepalived/keepalived.conf
! Configuration File for keepalived
global_defs {
    router_id k8s-node3
    enable_script_security
    script_user root
}
vrrp_script check_apiserver {
    script &quot;/etc/keepalived/check_apiserver.sh&quot;
    interval 5
    weight -10
    fall 2
    rise 2
}
vrrp_instance VI_1 {
    state BACKUP
    interface enp0s8
    mcast_src_ip 172.17.8.103
    virtual_router_id 51
    priority 50
    advert_int 5
    authentication {
        auth_type PASS
        auth_pass Password1
    }
    virtual_ipaddress {
        172.17.8.100/24 brd 172.17.8.255 dev enp0s8
    }
    track_script {
       check_apiserver
    }
}
EOF

systemctl start keepalived
systemctl enable keepalived
</code></pre>

<pre><code>kubectl scale deploy/coredns  --replicas=3 -n kube-system
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[K8s ERROR at kubectl logs]]></title>
            <link href="https://devopstales.github.io/home/k8s-error-at-kubectl-logs/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-error-at-kubectl-logs/?utm_source=atom_feed" rel="related" type="text/html" title="K8s ERROR at kubectl logs" />
                <link href="https://devopstales.github.io/home/k8s-kuberos/?utm_source=atom_feed" rel="related" type="text/html" title="Kubectl authentication with Kuberos" />
                <link href="https://devopstales.github.io/home/helm3-loki/?utm_source=atom_feed" rel="related" type="text/html" title="Install Grafana Loki with Helm3" />
                <link href="https://devopstales.github.io/home/k8s-ceph-storage-with-csi-driver/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD volume with CSI driver" />
                <link href="https://devopstales.github.io/home/grafana-loki/?utm_source=atom_feed" rel="related" type="text/html" title="Grafana Loki" />
            
                <id>https://devopstales.github.io/home/k8s-error-at-kubectl-logs/</id>
            
            
            <published>2020-03-06T00:00:00+00:00</published>
            <updated>2020-03-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I installed a kubernetes cluster in a Vagran environment. First everything was fine, but wen I try to add the command <code>kubectl logs</code> I got this error:</p>

<pre><code>$ kubectl logs busybox-7c9687585b-12d75
Error from server (NotFound): the server could not find the requested resource ( pods/log busybox-7c9687585b-12d75)
</code></pre>

<p>and another for port-forward and exec:</p>

<pre><code>$ kubectl exec -it busybox-7c9687585b-12d75 -- /bin/sh
error: unable to upgrade connection: pod does not exist
</code></pre>

<p>When I eun with <code>-v=9</code> I hot HTTP Error Code 404 all around. I wondered if the connection with the kubelet is wrong because of the multiple interfaces of the VMs? So I tested it:</p>

<pre><code>$ kubectl get nodes worker1 -o yaml
apiVersion: v1
kind: Node
...
status:
  addresses:
  - address: 10.0.2.15
...
</code></pre>

<p>The was the problem, the address is came from the NAT interface of the VM not from the bridged. But why? On the master I explicitly set the <code>--apiserver-advertise-address</code> for the bridged interface&rsquo;s IP. I needed to add an explicit IP address of the bridged interface on the workers too. The problem is there is no option for that in kubeadm.</p>

<p>I looked at the man page of the kubelet and I found the following option:</p>

<pre><code>--node-ip string    IP address of the node. If set, kubelet will use this IP address for the node
</code></pre>

<p>That is wat I need to set. So I added a no job to the end of my bootstrap scripts for master and worker nodes too.</p>

<pre><code>echo KUBELET_EXTRA_ARGS=\&quot;--node-ip=`ip addr show enp0s8 | grep inet | grep -E -o &quot;([0-9]{1,3}[\.]){3}[0-9]{1,3}/&quot; | tr -d '/'`\&quot; &gt; /etc/sysconfig/kubelet
systemctl restart kubelet
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[PhotonOS Basics]]></title>
            <link href="https://devopstales.github.io/home/photon_basics/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/protonos_vagrant_box/?utm_source=atom_feed" rel="related" type="text/html" title="How to create Vagrant box?" />
                <link href="https://devopstales.github.io/cloud/photon_basics/?utm_source=atom_feed" rel="related" type="text/html" title="PhotonOS Basics" />
                <link href="https://devopstales.github.io/cloud/protonos_vagrant_box/?utm_source=atom_feed" rel="related" type="text/html" title="How to create Vagrant box?" />
                <link href="https://devopstales.github.io/home/openshift_4/?utm_source=atom_feed" rel="related" type="text/html" title="OpenShift 4.2 with Red Hat CodeReady Containers" />
                <link href="https://devopstales.github.io/home/ceph_backup_benji/?utm_source=atom_feed" rel="related" type="text/html" title="CEPH backup with Benji" />
            
                <id>https://devopstales.github.io/home/photon_basics/</id>
            
            
            <published>2020-03-04T00:00:00+00:00</published>
            <updated>2020-03-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Project Photon OS is an open source, minimal Linux container host that is optimized for cloud-native applications, cloud platforms, and VMware infrastructure.</p>

<h3 id="star-in-vagrant">Star in Vagrant</h3>

<pre><code>nano Vagrantfile
Vagrant.configure(&quot;2&quot;) do |config|
  config.vm.box = &quot;devopstales/photon3&quot;
  config.vm.synced_folder &quot;.&quot;, &quot;/vagrant&quot;, type: &quot;virtualbox&quot;

  config.timezone.value = :host
  config.vm.provider &quot;virtualbox&quot; do |vb|
    vb.name = &quot;photon01&quot;
      vb.memory = 4096

      vb.cpus = 2
      vb.linked_clone = true
      vb.customize [&quot;modifyvm&quot;, :id, &quot;--vram&quot;, &quot;8&quot;]
  end

  config.vm.define &quot;pothon01&quot; do |vbox|
    vbox.vm.network :public_network, ip: &quot;192.168.0.112&quot; , bridge: &quot;wlan0&quot; #, adapter: &quot;1&quot;
    vbox.vm.hostname = &quot;pothon01.mydomain.intra&quot;
    vbox.hostsupdater.remove_on_suspend = false
    vbox.vbguest.auto_update = false
  end
end
</code></pre>

<pre><code>vagrant up
vagrant ssh
</code></pre>

<p>When you try to login the server wants you to change the passowrd of the <code>vagrant</code> user. The base password is <code>vagrant</code> as usual.</p>

<pre><code>vagrant ssh
You are required to change your password immediately (password expired)
Last login: Sun Apr 14 20:30:22 2019 from 10.0.2.2
WARNING: Your password has expired.
You must change your password now and login again!
Changing password for vagrant.
Current password:
New password:
Retype new password:
passwd: password updated successfully
Connection to 127.0.0.1 closed.
</code></pre>

<p>Vagrant can&rsquo;t configure the ip, hostname and mount the <code>/vagrant</code> fonder.</p>

<h3 id="package-management">Package management</h3>

<p>On Photon OS, <code>tdnf</code> is the default package manager for installing new packages. It is a C implementation of the <code>DNF</code> package manager without Python dependencies. <code>DNF</code> is the next upcoming major version of yum.</p>

<p>Let&rsquo;s install packages for the next steps.</p>

<pre><code>tdnf install nano awk tar build-essential linux-devel less -y
</code></pre>

<h3 id="install-virtualbox-guest-additions">Install virtualbox guest additions</h3>

<p><img src="/img/include/photon_base_1.png" alt="Example image" /><br><br></p>

<pre><code>mount /dev/cdrom /mnt/cdrom
cd /mnt/cdrom
./VBoxLinuxAdditions.run
</code></pre>

<h3 id="configure-static-ip">Configure static ip</h3>

<p>PhotonOS use systemd-networkd to manage network configurations. systemd-networkd configorations is located under <code>/etc/systemd/network/</code>.</p>

<pre><code>cat /etc/systemd/network/99-dhcp-en.network
[Match]
Name=e*

[Network]
DHCP=yes
IPv6AcceptRA=no
</code></pre>

<pre><code>cat &gt; /etc/systemd/network/20-static-eth1.network &lt;&lt; &quot;EOF&quot;
[Match]
Name=eth1

[Network]
DHCP=no
Address=192.168.0.112/24
Gateway=192.168.0.1
DNS=8.8.8.8
Domains=mydomain.intra
NTP=0.pool.ntp.org
EOF
</code></pre>

<pre><code>chmod 644 /etc/systemd/network/20-static-eth1.network
systemctl restart systemd-networkd
systemctl status systemd-networkd -l
</code></pre>

<h3 id="configure-hostname">Configure hostname</h3>

<pre><code>hostnamectl set-hostname &quot;pothon01.mydomain.intra&quot;
</code></pre>

<pre><code>hostnamectl status
   Static hostname: pothon01.mydomain.intra
         Icon name: computer-vm
           Chassis: vm
        Machine ID: 2c230d2255834f75bff4872bff234df4
           Boot ID: 2708cffec4cf4c6e91053cc11825b590
    Virtualization: oracle
  Operating System: VMware Photon OS/Linux
            Kernel: Linux 4.19.32-3.ph3
      Architecture: x86-64
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to create Vagrant box?]]></title>
            <link href="https://devopstales.github.io/home/protonos_vagrant_box/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/protonos_vagrant_box/?utm_source=atom_feed" rel="related" type="text/html" title="How to create Vagrant box?" />
                <link href="https://devopstales.github.io/home/openshift_4/?utm_source=atom_feed" rel="related" type="text/html" title="OpenShift 4.2 with Red Hat CodeReady Containers" />
                <link href="https://devopstales.github.io/home/ceph_backup_benji/?utm_source=atom_feed" rel="related" type="text/html" title="CEPH backup with Benji" />
                <link href="https://devopstales.github.io/home/networkmanagger-dnsmasq/?utm_source=atom_feed" rel="related" type="text/html" title="Using the NetworkManager’s DNSMasq plugin" />
                <link href="https://devopstales.github.io/home/pgbackrest_backup_to_s3/?utm_source=atom_feed" rel="related" type="text/html" title="pgBackRest Backup to S3" />
            
                <id>https://devopstales.github.io/home/protonos_vagrant_box/</id>
            
            
            <published>2020-03-03T00:00:00+00:00</published>
            <updated>2020-03-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how to create a vagrant box from pothonos ISO.</p>

<h3 id="base-vm">Base VM</h3>

<p>Fist we need to create a virtualbox vm and install the <a href="https://github.com/vmware/photon/wiki/Downloading-Photon-OS">latest PothonOS ISO</a>.</p>

<ul>
<li>Name: Photon-base</li>
<li>Type: Linux</li>
<li>Version: Other Linux (64-bit)</li>
<li>Memory Size: 1024MB</li>
<li>New Virtual Disk: [Type: VDI, Size: 40 GB]</li>
</ul>

<p>Modify the hardware settings of the virtual machine:</p>

<ul>
<li>Disable audio</li>
<li>Disable USB</li>
<li>Ensure Network Adapter 1 is set to NAT</li>
<li>Add this port-forwarding rule:

<ul>
<li>Name: SSH</li>
<li>Protocol: TCP</li>
<li>Host IP: blank</li>
<li>Host Port: 2222</li>
<li>Guest IP: blank</li>
<li>Guest Port: 22</li>
</ul></li>
</ul>

<h3 id="configure-system">Configure System</h3>

<p>Change the root users password to <code>vagrant</code></p>

<pre><code>passwd
chage -I -1 -m 0 -M 99999 -E -1 root
</code></pre>

<p>Upgrade system and install packages:</p>

<pre><code>tdnf upgrade -y
tdnf install sudo nano wget awk tar build-essential linux-devel less -y
</code></pre>

<h3 id="create-the-vagrant-account">Create the vagrant account</h3>

<p>Next you need to create the default vagrant user account:</p>

<pre><code>useradd -m -G sudo vagrant
passwd vagrant
chage -I -1 -m 0 -M 99999 -E -1 vagrant

cat &gt; /etc/sudoers.d/vagrant &lt; EOF
# add vagrant user
vagrant ALL=(ALL) NOPASSWD:ALL
EOF
</code></pre>

<h3 id="change-ssh-config">Change ssh config</h3>

<pre><code>nano /etc/ssh/sshd_config
AuthorizedKeysFile     %h/.ssh/authorized_keys
</code></pre>

<pre><code>systemctl restart sshd
</code></pre>

<h3 id="add-vagrant-ssh-keys-to-vagrant-user">Add vagrant ssh keys to vagrant user</h3>

<pre><code>su - vagrant

mkdir -p /home/vagrant/.ssh
chmod 0700 /home/vagrant/.ssh
</code></pre>

<pre><code>wget --no-check-certificate \
    https://raw.github.com/mitchellh/vagrant/master/keys/vagrant.pub \
    -O /home/vagrant/.ssh/authorized_keys
</code></pre>

<pre><code>chmod 0600 /home/vagrant/.ssh/authorized_keys
chown –R vagrant /home/vagrant/.ssh
</code></pre>

<h3 id="add-virtualboxadditions">Add VirtualBoxadditions</h3>

<p>Go to your virtualbox menu for the VM and select <code>Devices / Insert Guest Additions CD Image</code></p>

<p><img src="/img/include/photon_base_1.png" alt="Example image" /><br><br></p>

<pre><code>mount /dev/cdrom /mnt/cdrom
cd /mnt/cdrom
./VBoxLinuxAdditions.run
</code></pre>

<h3 id="compress-vm">Compress vm</h3>

<pre><code>sudo dd if=/dev/zero of=/EMPTY bs=1M
sudo rm -f /EMPTY
</code></pre>

<h3 id="package-box">Package box</h3>

<pre><code>vagrant package --base &quot;Photon-base&quot;

vagrant box add &lt;user-name&gt;/photon3 package.box
vagrant box add devopstales/photon3 package.box
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[OpenShift 4.2 with Red Hat CodeReady Containers]]></title>
            <link href="https://devopstales.github.io/home/openshift_4/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-secondary-router/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift secondary route" />
                <link href="https://devopstales.github.io/home/openshift-gitlab-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Installing GitLab on OpenShift" />
                <link href="https://devopstales.github.io/home/openshift-kompose/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Run docker-compoe in Openshift" />
                <link href="https://devopstales.github.io/home/openshift-add-node/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Add Nodes to a Cluster" />
                <link href="https://devopstales.github.io/home/openshift-letsencrypt/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Letsencrypt certificates" />
            
                <id>https://devopstales.github.io/home/openshift_4/</id>
            
            
            <published>2020-03-02T00:00:00+00:00</published>
            <updated>2020-03-02T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>The Red Hat CodeReady Containers enables you to run a minimal OpenShift 4.2 or newer cluster on your local laptop or desktop computer.</p>

<h3 id="download-crc">Download CRC</h3>

<pre><code>wget https://mirror.openshift.com/pub/openshift-v4/clients/crc/latest/crc-linux-amd64.tar.xz
tar xvf crc-linux-amd64.tar.xz
sudo cp crc-linux-*-amd64/crc /usr/local/sbin/
crc version
</code></pre>

<h3 id="setup-crs">Setup CRS</h3>

<pre><code>crc setup
</code></pre>

<pre><code>INFO Checking if running as non-root
INFO Caching oc binary
INFO Setting up virtualization
INFO Setting up KVM
INFO Installing libvirt service and dependencies
INFO Adding user to libvirt group
INFO Enabling libvirt
INFO Starting libvirt service
INFO Will use root access: start libvirtd service
[sudo] password for devopstales:
INFO Checking if a supported libvirt version is installed
INFO Installing crc-driver-libvirt
INFO Removing older system-wide crc-driver-libvirt
INFO Setting up libvirt 'crc' network
INFO Starting libvirt 'crc' network
INFO Checking if NetworkManager is installed
INFO Checking if NetworkManager service is running
INFO Writing Network Manager config for crc
INFO Will use root access: write NetworkManager config in /etc/NetworkManager/conf.d/crc-nm-dnsmasq.conf
INFO Will use root access: execute systemctl daemon-reload command
INFO Will use root access: execute systemctl stop/start command
INFO Writing dnsmasq config for crc
INFO Will use root access: write dnsmasq configuration in /etc/NetworkManager/dnsmasq.d/crc.conf
INFO Will use root access: execute systemctl daemon-reload command
INFO Will use root access: execute systemctl stop/start command
INFO Unpacking bundle from the CRC binary
Setup is complete, you can now run 'crc start' to start the OpenShift cluster
</code></pre>

<p>Thec configuration creats two dnsmasq config:</p>

<pre><code>cat /etc/NetworkManager/conf.d/crc-nm-dnsmasq.conf
[main]
dns=dnsmasq

cat /etc/NetworkManager/dnsmasq.d/crc.conf
server=/apps-crc.testing/192.168.130.11
server=/crc.testing/192.168.130.11
</code></pre>

<p>The first enable the NetworkManager&rsquo;s dnsmasq plugin to be used as a dns server and the second points two dns zone the <code>*.apps-crc.testing</code> and the <code>*.crc.testing</code> to the ip of the new vm&rsquo;s ip. The crc creats a kvm network for the vm whit this ip range.</p>

<pre><code>sudo virsh net-list
 Name      State    Autostart   Persistent
--------------------------------------------
 crc       active   yes         yes
 default   active   yes         yes
</code></pre>

<pre><code>sudo virsh net-edit
 &lt;network&gt;
  &lt;name&gt;crc&lt;/name&gt;
  &lt;uuid&gt;49eee855-d342-46c3-9ed3-b8d1758814cd&lt;/uuid&gt;
  &lt;forward mode='nat'&gt;
    &lt;nat&gt;
      &lt;port start='1024' end='65535'/&gt;
    &lt;/nat&gt;
  &lt;/forward&gt;
  &lt;bridge name='crc' stp='on' delay='0'/&gt;

  &lt;mac address='52:54:00:fd:be:d0'/&gt;
  &lt;ip family='ipv4' address='192.168.130.1' prefix='24'&gt;
    &lt;dhcp&gt;
      &lt;host mac='52:fd:fc:07:21:82' ip='192.168.130.11'/&gt;
    &lt;/dhcp&gt;
  &lt;/ip&gt;
&lt;/network&gt;
</code></pre>

<h3 id="start-crs">Start CRS</h3>

<pre><code>crc start
</code></pre>

<pre><code>INFO Checking if running as non-root
INFO Checking if oc binary is cached
INFO Checking if Virtualization is enabled
INFO Checking if KVM is enabled
INFO Checking if libvirt is installed
INFO Checking if user is part of libvirt group
INFO Checking if libvirt is enabled
INFO Checking if libvirt daemon is running
INFO Checking if a supported libvirt version is installed
INFO Checking if crc-driver-libvirt is installed
INFO Checking if libvirt 'crc' network is available
INFO Checking if libvirt 'crc' network is active
INFO Checking if NetworkManager is installed
INFO Checking if NetworkManager service is running
INFO Checking if /etc/NetworkManager/conf.d/crc-nm-dnsmasq.conf exists
INFO Checking if /etc/NetworkManager/dnsmasq.d/crc.conf exists
? Image pull secret [? for help]
</code></pre>

<p>Please note that a valid OpenShift user pull secret is required during installation. The pull secret can be copied or downloaded from the Pull Secret section of the <a href="https://cloud.redhat.com/openshift/install/crc/installer-provisioned">Install on Laptop: Red Hat CodeReady Containers</a> page on cloud.redhat.com.</p>

<pre><code>INFO To access the cluster, first set up your environment by following 'crc oc-env' instructions
INFO Then you can access it by running 'oc login -u developer -p developer https://api.crc.testing:6443'
INFO To login as an admin, run 'oc login -u kubeadmin -p 7z6T5-qmTth-oxaoD-p3xQF https://api.crc.testing:6443'
INFO
INFO You can now run 'crc console' and use these credentials to access the OpenShift web console
</code></pre>

<p>Go to the console:</p>

<pre><code>crc console
</code></pre>

<p><img src="/img/include/okd4.png" alt="Example image" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[CEPH backup with Benji]]></title>
            <link href="https://devopstales.github.io/home/ceph_backup_benji/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/ceph_backup_benji/?utm_source=atom_feed" rel="related" type="text/html" title="CEPH backup with Benji" />
                <link href="https://devopstales.github.io/home/k8s-ceph-storage-with-csi-driver/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD volume with CSI driver" />
                <link href="https://devopstales.github.io/home/who-mapping-rbd-device/?utm_source=atom_feed" rel="related" type="text/html" title="Ceph: who&#39;s mapping a RBD device" />
                <link href="https://devopstales.github.io/home/k8s-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/install-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ceph cluster" />
            
                <id>https://devopstales.github.io/home/ceph_backup_benji/</id>
            
            
            <published>2020-02-24T00:00:00+00:00</published>
            <updated>2020-02-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this article I will show you how to use benji to backup CEPH RBD incrementally.</p>

<p>Benji Backup is a block based deduplicating backup software. It builds on the excellent foundations and concepts of backy² by Daniel Kraft.</p>

<p>Benji requires <code>Python 3.6.5</code> or newer because older Python versions have some shortcomings in the <code>concurrent.futures</code> implementation which lead to an excessive memory usage. So in this example I will use docker to make easier to implement.</p>

<pre><code>mkdir -p /opt/benji/config/
mkdir -p /opt/benji/postgresql/data

nano /opt/benji/docker-compose.yaml
version: '3.5'

services:
  benji:
    image: elementalnet/benji-k8s:0.5.0
    restart: always
    volumes:
      - /etc/ceph:/etc/ceph
      - /backup/benji-ceph:/data
      - /opt/benji/config/benji.yaml:/benji/etc/benji.yaml
      - /opt/benji/config/backup-ceph:/benji/scripts/backup-ceph
      - /opt/benji/config/crontab:/benji/etc/crontab

  postgres:
    image: postgres:11
    restart: always
    environment:
      - &quot;POSTGRES_DB=benji-prod&quot;
      - &quot;POSTGRES_USER=benji-prod&quot;
      - &quot;POSTGRES_PASSWORD=Password1&quot;
    volumes:
      - /opt/benji/postgresql/data:/var/lib/postgresql/data
</code></pre>

<h3 id="create-benji-config">Create benji config</h3>

<pre><code>nano /opt/benji/config/benji.yaml
configurationVersion: '1'
databaseEngine: postgresql://benji-prod:Password1@postgres:5432/benji-prod
defaultStorage: storage-1
storages:
  - name: storage-1
    storageId: 1
    module: file
    configuration:
      path: /data
ios:
  - name: rbd
    module: rbd
</code></pre>

<h3 id="create-backup-script">Create backup script</h3>

<pre><code>nano /opt/benji/config/backup-ceph
#!/bin/bash

. common.sh
. prometheus.sh
. ceph.sh
. hooks.sh

FSFREEZE=no
BENJI_INSTANCE:devopstales
POOL=testpool
EXCLUDE=&quot;test1&quot;

# benji::backup::ceph test/test test test test/test

echo &quot;POOL :&quot; $POOL;
for IMAGE in `rbd ls $POOL | grep -v @ | grep -v $EXCLUDE`;
do
#   echo $POOL/$IMAGE
   benji::backup::ceph $POOL/$IMAGE $POOL $IMAGE $POOL/$IMAGE
done
</code></pre>

<h3 id="create-crontab">Create crontab</h3>

<pre><code>nano /opt/benji/config/crontab
BENJI_INSTANCE:devopstales
00 00 * * * root backup-ceph
00 04 * * * root benji-command enforce days8,weeks3,months2
00 05 * * * root benji-command cleanup
30 05 * * * root benji-versions-status
00 06 * * * root benji-command batch-deep-scrub
</code></pre>

<h3 id="perform-a-backup">Perform a backup</h3>

<pre><code>cd /opt/benji/
docker-compose up -d
docker exec -it benji_benji_1 bash
benji database-init
benji backup rbd:testpool/test2 devopstales
benji ls
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Using the NetworkManager’s DNSMasq plugin]]></title>
            <link href="https://devopstales.github.io/home/networkmanagger-dnsmasq/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/ceph_backup_benji/?utm_source=atom_feed" rel="related" type="text/html" title="CEPH backup with Benji" />
                <link href="https://devopstales.github.io/linux/networkmanagger-dnsmasq/?utm_source=atom_feed" rel="related" type="text/html" title="Using the NetworkManager’s DNSMasq plugin" />
                <link href="https://devopstales.github.io/home/pgbackrest_backup_to_s3/?utm_source=atom_feed" rel="related" type="text/html" title="pgBackRest Backup to S3" />
                <link href="https://devopstales.github.io/home/pgbackrest_backup_server/?utm_source=atom_feed" rel="related" type="text/html" title="pgBackRest Backup server" />
                <link href="https://devopstales.github.io/home/postgresql_pgbackrest/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL backup with pgBackRest" />
            
                <id>https://devopstales.github.io/home/networkmanagger-dnsmasq/</id>
            
            
            <published>2020-02-24T00:00:00+00:00</published>
            <updated>2020-02-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Imagine you want to test something in a demo setup with 5 machines. You create the necessary VMs in your local environment – but you cannot address them properly by name. With 5 machines you also need to write down the appropriate IP addresses – that’s hardly practical.</p>

<p>Luckily, there is an elegant solution: The dnsmasq plugin is a hidden gem of NetworkManager.</p>

<h3 id="enable-networkmanager-s-dnsmasq">Enable NetworkManager&rsquo;s dnsmasq</h3>

<pre><code># /etc/NetworkManager/conf.d/00-use-dnsmasq.conf
#
# This enabled the dnsmasq plugin.
[main]
dns=dnsmasq
</code></pre>

<pre><code># /etc/NetworkManager/dnsmasq.d/00-homelab.conf
# This file sets up the local lablab domain and
# defines some aliases and a wildcard.
local=/homelab/

# The below defines a Wildcard DNS Entry.
address=/.ose.homelab/192.168.101.125

# Below I define some host names.  I also pull in   
address=/openshift.homelab/192.168.101.120
address=/openshift-int.homelab/192.168.101.120
</code></pre>

<pre><code># /etc/NetworkManager/dnsmasq.d/02-add-hosts.conf
# By default, the plugin does not read from /etc/hosts.  
# This forces the plugin to slurp in the file.
#
# If you didn't want to write to the /etc/hosts file.  This could
# be pointed to another file.
#
addn-hosts=/etc/hosts
</code></pre>

<p>Restart your network managger <code>systemctl restart NetworkManager</code>. If everything is working right, you should see that your resolv.conf points to 127.0.0.1 and a new dnsmasq process spawned.</p>

<pre><code>cat /etc/resolv.conf
# Generated by NetworkManager
nameserver 127.0.0.1
</code></pre>

<h3 id="configurate-networkmanager-fo-libvirt-s-domain">Configurate NetworkManager fo libvirt&rsquo;s domain</h3>

<p>Libvirt comes with its own in-build DNS server, dnsmasq to serve DHCP and DNS to servers for vms.
Additionally, NetworkManager can be configured to use its dnsmasq plugin to forwarding DNS requests to the libvirt instance if needed.</p>

<pre><code># /etc/NetworkManager/dnsmasq.d/01-libvirt_dnsmasq.conf
server=/qxyz.intra/192.168.122.1
</code></pre>

<h3 id="configuring-libvirt">Configuring libvirt</h3>

<p>First of all, libvirt needs to be configured. Given that the network &ldquo;default&rdquo; is assigned to the relevant VMs, the configuration should look like this:</p>

<pre><code>sudo virsh net-edit default
&lt;network connections='1'&gt;
  &lt;name&gt;default&lt;/name&gt;
  &lt;uuid&gt;158880c3-9adb-4a44-ab51-d0bc1c18cddc&lt;/uuid&gt;
  &lt;forward mode='nat'&gt;
    &lt;nat&gt;
      &lt;port start='1024' end='65535'/&gt;
    &lt;/nat&gt;
  &lt;/forward&gt;
  &lt;bridge name='virbr0' stp='on' delay='0'/&gt;
  &lt;mac address='52:54:00:fa:cb:e5'/&gt;
  &lt;domain name='qxyz.de' localOnly='yes'/&gt;
  &lt;ip address='192.168.122.1' netmask='255.255.255.0'&gt;
    &lt;dhcp&gt;
      &lt;range start='192.168.122.128' end='192.168.122.254'/&gt;
    &lt;/dhcp&gt;
  &lt;/ip&gt;
&lt;/network&gt;
</code></pre>

<h3 id="configuring-the-vm-guests">Configuring the VM guests</h3>

<pre><code>sudo hostnamectl set-hostname neon.qxyz.intra
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[pgBackRest Backup to S3]]></title>
            <link href="https://devopstales.github.io/home/pgbackrest_backup_to_s3/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/pgbackrest_backup_server/?utm_source=atom_feed" rel="related" type="text/html" title="pgBackRest Backup server" />
                <link href="https://devopstales.github.io/home/postgresql_pgbackrest/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL backup with pgBackRest" />
                <link href="https://devopstales.github.io/home/postgresql_replication/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL Streamin replication" />
                <link href="https://devopstales.github.io/home/install-postgresql/?utm_source=atom_feed" rel="related" type="text/html" title="Install PostgreSQL" />
                <link href="https://devopstales.github.io/home/postgresql_pg_rewind/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL: pg_rewind" />
            
                <id>https://devopstales.github.io/home/pgbackrest_backup_to_s3/</id>
            
            
            <published>2020-02-21T00:00:00+00:00</published>
            <updated>2020-02-21T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this article I will show you how to use pgBackRest to backup PostgreSQL servers to S3 buckets.</p>

<p>For the purpose of this demo setup, we’ll use MinIO server, which is an Amazon S3 Compatible Object Storage.</p>

<h3 id="s3cmd">s3cmd</h3>

<pre><code>yum install -y epel-release
yum --enablerepo epel-testing install -y s3cmd
</code></pre>

<pre><code>nano ~/.s3cfg
host_base = minio.mydomain.intra:9000
host_bucket = minio.mydomain.intra:9000
bucket_location = us-east-1
use_https = false
access_key = &lt;minop_access_key&gt;
secret_key = &lt;minio_secret_key&gt;
signature_v2 = False
</code></pre>

<p>Create bucket and folders</p>

<pre><code>s3cmd mb --no-check-certificate s3://pgbackrest
mkdir postgresql1
s3cmd cp postgresql1 --no-check-certificate s3://pgbackrest
s3cmd ls --no-check-certificate s3://pgbackrest/postgresql1
                       DIR   s3://pgbackrest/postgresql1/
</code></pre>

<h3 id="installation">Installation</h3>

<pre><code>yum install epel-release -y
yum update -y
rpm -Uvh https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-centos10-10-2.noarch.rpm
yum install -y pgbackrest
</code></pre>

<h3 id="configure-pgbackrest">Configure pgBackRest</h3>

<pre><code>nano /etc/pgbackrest.conf

[global]
repo1-path=/postgresql1
repo1-type=s3
repo1-s3-endpoint=minio.mydomain.intra
repo1-s3-port=9000
repo1-s3-bucket=pgbackrest
repo1-s3-verify-tls=n
repo1-s3-key=&lt;minop_access_key&gt;
repo1-s3-key-secret=&lt;minio_secret_key&gt;
repo1-s3-region=us-east-1

repo1-retention-full=1
process-max=2
log-level-console=info
log-level-file=debug
start-fast=y
delta=y

[postgresql1]
pg1-path=/var/lib/pgsql/12/data
</code></pre>

<h3 id="configure-postgresql-on-postgresql1">Configure PostgreSQL on postgresql1</h3>

<pre><code>nano /var/lib/pgsql/12/data/postgresql.conf
...
archive_mode = on
archive_command = 'pgbackrest --stanza=postgresql1 archive-push %p'
...

systemctl restart postgresql12
</code></pre>

<h3 id="perform-a-backup">Perform a backup</h3>

<p>Before we can start a backup we need to initialize the backup repository.</p>

<pre><code>sudo -iu postgres pgbackrest --stanza=postgresql1 stanza-create
sudo -iu postgres pgbackrest --stanza=postgresql1 check
</code></pre>

<pre><code>sudo -iu postgres pgbackrest --stanza=postgresql1 --type=full backup
</code></pre>

<h3 id="show-backup-information">Show backup information</h3>

<pre><code>postgresql1 -iu postgres pgbackrest info
stanza: postgresql1
    status: ok
    cipher: none

    db (current)
        wal archive min/max (11-1): 000000010000000000000005/000000010000000000000005

        full backup: 20200219-091209F
            timestamp start/stop: 2020-02-19 09:12:09 / 2020-02-19 09:12:21
            wal start/stop: 000000010000000000000005 / 000000010000000000000005
            database size: 23.5MB, backup size: 23.5MB
            repository size: 2.8MB, repository backup size: 2.8MB
</code></pre>

<h3 id="restore-a-backup">Restore a backup</h3>

<p>The restore command can then be used on the postgresql1 host.</p>

<pre><code>sudo systemctl stop postgresql12

sudo -iu postgres pgbackrest --stanza=postgresql1 restore
</code></pre>

<p>Restore only test database:</p>

<pre><code>sudo -iu postgres pgbackrest --stanza=postgresql1 --delta --db-include=test restore
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[pgBackRest Backup server]]></title>
            <link href="https://devopstales.github.io/home/pgbackrest_backup_server/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/postgresql_pgbackrest/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL backup with pgBackRest" />
                <link href="https://devopstales.github.io/home/postgresql_replication/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL Streamin replication" />
                <link href="https://devopstales.github.io/home/install-postgresql/?utm_source=atom_feed" rel="related" type="text/html" title="Install PostgreSQL" />
                <link href="https://devopstales.github.io/home/postgresql_pg_rewind/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL: pg_rewind" />
                <link href="https://devopstales.github.io/linux/pgbackrest_backup_server/?utm_source=atom_feed" rel="related" type="text/html" title="pgBackRest Backup server" />
            
                <id>https://devopstales.github.io/home/pgbackrest_backup_server/</id>
            
            
            <published>2020-02-20T00:00:00+00:00</published>
            <updated>2020-02-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this article I will show you how to use pgBackRest in a dedicated backup server to backup remote PostgreSQL servers.</p>

<h3 id="installation">Installation</h3>

<pre><code>yum install epel-release -y
yum update -y
rpm -Uvh https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-centos10-10-2.noarch.rpm
yum install -y pgbackrest
</code></pre>

<h3 id="allow-passwordles-ssh-for-hosts">Allow passwordles ssh for hosts</h3>

<p>Generate ssh key on all hosts and copy ssh keys</p>

<pre><code>sudo -u postgres ssh-keygen -f /var/lib/pgsql/.ssh/id_rsa
sudo -u postgres restorecon -R /var/lib/pgsql/.ssh
</code></pre>

<pre><code>sudo -u postgres ssh-copy-id -i /var/lib/pgsql/.ssh/id_rsa postgres@backup-server

sudo -u postgres ssh-copy-id -i /var/lib/pgsql/.ssh/id_rsa postgres@postgresql1
</code></pre>

<h3 id="configure-pgbackrest-on-postgresql1">Configure pgBackRest on postgresql1</h3>

<pre><code>nano /etc/pgbackrest.conf

[global]
repo1-host=backup-server
repo1-host-user=postgres
process-max=2
log-level-console=info
log-level-file=debug

[postgresql1]
pg1-path=/var/lib/pgsql/12/data
</code></pre>

<h3 id="configure-postgresql-on-postgresql1">Configure PostgreSQL on postgresql1</h3>

<pre><code>nano /var/lib/pgsql/12/data/postgresql.conf
...
archive_mode = on
archive_command = 'pgbackrest --stanza=postgresql1 archive-push %p'
...

systemctl restart postgresql12
</code></pre>

<h3 id="configure-pgbackrest-on-backup-server">Configure pgBackRest on backup-server</h3>

<pre><code>nano /etc/pgbackrest.conf

[global]
repo1-path=/var/lib/pgbackrest
repo1-retention-full=1
process-max=2
log-level-console=info
log-level-file=debug
start-fast=y
stop-auto=y

[postgresql1]
pg1-path=/var/lib/pgsql/11/data
pg1-host=postgresql1
pg1-host-user=postgres
</code></pre>

<h3 id="perform-a-backup">Perform a backup</h3>

<p>Before we can start a backup we need to initialize the backup repository on backup-server.</p>

<pre><code>sudo -iu postgres pgbackrest --stanza=postgresql1 stanza-create
</code></pre>

<p>Finally, check the configuration on all hosts.</p>

<pre><code>sudo -iu postgres pgbackrest --stanza=postgresql1 check
</code></pre>

<pre><code>sudo -iu postgres pgbackrest --stanza=postgresql1 --type=full backup
</code></pre>

<h3 id="show-backup-information">Show backup information</h3>

<pre><code>postgresql1 -iu postgres pgbackrest info
stanza: postgresql1
    status: ok
    cipher: none

    db (current)
        wal archive min/max (11-1): 000000010000000000000005/000000010000000000000005

        full backup: 20200219-091209F
            timestamp start/stop: 2020-02-19 09:12:09 / 2020-02-19 09:12:21
            wal start/stop: 000000010000000000000005 / 000000010000000000000005
            database size: 23.5MB, backup size: 23.5MB
            repository size: 2.8MB, repository backup size: 2.8MB
</code></pre>

<h3 id="restore-a-backup">Restore a backup</h3>

<p>The restore command can then be used on the postgresql1 host.</p>

<pre><code>sudo systemctl stop postgresql12

sudo -iu postgres pgbackrest --stanza=postgresql1 restore
</code></pre>

<p>Restore only test database:</p>

<pre><code>sudo -iu postgres pgbackrest --stanza=postgresql1 --delta --db-include=test restore
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[PostgreSQL backup with pgBackRest]]></title>
            <link href="https://devopstales.github.io/home/postgresql_pgbackrest/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/postgresql_replication/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL Streamin replication" />
                <link href="https://devopstales.github.io/home/install-postgresql/?utm_source=atom_feed" rel="related" type="text/html" title="Install PostgreSQL" />
                <link href="https://devopstales.github.io/home/postgresql_pg_rewind/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL: pg_rewind" />
                <link href="https://devopstales.github.io/linux/postgresql_pgbackrest/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL backup with pgBackRest" />
                <link href="https://devopstales.github.io/home/mattermost-keycloak-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Free sso for Mattermost Teams Edition" />
            
                <id>https://devopstales.github.io/home/postgresql_pgbackrest/</id>
            
            
            <published>2020-02-19T00:00:00+00:00</published>
            <updated>2020-02-19T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this article I will show you how to use pgBackRest as a backup program for PostgreSQL.</p>

<p>pgBackRest aims to be a simple, reliable backup and restore solution that can seamlessly scale up to the largest databases and workloads by utilizing algorithms that are optimized for database-specific requirements.</p>

<h3 id="installation">Installation</h3>

<pre><code>yum install epel-release -y
yum update -y
rpm -Uvh https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-centos10-10-2.noarch.rpm
yum install -y pgbackrest
</code></pre>

<h3 id="configure-pgbackrest">Configure pgBackRest</h3>

<pre><code>cp /etc/pgbackrest.conf /etc/pgbackrest.conf.bck

nano /etc/pgbackrest.conf
[global]
repo1-path=/var/lib/pgsql/12/backups
log-level-console=info
log-level-file=debug
start-fast=y

[backup_stanza]
pg1-path=/var/lib/pgsql/12/data
repo1-retention-full=1
</code></pre>

<p>The configuration is based on two section the <code>global</code> what is specify the repository where you stores the backups and WAL segments archives and a <code>stanza</code>, in my case <code>backup_stanza</code>.</p>

<p>A stanza defines the backup configuration for a specific PostgreSQL database cluster. The stanza section must define the database cluster path and host/user if the database cluster is remote. Also, any global configuration sections can be overridden to define stanza-specific settings.</p>

<h3 id="configure-postgresql">Configure PostgreSQL</h3>

<pre><code>nano /var/lib/pgsql/12/data/postgresql.conf
...
archive_mode = on
archive_command = 'pgbackrest --stanza=backup_stanza archive-push %p'
...

systemctl restart postgresql12
</code></pre>

<h3 id="perform-a-backup">Perform a backup</h3>

<p>Before we can start a backup we need to initialize the backup repository.</p>

<pre><code>$ sudo -iu postgres pgbackrest --stanza=backup_stanza stanza-create
P00   INFO: stanza-create command begin 2.23: ...
P00   INFO: stanza-create command end: completed successfully

$ sudo -iu postgres pgbackrest --stanza=backup_stanza check
P00   INFO: check command begin 2.23: ...
P00   INFO: WAL segment ... successfully stored in the archive at ...
P00   INFO: check command end: completed successfully
</code></pre>

<pre><code>$ sudo -iu postgres pgbackrest --stanza=backup_stanza --type=full backup
P00   INFO: backup command begin 2.23: ...
P00   INFO: execute non-exclusive pg_start_backup() with label
        &quot;pgBackRest backup started at ...&quot;: backup begins after the requested immediate checkpoint completes
P00   INFO: backup start archive = 000000010000000000000005, lsn = 0/5000028
P00   INFO: full backup size = 23.5MB
P00   INFO: execute non-exclusive pg_stop_backup() and wait for all WAL segments to archive
P00   INFO: backup stop archive = 000000010000000000000005, lsn = 0/5000130
P00   INFO: new backup label = 20200219-091209F
P00   INFO: backup command end: completed successfully
P00   INFO: expire command begin
P00   INFO: expire full backup 20200219-090152F
P00   INFO: remove expired backup 20200219-090152F
P00   INFO: expire command end: completed successfully
</code></pre>

<h3 id="show-backup-information">Show backup information</h3>

<pre><code>$ sudo -iu postgres pgbackrest info
stanza: backup_stanza
    status: ok
    cipher: none

    db (current)
        wal archive min/max (11-1): 000000010000000000000005/000000010000000000000005

        full backup: 20200219-091209F
            timestamp start/stop: 2020-02-19 09:12:09 / 2020-02-19 09:12:21
            wal start/stop: 000000010000000000000005 / 000000010000000000000005
            database size: 23.5MB, backup size: 23.5MB
            repository size: 2.8MB, repository backup size: 2.8MB
</code></pre>

<h3 id="restore-a-backup">Restore a backup</h3>

<pre><code>sudo systemctl stop postgresql12

sudo -iu postgres pgbackrest --stanza=backup_stanza restore
P00   INFO: restore command begin 2.15: ...
P00   INFO: restore backup set 20200219-091209F
P00   INFO: write /var/lib/pgsql/11/data/recovery.conf
P00   INFO: restore global/pg_control (performed last to ensure aborted restores cannot be started)
P00   INFO: restore command end: completed successfully
</code></pre>

<p>Restore only test database:</p>

<pre><code>sudo -iu postgres pgbackrest --stanza=backup_stanza --delta --db-include=test restore
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Free sso for Mattermost Teams Edition]]></title>
            <link href="https://devopstales.github.io/home/mattermost-keycloak-sso/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/sso/mattermost-keycloak-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Free sso for Mattermost Teams Edition" />
                <link href="https://devopstales.github.io/home/k8s-kuberos/?utm_source=atom_feed" rel="related" type="text/html" title="Kubectl authentication with Kuberos" />
                <link href="https://devopstales.github.io/home/k8s-gangway/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak" />
                <link href="https://devopstales.github.io/home/k8s-dasboard-auth/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak" />
                <link href="https://devopstales.github.io/home/grafana-sso/?utm_source=atom_feed" rel="related" type="text/html" title="SSO login to Grafana" />
            
                <id>https://devopstales.github.io/home/mattermost-keycloak-sso/</id>
            
            
            <published>2020-02-16T00:00:00+00:00</published>
            <updated>2020-02-16T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this article I will show you how to use Keycloak as an authentication provider for  Mattermost Teams Edition.</p>

<p>Slack and Mattermost are very similar products. With Mattermost Teams Edition you can get almost all the functionality of the payed Slack. The only problem is sso. In Mattermost you can use google sso login in the E20 licensing what is more costly the Slack. (Slack: 8$/user Mattermost E20: 8.5$/user) In  Mattermost Teams Edition (the free edition of Mattermost) the only authentication provider you can use is gitlab, but thanks to <a href="https://qiita.com/wadahiro/items/8b118c34aae904353865">wadahiro</a> we can use Keycloak instead of gitlab.</p>

<h3 id="configurate-mattermost">Configurate MAttermost</h3>

<pre><code>nano /etc/mattermost/config.json
...
&quot;GitLabSettings&quot;: {
    &quot;Enable&quot;: false,
    &quot;Secret&quot;: &quot;&lt;secret&gt;&quot;,
    &quot;Id&quot;: &quot;mattermost&quot;,
    &quot;Scope&quot;: &quot;&quot;,
    &quot;AuthEndpoint&quot;: &quot;https://keycloak.mydomain.intra/auth/realms/mydomain/protocol/openid-connect/auth&quot;,
    &quot;TokenEndpoint&quot;: &quot;https://keycloak.mydomain.intra/auth/realms/mydomain/protocol/openid-connect/token&quot;,
    &quot;UserApiEndpoint&quot;: &quot;https://keycloak.mydomain.intra/auth/realms/mydomain/protocol/openid-connect/userinfo&quot;
},
</code></pre>

<h3 id="create-client-on-keycloak">Create Client on Keycloak</h3>

<p>In keycloak go to <code>Configure &gt; Clients</code> and create a new client for Mattermostw With the fallowing data:</p>

<ul>
<li>Standard Flow Enabled: <code>ON</code></li>
<li>Access Type: <code>confidential</code></li>
<li>Valid Redirect URIs: <code>http://&lt;Mattermost-FQDN&gt;/signup/gitlab/complete</code></li>
</ul>

<p><img src="/img/include/mattermost_sso1.png" alt="image" /> <br></p>

<h3 id="create-mapper-for-correct-data">Create mapper for correct data</h3>

<p>Mattermost <a href="https://github.com/mattermost/mattermost-server/blob/v4.10.0/model/gitlab/gitlab.go#L19-L25">want</a> the fallowing data from the authentication provider:</p>

<pre><code>type GitLabUser struct {
	Id       int64  `json:&quot;id&quot;`
	Username string `json:&quot;username&quot;`
	Login    string `json:&quot;login&quot;`
	Email    string `json:&quot;email&quot;`
	Name     string `json:&quot;name&quot;`
}
</code></pre>

<p>Create mapping for username:</p>

<p><img src="/img/include/mattermost_sso2.png" alt="image" /> <br></p>

<p>The only problematic data is the ID. For the test run you can use a self generated id like this:</p>

<p><img src="/img/include/mattermost_sso3.png" alt="image" /> <br></p>

<h3 id="create-user-for-test">Create user for test</h3>

<p>For testing purposes we’re going to create a local user in Keycloak.</p>

<p><img src="/img/include/mattermost_sso4.png" alt="image" /> <br></p>

<p>Now we need to create an attribute for this user with the mattermost id and the value should be an integer between 1 and 9999999999999999999.</p>

<p><img src="/img/include/mattermost_sso5.png" alt="image" /> <br></p>

<p>If you have many users you didn&rsquo;t want to create id for them manually. If you use LDAP for users the best fit for this need was the employeeNumber or EmployeeId ldap attribute.</p>

<p><img src="/img/include/mattermost_sso6.png" alt="image" /> <br></p>

<p>If you use ActiveDirectory this attribute maybe null so we need to generate it with a powershell script.</p>

<pre><code>ForEach ($User in ((Get-ADUser -Filter * -Properties SamAccountName,EmployeeId)))
{
if ( ([string]::IsNullOrEmpty($User.EmployeeId)))
{
$DATE = (Get-ADuser $User.SamAccountName -Properties whencreated).whencreated.ToString('yyMMddHHmmss')
$RANDOM = (Get-Random -Maximum 9999)
$DATA = -join ($DATE, $RANDOM)
$User.SamAccountName
$DATA
Set-ADUser $User.SamAccountName -employeeID $DATA
}
}
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[PostgreSQL: pg_rewind]]></title>
            <link href="https://devopstales.github.io/home/postgresql_pg_rewind/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/postgresql_replication/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL Streamin replication" />
                <link href="https://devopstales.github.io/linux/postgresql_pg_rewind/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL: pg_rewind" />
                <link href="https://devopstales.github.io/home/install-postgresql/?utm_source=atom_feed" rel="related" type="text/html" title="Install PostgreSQL" />
                <link href="https://devopstales.github.io/linux/postgresql_replication/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL Streamin replication" />
                <link href="https://devopstales.github.io/home/k8s-longhorn/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes project longhorn" />
            
                <id>https://devopstales.github.io/home/postgresql_pg_rewind/</id>
            
            
            <published>2020-02-05T00:00:00+00:00</published>
            <updated>2020-02-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this pos I will show you how to perform a rewind on a broken Streamin replication.</p>

<h3 id="create-test-db">create test db</h3>

<pre><code># master:
sudo -iu postgres psql -c &quot;create database test1;&quot;
</code></pre>

<h3 id="break-the-replication">break the replication</h3>

<pre><code># slaeve:
sudo -u postgres /usr/pgsql-12/bin/pg_ctl stop -m fast -D /var/lib/pgsql/12/data/
</code></pre>

<p>generate data in master</p>

<pre><code># master:
sudo -iu postgres /usr/pgsql-12/bin/pgbench -i -s 100 -d test1
</code></pre>

<h3 id="perform-the-rewind">perform the rewind</h3>

<pre><code># slaeve:
sudo -u postgres /usr/pgsql-12/bin/pg_rewind  --target-pgdata=/var/lib/pgsql/12/data/ --source-server=&quot;host=192.168.0.110 user=admin password=Password1 dbname=test1&quot; -P
systemctl start postgresql-12
systemctl status postgresql-12
</code></pre>

<h3 id="test-the-replication-status">test the replication status</h3>

<pre><code>/usr/pgsql-12/bin/pg_controldata -D /var/lib/pgsql/12/data/ | grep cluster
Database cluster state:               in archive recovery
</code></pre>

<pre><code># master:
sudo -u postgres psql -x -c &quot;select * from pg_stat_replication&quot;
could not change directory to &quot;/root&quot;: Permission denied
-[ RECORD 1 ]----+------------------------------
pid              | 11548
usesysid         | 16384
usename          | replica_user
application_name | walreceiver
client_addr      | 192.168.0.111
client_hostname  |
client_port      | 48592
backend_start    | 2020-02-08 10:49:45.449591+01
backend_xmin     |
state            | streaming
sent_lsn         | 1/448638D0
write_lsn        | 1/448638D0
flush_lsn        | 1/448638D0
replay_lsn       | 1/448638D0
write_lag        |
flush_lag        |
replay_lag       |
sync_priority    | 0
sync_state       | async
reply_time       | 2020-02-08 10:50:03.026266+01
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[PostgreSQL Streamin replication]]></title>
            <link href="https://devopstales.github.io/home/postgresql_replication/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/install-postgresql/?utm_source=atom_feed" rel="related" type="text/html" title="Install PostgreSQL" />
                <link href="https://devopstales.github.io/linux/postgresql_replication/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL Streamin replication" />
                <link href="https://devopstales.github.io/home/k8s-longhorn/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes project longhorn" />
                <link href="https://devopstales.github.io/home/speed_up_zfs/?utm_source=atom_feed" rel="related" type="text/html" title="How to speed up zfs resilver?" />
                <link href="https://devopstales.github.io/home/k8s-local-pv/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
            
                <id>https://devopstales.github.io/home/postgresql_replication/</id>
            
            
            <published>2020-02-03T00:00:00+00:00</published>
            <updated>2020-02-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this pos I will show you how to configure Postgresql Streamin replication.</p>

<h3 id="wal-segments">WAL segments</h3>

<p>At all times, PostgreSQL maintains a write ahead log (WAL) in the pg_wal/ subdirectory of the cluster&rsquo;s data directory. The log describes every change made to the database&rsquo;s data files. This log exists primarily for crash-safety purposes: if the system crashes, the database can be restored to consistency by “replaying&rdquo; the log entries made since the last checkpoint. This wal segments san be used to repliyate the transactions to a second read database server.</p>

<h3 id="requirements">requirements</h3>

<pre><code>yum install https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm -y
yum install postgresql12 postgresql12-server postgresql12-contrib -y
</code></pre>

<pre><code>/usr/pgsql-12/bin/postgresql-12-setup initdb
</code></pre>

<p>Set up the streaming replication related parameters on the two servers</p>

<pre><code>$ nano /var/lib/pgsql/12/data/postgresql.conf
--------------------------------------------------------
listen_addresses = '*'

wal_level = hot_standby

max_wal_senders = 6

#16 Mb * 32 = 512 Mb
wal_keep_segments = 32

max_wal_size = 2GB

# for pg dump on slave
hot_standby_feedback = on

# for pg_rewind
full_page_writes = on
wal_log_hints = on
</code></pre>

<h3 id="master-config">master config</h3>

<pre><code>$ nano /var/lib/pgsql/12/data/postgresql.conf
--------------------------------------------------------
archive_mode = on
archive_command = 'cp %p /var/lib/pgsql/archive/%f'
</code></pre>

<pre><code>systemctl enable postgresql-12
systemctl start postgresql-12
</code></pre>

<pre><code>su - postgres
pgsql
ALTER SYSTEM SET listen_addresses TO '*';
CREATE ROLE replica_user WITH REPLICATION LOGIN;
\du
\q
exit
</code></pre>

<pre><code>echo &quot;host    replication     replica_user    192.168.0.111/32      trust&quot;&gt;&gt;/var/lib/pgsql/12/data/pg_hba.conf
systemctl restart postgresql-12
</code></pre>

<h3 id="slave-config">slave config</h3>

<pre><code>rm -rf /var/lib/pgsql/12/data/*

su - postgres
/usr/pgsql-12/bin/pg_basebackup --host=192.168.0.110 --pgdata=/var/lib/pgsql/12/data/ --username=replica_user --verbose --progress --wal-method=stream --write-recovery-conf --checkpoint=fast

--create-slot --slot=Slot_name

ll /var/lib/pgsql/12/data/standby.signal
cat /var/lib/pgsql/12/data/postgresql.auto.conf

echo &quot;restore_command = 'cp /var/lib/pgsql/archive/%f %p'&quot; &gt;&gt; /var/lib/pgsql/12/data/postgresql.auto.conf

systemctl start postgresql-12
</code></pre>

<h3 id="slave-test">slave  test</h3>

<pre><code>SELECT * FROM pg_stat_wal_receiver;
sudo -u postgres psql -x -c &quot;select * from pg_stat_wal_receiver&quot;
</code></pre>

<h3 id="master-test">master test</h3>

<pre><code>SELECT * FROM pg_stat_replication;
sudo -u postgres psql -x -c &quot;select * from pg_stat_replication&quot;
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes project longhorn]]></title>
            <link href="https://devopstales.github.io/home/k8s-longhorn/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-longhorn/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes project longhorn" />
                <link href="https://devopstales.github.io/home/k8s-gangway/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak" />
                <link href="https://devopstales.github.io/home/k8s-dasboard-auth/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak" />
                <link href="https://devopstales.github.io/home/speed_up_zfs/?utm_source=atom_feed" rel="related" type="text/html" title="How to speed up zfs resilver?" />
                <link href="https://devopstales.github.io/home/k8s-local-pv/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
            
                <id>https://devopstales.github.io/home/k8s-longhorn/</id>
            
            
            <published>2020-01-18T00:00:00+00:00</published>
            <updated>2020-01-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Longhorn is lightweight, reliable, and powerful distributed block storage system for Kubernetes..</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
<li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-contour-ingress --></li>
<li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
<li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
<li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... --></li>
<li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
<li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
<li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<p>You can install Longhorn on an existing Kubernetes cluster with one <code>kubectl apply</code> command or using Helm charts. Once Longhorn is installed, it adds persistent volume support to the Kubernetes cluster.</p>

<h3 id="install-dependency">Install dependency</h3>

<pre><code>yum install iscsi-initiator-utils

modprobe iscsi_tcp
echo &quot;iscsi_tcp&quot; &gt;/etc/modules-load.d/iscsi-tcp.conf
</code></pre>

<h3 id="deploy-longhorn-and-storageclass">Deploy Longhorn and storageclass</h3>

<pre><code>kubectl apply -f https://raw.githubusercontent.com/rancher/longhorn/master/deploy/longhorn.yaml
kubectl apply -f https://raw.githubusercontent.com/rancher/longhorn/master/examples/storageclass.yaml

kubectl get storageclass
NAME       PROVISIONER          RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
longhorn   driver.longhorn.io   Delete          Immediate           false                  11m
</code></pre>

<p>Patch longhorn storageclass to be the default storageclass.</p>

<pre><code>kubectl patch storageclass longhorn -p \
  '{&quot;metadata&quot;: {&quot;annotations&quot;:{&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;true&quot;}}}'

kubectl get storageclass
NAME                 PROVISIONER          RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
longhorn (default)   driver.longhorn.io   Delete          Immediate           false                  12m
</code></pre>

<h3 id="deploy-admin-gui">Deploy admin gui</h3>

<pre><code>kubectl -n longhorn-system get svc

cat minio-sec.yaml
---
apiVersion: v1
kind: Secret
metadata:
  namespace: longhorn-system
  name: longhorn-minio
type: Opaque
data:
  AWS_ACCESS_KEY_ID: bWluaW8=
  AWS_SECRET_ACCESS_KEY: bWluaW8xMjM=
  AWS_ENDPOINTS: aHR0cDovL21pbmlvLmxvbmdob3JuLXN5c3RlbTo5MDAw
</code></pre>

<p><img src="/img/include/longhorn0.png" alt="Example image" /></br>
<img src="/img/include/longhorn2.png" alt="Example image" /></br>
<img src="/img/include/longhorn1.png" alt="Example image" /></br></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to speed up zfs resilver?]]></title>
            <link href="https://devopstales.github.io/home/speed_up_zfs/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-local-pv/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
                <link href="https://devopstales.github.io/home/k8s-kuberos/?utm_source=atom_feed" rel="related" type="text/html" title="Kubectl authentication with Kuberos" />
                <link href="https://devopstales.github.io/home/k8s-gangway/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak" />
                <link href="https://devopstales.github.io/home/k8s-dasboard-auth/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak" />
                <link href="https://devopstales.github.io/home/helm3-loki/?utm_source=atom_feed" rel="related" type="text/html" title="Install Grafana Loki with Helm3" />
            
                <id>https://devopstales.github.io/home/speed_up_zfs/</id>
            
            
            <published>2020-01-12T00:00:00+00:00</published>
            <updated>2020-01-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this article I will show you how speed up zfs.</p>

<p>First we need to understand there is two type of zfs the FreeBSD/Solaris based and Linux based cald Zfs On Linux or ZOL.</p>

<p>When a device is replaced, a resilvering operation is initiated to move data from the good copies to the new device. This action is a form of disk scrubbing. RAIDz resilvering is very slow in OpenZFS-based zpools. Basically, it starts with every transaction that’s ever happened in the pool and plays them back one-by-one to the new drive. This is very IO-intensive. Some say if you’re using hard drives larger than 1TB and you are using OpenZFS, use mirror, not RAIDz* or use only SSD for RAIDz*.</p>

<p>In ZOL 0.8.0 they changed to a new resilvering solution. The previous resilvering algorithm repairs blocks from oldest to newest, which can degrade into a lot of small random I/O. The new resilvering algorithm uses a two-step process to sort and resilver blocks in LBA order. If you have an older version of ZOL or want even better performance you can tweak with the zfs configuration.</p>

<h3 id="speed-up-resilvering">Speed up resilvering</h3>

<pre><code>echo 0 &gt; /sys/module/zfs/parameters/zfs_resilver_delay
echo 0 &gt; /sys/module/zfs/parameters/zfs_scrub_delay
echo 512 &gt; /sys/module/zfs/parameters/zfs_top_maxinflight
echo 8192 &gt; /sys/module/zfs/parameters/zfs_top_maxinflight
echo 8000 &gt; /sys/module/zfs/parameters/zfs_resilver_min_time_ms
</code></pre>

<h3 id="speed-up-on-pool">Speed up on pool</h3>

<pre><code>zpool set autoreplace=on zpool

echo 0 &gt; /sys/module/zfs/parameters/zfs_scan_idle
echo 24 &gt; /sys/module/zfs/parameters/zfs_vdev_scrub_min_active
echo 64 &gt; /sys/module/zfs/parameters/zfs_vdev_scrub_max_active
echo 64 &gt; /sys/module/zfs/parameters/zfs_vdev_sync_write_min_active
echo 32 &gt; /sys/module/zfs/parameters/zfs_vdev_sync_write_max_active
echo 8 &gt; /sys/module/zfs/parameters/zfs_vdev_sync_read_min_active
echo 32 &gt; /sys/module/zfs/parameters/zfs_vdev_sync_read_max_active
echo 8 &gt; /sys/module/zfs/parameters/zfs_vdev_async_read_min_active
echo 32 &gt; /sys/module/zfs/parameters/zfs_vdev_async_read_max_active
echo 8 &gt; /sys/module/zfs/parameters/zfs_vdev_async_write_min_active
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes nginx ingress with helm]]></title>
            <link href="https://devopstales.github.io/home/k8s-local-pv/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-velero-backup/?utm_source=atom_feed" rel="related" type="text/html" title="Backup your Kubernetes Cluster with Velero" />
                <link href="https://devopstales.github.io/home/k8s-cert-manager/?utm_source=atom_feed" rel="related" type="text/html" title="Install cert-manager for Kubernetes" />
                <link href="https://devopstales.github.io/home/kind-install/?utm_source=atom_feed" rel="related" type="text/html" title="Starting local Kubernetes using kind" />
                <link href="https://devopstales.github.io/home/k8s-metallb/?utm_source=atom_feed" rel="related" type="text/html" title="Install MetalLB load balancer for Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-tillerless-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Tillerless helm2 install" />
            
                <id>https://devopstales.github.io/home/k8s-local-pv/</id>
            
            
            <published>2020-01-08T00:00:00+00:00</published>
            <updated>2020-01-08T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how to use a local folder as a persistent volume in Kubernetes.</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
<li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-contour-ingress --></li>
<li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
<li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
<li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... --></li>
<li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
<li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
<li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<p>For a production environment this is not an ideal structure because if you store the data on a single host if the host dies your data will be lost. For this Demo I will use a separate disk for storing the PV&rsquo;s folders. So you can backup or replicate this disk separately.</p>

<h3 id="configure-the-disk">Configure the disk</h3>

<pre><code>vgcreate local-vg /dev/sdd
lvcreate -l 100%FREE -n local-lv local-vg /dev/sdd
mkfs.xfs -f /dev/local-vg/local-lv
mkdir -p /mnt/local-storage/
mount /dev/local-vg/local-lv /mnt/local-storage
echo &quot;/dev/local-vg/local-lv        /mnt/local-storage              xfs defaults 0 0&quot; &gt;&gt; /etc/fstab
rm -rf /mnt/local-storage/lost+found
</code></pre>

<p>Now you can create every PV and PVC manually.</p>

<pre><code>mkdir /mnt/local-storage/pv-tst

cat pv-tst.yaml
---
kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv-tst
spec:
  capacity:
    storage: 1Gi
  local:
    path: /mnt/local-storage/pv-tst
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local
  nodeAffinity:
    required:
      nodeSelectorTerms:
        - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
                - kubernetes03.devopstales.intra
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pv-tst
  namespace: tst
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  volumeName: pv-tst
  storageClassName: local
</code></pre>

<h3 id="add-automated-hostpath-provisioner">Add automated hostpath-provisioner</h3>

<p>This is a Persistent Volume Claim (PVC) provisioner for Kubernetes. It dynamically provisions hostPath volumes to provide storage for PVCs.</p>

<pre><code>git clone https://github.com/torchbox/k8s-hostpath-provisioner
cd k8s-hostpath-provisioner
kubectl apply -f deployment.yaml

nano local-sc.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: auto-local
  annotations:
    storageclass.kubernetes.io/is-default-class: &quot;true&quot;
provisioner: torchbox.com/hostpath
parameters:
  pvDir: /mnt/local-storage
</code></pre>

<p>Test the provisioner by creating a new PVC:</p>

<pre><code>cat testpvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: testpvc
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 50Gi

kubectl create -f testpvc.yaml
kubectl get pvc
NAME      STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
testpvc   Bound     pvc-145c785e-ab83-11e7-9432-4201ac1fd019   50Gi       RWX            auto-local     10s
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubectl authentication with Kuberos]]></title>
            <link href="https://devopstales.github.io/home/k8s-kuberos/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/sso/k8s-kuberos/?utm_source=atom_feed" rel="related" type="text/html" title="Kubectl authentication with Kuberos" />
                <link href="https://devopstales.github.io/home/k8s-gangway/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak" />
                <link href="https://devopstales.github.io/home/k8s-dasboard-auth/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak" />
                <link href="https://devopstales.github.io/home/grafana-sso/?utm_source=atom_feed" rel="related" type="text/html" title="SSO login to Grafana" />
                <link href="https://devopstales.github.io/home/openshift-sso-2/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift SSO with Gitlab" />
            
                <id>https://devopstales.github.io/home/k8s-kuberos/</id>
            
            
            <published>2020-01-07T00:00:00+00:00</published>
            <updated>2020-01-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Kuberos is an OIDC authentication helper for Kubernetes&rsquo; kubectl</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
<li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-contour-ingress --></li>
<li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
<li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
<li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... --></li>
<li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
<li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
<li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<pre><code>nano /etc/kubernetes/manifests/kube-apiserver.yaml
...
    command:
    - /hyperkube
    - apiserver
    - --advertise-address=10.10.40.30
...

    - --oidc-issuer-url=https://keycloak.devopstales.intra/auth/realms/mydomain
    - --oidc-client-id=k8s
    - --oidc-username-claim=email
    - --oidc-groups-claim=groups
...

systemctl restart docker kubelet
</code></pre>

<pre><code>cat &gt;&gt;EOF&lt; values.yaml
replicaCount: 1

kuberos:
  oidcClientURL: https://keycloak.devopstales.intra/auth/realms/mydomain
  oidcClientID: k8s
  oidcSecret: 43219919-0904-4338-bc0f-c986e1891a7a
  clusters:
  - name: openshift
    apiServer: https://192.168.0.106:6443
    # `apiServer` is the url for kubectl
    #   This is typically  https://api.fqdn
    caCrt: |-
      -----BEGIN CERTIFICATE-----
      MIIDZDCCAkygAwIBAgIIe/R9sc8oJiAwDQYJKoZIhvcNAQELBQAwFTETMBEGA1UE
      AxMKa3ViZXJuZXRlczAeFw0xOTEyMjcxNzM3MzlaFw0yMDEyMjYxNzM3MzlaMBkx
      FzAVBgNVBAMTDmt1YmUtYXBpc2VydmVyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8A
      MIIBCgKCAQEAzAg7MflA/HVTVcZPsGJH71cfcJ/U1CtEYbXfu/AQbhGg09XKmeK9
      aGEK3kSgi/Hyoi7M+e/ntx1+Gp/jwc8kanMFRLgxdKCxxi4MOswZF/q2loUdNoE/
      OQVPWQi8Hgznubw/0gINUkIq8mRx9Bb+RcRnJEfD3CXkxDhUNeCvvjeTrujguF0h
      pgfzrLoc2kGdJYpHiLqow8jRq7XXk0RzZaqCQjAEZgqWamwbTTqFZh3v+1gF/2s0
      EbFVVL2Ctu1dOGe1FkZxte7/Po1XBkPLQuRXbH3QRiJkPfyOW16T1nWk1QTcpCdH
      HO/l+CY2nLPFZL1BM83QuVmPgR1T1p+5tQIDAQABo4GzMIGwMA4GA1UdDwEB/wQE
      AwIFoDATBgNVHSUEDDAKBggrBgEFBQcDATCBiAYDVR0RBIGAMH6CEms4cy5teWRv
      bWFpbi5pbnRyYYIKa3ViZXJuZXRlc4ISa3ViZXJuZXRlcy5kZWZhdWx0ghZrdWJl
      cm5ldGVzLmRlZmF1bHQuc3ZjgiRrdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0
      ZXIubG9jYWyHBApgAAGHBMCoAGowDQYJKoZIhvcNAQELBQADggEBAJZ7jxPR72V6
      PYL3SKCWaS+RgTuGuSm0pYu26cBmPOjsugd8DUrJ7+iAnKDHUmmw22sWheLLCokc
      YU/AIfdbbsz0+f+/qthkO7zJmAJgdIAOMJ5MQCbxMBt+6L813r1R3QI7kAGxHvzV
      loKJVIIHq/6K3gFEZDfo0myvNvtOIpBCeMnZRK+8hx3UNcHckZbhkan1Z1j9t9iw
      b6Vv5jY1+9t2Iltd2wuNaUvHicx+3X6JPAqVR6H0jI3i+QSyT1EHXtBtbQBBpP4T
      5WDz+9uDa1mIDHtww7DTnJwY+hGI7fVF2H7XQaM4xwhGnwIwbkSh45JWVtUEHMou
      Q7T4bTyrwuQ=
      -----END CERTIFICATE-----
    # `caCrt` is the public / CA cert for the cluster
    # cat /etc/kubernetes/pki/apiserver.crt

ingress:
  enabled: true
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/proxy-buffer-size: &quot;64k&quot;
    cert-manager.io/cluster-issuer: ca-issuer
    ingress.kubernetes.io/force-ssl-redirect: &quot;true&quot;
  path: /
  hosts:
    - kubectl.devopstales.intra
  tls:
    - secretName: default-cert
      hosts:
        - kubectl.devopstales.intra

image:
  repository: negz/kuberos
  tag: ede4085
  pullPolicy: IfNotPresent

service:
  type: ClusterIP
  port: 80
  annotations: {}
  # Add your service annotations here.

resources: {}
EOF
</code></pre>

<pre><code>helm upgrade --install kuberos stable/kuberos --namespace kuberos -f values.yaml
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Dashboard authentication with Keycloak]]></title>
            <link href="https://devopstales.github.io/home/k8s-gangway/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-dasboard-auth/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak" />
                <link href="https://devopstales.github.io/sso/k8s-gangway/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak" />
                <link href="https://devopstales.github.io/sso/k8s-dasboard-auth/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak" />
                <link href="https://devopstales.github.io/home/grafana-sso/?utm_source=atom_feed" rel="related" type="text/html" title="SSO login to Grafana" />
                <link href="https://devopstales.github.io/home/openshift-sso-2/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift SSO with Gitlab" />
            
                <id>https://devopstales.github.io/home/k8s-gangway/</id>
            
            
            <published>2020-01-06T00:00:00+00:00</published>
            <updated>2020-01-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Kuberos is an OIDC authentication helper for Kubernetes&rsquo; kubectl</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
<li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-contour-ingress --></li>
<li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
<li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
<li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... --></li>
<li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
<li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
<li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<pre><code>nano /etc/kubernetes/manifests/kube-apiserver.yaml
...
    command:
    - /hyperkube
    - apiserver
    - --advertise-address=10.10.40.30
...

    - --oidc-issuer-url=https://keycloak.devopstales.intra/auth/realms/mydomain
    - --oidc-client-id=k8s
    - --oidc-username-claim=email
    - --oidc-groups-claim=groups
...

systemctl restart docker kubelet
</code></pre>

<pre><code>nano gangway.yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: gangway
---
apiVersion: v1
kind: Secret
metadata:
  name: gangway-key
  labels:
    app.kubernetes.io/name: gangway
type: Opaque
data:
  sessionkey: &quot;ZTZNYlJUbDdHcHlSeXVFU0J6ZDZmbUs5Mks5a21NWEo=&quot;
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: gangway
  namespace: gangway
data:
  gangway.yaml: |
    clusterName: &quot;minikube&quot;
    authorizeURL: &quot;https://keycloak.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/auth&quot;
    tokenURL: &quot;https://keycloak.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/token&quot;
    audience: &quot;https://keycloak.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/userinfo&quot;
    # Used to specify the scope of the requested Oauth authorization.
    # scopes: [&quot;openid&quot;, &quot;profile&quot;, &quot;email&quot;, &quot;offline_access&quot;]
    # scopes: [&quot;groups&quot;]
    # scopes: [&quot;openid&quot;, &quot;profile&quot;, &quot;email&quot;, &quot;offline_access&quot;, &quot;groups&quot;]
    # Where to redirect back to. This should be a URL where gangway is reachable.
    # Typically this also needs to be registered as part of the oauth application
    # with the oAuth provider.
    # Env var: GANGWAY_REDIRECT_URL
    redirectURL: &quot;https://gangway.devopstales.intra/callback&quot;
    clientID: &quot;k8s&quot;
    clientSecret: &quot;43219919-0904-4338-bc0f-c986e1891a7a&quot;
    # The JWT claim to use as the username. This is used in UI.
    # Default is &quot;nickname&quot;. This is combined with the clusterName
    # for the &quot;user&quot; portion of the kubeconfig.
    # Env var: GANGWAY_USERNAME_CLAIM
    # usernameClaim: &quot;sub&quot;
    usernameClaim: &quot;preferred_username&quot;
    emailClaim: &quot;email&quot;
    # The API server endpoint used to configure kubectl
    # Env var: GANGWAY_APISERVER_URL
    # apiServerURL: &quot;https://kube.codeformuenster.org:6443&quot;
    apiServerURL: &quot;https://192.168.0.106:8443&quot;
    # The path to find the CA bundle for the API server. Used to configure kubectl.
    # This is typically mounted into the default location for workloads running on
    # a Kubernetes cluster and doesn't need to be set.
    # Env var: GANGWAY_CLUSTER_CA_PATH
    # cluster_ca_path: &quot;/var/run/secrets/kubernetes.io/serviceaccount/ca.crt&quot;
    # The path to a root CA to trust for self signed certificates at the Oauth2 URLs
    # Env var: GANGWAY_TRUSTED_CA_PATH
    # for self signd certificate:
    trustedCAPath: /gangway/rootca.crt
    # The path gangway uses to create urls (defaults to &quot;&quot;)
    # Env var: GANGWAY_HTTP_PATH
    # httpPath: &quot;https://${GANGWAY_HTTP_PATH}&quot;
    # for self signd certificate:
  rootca.crt: |
    -----BEGIN CERTIFICATE-----
    MIIEATCCAumgAwIBAgIUXHOQsGW+UHpCphHViHSvyBny8BwwDQYJKoZIhvcNAQEL
    BQAwgY4xCzAJBgNVBAYTAkhVMQ0wCwYDVQQIDARQZXN0MREwDwYDVQQHDAhCdWRh
    cGVzdDETMBEGA1UECgwKTXkgQ29tcGFueTELMAkGA1UECwwCT1UxFzAVBgNVBAMM
    Dm15ZG9tYWluLmludHJhMSIwIAYJKoZIhvcNAQkBFhNyb290QG15ZG9tYWluLmlu
    dHJhMCAXDTE5MTIyNzE3MTk0OVoYDzIxMTkxMjAzMTcxOTQ5WjCBjjELMAkGA1UE
    BhMCSFUxDTALBgNVBAgMBFBlc3QxETAPBgNVBAcMCEJ1ZGFwZXN0MRMwEQYDVQQK
    DApNeSBDb21wYW55MQswCQYDVQQLDAJPVTEXMBUGA1UEAwwObXlkb21haW4uaW50
    cmExIjAgBgkqhkiG9w0BCQEWE3Jvb3RAbXlkb21haW4uaW50cmEwggEiMA0GCSqG
    SIb3DQEBAQUAA4IBDwAwggEKAoIBAQDevUZouW101hAu68qojvfmnC3fUIA9L5nj
    J+OTbgwDhxYduHVcmwFcrZJzBn/udm72sAUsjmoc34ZoEQVr1mCjrnDdb3NtOWBI
    XYmN7/RySzu5DSFLFv8Sj+27VvGLpYTXgDEt+IQpV4EgosX6DzjYK7BtmqaWCY3t
    aClGnzxEotlxMakTCt9eALD+l/ffV4NbiS6sPNaOFHbG8CKRnfzDzqh78qYaSH8d
    wWxGLGAvciNm1wv1G3NIkjMIZlkMqAv6uTzEtfOPQrHigG8sbb4hHAg8a9RtH2Sk
    nXjZRb3Wfo+XJ2eUCZyC6pwvZfEuZBuRAAo12Ycp/Ve2FC3kvpzLAgMBAAGjUzBR
    MB0GA1UdDgQWBBTRPLiCReojvBQXna2zkBBTsKdsnzAfBgNVHSMEGDAWgBTRPLiC
    ReojvBQXna2zkBBTsKdsnzAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUA
    A4IBAQBXp676KZ7VTPc/RPI+QIyb0ugPE+w6fkpw1PIzG+y0h53AsOu15tXxKX5L
    SwotjXoDuTnqQqLZ5wTFSiolscay+MpEDnoIdo+Pw7u3q3bpn6GmDjae1BaIL/En
    wvxvvJQsOJrXfEUQeC6M75i/MrYPSwhWNDAbqJTY2qEuRXcj/AALGrnlF5DEEd+O
    RYw79sj+xU88/kCOVWI35LwiH+/0QWFyKcPQvY8nER69nt5evFGqUQPE6qlOJKg/
    YD8dK+OF26Ta/qz0iKNAfh3WDgYU4lHAawKtwAbpBVBLlzLl+bD11BQvn6zDWVWA
    rfrwKUIO+dwSL3ZKS0kA0OlN3dyy
    -----END CERTIFICATE-----
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gangway
  namespace: gangway
  labels:
    app: gangway
spec:
  replicas: 1
  selector:
    matchLabels:
      app: gangway
  strategy:
  template:
    metadata:
      labels:
        app: gangway
    spec:
      containers:
      - name: gangway
        image: gcr.io/heptio-images/gangway:v3.2.0
        command: [&quot;gangway&quot;, &quot;-config&quot;, &quot;/gangway/gangway.yaml&quot;]
        env:
        - name: GANGWAY_SESSION_SECURITY_KEY
          valueFrom:
            secretKeyRef:
              key: sessionkey
              name: gangway-key
        - name: GANGWAY_PORT
          value: &quot;8080&quot;
        ports:
        - name: http
          containerPort: 8080
          protocol: TCP
        resources:
          requests:
            cpu: &quot;100m&quot;
            memory: &quot;128Mi&quot;
          limits:
            cpu: &quot;200m&quot;
            memory: &quot;512Mi&quot;
        volumeMounts:
        - name: gangway
          mountPath: /gangway/
        livenessProbe:
          httpGet:
            path: /
            port: 8080
          initialDelaySeconds: 20
          timeoutSeconds: 1
          periodSeconds: 60
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /
            port: 8080
          timeoutSeconds: 1
          periodSeconds: 10
          failureThreshold: 3
      volumes:
      - name: gangway
        configMap:
          name: gangway
---
kind: Service
apiVersion: v1
metadata:
  name: gangway
  namespace: gangway
  labels:
    app: gangway
spec:
  type: ClusterIP
  ports:
    - name: &quot;http&quot;
      protocol: TCP
      port: 80
      targetPort: &quot;http&quot;
  selector:
    app: gangway
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: gangway
  namespace: gangway
  annotations:
    ingress.kubernetes.io/rewrite-target: /
    cert-manager.io/cluster-issuer: ca-issuer
    ingress.kubernetes.io/force-ssl-redirect: &quot;true&quot;
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/proxy-buffer-size: &quot;64k&quot;
spec:
  rules:
  - host: gangway.devopstales.intra
    http:
      paths:
      - backend:
          serviceName: gangway
          servicePort: http
  tls:
  - secretName: gangway-tls
    hosts:
    - gangway.devopstales.intra
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Dashboard authentication with Keycloak]]></title>
            <link href="https://devopstales.github.io/home/k8s-dasboard-auth/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/sso/k8s-dasboard-auth/?utm_source=atom_feed" rel="related" type="text/html" title="Dashboard authentication with Keycloak" />
                <link href="https://devopstales.github.io/home/grafana-sso/?utm_source=atom_feed" rel="related" type="text/html" title="SSO login to Grafana" />
                <link href="https://devopstales.github.io/home/openshift-sso-2/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift SSO with Gitlab" />
                <link href="https://devopstales.github.io/home/nextcloud-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Nextcloud SSO" />
                <link href="https://devopstales.github.io/home/nexus-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Sonatype Nexus SSO" />
            
                <id>https://devopstales.github.io/home/k8s-dasboard-auth/</id>
            
            
            <published>2020-01-03T00:00:00+00:00</published>
            <updated>2020-01-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how to add a keycloak gatekeeper authentication proxy for Kubernetes Dashboard.</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
<li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-contour-ingress --></li>
<li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
<li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
<li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... --></li>
<li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
<li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
<li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<p>Kubernetes does not have its own user management and relies on external providers like Keycloak. First we need to integrate an OpeniD prodiver (for me keycloak) with the kubernetes api server.</p>

<pre><code>nano /etc/kubernetes/manifests/kube-apiserver.yaml
...
    command:
    - /hyperkube
    - apiserver
    - --advertise-address=10.10.40.30
...

    - --oidc-issuer-url=https://keycloak.devopstales.intra/auth/realms/mydomain
    - --oidc-client-id=k8s
    - --oidc-username-claim=email
    - --oidc-groups-claim=groups
    # for self sign cert or custom ca
    #- --oidc-ca-file=/etc/kubernetes/pki/rootca.pem
...

systemctl restart docker kubelet
</code></pre>

<p>We need an authentication proxy before the dasboard. I will use keycloak-gatekeeper for that purpose.</p>

<pre><code>nano proxy-deplayment.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dasboard-proxy
  labels:
    app.kubernetes.io/name: dasboard-proxy
  namespace: kubernetes-dashboard
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: dasboard-proxy
  template:
    metadata:
      labels:
        app.kubernetes.io/name: dasboard-proxy
    spec:
      containers:
        - name: dasboard-proxy
          image: &quot;keycloak/keycloak-gatekeeper:latest&quot;
          command:
            - /opt/keycloak-gatekeeper
            - --discovery-url=https://keycloak.devopstales.intra/auth/realms/mydomain/.well-known/openid-configuration
            - --client-id=k8s
            - --client-secret=43219919-0904-4338-bc0f-c986e1891a7a
            - --listen=0.0.0.0:3000
            - --encryption-key=AgXa7xRcoClDEU0ZDSH4X0XhL5Qy2Z2j
            - --redirection-url=https://dashboard.devopstales.intra
            - --enable-refresh-tokens=true
            - --upstream-url=https://kubernetes-dashboard
            # debug:
            #- --upstream-url=http://echo:8080
            # for self sign cert or custom ca
            #- --skip-upstream-tls-verify
            #- --skip-openid-provider-tls-verify
          ports:
            - name: http
              containerPort: 3000
              protocol: TCP
---
apiVersion: v1
kind: Service
metadata:
  name: dasboard-proxy
  labels:
    app.kubernetes.io/name: dasboard-proxy
  namespace: kubernetes-dashboard
spec:
  type: ClusterIP
  ports:
    - port: 3000
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: dasboard-proxy
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: dasboard-proxy
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/proxy-buffer-size: &quot;64k&quot;
    cert-manager.io/cluster-issuer: ca-issuer
  namespace: kubernetes-dashboard
spec:
  tls:
    - hosts:
        - dashboard.devopstales.intra
      secretName: dasboard-proxy-tls
  rules:
    - host: dashboard.devopstales.intra
      http:
        paths:
          - backend:
             serviceName: dasboard-proxy
             servicePort: 3000
</code></pre>

<p>Now you can login at dashboard.devopstales.intra but you haven&rsquo;t got any privileges so lets create. some.</p>

<pre><code>nano devops-group-rbac.yaml
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: devops-cluster-admin
  namespace: kubernetes-dashboard
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: devopstales
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
subjects:
- kind: User
  name: &quot;devopstales&quot;
  namespace: &quot;kube-system&quot;
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name:  cluster-admin
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Grafana Loki with Helm3]]></title>
            <link href="https://devopstales.github.io/home/helm3-loki/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/grafana-loki/?utm_source=atom_feed" rel="related" type="text/html" title="Grafana Loki" />
                <link href="https://devopstales.github.io/kubernetes/helm3-loki/?utm_source=atom_feed" rel="related" type="text/html" title="Install Grafana Loki with Helm3" />
                <link href="https://devopstales.github.io/monitoring/grafana-loki/?utm_source=atom_feed" rel="related" type="text/html" title="Grafana Loki" />
                <link href="https://devopstales.github.io/home/graylog-pfsense-squid/?utm_source=atom_feed" rel="related" type="text/html" title="Analyzing PFsense squid logs in Graylog" />
                <link href="https://devopstales.github.io/home/k8s-ceph-storage-with-csi-driver/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD volume with CSI driver" />
            
                <id>https://devopstales.github.io/home/helm3-loki/</id>
            
            
            <published>2020-01-03T00:00:00+00:00</published>
            <updated>2020-01-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Helm is a template based package management system for kubernetes applications.</p>

<h3 id="wath-is-new-in-helm3">Wath is new in Helm3</h3>

<p>The most important change in Helm3, tiller was removed completely. Tiller was the server component (rinninf in a pod on the Kubernetes cluster) for helm&rsquo;s cli. When Helm 2 was developed, Kubernetes did not yet have role-based access control (RBAC) therefore to achieve mentioned goal, Helm had to take care of that itself. After Kubernetes 1.6 RBAC is enabled by default so you had to create a serviceaccount for tiller. With Tiller gone, Helm permissions are now simply evaluated using kubeconfig file.</p>

<p>Tiller was also used as a central hub for Helm release information and for maintaining the Helm state. In Helm 3 the same information are fetched directly from Kubernetes API Server and Charts are rendered client-side.</p>

<p>Helm 2 stored the informations of the releases in configmaps now in Helm 3 that is stored in secrets for better security.</p>

<h3 id="install-chart-with-helm3">Install Chart with Helm3</h3>

<p>The removal of Tiller means you didn&rsquo;t need a <code>helm init</code> for initializing the tiller.</p>

<pre><code>helm repo add loki https://grafana.github.io/loki/charts
helm repo add stable https://kubernetes-charts.storage.googleapis.com
helm repo update

kubectl create namespace loki-stackhtop

helm upgrade --install loki --namespace=loki-stack loki/loki-stack
elm3 upgrade --install grafana --namespace=loki-stack stable/grafana
</code></pre>

<p>Namespaces are important now. <code>helm ls</code> won’t show anything, we have to specify the namespace with it:</p>

<pre><code>helm -n loki-stack ls
</code></pre>

<h3 id="access-grafana-interface">Access Grafana Interface</h3>

<pre><code>kubectl get secret -n loki-stack grafana -o jsonpath=&quot;{.data.admin-password}&quot; | base64 --decode
kubectl port-forward -n loki-stack service/grafana 3000:80

# go to localhost:3000
# add loki datasource URL http://loki:3100 and press Save &amp; Test
</code></pre>

<p><img src="/img/include/loki_1.png" alt="Example image" /></p>

<p><img src="/img/include/loki_2.png" alt="Example image" /></p>

<p><img src="/img/include/loki_3.png" alt="Example image" /></p>

<p><img src="/img/include/loki_4.png" alt="Example image" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Backup your Kubernetes Cluster with Velero]]></title>
            <link href="https://devopstales.github.io/home/k8s-velero-backup/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-velero-backup/?utm_source=atom_feed" rel="related" type="text/html" title="Backup your Kubernetes Cluster with Velero" />
                <link href="https://devopstales.github.io/home/k8s-cert-manager/?utm_source=atom_feed" rel="related" type="text/html" title="Install cert-manager for Kubernetes" />
                <link href="https://devopstales.github.io/home/kind-install/?utm_source=atom_feed" rel="related" type="text/html" title="Starting local Kubernetes using kind" />
                <link href="https://devopstales.github.io/home/k8s-metallb/?utm_source=atom_feed" rel="related" type="text/html" title="Install MetalLB load balancer for Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-tillerless-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Tillerless helm2 install" />
            
                <id>https://devopstales.github.io/home/k8s-velero-backup/</id>
            
            
            <published>2020-01-02T00:00:00+00:00</published>
            <updated>2020-01-02T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Velero (formerly Heptio Ark) gives you tools to back up and restore your Kubernetes cluster resources and persistent volumes. You can run Velero with a cloud provider or on-premises.</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
<li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-contour-ingress --></li>
<li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
<li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
<li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... --></li>
<li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
<li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
<li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<h3 id="how-it-s-work">How it&rsquo;s work</h3>

<p>Each Velero operation (on-demand backup, scheduled backup, restore) is a custom resource, stored in etcd. A backup opertaion is uploads a tarball of copied Kubernetes objects into cloud object storage. After that calls the cloud provider API to make disk snapshots of persistent volumes, if specified. Optionally you can specify hooks to be executed during the backup. When you create a backup, you can specify a TTL by adding the flag <code>--ttl &lt;DURATION&gt;</code>.</p>

<h3 id="velero-supported-providers">Velero supported providers</h3>

<table>
<thead>
<tr>
<th>Object Store</th>
<th>Volume Snapshotter</th>
</tr>
</thead>

<tbody>
<tr>
<td>AWS S3</td>
<td>AWS EBS</td>
</tr>

<tr>
<td>Google Cloud Storage</td>
<td>Google Compute Engine Disks</td>
</tr>

<tr>
<td>Azure Blob Storage</td>
<td>Azure Managed Disks</td>
</tr>

<tr>
<td>-</td>
<td>Portworx Volume</td>
</tr>

<tr>
<td>-</td>
<td>OpenEBS CStor Volume</td>
</tr>
</tbody>
</table>

<h3 id="install-cli">Install cli</h3>

<pre><code>wget https://github.com/vmware-tanzu/velero/releases/download/v1.2.0/velero-v1.2.0-linux-amd64.tar.gz
tar -xzf velero-v1.2.0-linux-amd64.tar.gz
sudo cp velero-v1.2.0-linux-amd64/velero /usr/local/sbin
</code></pre>

<h2 id="deploy-minio-and-deno-app">Deploy minio and deno app</h2>

<pre><code>kubctl apply -f velero-v1.2.0-linux-amd64/examples/minio/00-minio-deployment.yaml
kubctl apply -f velero-v1.2.0-linux-amd64/examples/nginx-app/base.yaml
</code></pre>

<h3 id="deploy-server-component">Deploy server component</h3>

<pre><code>nano velero.yaml
image:
  repository: velero/velero
  tag: v1.2.0
  pullPolicy: IfNotPresent

initContainers:
  - name: aws
    image: velero/velero-plugin-for-aws:v1.0.0
    imagePullPolicy: IfNotPresent
    volumeMounts:
      - mountPath: /target
        name: plugins

metrics:
  enabled: true
  scrapeInterval: 30s

  # Pod annotations for Prometheus
  podAnnotations:
    prometheus.io/scrape: &quot;true&quot;
    prometheus.io/port: &quot;8085&quot;
    prometheus.io/path: &quot;/metrics&quot;

  serviceMonitor:
    enabled: false
    additionalLabels: {}



configuration:
  provider: aws
  backupStorageLocation:
    name: aws
    bucket: velero
    config:
      region: minio
      s3ForcePathStyle: true
      publicUrl: https://minio.devopstales.intra
      s3Url: http://minio:9000
  volumeSnapshotLocation:
    name: aws
    bucket: kubernetes-pv
    config:
      region: minio
      s3ForcePathStyle: true
      publicUrl: https://minio.devopstales.intra
      s3Url: http://minio:9000

credentials:
  useSecret: true
  secretContents:
    cloud: |
      [default]
      aws_access_key_id = minio
      aws_secret_access_key = minio123

snapshotsEnabled: true
deployRestic: true
</code></pre>

<pre><code>helm repo add vmware-tanzu https://vmware-tanzu.github.io/helm-charts
helm repo update

helm install velero vmware-tanzu/velero --namespace velero -f velero.yaml
</code></pre>

<h3 id="create-backup">Create Backup</h3>

<pre><code>velero backup create nginx-backup --selector app=nginx
velero backup describe nginx-backup
velero backup logs nginx-backup
velero backup get

velero schedule create nginx-daily --schedule=&quot;0 1 * * *&quot; --selector app=nginx
velero schedule get
velero backup get
</code></pre>

<h3 id="restore-test">Restore test</h3>

<pre><code>kubectl delete ns nginx-example

velero restore create --from-backup nginx-backup
velero restore get

kubectl get po -n nginx-example
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install cert-manager for Kubernetes]]></title>
            <link href="https://devopstales.github.io/home/k8s-cert-manager/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/kind-install/?utm_source=atom_feed" rel="related" type="text/html" title="Starting local Kubernetes using kind" />
                <link href="https://devopstales.github.io/home/k8s-metallb/?utm_source=atom_feed" rel="related" type="text/html" title="Install MetalLB load balancer for Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-tillerless-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Tillerless helm2 install" />
                <link href="https://devopstales.github.io/home/k8s-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/k8s-nginx-ingress/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
            
                <id>https://devopstales.github.io/home/k8s-cert-manager/</id>
            
            
            <published>2019-12-29T00:00:00+00:00</published>
            <updated>2019-12-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this tutorial I will show you how to install  cert-manager running on Kubernetes (k8s).</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
<li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-contour-ingress --></li>
<li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
<li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
<li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... --></li>
<li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
<li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
<li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<p>cert-manager is a native Kubernetes certificate management controller. It can help with issuing certificates from a variety of sources, such as Let’s Encrypt, HashiCorp Vault, Venafi, a simple signing key pair, or self signed.</p>

<h3 id="install-cert-managger">Install cert-managger</h3>

<p>In order to install cert-manager, we must first create a namespace to run it in.</p>

<pre><code>kubectl create namespace cert-manager
</code></pre>

<p>Install the <code>CustomResourceDefinitions</code> and cert-manager itself</p>

<pre><code>kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.12.0/cert-manager.yaml
</code></pre>

<p>Verifying the installation</p>

<pre><code>kubectl get pods --namespace cert-manager

NAME                                       READY   STATUS    RESTARTS   AGE
cert-manager-5c6866597-zw7kh               1/1     Running   0          2m
cert-manager-cainjector-577f6d9fd7-tr77l   1/1     Running   0          2m
cert-manager-webhook-787858fcdb-nlzsq      1/1     Running   0          2m
</code></pre>

<h3 id="create-a-clusterissuer">Create a ClusterIssuer</h3>

<p>Before you can begin issuing certificates, you must configure at least one <code>Issuer</code> or <code>ClusterIssuer</code> resource in your cluster. These resources represent a particular signing authority and detail how the certificate requests are going to be honored. For this Demo I will use my own CA as an Issuer.</p>

<pre><code>cat issuer.yaml
apiVersion: cert-manager.io/v1alpha2
kind: ClusterIssuer
metadata:
  name: ca-issuer
  namespace: cert-manager
spec:
  ca:
    secretName: ca-key-pair

kubectl apply -f issuer.yaml
</code></pre>

<p>In order to create my certs, I must submit my CA certificate and singing private key to the Kubernetes Cluster so that cert-manager is able to use them and sign certificates.</p>

<pre><code>cat  rootCA.key | base64
LS0tLS1CRUdJTiB...
cat rootCA.crt | base64
LS0tLSD5DUdJTiB...

cat ca-key-pair.yaml
apiVersion: v1
kind: Secret
metadata:
  name: ca-key-pair
  namespace: cert-manager
data:
  tls.key: LS0tLS1CRUdJTiB...
  tls.crt: LS0tLSD5DUdJTiB...

kubectl apply -f ca-key-pair.yaml
</code></pre>

<h3 id="demo">demo</h3>

<p>Create cert for test</p>

<pre><code>cat test-resources.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: cert-manager-test
---
apiVersion: cert-manager.io/v1alpha2
kind: Issuer
metadata:
  name: test-selfsigned
  namespace: cert-manager-test
spec:
  selfSigned: {}
---
apiVersion: cert-manager.io/v1alpha2
kind: Certificate
metadata:
  name: selfsigned-cert
  namespace: cert-manager-test
spec:
  commonName: example.com
  secretName: selfsigned-cert-tls
  issuerRef:
    name: test-selfsigned
---
apiVersion: cert-manager.io/v1alpha2
kind: Certificate
metadata:
  name: ca-cert
  namespace: cert-manager-test
spec:
  commonName: example.com
  secretName: ca-cert-tls
  issuerRef:
    name: ca-issuer
    kind: ClusterIssuer

kubectl apply -f test-resources.yaml
</code></pre>

<pre><code>kubectl describe certificate -n cert-manager-test

...
Spec:
  Common Name:  example.com
  Issuer Ref:
    Name:       test-selfsigned
  Secret Name:  selfsigned-cert-tls
Status:
  Conditions:
    Last Transition Time:  2019-12-29T17:34:30Z
    Message:               Certificate is up to date and has not expired
    Reason:                Ready
    Status:                True
    Type:                  Ready
  Not After:               2019-12-29T17:34:29Z
Events:
  Type    Reason      Age   From          Message
  ----    ------      ----  ----          -------
  Normal  CertIssued  4s    cert-manager  Certificate issued successfully
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift secondary route]]></title>
            <link href="https://devopstales.github.io/home/openshift-secondary-router/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-gitlab-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Installing GitLab on OpenShift" />
                <link href="https://devopstales.github.io/home/openshift-kompose/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Run docker-compoe in Openshift" />
                <link href="https://devopstales.github.io/home/openshift-add-node/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Add Nodes to a Cluster" />
                <link href="https://devopstales.github.io/home/openshift-letsencrypt/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Letsencrypt certificates" />
                <link href="https://devopstales.github.io/home/openshift-hostalreadyclaimed/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Error: HostAlreadyClaimed" />
            
                <id>https://devopstales.github.io/home/openshift-secondary-router/</id>
            
            
            <published>2019-12-20T00:00:00+00:00</published>
            <updated>2019-12-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I this tutorial I will show you how to create a secondari router for Openshift.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<h3 id="environment">Environment</h3>

<pre><code>192.168.1.40    deployer
192.168.1.41    openshift01 # master node
192.168.1.42    openshift02 # infra node
192.168.1.43    openshift03 # worker node with second router
192.168.1.44    openshift04 # worker node
</code></pre>

<h3 id="deploy-route">Deploy route</h3>

<pre><code>oc adm router router-public --replicas=2 --ports=&quot;8080:8080,8443:8443&quot; \
--stats-port=1937 --selector=&quot;router=public&quot; --labels=&quot;router=public&quot;

oc set env dc/router-public \
DEFAULT_CERTIFICATE_PATH=/etc/pki/tls/private/tls.crt \
NAMESPACE_LABELS=&quot;router=public&quot; \
ROUTER_ALLOW_WILDCARD_ROUTES=true \
ROUTER_ENABLE_HTTP2=true \
ROUTER_HAPROXY_CONFIG_MANAGER=true \
ROUTER_SERVICE_HTTP_PORT=8080 \
ROUTER_SERVICE_HTTPS_PORT=8443 \
ROUTER_TCP_BALANCE_SCHEME=roundrobin

oc label node openshift03 &quot;router=public&quot;
</code></pre>

<p>Configurate your firewall to create a NAT rule from publicIP:80 to openshift03:8080 and publicIP:443 to openshift03:8443</p>

<h3 id="demo">Demo</h3>

<pre><code>oc new-project test
oc label namespace test router=public
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Starting local Kubernetes using kind]]></title>
            <link href="https://devopstales.github.io/home/kind-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-metallb/?utm_source=atom_feed" rel="related" type="text/html" title="Install MetalLB load balancer for Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-tillerless-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Tillerless helm2 install" />
                <link href="https://devopstales.github.io/home/k8s-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/k8s-nginx-ingress/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
                <link href="https://devopstales.github.io/home/k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install kubernetes with kubeadm" />
            
                <id>https://devopstales.github.io/home/kind-install/</id>
            
            
            <published>2019-12-20T00:00:00+00:00</published>
            <updated>2019-12-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this article, I will show you how to run a cluster in single Docker container using kind.</p>

<h3 id="what-is-kind">What is kind?</h3>

<p>Kind (Kubernetes IN Docker) is a tool to start kubernetes nodes as a docker container. It is a cross-platform tool you can run with Docker for Windows too.</p>

<h3 id="install-kind-binary">Install kind binary</h3>

<pre><code>wget https://github.com/kubernetes-sigs/kind/releases/latest/download/kind-linux-amd64
chmod +x kind-linux-amd64
sudo mv kind-linux-amd64 /usr/local/sbin/kind
</code></pre>

<h3 id="start-a-cluster-for-ingress">Start a cluster for Ingress</h3>

<pre><code>cat &lt;&lt;EOF | kind create cluster --config=-
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  kubeadmConfigPatches:
  - |
    apiVersion: kubeadm.k8s.io/v1beta2
    kind: InitConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        node-labels: &quot;ingress-ready=true&quot;
        authorization-mode: &quot;AlwaysAllow&quot;
  extraPortMappings:
  - containerPort: 80
    hostPort: 80
  - containerPort: 443
    hostPort: 443
EOF
</code></pre>

<h3 id="install-ingress">Install ingress</h3>

<pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml

# patch ingress for kind
kubectl patch deployments -n ingress-nginx nginx-ingress-controller -p '{&quot;spec&quot;:{&quot;template&quot;:{&quot;spec&quot;:{&quot;containers&quot;:[{&quot;name&quot;:&quot;nginx-ingress-controller&quot;,&quot;ports&quot;:[{&quot;containerPort&quot;:80,&quot;hostPort&quot;:80},{&quot;containerPort&quot;:443,&quot;hostPort&quot;:443}]}],&quot;nodeSelector&quot;:{&quot;ingress-ready&quot;:&quot;true&quot;}}}}}'
</code></pre>

<h3 id="demo">Demo</h3>

<pre><code>kubectl apply -f https://kind.sigs.k8s.io/examples/ingress/usage.yaml

curl localhost/foo
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install icinga director modules to Icingaweb2]]></title>
            <link href="https://devopstales.github.io/home/icinga2_director/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/icinga2_nrpe/?utm_source=atom_feed" rel="related" type="text/html" title="Install nrpe tp Icinga2" />
                <link href="https://devopstales.github.io/home/icinga2_add_host/?utm_source=atom_feed" rel="related" type="text/html" title="Add host to Icinga2" />
                <link href="https://devopstales.github.io/home/icinga2_install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Icinga2" />
                <link href="https://devopstales.github.io/monitoring/icinga2_director/?utm_source=atom_feed" rel="related" type="text/html" title="Install icinga director modules to Icingaweb2" />
                <link href="https://devopstales.github.io/monitoring/icinga2_nrpe/?utm_source=atom_feed" rel="related" type="text/html" title="Install nrpe tp Icinga2" />
            
                <id>https://devopstales.github.io/home/icinga2_director/</id>
            
            
            <published>2019-12-13T00:00:00+00:00</published>
            <updated>2019-12-13T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this tutorial I will show you how to install Icingaweb2 module director.</p>

<p>Icinga irector is designed for those who want to automate their configuration deployment and those who want to grant easy access for there users to the Icinga2 configuration.</p>

<h3 id="install-dependency">Install dependency</h3>

<pre><code>yum install git -y
yum install rh-php71-php-curl rh-php71-php-pcntl rh-php71-php-posix rh-php71-php-sockets rh-php71-php-xml rh-php71-php-zip -y
</code></pre>

<pre><code>nano
local   director      director                        md5
host    director      director      127.0.0.1/32      md5
host    director      director      ::1/128           md5

systemctl restart postgresql-10
</code></pre>

<h3 id="install-icingaweb2-modules">Install Icingaweb2 modules</h3>

<pre><code>MODULE_NAME=ipl
MODULE_VERSION=v0.4.0
REPO=&quot;https://github.com/Icinga/icingaweb2-module-${MODULE_NAME}&quot;
MODULES_PATH=&quot;/usr/share/icingaweb2/modules&quot;
git clone ${REPO} &quot;${MODULES_PATH}/${MODULE_NAME}&quot; --branch &quot;${MODULE_VERSION}&quot;
icingacli module enable &quot;${MODULE_NAME}&quot;

MODULE_NAME=incubator
MODULE_VERSION=v0.5.0
REPO=&quot;https://github.com/Icinga/icingaweb2-module-${MODULE_NAME}&quot;
MODULES_PATH=&quot;/usr/share/icingaweb2/modules&quot;
git clone ${REPO} &quot;${MODULES_PATH}/${MODULE_NAME}&quot; --branch &quot;${MODULE_VERSION}&quot;
icingacli module enable &quot;${MODULE_NAME}&quot;

MODULE_NAME=reactbundle
MODULE_VERSION=v0.7.0
REPO=&quot;https://github.com/Icinga/icingaweb2-module-${MODULE_NAME}&quot;
MODULES_PATH=&quot;/usr/share/icingaweb2/modules&quot;
git clone ${REPO} &quot;${MODULES_PATH}/${MODULE_NAME}&quot; --branch &quot;${MODULE_VERSION}&quot;
icingacli module enable &quot;${MODULE_NAME}&quot;

ICINGAWEB_MODULEPATH=&quot;/usr/share/icingaweb2/modules&quot;
REPO_URL=&quot;https://github.com/icinga/icingaweb2-module-director&quot;
TARGET_DIR=&quot;${ICINGAWEB_MODULEPATH}/director&quot;
MODULE_VERSION=&quot;1.7.2&quot;
git clone &quot;${REPO_URL}&quot; &quot;${TARGET_DIR}&quot;
cd &quot;${TARGET_DIR}&quot;
git fetch &amp;&amp; git fetch --tags
git checkout &quot;{MODULE_VERSION}&quot;
restorecon -R &quot;${TARGET_DIR}&quot;

MODULE_NAME=fileshipper
MODULE_VERSION=v1.1.0
REPO=&quot;https://github.com/Icinga/icingaweb2-module-${MODULE_NAME}&quot;
MODULES_PATH=&quot;/usr/share/icingaweb2/modules&quot;
git clone ${REPO} &quot;${MODULES_PATH}/${MODULE_NAME}&quot; --branch &quot;${MODULE_VERSION}&quot;
icingacli module enable &quot;${MODULE_NAME}&quot;
</code></pre>

<h3 id="create-db-for-director">Create db for director</h3>

<pre><code>sudo -u postgres psql -c &quot;CREATE DATABASE director WITH ENCODING 'utf8';&quot;
sudo -u postgres psql director -q -c &quot;CREATE USER director WITH PASSWORD 'director';
GRANT ALL PRIVILEGES ON DATABASE director TO director;
CREATE EXTENSION pgcrypto;&quot;
sudo -u postgres psql director &lt; /usr/share/icingaweb2/modules/director/schema/pgsql.sql
</code></pre>

<h3 id="edit-director-configuration">Edit director configuration</h3>

<pre><code>cat &lt;&lt;EOF &gt;&gt; /etc/icinga2/zones.conf

object Zone &quot;director-global&quot; {
  global = true
}
EOF

cat &lt;&lt;EOF &gt;&gt; /etc/icinga2/conf.d/api-users.conf

object ApiUser &quot;director&quot; {
        password = &quot;director&quot;
        permissions = [ &quot;*&quot; ]
}
EOF

cat &lt;&lt;EOF &gt;&gt; /etc/icingaweb2/resources.ini

[icinga_director]
type = &quot;db&quot;
db = &quot;pgsql&quot;
host = &quot;localhost&quot;
port = &quot;5432&quot;
dbname = &quot;director&quot;
username = &quot;director&quot;
password = &quot;director&quot;
charset = &quot;utf8&quot;
use_ssl = &quot;0&quot;
EOF

mkdir /etc/icingaweb2/modules/director/
cat &lt;&lt;EOF &gt;&gt; /etc/icingaweb2/modules/director/config.ini
[db]
resource = &quot;icinga_director&quot;
EOF

cat &lt;&lt;EOF &gt;&gt; /etc/icingaweb2/modules/director/kickstart.ini
[config]
endpoint = icinga.devopstales.intra
host = 127.0.0.1
port = 5665
username = director
password = director
EOF
</code></pre>

<pre><code>icingacli module enable director
icingacli director kickstart run
icingacli director migration run --verbose
icingacli director migration pending --verbose
</code></pre>

<pre><code>mkdir /etc/icingaweb2/modules/fileshipper
cat &lt;&lt;EOF &gt;&gt; /etc/icingaweb2/modules/fileshipper/imports.ini
[icinga2 - groups]
basedir = &quot;/etc/icinga2/conf.d/groups/

[icinga2 - hosts]
basedir = &quot;/etc/icinga2/conf.d/hosts/
EOF
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install nrpe tp Icinga2]]></title>
            <link href="https://devopstales.github.io/home/icinga2_nrpe/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/icinga2_add_host/?utm_source=atom_feed" rel="related" type="text/html" title="Add host to Icinga2" />
                <link href="https://devopstales.github.io/home/icinga2_install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Icinga2" />
                <link href="https://devopstales.github.io/monitoring/icinga2_nrpe/?utm_source=atom_feed" rel="related" type="text/html" title="Install nrpe tp Icinga2" />
                <link href="https://devopstales.github.io/home/nagios-nrpe/?utm_source=atom_feed" rel="related" type="text/html" title="Nagios Remote Plugin Executor" />
                <link href="https://devopstales.github.io/monitoring/icinga2_add_host/?utm_source=atom_feed" rel="related" type="text/html" title="Add host to Icinga2" />
            
                <id>https://devopstales.github.io/home/icinga2_nrpe/</id>
            
            
            <published>2019-12-12T00:00:00+00:00</published>
            <updated>2019-12-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this tutorial I will show you how to nrpe check in Icinga2.</p>

<h3 id="install-nrpe-on-the-clients">install nrpe on the clients</h3>

<pre><code>yum install nrpe nagios-plugins-all -y

firewall-cmd --permanent --add-port=5665/tcp
firewall-cmd --reload
</code></pre>

<h3 id="install-nrpe-on-the-server">install nrpe on the server</h3>

<pre><code>yum install nrpe nagios-plugins-all -y

firewall-cmd --permanent --add-port=5665/tcp
firewall-cmd --reload
</code></pre>

<h3 id="configurate-icinga-to-use-nrpe">configurate icinga to use nrpe</h3>

<pre><code>vi /etc/icinga2/conf.d/linux_services.conf
apply Service &quot;nrpe-disk-root&quot; {
  import &quot;generic-service&quot;
  check_command = &quot;nrpe&quot;
  vars.nrpe_command = &quot;check_disk&quot;
  vars.nrpe_arguments = [ &quot;20%&quot;, &quot;10%&quot;, &quot;/&quot; ]
  assign where &quot;linux-servers&quot; in host.groups
  ignore where match(&quot;*icinga*&quot;, host.name)
}

vi /etc/icinga2/conf.d/CLIENTS/server1.conf
object Host &quot;server1&quot; {
  address = &quot;192.168.10.60&quot;
  check_command = &quot;ping&quot;
  vars.os = &quot;Linux&quot;
}
</code></pre>

<p>Test the config and restart:</p>

<pre><code>icinga2 daemon -C
systemctl restart icinga2
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Add host to Icinga2]]></title>
            <link href="https://devopstales.github.io/home/icinga2_add_host/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/icinga2_install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Icinga2" />
                <link href="https://devopstales.github.io/monitoring/icinga2_add_host/?utm_source=atom_feed" rel="related" type="text/html" title="Add host to Icinga2" />
                <link href="https://devopstales.github.io/monitoring/icinga2_install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Icinga2" />
                <link href="https://devopstales.github.io/home/nagios-ncpa/?utm_source=atom_feed" rel="related" type="text/html" title="Nagios Cross Platform Agent" />
                <link href="https://devopstales.github.io/home/nagios-nrpe/?utm_source=atom_feed" rel="related" type="text/html" title="Nagios Remote Plugin Executor" />
            
                <id>https://devopstales.github.io/home/icinga2_add_host/</id>
            
            
            <published>2019-12-11T00:00:00+00:00</published>
            <updated>2019-12-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this tutorial I will show you how to add hosts to Icinga2.</p>

<h3 id="create-new-host">Create new host</h3>

<pre><code>nano /etc/icinga2/conf.d/my_router.conf
object Host &quot;Router&quot; {
  address = &quot;192.168.1.1&quot;
  check_command = &quot;hostalive&quot;
}

</code></pre>

<p>If we want to check the web admin interface of the router we can add a http check to see if the HTTP server is alive and responds with the proper HTTP codes</p>

<pre><code>nano /etc/icinga2/conf.d/my_router.conf
...
object Service &quot;http&quot; {
  host_name = &quot;Router&quot;
  check_command = &quot;http&quot;
}
</code></pre>

<h3 id="ad-custom-check">Ad custom check</h3>

<p>We will us the check_udpport script. This is the syntax.</p>

<pre><code>/usr/lib64/nagios/plugins/check_udpport –H 127.0.0.1 –p 69
</code></pre>

<p>Now we need to create a nwe command in icinga2:</p>

<pre><code>nano /etc/icinga2/conf.d/commands.conf
object CheckCommand &quot;myudp&quot; {
  command = [ PluginDir + &quot;/check_udpport&quot; ]

    arguments = {
    &quot;-H&quot; = &quot;$addr$&quot;
    &quot;-p&quot; = &quot;$port$&quot;
  }
  vars.addr = &quot;$address$&quot;
}
</code></pre>

<p>We have a nwe command so we can create a service to use rhis command:</p>

<pre><code>nano /etc/icinga2/conf.d/my_router.conf
...
object Service &quot;dhcp&quot; {
  host_name = Router&quot;
  check_command = &quot;myudp&quot;
vars.port = &quot;67&quot;
}
</code></pre>

<p>Test the config and restart:</p>

<pre><code>icinga2 daemon -C
systemctl restart icinga2
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configure Rundeck ACL]]></title>
            <link href="https://devopstales.github.io/home/rundeck-acl/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/rundeck-acl/</id>
            
            
            <published>2019-12-10T00:00:00+00:00</published>
            <updated>2019-12-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will configure access control in Rundeck.</p>

<h3 id="configurate-ad-groups-in-rundeck">Configurate AD groups in rundeck</h3>

<p>The modification of the <code>web.xml</code> no longer needed after 3.0.x.</p>

<pre><code>nano /var/lib/rundeck/exp/webapp/WEB-INF/web.xml
        &lt;security-role&gt;
               &lt;role-name&gt;rundeck-administrators&lt;/role-name&gt;
               &lt;role-name&gt;rundeck-project&lt;/role-name&gt;
        &lt;/security-role&gt;
</code></pre>

<h3 id="configure-the-privilege-for-ad-group">Configure the privilege for AD group</h3>

<pre><code>nano /etc/rundec/admin.aclpolicy

description: Admin, all access.
context:
  project: '.*' # all projects
for:
  resource:
    - allow: '*' # allow read/create all kinds
  adhoc:
    - allow: '*' # allow read/running/killing adhoc jobs
  job:
    - allow: '*' # allow read/write/delete/run/kill of all jobs
  node:
    - allow: '*' # allow read/run for all nodes
by:
  group: rundeck-administrators

---

description: Admin, all access.
context:
  application: 'rundeck'
for:
  resource:
    - allow: '*' # allow create of projects
  project:
    - allow: '*' # allow view/admin of all projects
  project_acl:
    - allow: '*' # allow admin of all project-level ACL policies
  storage:
    - allow: '*' # allow read/create/update/delete for all /keys/* storage content
by:
  group: rundeck-administrators
---

description: rundeck-project  PROJECT all access.
context:
  project: 'PROJECT'
for:
  resource:
    - allow: '*' # allow read/create all kinds
  adhoc:
    - allow: '*' # allow read/running/killing adhoc jobs
  job:
    - allow: '*' # allow read/write/delete/run/kill of all jobs
  node:
    - allow: '*' # allow read/run for all nodes
by:
  group: rundeck-project

---

description: rundeck-project, all access.
context:
  application: 'rundeck'
for:
  project:
    - match:
        name: 'PROJECT'
      allow: [read]
  system:
    - match:
        name: '.*'
      allow: [read]
  storage:
    - equals:
        path: 'keys'
      allow: [read]
    - match:
        path: 'keys/id_rsa*'
      allow: [read]
by:
  group: rundeck-project
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Icinga2]]></title>
            <link href="https://devopstales.github.io/home/icinga2_install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/monitoring/icinga2_install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Icinga2" />
                <link href="https://devopstales.github.io/home/nagios-ncpa/?utm_source=atom_feed" rel="related" type="text/html" title="Nagios Cross Platform Agent" />
                <link href="https://devopstales.github.io/home/nagios-nrpe/?utm_source=atom_feed" rel="related" type="text/html" title="Nagios Remote Plugin Executor" />
                <link href="https://devopstales.github.io/home/centreon-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Centreon on Centos 7" />
                <link href="https://devopstales.github.io/home/naemon-influxdb/?utm_source=atom_feed" rel="related" type="text/html" title="Install Nemon with Influxdb storage" />
            
                <id>https://devopstales.github.io/home/icinga2_install/</id>
            
            
            <published>2019-12-10T00:00:00+00:00</published>
            <updated>2019-12-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this tutorial I will show you how to install Icinga2 and Icingaweb2 webinterface.</p>

<p>Icinga 2 is an open source, scalable and extensible monitoring tool which checks the availability of your network resources, notifies users of outages, and generates performance data for reporting.</p>

<h3 id="configure-syslinux-and-firewall-for-the-install">Configure syslinux and Firewall for the install</h3>

<pre><code>setsebool -P httpd_can_network_connect_db 1
setsebool -P httpd_can_network_connect 1

firewall-cmd --permanent --add-service=http
firewall-cmd --permanent --add-service=https
firewall-cmd --permanent --add-service=postgresql
firewall-cmd --permanent --add-port=5665/tcp
firewall-cmd --reload
</code></pre>

<h3 id="install-postgresql">Install postgresql</h3>

<pre><code>yum install postgresql-server postgresql
postgresql-setup initdb

nano /var/lib/pgsql/data/pg_hba.conf
# icinga
local   icinga      icinga                            md5
host    icinga      icinga      127.0.0.1/32          md5
host    icinga      icinga      ::1/128               md5
local   icinga_web  icinga_web                        md5
host    icinga_web  icinga_web  127.0.0.1/32          md5
host    icinga_web  icinga_web  ::1/128               md5

# &quot;local&quot; is for Unix domain socket connections only
local   all         all                               ident
# IPv4 local connections:
host    all         all         127.0.0.1/32          ident
# IPv6 local connections:
host    all         all         ::1/128               ident

systemctl enable  --now postgresql-10
</code></pre>

<h3 id="install-icinga2">Install Icinga2</h3>

<pre><code>yum install https://packages.icinga.com/epel/icinga-rpm-release-7-latest.noarch.rpm
yum install epel-release

yum install icinga2 icinga2-selinux nagios-plugins-all nano-icinga2

systemctl enable --now httpd

echo 'include &quot;/usr/share/nano/icinga2.nanorc&quot;' &gt;&gt; /etc/nanorc
cp /etc/nanorc ~/.nanorc

yum install icinga2-ido-pgsql

cd /tmp
sudo -u postgres psql -c &quot;CREATE ROLE icinga WITH LOGIN PASSWORD 'icinga'&quot;
sudo -u postgres createdb -O icinga -E UTF8 icinga

export PGPASSWORD=icinga
psql -U icinga -d icinga &lt; /usr/share/icinga2-ido-pgsql/schema/pgsql.sql

icinga2 feature enable ido-pgsql

cat &lt;&lt; EOF | sudo tee /etc/icinga2/features-enabled/ido-pgsql.conf
/**
 * The db_ido_pgsql library implements IDO functionality
 * for PostgreSQL.
 */

library &quot;db_ido_pgsql&quot;

object IdoPgsqlConnection &quot;ido-pgsql&quot; {
  user = &quot;icinga&quot;,
  password = &quot;icinga&quot;,
  host = &quot;localhost&quot;,
  database = &quot;icinga&quot;
}
EOF

icinga2 api setup


ecjo '
object ApiUser &quot;icingaweb2&quot; {
  password = &quot;Wijsn8Z9eRs5E25d&quot;
  permissions = [ &quot;status/query&quot;, &quot;actions/*&quot;, &quot;objects/modify/*&quot;, &quot;objects/query/*&quot; ]
}' &gt;&gt; /etc/icinga2/conf.d/api-users.conf

icinga2 feature enable command
</code></pre>

<pre><code>icinga2 node wizard
Welcome to the Icinga 2 Setup Wizard!

We'll guide you through all required configuration details.

Please specify if this is a satellite setup ('n' installs a master setup) [Y/n]: n
Starting the Master setup routine...
Please specifiy the common name (CN) [icinga.devopstales.intra]:
Checking for existing certificates for common name 'icinga.devopstales.intra'...
Certificates not yet generated. Running 'api setup' now.
information/cli: Generating new CA.
information/base: Writing private key to '/var/lib/icinga2/ca/ca.key'.
information/base: Writing X509 certificate to '/var/lib/icinga2/ca/ca.crt'.
information/cli: Generating new CSR in '/etc/icinga2/pki/icinga.devopstales.intra.csr'.
information/base: Writing private key to '/etc/icinga2/pki/icinga.devopstales.intra.key'.
information/base: Writing certificate signing request to '/etc/icinga2/pki/icinga.devopstales.intra.csr'.
information/cli: Signing CSR with CA and writing certificate to '/etc/icinga2/pki/icinga.devopstales.intra.crt'.
information/cli: Copying CA certificate to '/etc/icinga2/pki/ca.crt'.
Generating master configuration for Icinga 2.
information/cli: Adding new ApiUser 'root' in '/etc/icinga2/conf.d/api-users.conf'.
information/cli: Enabling the 'api' feature.
Enabling feature api. Make sure to restart Icinga 2 for these changes to take effect.
information/cli: Dumping config items to file '/etc/icinga2/zones.conf'.
information/cli: Created backup file '/etc/icinga2/zones.conf.orig'.
Please specify the API bind host/port (optional):
Bind Host []: Hit Enter
Bind Port []: Hit Enter
information/cli: Created backup file '/etc/icinga2/features-available/api.conf.orig'.
information/cli: Updating constants.conf.
information/cli: Created backup file '/etc/icinga2/constants.conf.orig'.
information/cli: Updating constants file '/etc/icinga2/constants.conf'.
information/cli: Updating constants file '/etc/icinga2/constants.conf'.
information/cli: Updating constants file '/etc/icinga2/constants.conf'.
Done.

Now restart your Icinga 2 daemon to finish the installation!

systemctl restart icinga2
</code></pre>

<h3 id="install-icingaweb2">Install IcingaWeb2</h3>

<pre><code>yum install centos-release-scl
yum install icingaweb2 icingacli icingaweb2-selinux

yum install httpd
systemctl start httpd.service
systemctl enable httpd.service

icingacli setup config webserver apache

## OR

yum install nginx rh-php71-php-fpm rh-php71-php-pgsql
systemctl enable --now rh-php71-php-fpm.service

## config
icingacli setup config webserver nginx &gt; /etc/nginx/default.d//icinga.conf
systemctl enable --now nginx

sudo -u postgres psql -c &quot;CREATE ROLE icinga_web WITH LOGIN PASSWORD 'icinga_web'&quot;
sudo -u postgres createdb -O icinga_web -E UTF8 icinga_web

icingacli setup token create

# go to
https://icinga.devopstales.intra/icingaweb2/
</code></pre>

<p><img src="/img/include/icingaweb1.png" alt="Example image" /></p>

<p><img src="/img/include/icingaweb2.png" alt="Example image" /></p>

<p><img src="/img/include/icingaweb3.png" alt="Example image" /></p>

<p><img src="/img/include/icingaweb4.png" alt="Example image" /></p>

<p><img src="/img/include/icingaweb5.png" alt="Example image" /></p>

<p><img src="/img/include/icingaweb6.png" alt="Example image" /></p>

<p><img src="/img/include/icingaweb7.png" alt="Example image" /></p>

<p><img src="/img/include/icingaweb8.png" alt="Example image" /></p>

<p><img src="/img/include/icingaweb9.png" alt="Example image" /></p>

<p><img src="/img/include/icingaweb10.png" alt="Example image" /></p>

<p><img src="/img/include/icingaweb11.png" alt="Example image" /></p>

<p><img src="/img/include/icingaweb12.png" alt="Example image" /></p>

<p><img src="/img/include/icingaweb13.png" alt="Example image" /></p>

<p><img src="/img/include/icingaweb14.png" alt="Example image" /></p>

<p><img src="/img/include/icingaweb15.png" alt="Example image" /></p>

<p><img src="/img/include/icingaweb16.png" alt="Example image" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configure Rundeck LADAP]]></title>
            <link href="https://devopstales.github.io/home/rundeck-ldap/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/rundeck-ldap/</id>
            
            
            <published>2019-12-09T00:00:00+00:00</published>
            <updated>2019-12-09T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will configure Rundeck to use LDAP as a User backend.</p>

<h3 id="rundeck-ldap-config-file">Rundeck LDAP config file</h3>

<pre><code>nano /etc/rundeck/jaas-ldap.conf

# openldap
ldap {
      com.dtolabs.rundeck.jetty.jaas.JettyCachingLdapLoginModule required
      contextFactory=&quot;com.sun.jndi.ldap.LdapCtxFactory&quot;
      debug=&quot;true&quot;
      providerUrl=&quot;ldap://openldap:389&quot;
      bindDn=&quot;cn=admin,dc=mydomain,dc=intra&quot;
      bindPassword=&quot;Password1&quot;
      authenticationMethod=&quot;simple&quot;
      forceBindingLogin=&quot;true&quot;
      userBaseDn=&quot;dc=mydomain,dc=intra&quot;
      userRdnAttribute=&quot;cn&quot;
      userIdAttribute=&quot;cn&quot;
      userPasswordAttribute=&quot;userPassword&quot;
      userObjectClass=&quot;inetOrgPerson&quot;
      roleBaseDn=&quot;dc=mydomain,dc=intra&quot;
      roleNameAttribute=&quot;cn&quot;
      roleMemberAttribute=&quot;uniqueMember&quot;
      roleObjectClass=&quot;groupOfUniqueNames&quot;
      supplementalRoles=&quot;admin, user&quot;;
      };

# windows AD
ldap {
      com.dtolabs.rundeck.jetty.jaas.JettyCachingLdapLoginModule required
      contextFactory=&quot;com.sun.jndi.ldap.LdapCtxFactory&quot;
      debug=&quot;true&quot;
      providerUrl=&quot;ldap://devopstales.intra:389&quot;
      bindDn=&quot;cn=admin,dc=mydomain,dc=intra&quot;
      bindPassword=&quot;Password1&quot;
      authenticationMethod=&quot;simple&quot;
      forceBindingLogin=&quot;true&quot;
      userBaseDn=&quot;dc=mydomain,dc=intra&quot;
      userRdnAttribute=&quot;sAMAccountName&quot;
      userIdAttribute=&quot;sAMAccountName&quot;
      userPasswordAttribute=&quot;unicodePwd&quot;
      userObjectClass=&quot;user&quot;
      roleBaseDn=&quot;dc=mydomain,dc=intra&quot;
      roleNameAttribute=&quot;cn&quot;
      roleMemberAttribute=&quot;member&quot;
      roleObjectClass=&quot;group&quot;
      supplementalRoles=&quot;admin, user&quot;;
      };
</code></pre>

<h3 id="rundeck-multibackend-config-file">Rundeck multibackend config file</h3>

<pre><code>nano /etc/rundeck/jaas-multiauth.conf

multiauth {
      com.dtolabs.rundeck.jetty.jaas.JettyCombinedLdapLoginModule required
      contextFactory=&quot;com.sun.jndi.ldap.LdapCtxFactory&quot;
      debug=&quot;true&quot;
      providerUrl=&quot;ldap://ad1:389&quot;
      bindDn=&quot;cn=admin,dc=mydomain,dc=intra&quot;
      bindPassword=&quot;Password1&quot;
      authenticationMethod=&quot;simple&quot;
      forceBindingLogin=&quot;true&quot;
      userBaseDn=&quot;ou=Users,dc=mydomain,dc=intra&quot;
      userRdnAttribute=&quot;cn&quot;
      userIdAttribute=&quot;cn&quot;
      userPasswordAttribute=&quot;userPassword&quot;
      userObjectClass=&quot;inetOrgPerson&quot;
      roleBaseDn=&quot;ou=Groups,dc=mydomain,dc=intra&quot;
      roleNameAttribute=&quot;cn&quot;
      roleMemberAttribute=&quot;uniqueMember&quot;
      roleObjectClass=&quot;groupOfUniqueNames&quot;

      ignoreRoles=&quot;true&quot;
      storePass=&quot;true&quot;
      clearPass=&quot;true&quot;
      useFirstPass=&quot;false&quot;
      tryFirstPass=&quot;false&quot;
      supplementalRoles=&quot;admin, user&quot;;

      org.rundeck.jaas.jetty.JettyRolePropertyFileLoginModule required
      debug=&quot;true&quot;
      useFirstPass=&quot;true&quot;
      file=&quot;/etc/rundeck/realm.properties&quot;;
      };
</code></pre>

<h3 id="configure-rundeck-to-use-multibackend-config-file">Configure Rundeck to use multibackend config file</h3>

<pre><code>nano /etc/rundeck/profile

JAAS_CONF=&quot;${JAAS_CONF:-$RDECK_CONFIG/jaas-ldap.conf}&quot;
LOGIN_MODULE=&quot;ldap&quot;

# OR based on your distro
nano /etc/default/rundeckd

export JAAS_CONF=&quot;/etc/rundeck/jaas-ldap.conf&quot;
export LOGIN_MODULE=&quot;ldap&quot;
</code></pre>

<h3 id="configure-rundeck-to-use-ldap-config-file">Configure Rundeck to use LDAP config file</h3>

<pre><code>nano /etc/rundeck/profile

JAAS_CONF=&quot;${JAAS_CONF:-$RDECK_CONFIG/jaas-multiauth.conf}&quot;
LOGIN_MODULE=&quot;multiauth&quot;

# OR based on your distro
nano /etc/default/rundeckd

export JAAS_CONF=&quot;/etc/rundeck/jaas-multiauth.conf&quot;
export LOGIN_MODULE=&quot;multiauth&quot;
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install docker on fedora 31]]></title>
            <link href="https://devopstales.github.io/home/docker-on-fedora31/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/docker-on-fedora31/?utm_source=atom_feed" rel="related" type="text/html" title="Install docker on fedora 31" />
                <link href="https://devopstales.github.io/home/graylog-pfsense-squid/?utm_source=atom_feed" rel="related" type="text/html" title="Analyzing PFsense squid logs in Graylog" />
                <link href="https://devopstales.github.io/home/openshift-gitlab-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Installing GitLab on OpenShift" />
                <link href="https://devopstales.github.io/home/proxmox-backup-error/?utm_source=atom_feed" rel="related" type="text/html" title="Solution for: Proxmox backup error due to iothread" />
                <link href="https://devopstales.github.io/home/foreman-pxe/?utm_source=atom_feed" rel="related" type="text/html" title="Install Foreman PXE boot" />
            
                <id>https://devopstales.github.io/home/docker-on-fedora31/</id>
            
            
            <published>2019-12-04T00:00:00+00:00</published>
            <updated>2019-12-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>On Fedora 31 after starting docker container some error as follows because of cgroups v2.</p>

<pre><code>docker: Error response from daemon: OCI runtime create failed: container_linux.go:346: starting container process caused &quot;process_linux.go:297: applying cgroup configuration for process caused \&quot;open /sys/fs/cgroup/docker/cpuset.cpus.effective: no such file or directory\&quot;&quot;: unknown.
</code></pre>

<p>This is beacaue Fedora 31 uses cgroups v2 by default and docker doesn’t yet support cgroupsv2. In this tutorial I am going to show you how to change cgroups to v1 as a quick fix to run docker.</p>

<pre><code>sudo nano /etc/default/grub
GRUB_CMDLINE_LINUX=&quot;systemd.unified_cgroup_hierarchy=0&quot;

grub2-mkconfig -o /boot/efi/EFI/fedora/grub.cfg
reboot
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Analyzing PFsense squid logs in Graylog]]></title>
            <link href="https://devopstales.github.io/home/graylog-pfsense-squid/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/graylog3-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Analyzing PFsense logs in Graylog3" />
                <link href="https://devopstales.github.io/home/graylog3-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Graylog3" />
                <link href="https://devopstales.github.io/linux/graylog-pfsense-squid/?utm_source=atom_feed" rel="related" type="text/html" title="Analyzing PFsense squid logs in Graylog" />
                <link href="https://devopstales.github.io/linux/graylog3-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Analyzing PFsense logs in Graylog3" />
                <link href="https://devopstales.github.io/linux/graylog3-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Graylog3" />
            
                <id>https://devopstales.github.io/home/graylog-pfsense-squid/</id>
            
            
            <published>2019-11-24T00:00:00+00:00</published>
            <updated>2019-11-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>We will parse the access log records generated by the PfSense&rsquo;s squd plugin. We already have our graylog server running and we will start preparing the terrain to capture those logs records.</p>

<p>Many thanks to opc40772 developed the original contantpack for pfsense squid log agregation what I updated for the new Graylog3 and Elasticsearch 6.</p>

<h3 id="celebro-localinstall">Celebro localinstall</h3>

<pre><code># celebro van to use port 9000 so change graylog3 bindport
nano /etc/graylog/server/server.conf
http_bind_address = 127.0.0.1:9400
nano /etc/nginx/conf.d/graylog.conf

systemctl restart graylog-server.service
systemctl restart nginx

wget https://github.com/lmenezes/cerebro/releases/download/v0.8.3/cerebro-0.8.3-1.noarch.rpm
yum localinstall cerebro-0.8.3-1.noarch.rpm
</code></pre>

<h3 id="create-indices">Create indices</h3>

<p>We now create the Pfsense indice on Graylog at <code>System / Indexes</code> <br>
<img src="/img/include/squid_pfsense1.png" alt="image" /> <br></p>

<h3 id="import-index-template-for-elasticsearch-6-x">Import index template for elasticsearch 6.x</h3>

<pre><code>systemctl stop graylog-server.service
</code></pre>

<p>Go to <code>celebro &gt; more &gt; index templates</code>
Create new with name: <code>pfsense-custom</code> and copy the template from file <code>squid_custom_template_el6.json</code>
Edit other pfsense template to (sorrend 0)</p>

<p>In Cerebro we stand on top of the pfsense index and unfold the options and select delete index.</p>

<h3 id="geoip-database">Geoip database</h3>

<pre><code>wget -t0 -c http://geolite.maxmind.com/download/geoip/database/GeoLite2-City.tar.gz
tar -xvf GeoLite2-City.tar.gz
cp GeoLite2-City_*/GeoLite2-City.mmdb /etc/graylog/server

systemctl start graylog-server.service
</code></pre>

<p>Enable geoip database at <code>System \ Imput &gt; Configurations &gt; Plugins &gt; Geo-Location Processor &gt; update</code>
Chane the order of the <code>Message Processors Configuration</code></p>

<ul>
<li>AWS Instance Name Lookup</li>
<li>Message Filter Chain</li>
<li>Pipeline Processor</li>
<li>GeoIP Resolver</li>
</ul>

<p>Enable geoip database</p>

<h3 id="import-contantpack">Import contantpack</h3>

<pre><code>git clone https://github.com/devopstales/Squid-Graylog.git
</code></pre>

<p>import
chaneg date timezone in Pipeline rule
Go tu Stream menu and edit stream <br>
<img src="/img/include/squid_pfsense7.png" alt="image" /> <br></p>

<p><code>System &gt; Pipelines</code>
Manage rules and then Edit rule (Change the timezone)</p>

<pre><code>rule &quot;timestamp_pfsense_for_grafana&quot;
 when
 has_field(&quot;timestamp&quot;)
then
// the following date format assumes there's no time zone in the string
 let source_timestamp = parse_date(substring(to_string(now(&quot;Europe/Budapest&quot;)),0,23), &quot;yyyy-MM-dd'T'HH:mm:ss.SSS&quot;);
 let dest_timestamp = format_date(source_timestamp,&quot;yyyy-MM-dd HH:mm:ss&quot;);
 set_field(&quot;real_timestamp&quot;, dest_timestamp);
end
</code></pre>

<p>Add the existing pipeline to the squid stream by clicking the Edit connections at Pipeline connections.</p>

<p><img src="/img/include/squid_pfsense8.png" alt="image" /></p>

<h3 id="confifure-pfsense">Confifure pfsense</h3>

<pre><code># http://pkg.freebsd.org/FreeBSD:11:amd64/latest/All/
pkg add http://pkg.freebsd.org/FreeBSD:11:amd64/latest/All/beats-6.7.1.txz

nano /usr/local/etc/filebeat.yml
filebeat.prospectors:
- input_type: log
  document_type: squid3
  paths:
    - /var/squid/logs/access.log

output.logstash:
  # The Logstash hosts
  hosts: [&quot;192.168.0.112:5044&quot;]

  # Optional SSL. By default is off.
  # List of root certificates for HTTPS server verifications
  bulk_max_size: 2048
 #ssl.certificate_authorities: [&quot;/etc/filebeat/logstash.crt&quot;]
  template.name: &quot;filebeat&quot;
  template.path: &quot;filebeat.template.json&quot;
  template.overwrite: false
  # Certificate for SSL client authentication
  #ssl.certificate: &quot;/etc/pki/client/cert.pem&quot;

  # Client Certificate Key
  #ssl.key: &quot;/etc/pki/client/cert.key&quot;

/usr/local/sbin/filebeat -c /usr/local/etc/filebeat.yml test config
cp /usr/local/etc/rc.d/filebeat /usr/local/etc/rc.d/filebeat.sh
echo &quot;filebeat_enable=yes&quot; &gt;&gt; /etc/rc.conf.local
echo &quot;filebeat_conf=/usr/local/etc/filebeat.yml&quot; &gt;&gt; /etc/rc.conf.local

/usr/local/etc/rc.d/filebeat.sh start
ps aux | grep beat
</code></pre>

<h3 id="install-grafana-dashboard">Install grafana Dashboard</h3>

<pre><code># install nececery plugins
grafana-cli plugins install grafana-piechart-panel
grafana-cli plugins install grafana-worldmap-panel
grafana-cli plugins install savantly-heatmap-panel
grafana-cli plugins install briangann-datatable-panel
systemctl restart grafana-server
</code></pre>

<p>Create new datasource: <br>
<img src="/img/include/squid_pfsense9.jpg" alt="image" /> <br></p>

<p>Import dashboadr. <br></p>

<hr />

<h5 id="contantpack">Contantpack:</h5>

<p><a href="https://github.com/devopstales/Squid-Graylog.git">https://github.com/devopstales/Squid-Graylog.git</a></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Squid proxy]]></title>
            <link href="https://devopstales.github.io/home/install-squid/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/install-squid/</id>
            
            
            <published>2019-11-18T00:00:00+00:00</published>
            <updated>2019-11-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Squid is the most popular Proxy server for Linux systems. The squid proxy server is also useful for the web packet filtering.</p>

<h3 id="install-squid">Install Squid</h3>

<p>Squid packages are available in default yum repositories.</p>

<pre><code>yum install squid
</code></pre>

<h3 id="configuring-squid">Configuring Squid</h3>

<pre><code>nano /etc/squid/squid.conf
    #
    # Recommended minimum configuration:
    ## Example rule allowing access from your local networks.
    # Adapt to list your (internal) IP networks from where browsing
    # should be allowed
    acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
    acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
    acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
    acl localnet src fc00::/7       # RFC 4193 local private network range
    acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)

    machinesacl SSL_ports port 443
    acl Safe_ports port 80          # http
    acl Safe_ports port 21          # ftp
    acl Safe_ports port 443         # https
    acl Safe_ports port 70          # gopher
    acl Safe_ports port 210         # wais
    acl Safe_ports port 1025-65535  # unregistered ports
    acl Safe_ports port 280         # http-mgmt
    acl Safe_ports port 488         # gss-http
    acl Safe_ports port 591         # filemaker
    acl Safe_ports port 777         # multiling http
    acl CONNECT method CONNECT#
    # Recommended minimum Access Permission configuration:
    #
    # Deny requests to certain unsafe ports
    http_access deny !Safe_ports# Deny CONNECT to other than secure SSL ports
    http_access deny CONNECT !SSL_ports# Only allow cachemgr access from localhost
    http_access allow localhost manager
    http_access deny manager# We strongly recommend the following be uncommented to protect innocent
    # web applications running on the proxy server who think the only
    # one who can access services on &quot;localhost&quot; is a local user
    #http_access deny to_localhost#
    # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
    ## Example rule allowing access from your local networks.
    # Adapt localnet in the ACL section to list your (internal) IP networks
    # from where browsing should be allowed
    http_access allow localnet
    http_access allow localhost# And finally deny all other access to this proxy
    http_access deny all# Squid normally listens to port 3128
    http_port 3128# Uncomment and adjust the following to add a disk cache directory.
    #cache_dir ufs /var/spool/squid 100 16 256# Leave coredumps in the first cache dir
    coredump_dir /var/spool/squid

# # Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%	1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%	0
refresh_pattern .               0       20%     4320
</code></pre>

<h3 id="allow-ip-address-to-use-the-internet-through-your-proxy-server">Allow IP Address to Use the Internet Through Your Proxy Server</h3>

<p>If your netwok is 110.220.330.0/24:</p>

<pre><code>acl localnet src 110.220.330.0/24
</code></pre>

<p>For changes to take effect you will need to restart your Squid server, use the following command for same.</p>

<pre><code>systemctl restart squid
</code></pre>

<h3 id="allow-a-specific-port-for-http-connections">Allow a Specific Port for HTTP Connections</h3>

<pre><code>acl Safe_ports port 8080
</code></pre>

<h3 id="using-basic-authentication-with-squid">Using Basic Authentication with Squid</h3>

<pre><code>yum -y install httpd-tools
touch /etc/squid/passwd &amp;&amp; chown squid /etc/squid/passwd

htpasswd /etc/squid/passwd proxyuser
    New password:
    Re-type new password:
    Adding password for user pxuser
</code></pre>

<pre><code>nano /etc/squid/squid.conf
...
auth_param basic program /usr/lib64/squid/basic_ncsa_auth /etc/squid/passwd
auth_param basic children 5
auth_param basic realm Squid Basic Authentication
auth_param basic credentialsttl 2 hours
acl auth_users proxy_auth REQUIRED
http_access allow auth_users
</code></pre>

<h3 id="blocking-websites">Blocking Websites</h3>

<pre><code>nano /etc/squid/blocked_sites
facebook.com
youtube.com
</code></pre>

<pre><code>nano /etc/squid/squid.conf
acl blocked_sites dstdomain &quot;/etc/squid/blocked_sites&quot;
http_access deny blocked_sites
</code></pre>

<h3 id="block-specific-keyword-with-squid">Block Specific Keyword with Squid</h3>

<pre><code>nano /etc/squid/blockkeywords.lst
yahoo
gmail
facebook
</code></pre>

<pre><code>acl blockkeywordlist url_regex &quot;/etc/squid/blockkeywords.lst&quot;
http_access deny blockkeywordlist
</code></pre>

<h3 id="disable-caching">Disable caching</h3>

<pre><code># Leave coredumps in the first cache dir
#coredump_dir /var/spool/squid
</code></pre>

<h3 id="changing-squid-port">Changing Squid Port</h3>

<pre><code>nano /etc/squid/squid.conf
http_port 3128
</code></pre>

<h3 id="export-proxy-server-settings">Export Proxy Server Settings</h3>

<pre><code>$ export http_proxy=&quot;http://PROXY_SERVER:PORT&quot;
$ export https_proxy=&quot;http://PROXY_SERVER:PORT&quot;
$ export ftp_proxy=&quot;http://PROXY_SERVER:PORT&quot;

$ export http_proxy=&quot;http://USER:PASSWORD@PROXY_SERVER:PORT&quot;
$ export https_proxy=&quot;http://USER:PASSWORD@PROXY_SERVER:PORT&quot;
$ export ftp_proxy=&quot;http://USER:PASSWORD@PROXY_SERVER:PORT&quot;
</code></pre>

<h3 id="test-caching">Test caching</h3>

<pre><code>env | grep proxy

tailf /var/log/squid/access.log
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Installing GitLab on OpenShift]]></title>
            <link href="https://devopstales.github.io/home/openshift-gitlab-helm/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-gitlabrunner/?utm_source=atom_feed" rel="related" type="text/html" title="Install Gitlab runner on Openshift" />
                <link href="https://devopstales.github.io/kubernetes/openshift-gitlab-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Installing GitLab on OpenShift" />
                <link href="https://devopstales.github.io/home/openshift-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Helm" />
                <link href="https://devopstales.github.io/home/openshift-kompose/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Run docker-compoe in Openshift" />
                <link href="https://devopstales.github.io/home/openshift-add-node/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Add Nodes to a Cluster" />
            
                <id>https://devopstales.github.io/home/openshift-gitlab-helm/</id>
            
            
            <published>2019-11-18T00:00:00+00:00</published>
            <updated>2019-11-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I  had to install Gitlab to Openshift recently. Turned out getting GitLab up and running on OpenShift is not so easy.</p>

<h3 id="create-new-project">Create new project</h3>

<pre><code>oc new-project gitlab-devopstales.intra
</code></pre>

<h3 id="deploy-helm">Deploy helm</h3>

<pre><code>nano helm-namespace-account.yaml
kind: ServiceAccount
apiVersion: v1
metadata:
  name: tiller-gitlab-devopstales.intra
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tiller-gitlab-devopstales.intra
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller-gitlab-devopstales.intra
    namespace: kube-system
</code></pre>

<p>Now set up Helm, install the Tiller plugin and add the GitLab repository.</p>

<pre><code>oc apply -f helm-namespace-account.yaml
oc get sa

helm init --service-account tiller-gitlab-devopstales.intra --tiller-namespace gitlab-mydomain-intra
oc get po -n kube-system

export TILLER_NAMESPACE=kube-system
echo $TILLER_NAMESPACE
helm version
</code></pre>

<h2 id="get-helmchart">Get helmchart</h2>

<pre><code>helm repo add gitlab https://charts.gitlab.io/
helm repo update

oc adm policy add-scc-to-user anyuid -z default -n gitlab-devopstales.intra
oc adm policy add-scc-to-user anyuid -z gitlab-runner -n gitlab-devopstales.intra

# gitlab-tst is the name of the helm deployment
oc adm policy add-scc-to-user anyuid -z gitlab-tst-shared-secrets
oc adm policy add-scc-to-user anyuid -z gitlab-tst-gitlab-runner
oc adm policy add-scc-to-user anyuid -z gitlab-tst-prometheus-server
oc adm policy add-scc-to-user anyuid -z default
</code></pre>

<h3 id="create-chart-values">Create chart values</h3>

<pre><code>nano gitlab-values.yml
certmanager:
  install: false
global:
  appConfig:
    enableUsagePing: true
    enableImpersonation: true
    defaultCanCreateGroup: true
    usernameChangingEnabled: true
    issueClosingPattern:
    defaultTheme:
    defaultProjectsFeatures:
      issues: true
      mergeRequests: true
      wiki: true
      snippets: true
      builds: true
      containerRegistry: true
    ldap:
      servers:
        main:
          base: dc=mydomain,dc=intra
          user_filter: (&amp;(objectClass=user)(memberof=cn=Users,dc=mydomain,dc=intra))
          bind_dn: Administrator@devopstales.intra
          host: 192.168.10.4
          label: devopstales.intra
          password:
            key: password
            secret: gitlab-ldap-secret
          port: 636
          encryption: simple_tls
          uid: sAMAccountName
          active_directory: true
          verify_certificates: false
          allow_username_or_email_login: true
    omniauth:
      enabled: true
      blockAutoCreatedUsers: false
      allowSingleSignOn: ['oauth2_generic']
      providers:
        - secret: gitlab-sso
          key: provider
    backups:
      bucket: gitlab-devopstales.intra
      tmpBucket: gitlab-devopstales.intra
      objectStorage:
        backend: s3
    lfs:
      bucket: gitlab-devopstales.intra
      connection:
        secret: ceph-storage
        key: gitlab
    artifacts:
      bucket: gitlab-devopstales.intra
      connection:
        secret: ceph-storage
        key: gitlab
    uploads:
      bucket: gitlab-devopstales.intra
      connection:
        secret: ceph-storage
        key: gitlab
    packages:
      bucket: gitlab-devopstales.intra
      connection:
        secret: ceph-storage
        key: gitlab
    externalDiffs:
      bucket: gitlab-devopstales.intra
      connection:
        secret: ceph-storage
        key: gitlab
    pseudonymizer:
      bucket: gitlab-devopstales.intra
      connection:
        secret: ceph-storage
        key: gitlab
  edition: ce
  email:
    from: gitlab@devopstales.intra
  hosts:
    domain: devopstales.intra
    externalIP: gitlab.devopstales.intra
    gitlab:
      name: gitlab.devopstales.intra
      https: false
    registry:
      name: gitlab-registry.devopstales.intra
      https: false
  ingress:
    enabled: false
    configureCertmanager: false
    tls:
      secretName: gitlab-certs
  smtp:
    address: mail.active.hu
    authentication: &quot;&quot;
    domain: devopstales.intra
    enabled: true
    port: 25
  gitlab-exporter:
    enabled: false
  registry:
    bucket: gitlab-registry
  minio:
    enabled: false
nginx-ingress:
  enabled: false
gitlab-runner:
  rbac:
    create: true
registry:
  enabled: true
  storage:
    secret: ceph-storage
    key: registry
  image:
    repository: docker.io/registry
    tag: 2.6.0
gitlab:
  task-runner:
    backups:
      objectStorage:
        config:
          secret: storage-config
          key: config
</code></pre>

<h3 id="create-secrets-for-deployment">Create secrets for deployment</h3>

<pre><code>nano ceph.gitlab-data.yaml
provider: AWS
region: default
aws_access_key_id: W3MNDO373H6LQUNCG4SG
aws_secret_access_key: vVFEWx3hqbcrGJyaZVie9YoFG6rPoRYmqnDzRwrn
endpoint: &quot;https://s3.devopstales.intra&quot;
enable_signature_v4_streaming: false

# admin jog kell a cephez
nano ceph.gitlab-registry.yaml
cache:
  blobdescriptor: inmemory
s3:
  region: default
  bucket: gitlab-registry
  accesskey: PZIOIH63CENHPG15XY42
  secretkey: K6K1lWO7Jtyp5rZiCwj77JC5BFMEAZ4a2PAkg9fB
  regionendpoint: https://s3.devopstales.intra
  rootdirectory: /
  secure: true
  v4auth: false
  encrypt: false
  chunksize: 5242880
redirect:
  disable: true
</code></pre>

<pre><code>nano ceph.backup.config
[default]
access_key = W3MNDO373H8LQUNCJ8QV
access_token = vVFEWx8hqbcrGJyaZVie8YoER8rPoRYmqnDzRwrn
host_base = s3.devopstales.intra
host_bucket = %(bucket)s.s3.devopstales.intra
bucket_location = US
use_https = True
check_ssl_certificate = False
</code></pre>

<pre><code>nano keycloak.sso.yaml
name: 'oauth2_generic'
label: 'mydomain'
app_id: 'gitlab'
app_secret: 'f2514bd4-92e4-40fa-bec4-382838db25f0'
args:
  client_options:
    site: 'https://sso.devopstales.intra'
    user_info_url: '/auth/realms/mydomain/protocol/openid-connect/userinfo'
    authorize_url: '/auth/realms/mydomain/protocol/openid-connect/auth'
    token_url: '/auth/realms/mydomain/protocol/openid-connect/token'
  user_response_structure:
    attributes:
      email: 'email'
      first_name: 'given_name'
      last_name: 'family_name'
      name: 'name'
      nickname: 'preferred_username'
    id_path: 'preferred_username'
</code></pre>

<h3 id="deploy-secrets">Deploy secrets</h3>

<pre><code># https ssl cert
oc create secret tls gitlab-certs --cert=tls.crt --key=tls.key

oc create secret generic storage-config --from-file=config=ceph.backup.config

oc create secret generic ceph-storage --from-file=registry=ceph.gitlab-registry.yaml --from-file=gitlab=ceph.gitlab-data.yaml

oc create secret generic gitlab-sso --from-file=provider=keycloak.sso.yaml
oc create secret generic gitlab-ldap-secret --from-literal=password=
</code></pre>

<h3 id="deploy-application-with-helm">Deploy application with helm</h3>

<pre><code>helm upgrade --install -f gitlab-values.yml gitlab-tst gitlab/gitlab --debug --dry-run
helm upgrade --install -f gitlab-values.yml gitlab-tst gitlab/gitlab --timeout 600
helm upgrade -f gitlab-values.yml gitlab-tst gitlab/gitlab --timeout 600

# https://docs.gitlab.com/charts/installation/version_mappings.html
helm upgrade -f gitlab-values.yml gitlab-tst gitlab/gitlab --version 2.3.5 --timeout 600

# gitlab-tst
oc get secret gitlab-tst-gitlab-initial-root-password -o jsonpath='{.data.password}' | base64 -d
</code></pre>

<p>###</p>

<pre><code>nano gitlab-ssh-nodeport-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: gitlab-shell-nodeport
  labels:
    app: gitlab-shell
    name: gitlab-shell-nodeport
spec:
  type: NodePort
  ports:
    - port: 2222
      nodePort: 32222
      name: ssh
  selector:
    app: gitlab-shell

</code></pre>

<pre><code>oc create -f gitlab-ssh-nodeport-svc.yaml
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Solution for: Proxmox backup error due to iothread]]></title>
            <link href="https://devopstales.github.io/home/proxmox-backup-error/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/proxmox-backup-pve-zsync/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with pve-zsync" />
                <link href="https://devopstales.github.io/home/proxmox-backup-sanoid/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with sanoid" />
                <link href="https://devopstales.github.io/home/proxmox-backup-znapzend/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with znapzend" />
                <link href="https://devopstales.github.io/home/proxmox-node-remove/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox node removal" />
                <link href="https://devopstales.github.io/home/openshift-gitlab-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Installing GitLab on OpenShift" />
            
                <id>https://devopstales.github.io/home/proxmox-backup-error/</id>
            
            
            <published>2019-11-18T00:00:00+00:00</published>
            <updated>2019-11-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>If you see the following error when trying to backup a KVM VM image on Proxmox:</p>

<pre><code>ERROR: Backup of VM 100 failed – disk ‘scsi0’ ‘zfsvols:vm-100-disk-1’ (iothread=on) can’t use backup feature currently. Please set backup=no for this drive at /usr/share/perl5/PVE/VZDump/QemuServer.pm line 77. INFO: Backup job finished with errors TASK ERROR: job errors
</code></pre>

<p>Posted on September 9, 2017 by Daniel Mettler
Solution for: Proxmox backup error due to iothread=1</p>

<p>If you see the following error when trying to backup a KVM VM image on Proxmox:</p>

<p>ERROR: Backup of VM 100 failed – disk ‘scsi0’ ‘zfsvols:vm-100-disk-1’ (iothread=on) can’t use backup feature currently. Please set backup=no for this drive at /usr/share/perl5/PVE/VZDump/QemuServer.pm line 77. INFO: Backup job finished with errors TASK ERROR: job errors</p>

<p>edit /etc/pve/qemu-server/100.conf, look for a line similar to</p>

<pre><code>scsi0: zfsvols:vm-100-disk-1,iothread=1,size=70G
</code></pre>

<p>and change it to</p>

<pre><code>scsi0: zfsvols:vm-100-disk-1,iothread=0,size=70G
# OR
scsi0: zfsvols:vm-100-disk-1,size=70G
</code></pre>

<p>After this you can backup the VM. This Problem was solvd in the proxmox 6 (pve-manager 6.0-11)</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Foreman PXE boot]]></title>
            <link href="https://devopstales.github.io/home/foreman-pxe/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/foreman-pxe/?utm_source=atom_feed" rel="related" type="text/html" title="Install Foreman PXE boot" />
                <link href="https://devopstales.github.io/home/clonedeploy/?utm_source=atom_feed" rel="related" type="text/html" title="Install clonedeploy pxeboot server" />
                <link href="https://devopstales.github.io/home/k8s-ceph-storage-with-csi-driver/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD volume with CSI driver" />
                <link href="https://devopstales.github.io/home/who-mapping-rbd-device/?utm_source=atom_feed" rel="related" type="text/html" title="Ceph: who&#39;s mapping a RBD device" />
                <link href="https://devopstales.github.io/home/gcp-vm-export/?utm_source=atom_feed" rel="related" type="text/html" title="Export GCP VM to S3" />
            
                <id>https://devopstales.github.io/home/foreman-pxe/</id>
            
            
            <published>2019-11-07T00:00:00+00:00</published>
            <updated>2019-11-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Foreman is a complete lifecycle management tool for physical and virtual servers. We give system administrators the power to easily automate repetitive tasks, quickly deploy applications, and proactively manage servers, on-premise or in the cloud.</p>

<p>I hawe a VM with two virtual interface the enp0s3 for NAT and enp0s9 with an internal network.</p>

<h3 id="install-dhcp-server">Install DHCP server</h3>

<pre><code>yum install -y dhcp nano -y

echo &quot;DHCPDARGS=enp0s9&quot; &gt;&gt; /etc/sysconfig/dhcpd
</code></pre>

<pre><code>cat &gt; /etc/dhcp/dhcpd.conf &lt;&lt; EOF
#DHCP configuration for PXE boot server
ddns-update-style interim;
ignore client-updates;
authoritative;
allow booting;
allow bootp;
allow unknown-clients;

# A slightly different configuration for an internal subnet.
subnet 192.168.100.0
netmask 255.255.255.0
{
range 192.168.100.101 192.168.100.200;
option domain-name-servers 192.168.100.100;
option routers 192.168.100.100;
default-lease-time 600;
max-lease-time 7200;

# PXE SERVER IP
next-server 192.168.100.100; #  PXE server ip
filename &quot;pxelinux.0&quot;;
}
EOF
</code></pre>

<pre><code>systemctl start dhcpd.service
systemctl enable dhcpd.service
systemctl status dhcpd.service
</code></pre>

<h3 id="install-dns-server">Install DNS server</h3>

<pre><code>yum -y install bind bind-utils

nano /etc/named.conf
options {
        ...
        // listen-on port 53 { 127.0.0.1; };
        // listen-on-v6 port 53 { ::1; };
        ...
        allow-query     { localhost; 192.168.100.0/24; };
        ...
        forwarders {
                8.8.8.8;
                8.8.4.4;
        };
};
include &quot;/etc/named.my.zones&quot;;
</code></pre>

<pre><code>touch /etc/named.my.zones
chown root:named /etc/named.my.zones

nano /etc/named.my.zones
zone &quot;devopstales.intra&quot; IN {
         type master;
         file &quot;devopstales.intra.db&quot;;
         allow-update { none; };
};
</code></pre>

<pre><code>nano /var/named/devopstales.intra.db
@   IN  SOA     primary.devopstales.intra. root.mydomain.intra. (
                                                1001    ;Serial
                                                3H      ;Refresh
                                                15M     ;Retry
                                                1W      ;Expire
                                                1D      ;Minimum TTL
                                                )

;Name Server Information
@      IN  NS      primary.devopstales.intra.

;IP address of Name Server
primary IN  A       192.168.100.100

;Mail exchanger
devopstales.intra. IN  MX 10   mail.mydomain.intra.

;A - Record HostName To IP Address
foreman IN  A       192.168.100.100
mail    IN  A       192.168.100.50
</code></pre>

<pre><code>systemctl restart named
systemctl status named
systemctl enable named
</code></pre>

<h3 id="install-vtftp">Install vtftp</h3>

<pre><code>yum install vsftpd -y

nano /etc/vsftpd/vsftpd.conf
anonymous_enable=YES
write_enable=NO

systemctl enable vsftpd
systemctl restart vsftpd
systemctl status vsftpd

cd /opt
wget http://ftp.bme.hu/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1908.iso
wget http://ftp.bme.hu/centos/7/isos/x86_64/sha256sum.txt

sha256sum CentOS-7-x86_64-Minimal-1908.iso
cat sha256sum.txt

mount -o loop /opt/CentOS-7-x86_64-Minimal-1908.iso  /mnt

mkdir /var/ftp/pub/CentOS_7_x86_64
rsync -rv --progress /mnt/ /var/ftp/pub/CentOS_7_x86_64/
umount /mnt
restorecon -Rv /var/ftp/pub/
</code></pre>

<h3 id="install-foreman">Install Foreman</h3>

<pre><code>yum -y install https://yum.puppet.com/puppet6-release-el-7.noarch.rpm
yum -y install http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
yum -y install https://yum.theforeman.org/releases/1.23/el7/x86_64/foreman-release.rpm
yum -y install foreman-installer

foreman-installer \
--foreman-initial-organization &quot;mydomain&quot; \
--foreman-initial-location &quot;office&quot; \
--enable-foreman-plugin-ansible \
--enable-foreman-proxy-plugin-ansible \
--enable-foreman-plugin-remote-execution \
--enable-foreman-proxy-plugin-remote-execution-ssh \
--enable-foreman-plugin-cockpit \
--enable-foreman-plugin-openscap
</code></pre>

<h3 id="configure-hammer">Configure hammer</h3>

<pre><code>nano ~/.hammer/cli.modules.d/foreman.yml
:foreman:
 :host: 'https://foreman.devopstales.intra/'
 :username: 'admin'
 :password: '**********'

hammer defaults add --param-name organization --param-value &quot;mydomain&quot;
hammer defaults add --param-name location --param-value &quot;office&quot;
hammer defaults list
</code></pre>

<h3 id="configurate-pxeboot">Configurate PXEboot</h3>

<pre><code>sudo ss -lnup | grep 69
grep disa /etc/xinetd.d/tftp
ls -l /var/lib/tftpboot/

# create subnet
hammer subnet create \
--name PXEnet \
--network-type IPv4 \
--network 192.168.100.0 \
--mask 255.255.255.0 \
--dns-primary 192.168.100.100 \
--domains devopstales.intra \
--tftp-id 1 \
--httpboot-id 1 \
--ipam &quot;Internal DB&quot; \
--from 192.168.100.101 \
--to 192.168.100.200 \
--boot-mode Static

hammer medium create \
--name &quot;CentOS7_DVD_FTP&quot; \
--os-family &quot;Redhat&quot; \
--path &quot;ftp://foreman.devopstales.intra/pub/CentOS_7_x86_64/&quot;
</code></pre>

<p>Create a file hardened_ptable.txt with the content below.</p>

<pre><code>&lt;%#
kind: ptable
name: Kickstart hardened
oses:
- CentOS
- Fedora
- RedHat
%&gt;

# System bootloader configuration
bootloader --location=mbr --boot-drive=sda --timeout=3
# Partition clearing information
clearpart --all --drives=sda
zerombr

# Disk partitioning information
part /boot --fstype=&quot;xfs&quot; --ondisk=sda --size=1024 --label=boot --fsoptions=&quot;rw,nodev,noexec,nosuid&quot;

# 30GB physical volume
part pv.01  --fstype=&quot;lvmpv&quot; --ondisk=sda --size=30720
volgroup vg_os pv.01

logvol /        --fstype=&quot;xfs&quot;  --size=4096 --vgname=vg_os --name=lv_root
logvol /home    --fstype=&quot;xfs&quot;  --size=512  --vgname=vg_os --name=lv_home --fsoptions=&quot;rw,nodev,nosuid&quot;
logvol /tmp     --fstype=&quot;xfs&quot;  --size=1024 --vgname=vg_os --name=lv_tmp  --fsoptions=&quot;rw,nodev,noexec,nosuid&quot;
logvol /var     --fstype=&quot;xfs&quot;  --size=6144 --vgname=vg_os --name=lv_var  --fsoptions=&quot;rw,nosuid&quot;
logvol /var/log --fstype=&quot;xfs&quot;  --size=512  --vgname=vg_os --name=lv_log  --fsoptions=&quot;rw,nodev,noexec,nosuid&quot;
logvol swap     --fstype=&quot;swap&quot; --size=2048 --vgname=vg_os --name=lv_swap --fsoptions=&quot;swap&quot;
</code></pre>

<pre><code>hammer partition-table create \
  --name &quot;Kickstart hardened&quot; \
  --os-family &quot;Redhat&quot; \
  --operatingsystems &quot;CentOS 7.4.1708&quot; \
  --file &quot;hardened_ptable.txt&quot;

hammer os create \
  --name &quot;CentOS&quot; \
  --major &quot;7&quot; \
  --minor &quot;4.1708&quot; \
  --family &quot;Redhat&quot; \
  --password-hash &quot;SHA512&quot; \
  --architectures &quot;x86_64&quot; \
  --media &quot;CentOS7_DVD_FTP&quot; \
  --partition-tables &quot;Kickstart hardened&quot;

hammer hostgroup create \
  --name &quot;el7_group&quot; \
  --description &quot;Host group for CentOS 7 servers&quot; \
  --lifecycle-environment &quot;stable&quot; \
  --content-view &quot;el7_content&quot; \
  --content-source-id &quot;1&quot; \
  --environment &quot;homelab&quot; \
  --puppet-proxy &quot;foreman.devopstales.intra&quot; \
  --puppet-ca-proxy &quot;foreman.devopstales.intra&quot; \
  --domain &quot;devopstales.intra&quot; \
  --subnet &quot;PXEnet&quot; \
  --architecture &quot;x86_64&quot; \
  --operatingsystem &quot;CentOS 4.1708&quot; \
  --medium &quot;CentOS7_DVD_FTP&quot; \
  --partition-table &quot;Kickstart hardened&quot; \
  --pxe-loader &quot;PXELinux BIOS&quot; \
  --root-pass &quot;Password1&quot;

hammer hostgroup set-parameter  \
  --name &quot;selinux-mode&quot; \
  --value &quot;disabled&quot; \
  --hostgroup &quot;el7_group&quot;

hammer hostgroup set-parameter  \
  --name &quot;disable-firewall&quot; \
  --value &quot;true&quot; \
  --hostgroup &quot;el7_group&quot;

hammer hostgroup set-parameter  \
  --name &quot;bootloader-append&quot; \
  --value &quot;net.ifnames=0 biosdevname=0&quot; \
  --hostgroup &quot;el7_group&quot;

hammer host create \
  --name &quot;pxe-test&quot; \
  --hostgroup &quot;el7_group&quot; \
  --interface &quot;type=interface,mac=08:00:27:fb:ad:17,ip=192.168.100.110,managed=true,primary=true,provision=true&quot;
</code></pre>

<pre><code>ll /var/lib/tftpboot/pxelinux.cfg/
cat /var/lib/tftpboot/pxelinux.cfg/01-08-00-27-fb-ad-17
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes Ceph RBD volume with CSI driver]]></title>
            <link href="https://devopstales.github.io/home/k8s-ceph-storage-with-csi-driver/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/k8s-ceph-storage-with-csi-driver/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD volume with CSI driver" />
                <link href="https://devopstales.github.io/home/who-mapping-rbd-device/?utm_source=atom_feed" rel="related" type="text/html" title="Ceph: who&#39;s mapping a RBD device" />
                <link href="https://devopstales.github.io/home/k8s-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/install-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ceph cluster" />
                <link href="https://devopstales.github.io/home/ceph-block-device/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph Block Device" />
            
                <id>https://devopstales.github.io/home/k8s-ceph-storage-with-csi-driver/</id>
            
            
            <published>2019-10-08T00:00:00+00:00</published>
            <updated>2019-10-08T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how can you use CEPH RBD with CSI driver for persistent storagi on Kubernetes.</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
<li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-contour-ingress --></li>
<li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
<li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
<li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... --></li>
<li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
<li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
<li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<p>The Container Storage Interface (CSI) is a standard for exposing arbitrary block and file storage storage systems to Kubernetes. Using CSI third-party storage providers can write and deploy plugins exposing storage systems in Kubernetes. Bbefore we begin lets ensure that we have the following requirements:</p>

<ul>
<li>Kubernetes cluster v1.14+</li>
<li>allow-privileged flag enabled for both kubelet and API server</li>

<li><p>Running Ceph cluster</p>

<pre><code>git clone https://github.com/ceph/ceph-csi.git
cd ceph-csi/deploy/rbd/kubernetes/v1.14+/

kubectl create -f csi-nodeplugin-rbac.yaml
kubectl create -f csi-provisioner-rbac.yaml
</code></pre>

<pre><code>nano csi-config-map.yaml
---
apiVersion: v1
kind: ConfigMap
data:
config.json: |-
[
  {
    &quot;clusterID&quot;: &quot;k8s-ceph&quot;,
    &quot;monitors&quot;: [
      &quot;192.168.1.31:6789&quot;,
      &quot;192.168.1.32:6789&quot;,
      &quot;192.168.1.33:6789&quot;
    ]
  }
]
metadata:
name: ceph-csi-config


kubectl create -fcsi-config-map.yaml
</code></pre>

<pre><code>kubectl create -f csi-rbdplugin-provisioner.yaml
kubectl create -f csi-rbdplugin.yaml
</code></pre>

<pre><code>ceph auth get-key client.admin|base64
QVFDTDliVmNEb21I32SHoPxXNGhmRkczTFNtcXM0ZW5VaXlTZEE977==

nano csi-rbd-secret.yaml
---
apiVersion: v1
kind: Secret
metadata:
name: csi-rbd-secret
namespace: default
data:
userID: admin
userKey: QVFDTDliVmNEb21I32SHoPxXNGhmRkczTFNtcXM0ZW5VaXlTZEE977==

nano rbd-csi-sc.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
name: csi-rbd
provisioner: rbd.csi.ceph.com
parameters:
monitors: 192.168.1.31:6790,192.168.1.32:6790,192.168.1.33:6790
clusterID: k8s-ceph
pool: rbd
imageFeatures: layering
csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
csi.storage.k8s.io/provisioner-secret-namespace: default
csi.storage.k8s.io/node-publish-secret-name: csi-rbd-secret
csi.storage.k8s.io/node-publish-secret-namespace: default
adminid: admin
csi.storage.k8s.io/fstype: ext4
reclaimPolicy: Delete
mountOptions:
- discard

kubectl create -f csi-rbd-secret.yaml
kubectl create -f rbd-csi-sc.yaml

kubectl get storageclass
NAME      PROVISIONER        AGE
csi-rbd   rbd.csi.ceph.com   15s
</code></pre>

<pre><code>nano raw-block-pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
name: raw-block-pvc
spec:
accessModes:
- ReadWriteMany
volumeMode: Block
resources:
requests:
  storage: 1Gi
storageClassName: csi-rbd

kubectl create -f raw-block-pvc.yaml

kubectl get pvc
NAME            STATUS    VOLUME                                  
raw-block-pvc   Bound     pvc-fd66b4d6-757d-22e9-8f9e-4f86e2356a59
</code></pre></li>
</ul>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Ceph: who's mapping a RBD device]]></title>
            <link href="https://devopstales.github.io/home/who-mapping-rbd-device/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/install-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ceph cluster" />
                <link href="https://devopstales.github.io/home/ceph-block-device/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph Block Device" />
                <link href="https://devopstales.github.io/home/ceph-cephfs/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph CephFS" />
                <link href="https://devopstales.github.io/kubernetes/who-mapping-rbd-device/?utm_source=atom_feed" rel="related" type="text/html" title="Ceph: who&#39;s mapping a RBD device" />
            
                <id>https://devopstales.github.io/home/who-mapping-rbd-device/</id>
            
            
            <published>2019-10-05T00:00:00+00:00</published>
            <updated>2019-10-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<h3 id="main-problem">Main problem</h3>

<p>I get this error in Openshift:</p>

<pre><code>  MountVolume.WaitForAttach failed for volume &quot;pvc-9fcd3d08-d14e-11e9-a958-66934f1af826&quot; :
  rbd image k8s-prod/kubernetes-dynamic-pvc-a0029613-d14e-11e9-aaf5-8611e6b1c395 is still being used
</code></pre>

<h3 id="solution">Solution</h3>

<p>So I Wanna know who is using this DBD device?</p>

<pre><code>rbd status k8s-prod/kubernetes-dynamic-pvc-a0029613-d14e-11e9-aaf5-8611e6b1c395
Watchers:
	watcher=192.168.1.43:0/3614154426 client.165467009 cookie=18446462598732840961
</code></pre>

<p>To solve this problem you need to restart Openshift&rsquo;s Kubernetes components.</p>

<pre><code>ssh 192.168.1.43
systemctl restart docker origin-node
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Export GCP VM to S3]]></title>
            <link href="https://devopstales.github.io/home/gcp-vm-export/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/gcp-vm-export/?utm_source=atom_feed" rel="related" type="text/html" title="Export GCP VM to S3" />
                <link href="https://devopstales.github.io/home/ansible-k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Kubernetes" />
                <link href="https://devopstales.github.io/home/alerta-on-centos8/?utm_source=atom_feed" rel="related" type="text/html" title="Install alerta on Centos8" />
                <link href="https://devopstales.github.io/home/alerta-on-centos7/?utm_source=atom_feed" rel="related" type="text/html" title="Install alerta on Centos7" />
                <link href="https://devopstales.github.io/home/openshift-restrict-access/?utm_source=atom_feed" rel="related" type="text/html" title="Restrict access to OpenShift routes by IP address" />
            
                <id>https://devopstales.github.io/home/gcp-vm-export/</id>
            
            
            <published>2019-10-04T00:00:00+00:00</published>
            <updated>2019-10-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Step by step guide to export virtual machine running in Google cloud computer engine to your S3 bucket.</p>

<pre><code>gcloud compute instances list
NAME        ZONE            MACHINE_TYPE               PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP  STATUS
demo1   europe-west1-b  g1-small                                10.132.0.3                TERMINATED
</code></pre>

<h3 id="create-snapshot">Create snapshot</h3>

<pre><code>gcloud compute disks snapshot europe-west1-b/disks/demo1 --storage-locatio europe-west1

gcloud compute snapshots list
NAME              DISK_SIZE_GB  SRC_DISK                        STATUS
demo1-backup  30            europe-west1-b/disks/demo1  READY
</code></pre>

<h3 id="create-custom-image">Create custom image</h3>

<pre><code>gcloud compute images create demo1-backup --source-snapshot demo1-backup
Created [https://www.googleapis.com/compute/v1/projects/demo-project-223110/global/images/demo1-backup].
NAME              PROJECT               FAMILY  DEPRECATED  STATUS
demo1-backup  demo-project-223110                      READY
</code></pre>

<h3 id="create-s3-storage">Create S3 storage</h3>

<pre><code>gsutil mb gs://backup-demo-project-223110/ -l europe-west1
</code></pre>

<h3 id="export-to-s3-storage">Export to S3 storage</h3>

<pre><code>gcloud compute images export --destination-uri gs://backup-demo-project-223110/demo1-beckup.tar.gz --image demo1-backup --export-format=vmdk
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Kubernetes]]></title>
            <link href="https://devopstales.github.io/home/ansible-k8s-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/ansible-operator-overview/?utm_source=atom_feed" rel="related" type="text/html" title="Ansible Operator Overview" />
                <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
                <link href="https://devopstales.github.io/home/openshift-restrict-access/?utm_source=atom_feed" rel="related" type="text/html" title="Restrict access to OpenShift routes by IP address" />
                <link href="https://devopstales.github.io/home/openshift-kompose/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Run docker-compoe in Openshift" />
                <link href="https://devopstales.github.io/home/openshift-add-node/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Add Nodes to a Cluster" />
            
                <id>https://devopstales.github.io/home/ansible-k8s-install/</id>
            
            
            <published>2019-10-03T00:00:00+00:00</published>
            <updated>2019-10-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Kubespray is a pre made ansible playbook for Kubernetes installation. In this Post I will show you how to use to install a new Kubernetes cluster.</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
<li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-contour-ingress --></li>
<li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
<li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
<li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... --></li>
<li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
<li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
<li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<h3 id="environment">Environment</h3>

<pre><code>192.168.1.60    deployer.devopstales.intra (LB)
192.168.1.61    master0.devopstales.intra  (master)
192.168.1.62    master1.devopstales.intra  (master)
192.168.1.63    master2.devopstales.intra  (master)
192.168.1.64    worker0.devopstales.intra  (worker)
192.168.1.65    worker1.devopstales.intra  (worker)

# hardware requirement
4 CPU
16G RAM
</code></pre>

<h3 id="prerequirement">Prerequirement</h3>

<pre><code># deployer

nano ~/.ssh/config
Host master0
    Hostname master0.devopstales.intra
    User ansible

Host master1
    Hostname master1.devopstales.intra
    User ansible

Host master2
    Hostname master2.devopstales.intra
    User ansible

Host worker0.devopstales.intra
    Hostname worker0.devopstales.intra
    User ansible

Host worker1
    Hostname worker1.devopstales.intra
    User ansible
</code></pre>

<pre><code>yum install epel-release -y
yum update -y
yum install python-pip git tmux nano -y
git clone https://github.com/kubernetes-sigs/kubespray.git
cd kubespray
pip install --user -r requirements.txt

cp -rfp inventory/sample inventory/mycluster
</code></pre>

<h3 id="configurate-installer">Configurate Installer</h3>

<pre><code>nano inventory/mycluster/inventory.ini
master0   ansible_host=192.168.1.61 ip=192.168.1.61
master1   ansible_host=192.168.1.62 ip=192.168.1.62
master2   ansible_host=192.168.1.63 ip=192.168.1.63
worker0   ansible_host=192.168.1.64 ip=192.168.1.64
worker1   ansible_host=192.168.1.65 ip=192.168.1.65

# ## configure a bastion host if your nodes are not directly reachable
# bastion ansible_host=x.x.x.x ansible_user=some_user

[kube-master]
master0
master1
master2

[etcd]
master0
master1
master2

[kube-node]
worker0
worker1

[calico-rr]

[k8s-cluster:children]
kube-master
kube-node
calico-rr
</code></pre>

<h3 id="run-the-installer">Run the Installer</h3>

<pre><code>tmux new -s kubespray
ansible-playbook -i inventory/mycluster/inventory.ini --become \
--user=centos --become-user=root cluster.yml

test install on node:
sudo -i
kubectl get node
NAME      STATUS   ROLES    AGE   VERSION
master0   Ready    master   92m   v1.15.3
master1   Ready    master   91m   v1.15.3
master2   Ready    master   91m   v1.15.3
worker0   Ready    &lt;none&gt;   90m   v1.15.3
worker1   Ready    &lt;none&gt;   90m   v1.15.3

kubectl config get-clusters
</code></pre>

<h3 id="let-s-configure-an-external-loadbalancer">Let’s configure an external loadbalancer</h3>

<pre><code>sudo firewall-cmd --add-port=6443/tcp --permanent
sudo firewall-cmd --reload

yum -y install haproxy

nano ..
listen k8s-apiserver-https
  bind *:6443
  option ssl-hello-chk
  mode tcp
  balance roundrobin
  timeout client 3h
  timeout server 3h
  server master0 192.168.1.61:6443
  server master1 192.168.1.62:6443
  server master2 192.168.1.63:6443

systemctl enable --now haproxy
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install alerta on Centos8]]></title>
            <link href="https://devopstales.github.io/home/alerta-on-centos8/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/alerta-on-centos7/?utm_source=atom_feed" rel="related" type="text/html" title="Install alerta on Centos7" />
                <link href="https://devopstales.github.io/monitoring/alerta-on-centos8/?utm_source=atom_feed" rel="related" type="text/html" title="Install alerta on Centos8" />
                <link href="https://devopstales.github.io/monitoring/alerta-on-centos7/?utm_source=atom_feed" rel="related" type="text/html" title="Install alerta on Centos7" />
                <link href="https://devopstales.github.io/home/gitlab-install/?utm_source=atom_feed" rel="related" type="text/html" title="Gitlab Install" />
                <link href="https://devopstales.github.io/home/prometheus-alertmanagger/?utm_source=atom_feed" rel="related" type="text/html" title="Install Alertmanagger" />
            
                <id>https://devopstales.github.io/home/alerta-on-centos8/</id>
            
            
            <published>2019-09-28T00:00:00+00:00</published>
            <updated>2019-09-28T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>AIn this post I will show you how to install alerta monitoring dashboard on Centos 8.</p>

<pre><code>yum install epel-release -y
yum upgrade -y
</code></pre>

<h3 id="install-and-configure-postgresql">Install and configure postgresql</h3>

<p>In a previous post I wrote about how to <a href="/linux/install-postgresql/#install-postgresql-10-on-centos-8">Install PostgreSQL 10</a></p>

<pre><code>sudo su - postgres
createuser alerta
createdb -O alerta alerta
psql
ALTER USER &quot;alerta&quot; WITH PASSWORD 'alerta';
\q
exit
</code></pre>

<h3 id="install-python3-packages">install python3 packages</h3>

<pre><code>yum install python3 python3-pip python3-setuptools python3-devel python3-psycopg2 gcc git tmux nginx
</code></pre>

<p>Create user for alerta and install python3 moduls with it.</p>

<pre><code>useradd alerta
usermod -aG alerta nginx
su - alerta
pip3 install --user wheel alerta-server alerta uwsgi
</code></pre>

<h1 id="alerta-server">alerta server</h1>

<pre><code>nano /etc/alertad.conf
DATABASE_URL = 'postgres://alerta:alerta@localhost:5432/alerta'
PLUGINS=['reject']
ALLOWED_ENVIRONMENTS=['Production', 'Development', 'Code']
</code></pre>

<p>We use uwsgi to create a unix socket wgere the alerta webgui can connect.</p>

<pre><code>sudo mkdir -p /var/log/uwsgi
sudo chown -R alerta:alerta /var/log/uwsgi
mkdir /var/run/alerta
chown -R alerta.alerta /var/run/alerta/

echo &quot;from alerta import app&quot; &gt; /usr/share/nginx/wsgi.py
</code></pre>

<pre><code>nano /etc/uwsgi.ini
[uwsgi]
chdir = /usr/share/nginx/
mount = /api=wsgi.py
callable = app
manage-script-name = true
env = BASE_URL=/api

master = true
processes = 5
#logger = syslog:alertad
logto = /var/log/uwsgi/%n.log

socket = /var/run/alerta/uwsgi.sock
chmod-socket = 664
uid = alerta
gid = alerta
vacuum = true

die-on-term = true
</code></pre>

<pre><code>nano /etc/systemd/system/uwsgi.service
[Unit]
Description=uWSGI service
After=syslog.target

[Service]
ExecStart=/home/alerta/.local/bin/uwsgi --ini /etc/uwsgi.ini
RuntimeDirectory=uwsgi
Type=notify
StandardError=syslog
NotifyAccess=all

[Install]
WantedBy=multi-user.target
</code></pre>

<pre><code>systemctl enable uwsgi
systemctl start uwsgi
systemctl status uwsgi
</code></pre>

<h1 id="alerta-webgui">alerta webgui</h1>

<pre><code>wget https://github.com/alerta/alerta-webui/releases/latest/download/alerta-webui.tar.gz
tar zxvf alerta-webui.tar.gz
mv dist/ /usr/share/nginx/alerta
echo '{&quot;endpoint&quot;: &quot;/api&quot;}' &gt; /usr/share/nginx/dist/config.json
</code></pre>

<pre><code>nano /etc/nginx/nginx.conf
    #server {
    #    listen       80 default_server;
    #    listen       [::]:80 default_server;
    #    server_name  _;
    #    root         /usr/share/nginx/html;

    #    # Load configuration files for the default server block.
    #    include /etc/nginx/default.d/*.conf;

     #   location / {
     #   }

     #   error_page 404 /404.html;
     #       location = /40x.html {
     #   }

     #   error_page 500 502 503 504 /50x.html;
     #       location = /50x.html {
     #   }
    #}
</code></pre>

<pre><code>nano /etc/nginx/conf.d/alerta.conf
server {
        listen 80 default_server;

        location /api { try_files $uri @api; }
        location @api {
            include uwsgi_params;
            uwsgi_pass unix:/var/run/alerta/uwsgi.sock;
            proxy_set_header Host $host:$server_port;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }

        location / {
                root /usr/share/nginx/alerta;
        }
}
</code></pre>

<pre><code>nginx -t
systemctl restart nginx
systemctl enable nginx
</code></pre>

<h3 id="test">test</h3>

<p>Send testalert with alerta client.</p>

<pre><code>su - alerta

echo '[DEFAULT]
endpoint = http://localhost/api' &gt;  $HOME/.alerta.conf

echo 'export PATH=$PATH:/home/alerta/.local/bin' &gt;&gt; ~/.bashrc
PATH=$PATH:/home/alerta/.local/bin

alerta query
alerta send --resource net01 --event down --severity critical --environment Code --service Network --text 'net01 is down.'
alerta query
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install alerta on Centos7]]></title>
            <link href="https://devopstales.github.io/home/alerta-on-centos7/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/monitoring/alerta-on-centos7/?utm_source=atom_feed" rel="related" type="text/html" title="Install alerta on Centos7" />
                <link href="https://devopstales.github.io/home/gitlab-install/?utm_source=atom_feed" rel="related" type="text/html" title="Gitlab Install" />
                <link href="https://devopstales.github.io/home/prometheus-alertmanagger/?utm_source=atom_feed" rel="related" type="text/html" title="Install Alertmanagger" />
                <link href="https://devopstales.github.io/home/openshift-restrict-access/?utm_source=atom_feed" rel="related" type="text/html" title="Restrict access to OpenShift routes by IP address" />
                <link href="https://devopstales.github.io/home/openshift-kompose/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Run docker-compoe in Openshift" />
            
                <id>https://devopstales.github.io/home/alerta-on-centos7/</id>
            
            
            <published>2019-09-27T00:00:00+00:00</published>
            <updated>2019-09-27T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>AIn this post I will show you how to install alerta monitoring dashboard on Centos 7.</p>

<pre><code>yum install epel-release nano -y
yum upgrade -y
</code></pre>

<h3 id="install-and-configure-postgresql">Install and configure postgresql</h3>

<p>In a previous post I wrote about how to <a href="/linux/install-postgresql/#install-postgresql-10-on-centos-7">Install PostgreSQL 10</a></p>

<pre><code>sudo su - postgres
createuser alerta
createdb -O alerta alerta
psql
ALTER USER &quot;alerta&quot; WITH PASSWORD 'alerta';
\q
</code></pre>

<h3 id="install-python3-packages">install python3 packages</h3>

<pre><code>yum install https://centos7.iuscommunity.org/ius-release.rpm -y
yum install python36u python36u-pip python36u-setuptools python36u-devel gcc git tmux
yum install uwsgi-plugin-psgi nginx -y
</code></pre>

<h1 id="alerta-server">alerta server</h1>

<pre><code>python3.6 -m venv alerta
alerta/bin/pip install --upgrade pip wheel alerta-server alerta uwsgi
</code></pre>

<pre><code>nano /etc/alertad.conf
DATABASE_URL = 'postgres://alerta:alerta@localhost:5432/alerta'
PLUGINS=['reject']
ALLOWED_ENVIRONMENTS=['Production', 'Development', 'Code']
</code></pre>

<p>We use uwsgi to create a unix socket wgere the alerta webgui can connect.</p>

<pre><code>sudo mkdir -p /var/log/uwsgi
sudo chown -R nginx:nginx /var/log/uwsgi
mkdir /var/run/alerta
chown -R nginx.nginx /var/run/alerta/

nano /var/www/wsgi.py
from alerta import app
</code></pre>

<pre><code>nano /etc/uwsgi.ini
[uwsgi]
chdir = /var/www
mount = /api=wsgi.py
callable = app
manage-script-name = true
env = BASE_URL=/api

master = true
processes = 5
#logger = syslog:alertad
logto = /var/log/uwsgi/%n.log

socket = /var/run/alerta/uwsgi.sock
chmod-socket = 664
uid = nginx
gid = nginx
vacuum = true

die-on-term = true
</code></pre>

<pre><code>nano /etc/systemd/system/alerta-app.service
[Unit]
Description=uWSGI service
After=syslog.target

[Service]
ExecStart=/opt/alerta/bin/uwsgi --ini /etc/uwsgi.ini
RuntimeDirectory=uwsgi
Type=notify
StandardError=syslog
NotifyAccess=all

[Install]
WantedBy=multi-user.target
</code></pre>

<pre><code>systemctl enable alerta-app
systemctl start alerta-app
systemctl status alerta-app
</code></pre>

<h1 id="alerta-webgui">alerta webgui</h1>

<pre><code>wget https://github.com/alerta/alerta-webui/releases/latest/download/alerta-webui.tar.gz
tar zxvf alerta-webui.tar.gz
mv dist/ /usr/share/nginx/alerta
echo '{&quot;endpoint&quot;: &quot;/api&quot;}' &gt; /usr/share/nginx/dist/config.json
</code></pre>

<pre><code>nano /etc/nginx/nginx.conf
    #server {
    #    listen       80 default_server;
    #    listen       [::]:80 default_server;
    #    server_name  _;
    #    root         /usr/share/nginx/html;

    #    # Load configuration files for the default server block.
    #    include /etc/nginx/default.d/*.conf;

     #   location / {
     #   }

     #   error_page 404 /404.html;
     #       location = /40x.html {
     #   }

     #   error_page 500 502 503 504 /50x.html;
     #       location = /50x.html {
     #   }
    #}
</code></pre>

<pre><code>nano /etc/nginx/conf.d/alerta.conf
server {
        listen 80 default_server;

        location /api { try_files $uri @api; }
        location @api {
            include uwsgi_params;
            uwsgi_pass unix:/var/run/alerta/uwsgi.sock;
            proxy_set_header Host $host:$server_port;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }

        location / {
                root /usr/share/nginx/alerta;
        }
}
</code></pre>

<pre><code>nginx -t
systemctl restart nginx
systemctl enable nginx
</code></pre>

<h3 id="test">test</h3>

<p>Send testalert with alerta client.</p>

<pre><code>nano $HOME/.alerta.conf
[DEFAULT]
endpoint = http://localhost/api

/opt/alerta/bin/alerta query
/opt/alerta/bin/alerta send --resource net01 --event down --severity critical --environment Code --service Network --text 'net01 is down.'
/opt/alerta/bin/alerta query
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Restrict access to OpenShift routes by IP address]]></title>
            <link href="https://devopstales.github.io/home/openshift-restrict-access/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure Openshift vSphere Cloud Provider" />
                <link href="https://devopstales.github.io/home/openshift-kompose/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Run docker-compoe in Openshift" />
                <link href="https://devopstales.github.io/home/ansible-operator-overview/?utm_source=atom_feed" rel="related" type="text/html" title="Ansible Operator Overview" />
                <link href="https://devopstales.github.io/home/openshift-add-node/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Add Nodes to a Cluster" />
                <link href="https://devopstales.github.io/home/openshift-letsencrypt/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Letsencrypt certificates" />
            
                <id>https://devopstales.github.io/home/openshift-restrict-access/</id>
            
            
            <published>2019-09-20T00:00:00+00:00</published>
            <updated>2019-09-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how can restrict access to the routes by source IP address.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<h3 id="restricting-access-to-a-route">Restricting access to a route</h3>

<p>After creating and exposing a route, you can add an annotation to the route specifying the IP address(es) that you would like to whitelist. Whitelisting a IP address automatically blacklists everything else.</p>

<pre><code>oc annotate route test-route haproxy.router.openshift.io/ip_whitelist=192.168.0.0/24
</code></pre>

<p>To allow several IP addresses through to the route, separate each IP with a space:</p>

<pre><code>oc annotate route test-route haproxy.router.openshift.io/ip_whitelist=192.168.1.10 180.5.61.153 192.168.1.0/24 192.168.0.0/24
</code></pre>

<p>To delete the IPs from the annotation, you can run the command:</p>

<pre><code>oc annotate route test-route haproxy.router.openshift.io/ip_whitelist-
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift: Run docker-compoe in Openshift]]></title>
            <link href="https://devopstales.github.io/home/openshift-kompose/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-add-node/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Add Nodes to a Cluster" />
                <link href="https://devopstales.github.io/home/openshift-letsencrypt/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Letsencrypt certificates" />
                <link href="https://devopstales.github.io/home/openshift-hostalreadyclaimed/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Error: HostAlreadyClaimed" />
                <link href="https://devopstales.github.io/home/openshift-gitlabrunner/?utm_source=atom_feed" rel="related" type="text/html" title="Install Gitlab runner on Openshift" />
                <link href="https://devopstales.github.io/home/openshift-extregistry/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: External registry" />
            
                <id>https://devopstales.github.io/home/openshift-kompose/</id>
            
            
            <published>2019-09-08T00:00:00+00:00</published>
            <updated>2019-09-08T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Kompose is a open source tool that uses &ldquo;docker-compose&rdquo; file to deploy on kubernetes. Openshift is also Kubernetes based and Kompose is support Openshift too.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<p>Let’s say we have project with multiple microsevices that needs to deploy on Openshift and they have docker-compose.yml and Dockerfile. How do we deploy on Openshift?<br></p>

<p>Use Kompose to convert the docker-compose file to openshift compatible kubernetes config.</p>

<pre><code>kompose convert -f docker-compose.yaml --provider=openshift
</code></pre>

<p>This command creates a separet yaml file for all kubernetes building block like  &ldquo;-imagestream.yaml&rdquo;, &ldquo;-service.yaml&rdquo;, &ldquo;-deploymentconfig.yaml&rdquo; for each microservice. We can use these config files to deploy on Openshift easily.</p>

<pre><code>kompose up --provider=openshift -f docker-compose.yml --build build-config --namespace=devopstales
</code></pre>

<p>If you have &ldquo;build&rdquo; option in docker-compose file, kompose automatically detects remote git url to deploy automatically.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to backup Graylog's logs in elasticsearch]]></title>
            <link href="https://devopstales.github.io/home/elasticsearch-backup/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/elasticsearch-backup/?utm_source=atom_feed" rel="related" type="text/html" title="How to backup Graylog&#39;s logs in elasticsearch" />
                <link href="https://devopstales.github.io/home/k8s-metallb/?utm_source=atom_feed" rel="related" type="text/html" title="Install MetalLB load balancer for Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-tillerless-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Tillerless helm2 install" />
                <link href="https://devopstales.github.io/home/k8s-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/clonedeploy/?utm_source=atom_feed" rel="related" type="text/html" title="Install clonedeploy pxeboot server" />
            
                <id>https://devopstales.github.io/home/elasticsearch-backup/</id>
            
            
            <published>2019-09-07T00:00:00+00:00</published>
            <updated>2019-09-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Graylog store the log data in elasticsearch so I will show you how to create and restore snapshot with elasticsearch.</p>

<h3 id="requirement">Requirement</h3>

<p>First you will need to add the repo.path location to your elasticsearch.yml. This is the local path of the folder where the snapshot files will store.</p>

<pre><code>mkdir -p /mnt/elasticsearch-backup
chown -R elasticsearch. /mnt/elasticsearch-backup

cat &gt;&gt; /etc/elasticsearch/elasticsearch.yml &lt;&lt; EOF
path.repo: [&quot;/mnt/elasticsearch-backup&quot;]
EOF

systemctl restart elasticsearch
</code></pre>

<h3 id="elasticsearch">Elasticsearch</h3>

<p>Elasticsearch needs to know the backup path by registering a backup repository:</p>

<pre><code>curl -XPUT 'http://localhost:9200/_snapshot/my_backup' -d {
  &quot;type&quot;: &quot;fs&quot;,
  &quot;settings&quot;: {
     &quot;location&quot;: &quot;/mnt/elasticsearch-backup&quot;,
     &quot;compress&quot;: true
  }
}'
</code></pre>

<h3 id="create-backup">Create Backup</h3>

<pre><code>curl -XPUT &quot;localhost:9200/_snapshot/my_backup/snapshot_1?wait_for_completion=true&quot;

# list snapshots:
curl -XGET 'localhost:9200/_snapshot/my_backup/_all?pretty'
</code></pre>

<h3 id="restore-backup">Restore backup</h3>

<pre><code>curl -XPOST &quot;localhost:9200/_snapshot/my_backup/snapshot_1/_restore?wait_for_completion=true&quot;
</code></pre>

<h3 id="delete-snapshot">Delete snapshot</h3>

<pre><code>curl -XDELETE 'localhost:9200/_snapshot/my_backup/snapshot_1'
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install MetalLB load balancer for Kubernetes]]></title>
            <link href="https://devopstales.github.io/home/k8s-metallb/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-nginx-ingress/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
                <link href="https://devopstales.github.io/kubernetes/k8s-metallb/?utm_source=atom_feed" rel="related" type="text/html" title="Install MetalLB load balancer for Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-tillerless-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Tillerless helm2 install" />
                <link href="https://devopstales.github.io/home/k8s-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install kubernetes with kubeadm" />
            
                <id>https://devopstales.github.io/home/k8s-metallb/</id>
            
            
            <published>2019-08-08T00:00:00+00:00</published>
            <updated>2019-08-08T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this tutorial I will show you how to install  Metal LB load balancer running on Kubernetes (k8s).</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
<li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-contour-ingress --></li>
<li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
<li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
<li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... --></li>
<li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
<li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
<li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<h3 id="enviroment">Enviroment</h3>

<pre><code>kubectl get node
NAME     STATUS   ROLES    EXTERNAL-IP
host-1   Ready    master   203.0.113.1
host-2   Ready    node     203.0.113.2
host-3   Ready    node     203.0.113.3
host-4   Ready    node     203.0.113.4
</code></pre>

<p>MetalLB provides a network load-balancer implementation for Kubernetes clusters that do not run on a supported cloud provider, effectively allowing the usage of LoadBalancer Services within ber-metal Installation. Kubernetes does not offer an implementation of network load-balancers (Services of type LoadBalancer) for bare metal clusters. The implementations of Network LB that Kubernetes does ship with are all glue code that calls out to various IaaS platforms (GCP, AWS, Azure…). If you’re not running on a supported IaaS platform (GCP, AWS, Azure…), LoadBalancers will remain in the “pending” state indefinitely when created.</p>

<p><img src="/img/include/metallb.jpg" alt="Example image" /></p>

<p>First we need to apply the MetalLB manifest.</p>

<pre><code>kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.8.1/manifests/metallb.yaml
</code></pre>

<p>Create a metallb-configmap.yaml file and modify your IP range accordingly.</p>

<pre><code>cat &lt; EOF &gt; metallb-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: my-ip-space
      protocol: layer2
      addresses:
      - 203.0.113.2-203.0.113.4
EOF

kubectl apply -f metallb-config.yaml
kubectl get pods -n metallb-system
</code></pre>

<p>Exposing a service through the load balancer</p>

<pre><code>cat &lt;EOF&gt;&gt; nginx.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  type: LoadBalancer
  selector:
    app: nginx
  ports:
  - port: 80
    name: http
EOF

kubectl apply -f nginx-deployment.yaml
kubectl get svc
NAME           TYPE           CLUSTER-IP       EXTERNAL-IP    PORT(S)        AGE
nginx          LoadBalancer   10.109.51.83     203.0.113.2    80:30452/TCP   5m
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install s3cmd with CEHP Radosgateway]]></title>
            <link href="https://devopstales.github.io/home/s3cmd-with-radosgw/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/s3cmd-with-radosgw/</id>
            
            
            <published>2019-08-04T00:00:00+00:00</published>
            <updated>2019-08-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>s3cmd is a cli utility for s3.</p>

<pre><code>root@pve1:~# apt-get install s3cmd
root@pve1:~# s3cmd --configure
Access Key: xxxxxxxxxxxxxxxxxxxxxx
Secret Key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

root@pve1:~#  s3cmd mb s3://devopstales
Bucket 's3://devopstales/' created
</code></pre>

<h3 id="create-user">Create User</h3>

<pre><code>adosgw-admin user create --uid={username} --display-name=&quot;{display-name}&quot; \[--email={email}\]
radosgw-admin user create --uid=devopstales --display-name=&quot;devopstales&quot; --email=devopstales@devopstales.intra
</code></pre>

<h3 id="user-info">User Info</h3>

<pre><code>radosgw-admin user info --uid=devopstales
</code></pre>

<h3 id="modify-user">Modify User</h3>

<pre><code>radosgw-admin user modify --uid=devopstales --display-name=&quot;John Doe&quot;
</code></pre>

<h3 id="disable-and-enable-user">Disable and Enable User</h3>

<pre><code>radosgw-admin user suspend --uid=devopstales
radosgw-admin user enable --uid=devopstales
</code></pre>

<h3 id="remove-user">Remove User</h3>

<pre><code>radosgw-admin user rm --uid=devopstales
</code></pre>

<h1 id="add-quota-for-user">Add quota for User</h1>

<pre><code>radosgw-admin quota set --uid=&quot;devopstales@devopstales.intra&quot; --quota-scope=bucket --max-size=30G
radosgw-admin quota enable --quota-scope=bucket --uid=&quot;devopstales@devopstales.intra&quot;
</code></pre>

<h3 id="set-bucket-policy">Set Bucket Policy</h3>

<p>Create a Bucket Policy fot devopstales@devopstales.intra to allow privileges for devopstales Bucket.</p>

<pre><code>cat &lt;EOF&gt; bucket-policy.json
{
  &quot;Version&quot;: &quot;2012-10-17&quot;,
  &quot;Statement&quot;: [{
    &quot;Effect&quot;: &quot;Allow&quot;,
    &quot;Principal&quot;: {&quot;AWS&quot;: [&quot;arn:aws:iam:::user/devopstales2@devopstales.intra&quot;]},
    &quot;Action&quot;: [&quot;s3:ListBucket&quot;, &quot;s3:GetObject&quot;, &quot;s3:PutObject&quot;, &quot;s3:DeleteObject&quot;],
    &quot;Resource&quot;: [
      &quot;arn:aws:s3:::devopstales&quot;,
      &quot;arn:aws:s3:::devopstales/*&quot;
    ]
  }]
}
EOF

s3cmd setpolicy ./bucket-policy.json s3://devopstales
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Tillerless helm2 install]]></title>
            <link href="https://devopstales.github.io/home/k8s-tillerless-helm/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-copy-secret/?utm_source=atom_feed" rel="related" type="text/html" title="Copying Kubernetes Secrets Between Namespaces" />
                <link href="https://devopstales.github.io/home/k8s-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/k8s-nginx-ingress/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
                <link href="https://devopstales.github.io/home/k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install kubernetes with kubeadm" />
                <link href="https://devopstales.github.io/home/openshift-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Helm" />
            
                <id>https://devopstales.github.io/home/k8s-tillerless-helm/</id>
            
            
            <published>2019-07-23T00:00:00+00:00</published>
            <updated>2019-07-23T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>It looks like it is not so hard to have Tillerless Helm. So let me go to more details.</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
<li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-contour-ingress --></li>
<li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
<li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
<li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... --></li>
<li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
<li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
<li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<p>Since Helm v2, helm got a server part called The Tiller Server which is interacts with the helm client, and the Kubernetes API server. By default helm init installs a Tiller deployment to Kubernetes clusters and communicates via gRPC.</p>

<p><img src="/img/include/tiller1.png" alt="Example image" /></p>

<p>The community voted that Helm v3 should be Tillerless. If we can run tiller localli we can achieve the same goal.</p>

<p><img src="/img/include/tiller2.png" alt="Example image" /></p>

<p>There is a helm plugin for this same purpose.</p>

<pre><code>$ helm plugin install https://github.com/rimusz/helm-tiller
Installed plugin: tiller
</code></pre>

<h3 id="use-this-plugin-locally">Use this plugin locally</h3>

<pre><code>helm tiller start
</code></pre>

<p>It will start the tiller locally and kube-system namespace will be used to store helm releases but you can change the name of the namespace if you want:</p>

<pre><code>helm tiller start my-team-namespace

# stop tiller
helm tiller stop
</code></pre>

<h3 id="how-to-use-this-plugin-in-ci-cd-pipelines">How to use this plugin in CI/CD pipelines</h3>

<pre><code>helm tiller start-ci
export HELM_HOST=localhost:44134
</code></pre>

<p>Then your helm will know where to connect to Tiller and you do not need to make any changes in your CI pipelines.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes Ceph RBD for dynamic provisioning]]></title>
            <link href="https://devopstales.github.io/home/k8s-ceph/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/install-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ceph cluster" />
                <link href="https://devopstales.github.io/home/ceph-block-device/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph Block Device" />
                <link href="https://devopstales.github.io/home/ceph-cephfs/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph CephFS" />
                <link href="https://devopstales.github.io/kubernetes/k8s-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/k8s-nginx-ingress/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
            
                <id>https://devopstales.github.io/home/k8s-ceph/</id>
            
            
            <published>2019-07-18T00:00:00+00:00</published>
            <updated>2019-07-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how can you use CEPH RBD for persistent storagi on Kubernetes.</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
<li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-contour-ingress --></li>
<li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
<li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
<li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... --></li>
<li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
<li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
<li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<h3 id="environment">Environment</h3>

<pre><code># openshift cluster
192.168.1.41  kubernetes01 # master node
192.168.1.42  kubernetes02 # frontend node
192.168.1.43  kubernetes03 # worker node
192.168.1.44  kubernetes04 # worker node
192.168.1.45  kubernetes05 # worker node

# ceph cluster
192.168.1.31    ceph01
192.168.1.32    ceph02
192.168.1.33    ceph03
</code></pre>

<h3 id="prerequirement">Prerequirement</h3>

<p>RBD volume provisioner needs admin key from Ceph to provision storage. To get the admin key from Ceph cluster use this command:</p>

<pre><code>sudo ceph --cluster ceph auth get-key client.admin

nano ceph-admin-secret.yaml
apiVersion: v1
data:
  key: QVFBOFF2SlZheUJQRVJBQWgvS2cwT1laQUhPQno3akZwekxxdGc9PQ==
kind: Secret
metadata:
  name: ceph-admin-secret
  namespace: kube-system
type: kubernetes.io/rbd

</code></pre>

<p>I will also create a separate Ceph pool for</p>

<pre><code>sudo ceph --cluster ceph osd pool create k8s 1024 1024
sudo ceph --cluster ceph auth get-or-create client.k8s mon 'allow r' osd 'allow rwx pool=k8s'
sudo ceph --cluster ceph auth get-key client.k8s

nano ceph-secret-k8s.yaml
apiVersion: v1
data:
  key: QVFBOFF2SlZheUJQRVJBQWgvS2ctS2htOFNSZnRvclJPRk1jdXc9PQ==
kind: Secret
metadata:
  name: ceph-secret-k8s
  namespace: kube-system
type: kubernetes.io/rbd

</code></pre>

<pre><code># on all openshift node
wget http://download.proxmox.com/debian/proxmox-ve-release-5.x.gpg \
-O /etc/apt/trusted.gpg.d/proxmox-ve-release-5.x.gpg
chmod +r /etc/apt/trusted.gpg.d/proxmox-ve-release-5.x.gpg

echo deb http://download.proxmox.com/debian/ceph-luminous $(lsb_release -sc) main \
&gt; /etc/apt/sources.list.d/ceph.list

apt-get update
apt-get install ceph-common -y
</code></pre>

<pre><code>cat &lt;&lt;EOF &gt; rbd-provisioner.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: rbd-provisioner
rules:
  - apiGroups: [&quot;&quot;]
    resources: [&quot;persistentvolumes&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;persistentvolumeclaims&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]
  - apiGroups: [&quot;storage.k8s.io&quot;]
    resources: [&quot;storageclasses&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;events&quot;]
    verbs: [&quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;services&quot;]
#    resourceNames: [&quot;kube-dns&quot;]
    verbs: [&quot;list&quot;, &quot;get&quot;]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: rbd-provisioner
subjects:
  - kind: ServiceAccount
    name: rbd-provisioner
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: rbd-provisioner
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: rbd-provisioner
rules:
- apiGroups: [&quot;&quot;]
  resources: [&quot;secrets&quot;]
  verbs: [&quot;get&quot;]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: rbd-provisioner
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: rbd-provisioner
subjects:
- kind: ServiceAccount
  name: rbd-provisioner
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: rbd-provisioner
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: rbd-provisioner
spec:
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: rbd-provisioner
    spec:
      containers:
      - name: rbd-provisioner
        image: &quot;quay.io/external_storage/rbd-provisioner:v1.0.0-k8s1.10&quot;
        env:
        - name: PROVISIONER_NAME
          value: ceph.com/rbd
      serviceAccount: rbd-provisioner
EOF

kubectl create -n kube-system -f rbd-provisioner.yaml
kubectl get pods -l app=rbd-provisioner -n kube-system
</code></pre>

<p>Please check that <code>quay.io/external_storage/rbd-provisioner:latest</code> image has the same Ceph version as your Ceph cluster.</p>

<pre><code>docker pull quay.io/external_storage/rbd-provisioner:latest
docker history quay.io/external_storage/rbd-provisioner:latest | grep CEPH_VERSION

# pfroxmox ceph use luminous so I'will use v1.0.0-k8s1.10
docker history quay.io/external_storage/rbd-provisioner:v1.0.0-k8s1.10 | grep CEPH_VERSION
&lt;missing&gt;           13 months ago       /bin/sh -c #(nop)  ENV CEPH_VERSION=luminous    0B
</code></pre>

<pre><code># on one openshift master node
nano  k8s-storage.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: &quot;true&quot;
  name: k8s
parameters:
  adminId: admin
  adminSecretName: ceph-admin-secret
  adminSecretNamespace: kube-system
  imageFeatures: layering
  imageFormat: &quot;2&quot;
  monitors: 192.168.1.31.xip.io:6789, 192.168.1.32.xip.io:6789, 192.168.1.33.xip.io:6789
  pool: k8s
  userId: k8s
  userSecretName: ceph-secret-k8s
provisioner: ceph.com/rbd
reclaimPolicy: Delete
volumeBindingMode: Immediate
allowVolumeExpansion: true


kubectl apply -f ceph-admin-secret.yaml
kubectl apply -f ceph-secret-k8s.yaml
kubectl apply -f k8s-storage.yaml
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install clonedeploy pxeboot server]]></title>
            <link href="https://devopstales.github.io/home/clonedeploy/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/clonedeploy/?utm_source=atom_feed" rel="related" type="text/html" title="Install clonedeploy pxeboot server" />
                <link href="https://devopstales.github.io/home/k8s-nginx-ingress/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
                <link href="https://devopstales.github.io/home/k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install kubernetes with kubeadm" />
                <link href="https://devopstales.github.io/home/ansible-operator-overview/?utm_source=atom_feed" rel="related" type="text/html" title="Ansible Operator Overview" />
                <link href="https://devopstales.github.io/home/graylog3-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Analyzing PFsense logs in Graylog3" />
            
                <id>https://devopstales.github.io/home/clonedeploy/</id>
            
            
            <published>2019-07-17T00:00:00+00:00</published>
            <updated>2019-07-17T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I’ll show you how to create network booting (PXE) with clusterdeploy.</p>

<h3 id="install-web-application">Install Web Application</h3>

<pre><code>yum -y install yum-utils
yum -y install epel-release
rpm --import &quot;http://keyserver.ubuntu.com/pks/lookup?op=get&amp;search=0x3FA7E0328081BFF6A14DA29AA6A19B38D3D831EF&quot;
yum-config-manager --add-repo http://download.mono-project.com/repo/centos7/

yum -y install mono-devel apache2-mod_mono httpd udpcast lz4 mkisofs wget
</code></pre>

<pre><code>wget &quot;https://sourceforge.net/projects/clonedeploy/files/CloneDeploy 1.4.0/clonedeploy-1.4.0.tar.gz&quot;
tar xvzf clonedeploy-1.4.0.tar.gz
cd clonedeploy
cp clonedeploy.conf /etc/httpd/conf.d/
mkdir /var/www/html/clonedeploy
cp -r frontend /var/www/html/clonedeploy
cp -r api /var/www/html/clonedeploy
cp -r tftpboot /

ln -s ../../images /tftpboot/proxy/bios/images
ln -s ../../images /tftpboot/proxy/efi32/images
ln -s ../../images /tftpboot/proxy/efi64/images
ln -s ../../kernels /tftpboot/proxy/bios/kernels
ln -s ../../kernels /tftpboot/proxy/efi32/kernels
ln -s ../../kernels /tftpboot/proxy/efi64/kernels

mkdir -p /cd_dp/images
mkdir /cd_dp/resources
mkdir /var/www/.mono
mkdir /usr/share/httpd/.mono
mkdir /etc/mono/registry
chown -R apache:apache /tftpboot /cd_dp /var/www/html/clonedeploy /var/www/.mono /usr/share/httpd/.mono /etc/mono/registry
chmod 1777 /tmp

sysctl fs.inotify.max_user_instances=1024
echo fs.inotify.max_user_instances=1024 &gt;&gt; /etc/sysctl.conf
chkconfig httpd on
</code></pre>

<h3 id="install-database">Install Database</h3>

<pre><code>echo &quot;[mariadb]
name = MariaDB
baseurl = http://yum.mariadb.org/10.3/centos7-amd64
gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDB
gpgcheck=1&quot; &gt;&gt; /etc/yum.repos.d/MariaDB.repo

yum -y install mariadb-server
service mariadb start
chkconfig mariadb on

mysql

create database clonedeploy;
CREATE USER 'cduser'@'localhost' IDENTIFIED BY 'Password1';
GRANT ALL PRIVILEGES ON clonedeploy.* TO 'cduser'@'localhost';
quit

mysql clonedeploy &lt; cd.sql -v
</code></pre>

<p>Open <code>/var/www/html/clonedeploy/api/Web.config</code> with a text editor and change the following values:<br>
<code>xx_marker1_xx</code> to your cduser database password you created earlier<br>
On that same line change <code>Uid=root to Uid=cduser</code><br>
<code>xx_marker2_xx</code> to some random characters(alphanumeric only), probably should be a minimum of 8<br></p>

<pre><code>service httpd restart
</code></pre>

<h3 id="install-samba-server">Install Samba Server</h3>

<pre><code>yum -y install samba
groupadd cdsharewriters
useradd cd_share_ro
useradd cd_share_rw -G cdsharewriters
usermod -a -G cdsharewriters apache

smbpasswd -a cd_share_ro
smbpasswd -a cd_share_rw

echo &quot;[cd_share]
path = /cd_dp
valid users = @cdsharewriters, cd_share_ro
create mask = 02775
directory mask = 02775
guest ok = no
writable = yes
browsable = yes
read list = @cdsharewriters, cd_share_ro
write list = @cdsharewriters
force create mode = 02775
force directory mode = 02775
force group = +cdsharewriters&quot; &gt;&gt; /etc/samba/smb.conf

chown -R apache:cdsharewriters /cd_dp
chmod -R 2775 /cd_dp
service smb restart
chkconfig smb on
</code></pre>

<h3 id="install-tftp-server">Install TFTP Server</h3>

<pre><code>yum -y install tftp-server
sed -i 's/\/var\/lib\/tftpboot/\/tftpboot -m \/tftpboot\/remap/g' /usr/lib/systemd/system/tftp.service
systemctl daemon-reload
service tftp restart
chkconfig tftp on
</code></pre>

<h3 id="create-firewall-exceptions">Create Firewall Exceptions</h3>

<pre><code>firewall-cmd --permanent --add-service=http
firewall-cmd --permanent --add-service=https
firewall-cmd --permanent --add-service=samba
firewall-cmd --permanent --add-service=tftp
firewall-cmd --permanent --add-service=dhcp
firewall-cmd --permanent --add-service=proxy-dhcp
firewall-cmd --permanent --add-port=9000-10002/udp
service firewalld reload
</code></pre>

<h3 id="post-install-setup">Post Install Setup</h3>

<pre><code># Open the CloneDeploy Web Interface
http://server-ip/clonedeploy

# éogin with
clonedeploy / password

# Upon login you will be greeted with the Initial Setup Page
# Fill out the fields and click Finalize Setup
</code></pre>

<pre><code>yum -y install syslinux vsftpd
cp -v /usr/share/syslinux/pxelinux.0 /tftpboot/
cp -v /usr/share/syslinux/vesamenu.c32 /tftpboot/

mkdir /var/ftp/pub/rhel7
mkdir -p /tftpboot/networkboot/rhel7/

cd /opt
wget  http://files.clonedeploy.org/CloneDeploy-Services.dll
cp CloneDeploy-Services.dll /var/www/html/clonedeploy/api/bin/

wget http://ftp.bme.hu/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1810.iso
wget http://ftp.bme.hu/centos/7/isos/x86_64/sha256sum.txt

sha256sum CentOS-7-x86_64-Minimal-1810.iso
cat sha256sum.txt

mount -o loop /opt/CentOS-7-x86_64-Minimal-1810.iso  /mnt
cp -raf /mnt/* /var/ftp/pub/rhel7
cp /var/ftp/pub/rhel7/images/pxeboot/{initrd.img,vmlinuz} /tftpboot/networkboot/rhel7/

systemctl enable vsftpd.service &amp;&amp; systemctl start vsftpd.service

LABEL CentOS 7 X64
MENU LABEL CentOS 7 X64
kernel /networkboot/rhel7/vmlinuz
append initrd=/networkboot/rhel7/initrd.img inst.repo=ftp://192.168.100.100/pub/rhel7 devfs=nomount
</code></pre>

<hr />

<p><a href="http://clonedeploy.org/docs/create-and-deploy-your-first-image/0">http://clonedeploy.org/docs/create-and-deploy-your-first-image/0</a></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install pxeboot server]]></title>
            <link href="https://devopstales.github.io/home/pxe1/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/pxe1/</id>
            
            
            <published>2019-07-16T00:00:00+00:00</published>
            <updated>2019-07-16T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Have you ever needed to troubleshoot or diagnose a problematic computer and you forgot where the utility CD is? We’ll show you how to utilize network booting (PXE) to make that problem a thing of the past.</p>

<h3 id="dhcp">dhcp</h3>

<pre><code>yum install -y dhcp nano

# ha több interfész van
echo &quot;DHCPDARGS=enp0s8&quot; &gt;&gt; /etc/sysconfig/dhcpd

cat &gt; /etc/dhcp/dhcpd.conf &lt;&lt; EOF
#DHCP configuration for PXE boot server
ddns-update-style interim;
ignore client-updates;
authoritative;
allow booting;
allow bootp;
allow unknown-clients;

# A slightly different configuration for an internal subnet.
subnet 192.168.100.0
netmask 255.255.255.0
{
range 192.168.100.101 192.168.100.200;
option domain-name-servers 192.168.100.100;
option routers 192.168.100.100;
default-lease-time 600;
max-lease-time 7200;

# PXE SERVER IP
next-server 192.168.100.100; #  DHCP server ip
filename &quot;pxelinux.0&quot;;
}
EOF

systemctl start dhcpd.service
systemctl enable dhcpd.service
firewall-cmd --permanent --add-service={dhcp,proxy-dhcp}
firewall-cmd --reload
</code></pre>

<pre><code>cd /opt
wget http://ftp.bme.hu/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1810.iso
wget http://ftp.bme.hu/centos/7/isos/x86_64/sha256sum.txt

sha256sum CentOS-7-x86_64-Minimal-1810.iso
cat sha256sum.txt

mount -o loop /opt/CentOS-7-x86_64-Minimal-1810.iso  /mnt
</code></pre>

<h3 id="ftp">ftp</h3>

<pre><code>yum install -y tftp-server vsftpd syslinux syslinux-tftpboot

cp -v /usr/share/syslinux/pxelinux.0 /var/lib/tftpboot/
cp -v /usr/share/syslinux/menu.c32 /var/lib/tftpboot/
cp -v /usr/share/syslinux/mboot.c32 /var/lib/tftpboot/
cp -v /usr/share/syslinux/chain.c32 /var/lib/tftpboot/

mkdir /var/lib/tftpboot/pxelinux.cfg
mkdir -p /var/lib/tftpboot/networkboot/rhel7
mkdir /var/ftp/pub/rhel7

cp -raf /mnt/* /var/ftp/pub/rhel7
cp /var/ftp/pub/rhel7/images/pxeboot/{initrd.img,vmlinuz} /var/lib/tftpboot/networkboot/rhel7/
umount /mnt
</code></pre>

<pre><code>cat &gt; /var/lib/tftpboot/pxelinux.cfg/default &lt;&lt; EOF
default menu.c32
prompt 0
timeout 30
menu title mydomail.local PXE Menu

label 1
menu label ^1) Boot from local drive
localboot 0x00

label 2
menu label ^2) CentOS 7 X64
kernel /networkboot/rhel7/vmlinuz
append initrd=/networkboot/rhel7/initrd.img ks=ftp://192.168.100.100/pub/rhel7/ks.cfg inst.repo=ftp://192.168.100.100/pub/rhel7 devfs=nomount

label 3
menu label ^3) CentOS 7 X64
kernel /networkboot/rhel7/vmlinuz
append initrd=/networkboot/rhel7/initrd.img inst.repo=ftp://192.168.100.100/pub/rhel7 devfs=nomount

#label 4
#menu label ^4) Install CentOS 7 x64 with Local Repo using VNC
#kernel /networkboot/rhel7/vmlinuz
#append  initrd=/networkboot/rhel7/initrd.img inst.repo=ftp://192.168.100.100/pub/rhel7 devfs=nomount inst.vnc inst.vncpassword=Aa123456
EOF
</code></pre>

<pre><code>echo '# Use network installation
url --url=&quot;ftp://192.168.100.100/pub/rhel7/&quot;
install

firstboot --disable
selinux --disabled
firewall --disabled
services --enabled=NetworkManager,sshd,chronyd
eula --agreed
ignoredisk --only-use=sda
# Keyboard layouts
keyboard --vckeymap=hu --xlayouts='hu'
# System language
lang en_US.UTF-8

#Text mode installation
text

# Network information
network  --bootproto=dhcp --device=eth0 --hostname=test.mydomain --noipv6 --activate
# Root password
authconfig --enableshadow --enablemd5
# Gen hash: openssl passwd -1 &quot;Password1&quot;
rootpw --iscrypted $1$Skulk114$/QcbiPYHtUnB/rJaAehAH0
# System timezone
timezone Europe/Budapest --ntpservers=0.hu.pool.ntp.org,.hu.pool.ntp.org,2.hu.pool.ntp.org

# System bootloader configuration
bootloader --append=&quot;net.ifnames=0 biosdevname=0&quot; --location=mbr
zerombr

# Disk partitioning information
clearpart --all --drives=sda
part /boot --fstype=ext4 --size=512
part pv.01 --size=1 --grow

volgroup vg00 pv.01
logvol swap --name=lv_swap --vgname=vg00 --size=1024
logvol / --name=lv_01 --vgname=vg00 --size=1 --grow

reboot --eject

%packages
@core
chrony
openssh-clients
net-tools
%end' &gt; /var/ftp/pub/rhel7/ks.cfg
</code></pre>

<pre><code>systemctl start tftp.service
systemctl enable tftp.service
systemctl enable vsftpd.service
systemctl start vsftpd.service

firewall-cmd --permanent --add-service=tftp
firewall-cmd --permanent --add-service=ftp
firewall-cmd --reload
</code></pre>

<p>CLIENT NEEDS AT LEASET 2 GB RAM</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes nginx ingress with helm]]></title>
            <link href="https://devopstales.github.io/home/k8s-nginx-ingress/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install kubernetes with kubeadm" />
                <link href="https://devopstales.github.io/home/k8s-copy-secret/?utm_source=atom_feed" rel="related" type="text/html" title="Copying Kubernetes Secrets Between Namespaces" />
                <link href="https://devopstales.github.io/home/k8s-helm-rbac/?utm_source=atom_feed" rel="related" type="text/html" title="RBAC permissions for Helm" />
                <link href="https://devopstales.github.io/home/kubectl-multi-cluster-config/?utm_source=atom_feed" rel="related" type="text/html" title="Configure kubectl for multiple clusters" />
                <link href="https://devopstales.github.io/kubernetes/k8s-nginx-ingress/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
            
                <id>https://devopstales.github.io/home/k8s-nginx-ingress/</id>
            
            
            <published>2019-07-14T00:00:00+00:00</published>
            <updated>2019-07-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how can you use install IngressControllert on Kubernetes with helm.</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
<li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-contour-ingress --></li>
<li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
<li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
<li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... --></li>
<li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
<li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
<li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<h3 id="environment">Environment</h3>

<pre><code># openshift cluster
192.168.1.41  kubernetes01 # master node
192.168.1.42  kubernetes02 # frontend node
192.168.1.43  kubernetes03 # worker node
192.168.1.44  kubernetes04 # worker node
192.168.1.45  kubernetes05 # worker node
</code></pre>

<h3 id="helm-with-cluster-admin-permissions">Helm with cluster-admin permissions</h3>

<pre><code>at &lt;&lt;EOF&gt; helm-cluster-admin.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller-admin
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tiller-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: tiller-admin
  namespace: kube-system
EOF
</code></pre>

<h3 id="init-helm">Init Helm</h3>

<pre><code>kubectl create -f helm-cluster-admin.yaml

helm init --service-account helm
kubectl get po --all-namespaces | grep tiller
</code></pre>

<h3 id="tag-node-for-ingress">Tag node for ingress</h3>

<pre><code>kubectl get nodes --show-labels
kubectl label nodes kubernetes02 node-role.kubernetes.io/frontend= --overwrite=true

helm install stable/nginx-ingress \
    --name nginx-ingress \
    --namespace=nginx-ingress \
    --set rbac.create=true \
    --set controller.kind=DaemonSet \
    --set controller.hostNetwork=true \
    --set controller.daemonset.useHostPort=true \
    --set controller.nodeSelector.&quot;node-role\.kubernetes\.io/frontend&quot;= \
    --set controller.stats.enabled=true \
    --set controller.metrics.enabled=true

kubectl --namespace nginx-ingress get services -o wide -w nginx-ingress-controller
kubectl create secret tls default-ingress-tls --key /path/to/private.pem --cert /path/to/cert.pem --namespace nginx-ingress
</code></pre>

<pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta1/aio/deploy/recommended.yaml

kubectl create secret tls default-ingress-tls --key /path/to/private.pem --cert /path/to/cert.pem --namespace kubernetes-dashboard

cat &lt;&lt;EOF&gt; dashboard_ingress.yml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kubernetes-dashboard-ingress
  namespace: kubernetes-dashboard
  annotations:
    kubernetes.io/tls-acme: 'true'
    ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-passthrough: &quot;true&quot;
    nginx.ingress.kubernetes.io/secure-backends: &quot;true&quot;
    ingress.kubernetes.io/ssl-redirect: &quot;true&quot;
    ingress.kubernetes.io/force-ssl-redirect: &quot;true&quot;
spec:
  tls:
  - hosts:
    - dashboard.devopstales.intra
    secretName: default-ingress-tls
  rules:
  - host: dashboard.devopstales.intra
    http:
     paths:
     - backend:
         serviceName: kubernetes-dashboard
         servicePort: 443
EOF

kubectl apply -f dashboard_ingress.yml
</code></pre>

<pre><code>kubectl create serviceaccount dashboard-admin-sa
kubectl create clusterrolebinding dashboard-admin-sa --clusterrole=cluster-admin --serviceaccount=default:dashboard-admin-sa

kubectl get secrets
NAME                  TYPE                                  DATA   AGE
dashboard-admin-sa-token-XXXXX   kubernetes.io/service-account-token   3      22h

kubectl describe secret dashboard-admin-sa-token-XXXXX
Name:         dashboard-admin-sa-token-bq9cr
...
token:      XXXXXXXXXXXXXXXXXXXXXXXXXX

# use this token to login
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install kubernetes with kubeadm]]></title>
            <link href="https://devopstales.github.io/home/k8s-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/k8s-copy-secret/?utm_source=atom_feed" rel="related" type="text/html" title="Copying Kubernetes Secrets Between Namespaces" />
                <link href="https://devopstales.github.io/home/k8s-helm-rbac/?utm_source=atom_feed" rel="related" type="text/html" title="RBAC permissions for Helm" />
                <link href="https://devopstales.github.io/home/kubectl-multi-cluster-config/?utm_source=atom_feed" rel="related" type="text/html" title="Configure kubectl for multiple clusters" />
                <link href="https://devopstales.github.io/kubernetes/k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install kubernetes with kubeadm" />
                <link href="https://devopstales.github.io/home/ansible-operator-overview/?utm_source=atom_feed" rel="related" type="text/html" title="Ansible Operator Overview" />
            
                <id>https://devopstales.github.io/home/k8s-install/</id>
            
            
            <published>2019-07-12T00:00:00+00:00</published>
            <updated>2019-07-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Kubeadm is a tool that helps you bootstrap a simple Kubernetes cluster and simplifies the deployment process.</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
<li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-contour-ingress --></li>
<li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
<li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
<li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... --></li>
<li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
<li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
<li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<pre><code>192.168.1.41  kubernetes01 # master node
192.168.1.42  kubernetes02 # frontend node
192.168.1.43  kubernetes03 # worker node
192.168.1.44  kubernetes04 # worker node
192.168.1.45  kubernetes05 # worker node

# hardware requirement
4 CPU
16G RAM
</code></pre>

<h3 id="install-docker">Install Docker</h3>

<pre><code>apt-get update
apt-get install apt-transport-https ca-certificates curl gnupg2 software-properties-common
curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add -
add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/debian $(lsb_release -cs) stable&quot;
apt-get update
apt-get install docker-ce docker-ce-cli containerd.io
systemct start docker
systemct enable docker
</code></pre>

<h3 id="configuuration">Configuuration</h3>

<pre><code>cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
EOF

echo '1' &gt; /proc/sys/net/ipv4/ip_forward
echo '1' &gt; /proc/sys/net/bridge/bridge-nf-call-iptables
</code></pre>

<h3 id="disable-swap">Disable swap</h3>

<pre><code>free -h
swapoff -a
swapoff -a
sed -i.bak -r 's/(.+ swap .+)/#\1/' /etc/fstab
free -h
</code></pre>

<h3 id="install-kubeadm">Install kubeadm</h3>

<pre><code>apt-get install ebtables ethtool apt-transport-https

curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF

apt-get update &amp;&amp; apt-get install -y kubelet kubeadm kubectl
apt-mark hold kubelet kubeadm kubect
systemctl enable kubelet &amp;&amp; systemctl start kubelet
kubeadm config images pul
</code></pre>

<h3 id="init-master">Init master</h3>

<pre><code>kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=192.168.1.41


mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config


kubectl apply -f https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml
</code></pre>

<h3 id="join-workers-to-cluster">Join workers to cluster</h3>

<pre><code>kubeadm join 192.168.1.41:6443 --token XXXXXXXX \
    --discovery-token-ca-cert-hash sha256:XXXXXXXX
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Ansible Operator Overview]]></title>
            <link href="https://devopstales.github.io/home/ansible-operator-overview/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
                <link href="https://devopstales.github.io/kubernetes/ansible-operator-overview/?utm_source=atom_feed" rel="related" type="text/html" title="Ansible Operator Overview" />
                <link href="https://devopstales.github.io/home/openshift-add-node/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Add Nodes to a Cluster" />
                <link href="https://devopstales.github.io/home/openshift-letsencrypt/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Letsencrypt certificates" />
                <link href="https://devopstales.github.io/home/openshift-hostalreadyclaimed/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Error: HostAlreadyClaimed" />
            
                <id>https://devopstales.github.io/home/ansible-operator-overview/</id>
            
            
            <published>2019-07-10T00:00:00+00:00</published>
            <updated>2019-07-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Operators make it easy to manage complex stateful applications on top of Kubernetes. In this pos I will show you how to read an ansible based Openshift operator.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<h3 id="why-an-operator">Why an Operator?</h3>

<p>Operators make it easy to manage complex stateful applications on top of Kubernetes. However writing an operator today can be difficult because of challenges such as using low level APIs, writing boilerplate, and a lack of modularity which leads to duplication.</p>

<h3 id="what-is-an-ansible-operator">What is an Ansible Operator?</h3>

<p>Ansible Operator is one of the available types of Operators that Operator SDK is able to generate. Operator SDK can create an operator using with Golang, Helm, or Ansible.
It is a collection of building blocks from Operator SDK that enables Ansible to handle the reconciliation logic for an Operator.</p>

<h3 id="how-ansible-operator-works">How Ansible Operator works?</h3>

<p>We want to trigger this Ansible logic when a Custom Resource changes. The Ansible Operator uses a Watches file, written in YAML, which holds the mapping between Custom Resources and Ansible Roles/Playbooks. The Operator expects this mapping file in a predefined location: <code>/opt/ansible/watches.yaml</code></p>

<p>Each mapping within the Watches file has mandatory fields:</p>

<ul>
<li>group: Group of the Custom Resource that you will be watching.</li>
<li>version: Version of the Custom Resource that you will be watching.</li>
<li>kind: Kind of the Custom Resource that you will be watching.</li>
<li>role (default): Path to the Role that should be run by the Operator for a particular Group-Version-Kind (GVK). This field is mutually exclusive with the &ldquo;playbook&rdquo; field.</li>
<li>playbook (optional): Path to the Playbook that should be run by the Operator.</li>
</ul>

<h3 id="initialize-new-operator-template">Initialize new operator template</h3>

<pre><code># istall sdk client
brew install operator-sdk

# generate base temlate
operator-sdk new memcached-operator --type=ansible --api-version=cache.example.com/v1alpha1 --kind=Memcached --skip-git-init
cd memcached-operator
</code></pre>

<p>The sdk cli generated the base structure for all the componets. The main parts are the <code>watches.yaml</code> the Dockerfile in <code>build/Dockerfile</code> and the ansible role in <code>roles/</code><br> folder. Fore this tutorial I will use this role <a href="https://github.com/dymurray/memcached-operator-role">https://github.com/dymurray/memcached-operator-role</a>.<br>
In the role we must use k8s ansible module to deploy kubernetes compnets to the cluster.<br></p>

<pre><code>nano roles/memcached/tasks/main.yml
---
- name: start memcached
  k8s:
    definition:
      kind: Deployment
      apiVersion: apps/v1
      metadata:
        name: '{{ meta.name }}-memcached'
        namespace: '{{ meta.namespace }}'
      spec:
        replicas: &quot;{{size}}&quot;
        selector:
          matchLabels:
            app: memcached
        template:
          metadata:
            labels:
              app: memcached
          spec:
            containers:
            - name: memcached
              command:
              - memcached
              - -m=64
              - -o
              - modern
              - -v
              image: &quot;memcached:1.4.36-alpine&quot;
              ports:
                - containerPort: 11211
</code></pre>

<p>You can use variables in ansible Jinja temlate from the CR spec or from kubernetes enviroment like namespace: <code>{{ meta.namespace }}</code>.<br>
Add default value to the <code>{{size}}</code> variable:</p>

<pre><code>nano roles/memcached/defaults/main.yml
---
size: 1
</code></pre>

<h4 id="variable-sharing-example">Variable sharing Example</h4>

<pre><code>apiVersion: &quot;foo.example.com/v1alpha1&quot;
kind: &quot;Foo&quot;
metadata:
  name: &quot;example&quot;
annotations:
  ansible.operator-sdk/reconcile-period: &quot;30s&quot;
  name: &quot;example&quot;
spec:
  message: &quot;Hello world 2&quot;
  newParameter: &quot;newParam&quot;
# associates GVK with Role
role: /opt/ansible/roles/Foo
</code></pre>

<pre><code>- debug:
    msg: &quot;message value from CR spec: {{ message }}&quot;

- debug:
    msg: &quot;newParameter value from CR spec: {{ new_parameter }}&quot;

- debug:
    msg: &quot;name: {{ meta.name }}, namespace: {{ meta.namespace }}&quot;
</code></pre>

<p>The Openshidt SDK created a simple Dockerfile in <code>build/Dockerfile</code> to run the newly created ansible role. We nead to build a docker image from this Dockerfile and use this image in our memcached-operator deployment.</p>

<pre><code># buid image
operator-sdk build memcached-operator:v0.0.1

# Edit deploy/operator.yaml to use the newly created memcached-operator:v0.0.1 docker image
sed -i 's|{{ REPLACE_IMAGE }}|memcached-operator:v0.0.1|g' deploy/operator.yaml

# If we did not want to download the image (besause we build it on the worker or it is representid on all of my workes) we can disable image pulling.
sed -i &quot;s|{{ pull_policy\|default('Always') }}|Never|g&quot; deploy/operator.yaml
</code></pre>

<h3 id="creating-the-operator-from-deploy-manifests">Creating the Operator from deploy manifests</h3>

<pre><code>oc create -f deploy/crds/cache_v1alpha1_memcached_crd.yaml

oc new-project tutorial
oc create -f deploy/service_account.yaml
oc create -f deploy/role.yaml
oc create -f deploy/role_binding.yaml
oc create -f deploy/operator.yaml

oc get deployment
</code></pre>

<p>Now that we have deployed our Operator, let&rsquo;s create a CR and deploy an instance of memcached.
There is a sample CR in the scaffolding created as part of the Operator SDK. Inspect <code>deploy/crds/cache_v1alpha1_memcached_cr.yaml</code>, and then use it to create a Memcached custom resource.</p>

<pre><code>oc create -f deploy/crds/cache_v1alpha1_memcached_cr.yaml



$ oc get deployment
NAME                 DESIRED CURRENT UP-TO-DATE AVAILABLE AGE
memcached-operator   1       1       1          1         2m
example-memcached    3       3       3          3         1m

#Check the pods to confirm 3 replicas were created:

$ oc get pods
NAME                                READY STATUS   RESTARTS AGE
example-memcached-6cc844747c-2hbln  1/1   Running  0        1m
example-memcached-6cc844747c-54q26  1/1   Running  0        1m
example-memcached-6cc844747c-7jfhc  1/1   Running  0        1m
memcached-operator-68b5b558c5-dxjwh 1/1   Running  0        2m
</code></pre>

<h3 id="removing-memcached-from-the-cluster">Removing Memcached from the cluster</h3>

<pre><code># First, delete the 'memcached' CR, which will remove the 4 Memcached pods and the associated deployment.
oc delete -f deploy/crds/cache_v1alpha1_memcached_cr.yaml

# Then, delete the memcached-operator deployment.
oc delete -f deploy/operator.yaml
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Analyzing PFsense logs in Graylog3]]></title>
            <link href="https://devopstales.github.io/home/graylog3-pfsense/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/graylog3-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Graylog3" />
                <link href="https://devopstales.github.io/linux/graylog3-pfsense/?utm_source=atom_feed" rel="related" type="text/html" title="Analyzing PFsense logs in Graylog3" />
                <link href="https://devopstales.github.io/linux/graylog3-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Graylog3" />
                <link href="https://devopstales.github.io/home/grafana-loki/?utm_source=atom_feed" rel="related" type="text/html" title="Grafana Loki" />
                <link href="https://devopstales.github.io/home/grafana-sso/?utm_source=atom_feed" rel="related" type="text/html" title="SSO login to Grafana" />
            
                <id>https://devopstales.github.io/home/graylog3-pfsense/</id>
            
            
            <published>2019-07-04T00:00:00+00:00</published>
            <updated>2019-07-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>We will parse the log records generated by the PfSense Firewall. We already have our graylog server running and we will start preparing the terrain to capture those logs records.</p>

<p>Many thanks to opc40772 developed the original contantpack for pfsense log agregation what I updated for the new Graylog3 and Elasticsearch 6.</p>

<h3 id="celebro-localinstall">Celebro localinstall</h3>

<pre><code># celebro van to use port 9000 so change graylog3 bindport
nano /etc/graylog/server/server.conf
http_bind_address = 127.0.0.1:9400
nano /etc/nginx/conf.d/graylog.conf

systemctl restart graylog-server.service
systemctl restart nginx

wget https://github.com/lmenezes/cerebro/releases/download/v0.9.3/cerebro-0.9.3-1.noarch.rpm
yum localinstall cerebro-0.9.3-1.noarch.rpm

sudo sed -i 's|# JAVA_OPTS=&quot;-Dpidfile.path=/var/run/cerebro/play.pid&quot;|JAVA_OPTS=&quot;-Dpidfile.path=/var/run/cerebro/play.pid -Dhttp.address=0.0.0.0&quot;|' /etc/default/cerebro

chown cerebro:cerebro -R /usr/share/cerebro

systemctl start cerebro
</code></pre>

<h3 id="create-indices">Create indices</h3>

<p>We now create the Pfsense indice on Graylog at <code>System / Indexes</code> <br>
<img src="/img/include/graylog_pfsense1.png" alt="image" /> <br></p>

<h3 id="import-index-template-for-elasticsearch-6-x">Import index template for elasticsearch 6.x</h3>

<pre><code>systemctl stop graylog-server.service

git clone https://github.com/devopstales/pfsense-graylog.git
cd pfsense-graylog/service-names-port-numbers/
cp service-names-port-numbers.csv /etc/graylog/server/
</code></pre>

<p>Go to <code>celebro &gt; more &gt; index templates</code>
Create new with name: <code>pfsense-custom</code> and copy the template from file <code>pfsense_custom_template_es6.json</code></p>

<p>In Cerebro we stand on top of the pfsense index and unfold the options and select delete index.</p>

<h3 id="geoip-database">Geoip database</h3>

<pre><code>wget -t0 -c http://geolite.maxmind.com/download/geoip/database/GeoLite2-City.tar.gz
tar -xvf GeoLite2-City.tar.gz
cp GeoLite2-City_*/GeoLite2-City.mmdb /etc/graylog/server

systemctl start graylog-server.service
</code></pre>

<p>Enable geoip database at <code>System &gt; Configurations &gt; Plugins &gt; Geo-Location Processor &gt; update</code>
Chane the order of the <code>Message Processors Configuration</code></p>

<ul>
<li>AWS Instance Name Lookup</li>
<li>Message Filter Chain</li>
<li>Pipeline Processor</li>
<li>GeoIP Resolver</li>
</ul>

<p>Enable geoip database</p>

<h3 id="import-contantpack">Import contantpack</h3>

<p>import
chaneg date timezone in Pipeline rule
Go tu Stream menu and edit stream <br>
<img src="/img/include/graylog_pfsense2.png" alt="image" /> <br></p>

<p><code>System &gt; Pipelines</code>
Manage rules and then Edit rule (Change the timezone)</p>

<pre><code>rule &quot;timestamp_pfsense_for_grafana&quot;
 when
 has_field(&quot;timestamp&quot;)
then
// the following date format assumes there's no time zone in the string
 let source_timestamp = parse_date(substring(to_string(now(&quot;Europe/Budapest&quot;)),0,23), &quot;yyyy-MM-dd'T'HH:mm:ss.SSS&quot;);
 let dest_timestamp = format_date(source_timestamp,&quot;yyyy-MM-dd HH:mm:ss&quot;);
 set_field(&quot;real_timestamp&quot;, dest_timestamp);
end
</code></pre>

<h3 id="confifure-pfsense">Confifure pfsense</h3>

<p><code>Status &gt; System Logs &gt; Settings</code> <br>
<img src="/img/include/graylog_pfsense3.png" alt="image" /> <br></p>

<h3 id="confifure-opnsense">Confifure Opnsense</h3>

<p>Access the Opnsense GUI
<code>System</code> menu, access the <code>Settings</code> sub-menu and select the  <code>Logging / Targets</code> option.
<img src="/img/include/graylog_pfsense13.png" alt="image" /> <br></p>

<p>Add a new logging target and perform the following configuration: <br></p>

<p><img src="/img/include/graylog_pfsense14.png" alt="image" /> <br></p>

<h3 id="install-grafana-dashboard">Install grafana Dashboard</h3>

<pre><code># install nececery plugins
grafana-cli plugins install grafana-piechart-panel
grafana-cli plugins install grafana-worldmap-panel
grafana-cli plugins install savantly-heatmap-panel
systemctl restart grafana-server
</code></pre>

<p>Create new datasource: <br><br></p>

<p><img src="/img/include/graylog_pfsense4.png" alt="image" /> <br></p>

<p>Import dashboadr from store: <br>
id: 5420</p>

<h2 id="image-img-include-graylog-pfsense12-png-br"><img src="/img/include/graylog_pfsense12.png" alt="image" /> <br></h2>

<h5 id="contantpack">Contantpack:</h5>

<p><a href="https://github.com/devopstales/pfsense-graylog">https://github.com/devopstales/pfsense-graylog</a></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[SSO login to Grafana]]></title>
            <link href="https://devopstales.github.io/home/grafana-sso/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-sso-2/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift SSO with Gitlab" />
                <link href="https://devopstales.github.io/home/nextcloud-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Nextcloud SSO" />
                <link href="https://devopstales.github.io/home/nexus-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Sonatype Nexus SSO" />
                <link href="https://devopstales.github.io/home/hashicorp-sso/?utm_source=atom_feed" rel="related" type="text/html" title="SSO for hashicorp vault" />
                <link href="https://devopstales.github.io/home/rundeck-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Rundeck SSO" />
            
                <id>https://devopstales.github.io/home/grafana-sso/</id>
            
            
            <published>2019-06-28T00:00:00+00:00</published>
            <updated>2019-06-28T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Configurate Gitab to use Keycloak as SSO Identity Proider.</p>

<h3 id="configurate-keycloak">Configurate Keycloak</h3>

<p>Login to Keycloak and create client for Grafana:
<img src="/img/include/gitlab-keycloak1.png" alt="Example image" /></p>

<h3 id="configurate-gitlab">Configurate Gitlab</h3>

<pre><code>nano /etc/grafana/grafana.ini
#################################### Generic OAuth ##########################
[auth.generic_oauth]
enabled = true
name = SSO
allow_sign_up = true
client_id = gitlab
client_secret = 47fd3013-4333-4825-bbfa-b7688548d9cf
# for old version
# scopes = user:email,read:org
scopes = openid email profile
auth_url = https://sso.devopstales.intra/auth/realms/gitlab/protocol/openid-connect/auth
token_url = https://sso.devopstales.intra/auth/realms/gitlab/protocol/openid-connect/token
api_url = https://sso.devopstales.intra/auth/realms/gitlab/protocol/openid-connect/userinfo
;team_ids =
;allowed_organizations =

</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift: Add Nodes to a Cluster]]></title>
            <link href="https://devopstales.github.io/home/openshift-add-node/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-letsencrypt/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Letsencrypt certificates" />
                <link href="https://devopstales.github.io/home/openshift-hostalreadyclaimed/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Error: HostAlreadyClaimed" />
                <link href="https://devopstales.github.io/home/openshift-gitlabrunner/?utm_source=atom_feed" rel="related" type="text/html" title="Install Gitlab runner on Openshift" />
                <link href="https://devopstales.github.io/home/openshift-extregistry/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: External registry" />
                <link href="https://devopstales.github.io/home/openshift-cert/?utm_source=atom_feed" rel="related" type="text/html" title="Change Certificates in Openshift" />
            
                <id>https://devopstales.github.io/home/openshift-add-node/</id>
            
            
            <published>2019-06-27T00:00:00+00:00</published>
            <updated>2019-06-27T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Add Nodes to an existing Cluster.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<p>In the last post I used the basic htpasswd authentication method for the installatipn.<br>
But I can use Ansible-openshift to configure an LDAP backed at the install for the authentication.</p>

<h3 id="environment">Environment</h3>

<pre><code>192.168.1.40    deployer
192.168.1.41    openshift01 # master node
192.168.1.42    openshift02 # infra node
192.168.1.43    openshift03 # worker node
192.168.1.44    openshift04 # new-worker node
192.168.1.45    openshift00 # new-master node
</code></pre>

<p>###</p>

<pre><code>useradd origin
passwd origin
echo -e 'Defaults:origin !requiretty\norigin ALL = (root) NOPASSWD:ALL' | tee /etc/sudoers.d/openshift
chmod 440 /etc/sudoers.d/openshift

# if Firewalld is running, allow SSH

firewall-cmd --add-service=ssh --permanent
firewall-cmd --reload

yum -y install centos-release-openshift-origin36 docker
vgcreate vg_origin01 /dev/sdb1

Volume group &quot;vg_origin01&quot; successfully created
echo VG=vg_origin01 &gt;&gt; /etc/sysconfig/docker-storage-setup
systemctl start docker
systemctl enable docker
</code></pre>

<h3 id="configurate-installer">Configurate Installer</h3>

<pre><code>nano /etc/ansible/hosts
# add into OSEv3 section

[OSEv3:children]
masters
nodes
new_nodes
new_masters
new_etcd

[new_nodes]
openshift04.devopstales.intra openshift_node_group_name='node-config-compute'
openshift00.devopstales.intra openshift_node_group_name='node-config-master'

[new_masters]
openshift00.devopstales.intra openshift_node_group_name='node-config-master'

[new_etcd]
openshift00.devopstales.intra containerized=true
</code></pre>

<h3 id="run-the-installer">Run the Installer</h3>

<p>Run Ansible Playbook for scaleout the Cluster.</p>

<pre><code># deployer
cd /usr/share/ansible/openshift-ansible/

ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-node/scaleup.yml
ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-master/scaleup.yml
ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/openshift-etcd/scaleup.yml
</code></pre>

<h3 id="configurate-installer-1">Configurate Installer</h3>

<p>After finishing to add new Nodes, Open [/etc/ansible/hosts] again and move new definitions to existing [nodes] section like follows.</p>

<pre><code>nano /etc/ansible/hosts
# add into OSEv3 section

[OSEv3:children]
masters
nodes
new_nodes
new_masters

[nodes]
openshift00.devopstales.intra openshift_node_group_name='node-config-master'
openshift01.devopstales.intra openshift_node_group_name='node-config-master'
openshift02.devopstales.intra openshift_node_group_name='node-config-infra'
openshift03.devopstales.intra openshift_node_group_name='node-config-compute'
openshift04.devopstales.intra openshift_node_group_name='node-config-compute

[new_nodes]

[masters]
openshift00.devopstales.intra openshift_node_group_name='node-config-master'
openshift01.devopstales.intra openshift_node_group_name='node-config-master'

[new_masters]

[etcd]
openshift01.devopstales.intra containerized=true
openshift00.devopstales.intra containerized=true

[new_etcd]
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Graylog3]]></title>
            <link href="https://devopstales.github.io/home/graylog3-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/graylog3-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Graylog3" />
                <link href="https://devopstales.github.io/home/grafana-loki/?utm_source=atom_feed" rel="related" type="text/html" title="Grafana Loki" />
                <link href="https://devopstales.github.io/home/gitlab-install/?utm_source=atom_feed" rel="related" type="text/html" title="Gitlab Install" />
                <link href="https://devopstales.github.io/home/openshift-sso-2/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift SSO with Gitlab" />
                <link href="https://devopstales.github.io/home/install-prometheus-for-gitlab/?utm_source=atom_feed" rel="related" type="text/html" title="Install Prometheus for Gitlab" />
            
                <id>https://devopstales.github.io/home/graylog3-install/</id>
            
            
            <published>2019-06-24T00:00:00+00:00</published>
            <updated>2019-06-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Graylog is defined in terms of log management platform for collecting, indexing, and analyzing both structured and unstructured data from almost any source.</p>

<h3 id="install-requirement">Install requirement</h3>

<pre><code>yum install epel-release -y
yum install java-1.8.0-openjdk-headless.x86_64 pwgen nano -y
java -version
</code></pre>

<h3 id="set-timezone">Set Timezone</h3>

<pre><code>rm -f /etc/localtime
ln -s /usr/share/zoneinfo/CET /etc/localtime

yum install -y ntp
ntpd
</code></pre>

<h3 id="elasticsearch">Elasticsearch</h3>

<pre><code>rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch

echo '[elasticsearch-6.x]
name=Elasticsearch repository for 6.x packages
baseurl=https://artifacts.elastic.co/packages/6.x/yum
gpgcheck=1
gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
enabled=1
autorefresh=1
type=rpm-md
' | tee /etc/yum.repos.d/elasticsearch.repo

sudo yum -y install elasticsearch

sudo -E sed -i -e 's/#cluster.name: my-application/cluster.name: graylog/' \
/etc/elasticsearch/elasticsearch.yml

systemctl restart elasticsearch
systemctl enable elasticsearch

curl -XGET 'http://localhost:9200/_cluster/health?pretty=true'
</code></pre>

<h3 id="mongodb">Mongodb</h3>

<pre><code>echo '[mongodb-org-4.0]
name=MongoDB Repository
baseurl=https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/4.0/x86_64/
gpgcheck=1
enabled=1
gpgkey=https://www.mongodb.org/static/pgp/server-4.0.asc' | tee /etc/yum.repos.d/mongodb-org.repo

yum -y install mongodb-org

systemctl restart mongod
systemctl enable  mongod
</code></pre>

<h3 id="graylog3">Graylog3</h3>

<pre><code>rpm -Uvh https://packages.graylog2.org/repo/packages/graylog-3.3-repository_latest.rpm
yum -y install graylog-server

SECRET=$(pwgen -s 96 1)
sudo -E sed -i -e 's/password_secret =.*/password_secret = '$SECRET'/' /etc/graylog/server/server.conf
PASSWORD=$(echo -n Password1 | sha256sum | awk '{print $1}')
sudo -E sed -i -e 's/root_password_sha2 =.*/root_password_sha2 = '$PASSWORD'/' /etc/graylog/server/server.conf

# Set to your timezone
sudo -E sed -i -e 's/#root_timezone = UTC/root_timezone = CET/' /etc/graylog/server/server.conf

# Set to your email
sudo -E sed -i -e 's/#root_email = &quot;&quot;/root_email = &quot;admin@devopstales.intra&quot;/' /etc/graylog/server/server.conf
sudo -E sed -i -e 's/elasticsearch_shards = 4/elasticsearch_shards = 1/' /etc/graylog/server/server.conf
sudo -E sed -i -e 's/#http_bind_address = 127.0.0.1:9000/http_bind_address = 127.0.0.1:9400/' /etc/graylog/server/server.conf

# got ta https://dev.maxmind.com/geoip/geoip2/geolite2/ and download
# or use an old one
wget -t0 -c https://github.com/DocSpring/geolite2-city-mirror/raw/master/GeoLite2-City.tar.gz
tar -xvf GeoLite2-City.tar.gz
cp GeoLite2-City_*/GeoLite2-City.mmdb /etc/graylog/server

systemctl daemon-reload
systemctl restart graylog-server
systemctl enable graylog-server

tailf /var/log/graylog-server/server.log

If everything goes well, you should see below message in the logfile:
2019-06-20T13:37:04.059Z INFO  [ServerBootstrap] Graylog server up and running.
</code></pre>

<h3 id="install-grafana">Install Grafana</h3>

<pre><code>echo '[grafana]
name=grafana
baseurl=https://packages.grafana.com/oss/rpm
repo_gpgcheck=1
enabled=1
gpgcheck=1
gpgkey=https://packages.grafana.com/gpg.key
sslverify=1
sslcacert=/etc/pki/tls/certs/ca-bundle.crt
' &gt; /etc/yum.repos.d/grafana.repo


sudo yum install -y grafana
grafana-cli plugins install grafana-piechart-panel

sudo -E sed -i -e 's/;http_addr =/http_addr = 127.0.0.1/' /etc/grafana/grafana.ini

systemctl start grafana-server
systemctl status grafana-server
systemctl enable grafana-server
</code></pre>

<h3 id="nginx-proxy">Nginx Proxy</h3>

<pre><code>yum install nginx -y

echo 'server {
    listen 80;
    server_name graylog.devopstales.intra;

    location / {
      proxy_set_header Host $http_host;
      proxy_set_header X-Forwarded-Host $host;
      proxy_set_header X-Forwarded-Server $host;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      proxy_set_header X-Graylog-Server-URL http://$server_name/;
      proxy_pass       http://127.0.0.1:9400;
    }
}' &gt; /etc/nginx/conf.d/graylog.conf

echo 'server {
    listen 80;
    server_name grafana.devopstales.intra;

    location / {
      proxy_set_header Host $http_host;
      proxy_set_header X-Forwarded-Host $host;
      proxy_set_header X-Forwarded-Server $host;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      proxy_pass       http://127.0.0.1:3000;
    }
}' &gt; /etc/nginx/conf.d/grafana.conf

nginx -t
systemctl restart nginx
systemctl enable nginx
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Grafana Loki]]></title>
            <link href="https://devopstales.github.io/home/grafana-loki/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/monitoring/grafana-loki/?utm_source=atom_feed" rel="related" type="text/html" title="Grafana Loki" />
                <link href="https://devopstales.github.io/home/naemon-influxdb/?utm_source=atom_feed" rel="related" type="text/html" title="Install Nemon with Influxdb storage" />
                <link href="https://devopstales.github.io/home/openshift-sso-2/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift SSO with Gitlab" />
                <link href="https://devopstales.github.io/home/install-prometheus-for-gitlab/?utm_source=atom_feed" rel="related" type="text/html" title="Install Prometheus for Gitlab" />
                <link href="https://devopstales.github.io/home/install-mattermost-for-gitlab/?utm_source=atom_feed" rel="related" type="text/html" title="Install mattermost for Gitlab" />
            
                <id>https://devopstales.github.io/home/grafana-loki/</id>
            
            
            <published>2019-06-22T00:00:00+00:00</published>
            <updated>2019-06-22T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Loki is a Prometheus-inspired logging service for cloud native infrastructure.
It’s similar to well-known ELK stack but more simple use and is intended to be used mostly Kubernetes.</p>

<h3 id="loki-components">Loki components</h3>

<p>Loki-stack consists of three main components:
- promtail – agent to collect logs on a host and push them to a Loki instance
- loki – TSDB (Time-series database) logs aggregation and processing server
- Grafana – for querying and displaying logs</p>

<h3 id="deployment">Deployment</h3>

<pre><code>yum install nginx -y
systemct start nginx

mkdir /opt/loki
cd /opt/loki/
</code></pre>

<pre><code>nano loki-promtail-conf.yml
server:

  http_listen_port: 9080
  grpc_listen_port: 0

positions:

  filename: /tmp/positions.yaml

client:

  url: http://loki:3100/api/prom/push

scrape_configs:

  - job_name: system
    entry_parser: raw
    static_configs:
    - targets:
        - localhost
      labels:
        job: varlogs
        __path__: /var/log/*log

  - job_name: nginx
    entry_parser: raw
    static_configs:
    - targets:
        - localhost
      labels:
        job: nginx
        __path__: /var/log/nginx/*log
</code></pre>

<pre><code>nano docker-compose.yml
version: &quot;3&quot;

networks:
  loki:

services:
  loki:
    image: grafana/loki:master
    ports:
      - &quot;3100:3100&quot;
    command: -config.file=/etc/loki/local-config.yaml
    networks:
      - loki

  promtail:
    image: grafana/promtail:master
    volumes:
       - /opt/loki/loki-promtail-conf.yml:/etc/promtail/docker-config.yaml
      - /var/log:/var/log
    command: -config.file=/etc/promtail/docker-config.yaml
    networks:
      - loki

  grafana:
    image: grafana/grafana:master
    ports:
      - &quot;3000:3000&quot;
    networks:
      - loki
</code></pre>

<pre><code>docker-compose up -d

# add datasource
key=”{job=\”nginx\”}” appeared – all good.
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift SSO with Gitlab]]></title>
            <link href="https://devopstales.github.io/home/openshift-sso-2/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift SSO authentication" />
                <link href="https://devopstales.github.io/home/nextcloud-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Nextcloud SSO" />
                <link href="https://devopstales.github.io/home/nexus-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Sonatype Nexus SSO" />
                <link href="https://devopstales.github.io/home/hashicorp-sso/?utm_source=atom_feed" rel="related" type="text/html" title="SSO for hashicorp vault" />
                <link href="https://devopstales.github.io/home/rundeck-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Rundeck SSO" />
            
                <id>https://devopstales.github.io/home/openshift-sso-2/</id>
            
            
            <published>2019-06-17T00:00:00+00:00</published>
            <updated>2019-06-17T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Configure Openshift Cluster to use Gitlab as a user backend for login with oauth2 and SSO.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<p>With Ansible-openshift you can not change the authetication method after Install !! If you installed the cluster with htpasswd, then change to LDAP the playbook trys to add a second authentication methot for the config. It is forbidden to add a second type of identity provider in the version 3.11 of Ansible-openshift. To solve this problem we must change the configuration manually.</p>

<h3 id="environment">Environment</h3>

<pre><code>192.168.1.40    deployer
192.168.1.41    openshift01 # master node
192.168.1.42    openshift02 # infra node
192.168.1.43    openshift03 # worker node
</code></pre>

<h3 id="configuration-gitlab">Configuration Gitlab</h3>

<p>Login to Gitlab and create client for the app:
<img src="/img/include/openshift_gitlab_sso.png" alt="Example image" /></p>

<h3 id="configurate-the-cluster">Configurate The cluster</h3>

<pre><code># on all openshift hosts
nano /etc/origin/master/master-config.yaml
...
  identityProviders:
  - name: gitlabsso
    challenge: true
    login: true
    mappingMethod: claim
    provider:
      apiVersion: v1
      kind: GitLabIdentityProvider
      legacy: true
      clientID: 7305abce637a123654a2c9dd4f8caec1156a1bc41cd80be4db0f14253fe24e58
      clientSecret: 2d5aebe7831c99383d876cc235febb401906263de748a29b03b058f62f15c2f7
      url: https://gitlab.devopstales.intra/
  - challenge: true
</code></pre>

<h3 id="reconfigurate-the-cluster">Reconfigurate the cluster</h3>

<pre><code># on all openshift hosts
master-restart api
master-restart controllers
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install CEHP Radosgateway on Proxmox]]></title>
            <link href="https://devopstales.github.io/home/proxmox-ceph-radosgw/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/proxmox-ceph-radosgw/</id>
            
            
            <published>2019-06-14T00:00:00+00:00</published>
            <updated>2019-06-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>RADOS Gateway  is an object storage interface in Ceph. It provides interfaces compatible with OpenStack Swift and Amazon S3.</p>

<p>First create a keyring than generated the keys and added them to the keyring:</p>

<pre><code>root@pve1:~# ceph-authtool --create-keyring /etc/ceph/ceph.client.radosgw.keyring

root@pve1:~# ceph-authtool /etc/ceph/ceph.client.radosgw.keyring -n client.radosgw.pve1 --gen-key
root@pve1:~# ceph-authtool /etc/ceph/ceph.client.radosgw.keyring -n client.radosgw.pve2 --gen-key
root@pve1:~# ceph-authtool /etc/ceph/ceph.client.radosgw.keyring -n client.radosgw.pve3 --gen-key
</code></pre>

<p>And then I added the proper capabilities and add the keys to the cluster:</p>

<pre><code>root@pve1:~# ceph-authtool -n client.radosgw.pve1 --cap osd 'allow rwx' --cap mon 'allow rwx' /etc/ceph/ceph.client.radosgw.keyring
root@pve1:~# ceph-authtool -n client.radosgw.pve2 --cap osd 'allow rwx' --cap mon 'allow rwx' /etc/ceph/ceph.client.radosgw.keyring
root@pve1:~# ceph-authtool -n client.radosgw.pve3 --cap osd 'allow rwx' --cap mon 'allow rwx' /etc/ceph/ceph.client.radosgw.keyring

root@pve1:~# ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.radosgw.pve1 -i /etc/ceph/ceph.client.radosgw.keyring
root@pve1:~# ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.radosgw.pve2 -i /etc/ceph/ceph.client.radosgw.keyring
root@pve1:~# ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.radosgw.pve3 -i /etc/ceph/ceph.client.radosgw.keyring
</code></pre>

<p>Copy the rings to the proxmox ClusterFS</p>

<pre><code>root@pve1:~# cp /etc/ceph/ceph.client.radosgw.keyring /etc/pve/priv
</code></pre>

<p>Add the following lines to <code>/etc/ceph/ceph.conf</code>:</p>

<pre><code>[client.radosgw.pve1]
        host = pve1
        keyring = /etc/pve/priv/ceph.client.radosgw.keyring
        log file = /var/log/ceph/client.radosgw.$host.log
        rgw_dns_name = s3.devopstales.intra

[client.radosgw.pve2]
        host = pve2
        keyring = /etc/pve/priv/ceph.client.radosgw.keyring
        log file = /var/log/ceph/client.radosgw.$host.log
        rgw_dns_name = s3.devopstales.intra

[client.radosgw.pve3]
        host = pve3
        keyring = /etc/pve/priv/ceph.client.radosgw.keyring
        log file = /var/log/ceph/client.rados.$host.log
        rgw_dns_name = s3.devopstales.intra
</code></pre>

<p>Install the pcakages and start the service. If all goes well, RADOSGW will create some default pools for you.</p>

<pre><code>root@pve1:~# apt install radosgw
root@pve1:~# service radosgw start

root@pve1:~# tail -f /var/log/ceph/client.rados.pve1.log
</code></pre>

<pre><code>root@pve1:~# ceph osd pool application enable .rgw.root rgw
root@pve1:~# ceph osd pool application enable default.rgw.control rgw
root@pve1:~# ceph osd pool application enable default.rgw.data.root rgw
root@pve1:~# ceph osd pool application enable default.rgw.gc rgw
root@pve1:~# ceph osd pool application enable default.rgw.log rgw
root@pve1:~# ceph osd pool application enable default.rgw.users.uid rgw
root@pve1:~# ceph osd pool application enable default.rgw.users.email rgw
root@pve1:~# ceph osd pool application enable default.rgw.users.keys rgw
root@pve1:~# ceph osd pool application enable default.rgw.buckets.index rgw
root@pve1:~# ceph osd pool application enable default.rgw.buckets.data rgw
root@pve1:~# ceph osd pool application enable default.rgw.lc rgw
</code></pre>

<pre><code>root@pve1:~#  ssh pve2 'apt install radosgw &amp;&amp; service radosgw start'
root@pve1:~#  ssh pve3 'apt install radosgw &amp;&amp; service radosgw start'

root@pve1:~#  ceph osd pool ls
</code></pre>

<pre><code>root@pve1:~# radosgw-admin user create --uid=devopstales --display-name=&quot;devopstales&quot; --email=devopstales@devopstales.intra
root@pve1:~# radosgw-admin user info devopstales

root@pve1:~# ceph osd pool application enable default.rgw.buckets.index rgw
root@pve1:~# ceph osd pool application enable default.rgw.buckets.data rgw
</code></pre>

<pre><code>root@pve1:~# apt-get install s3cmd
root@pve1:~# s3cmd --configure
Access Key: xxxxxxxxxxxxxxxxxxxxxx
Secret Key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

root@pve1:~#  s3cmd mb s3://devopstales
Bucket 's3://devopstales/' created
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Prometheus for Gitlab]]></title>
            <link href="https://devopstales.github.io/home/install-prometheus-for-gitlab/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/prometheus-alertmanagger/?utm_source=atom_feed" rel="related" type="text/html" title="Install Alertmanagger" />
                <link href="https://devopstales.github.io/monitoring/install-prometheus-for-gitlab/?utm_source=atom_feed" rel="related" type="text/html" title="Install Prometheus for Gitlab" />
                <link href="https://devopstales.github.io/home/pfsense-telegraf/?utm_source=atom_feed" rel="related" type="text/html" title="Install telegraf on pfsense" />
                <link href="https://devopstales.github.io/home/naemon-influxdb/?utm_source=atom_feed" rel="related" type="text/html" title="Install Nemon with Influxdb storage" />
                <link href="https://devopstales.github.io/home/install-mattermost-for-gitlab/?utm_source=atom_feed" rel="related" type="text/html" title="Install mattermost for Gitlab" />
            
                <id>https://devopstales.github.io/home/install-prometheus-for-gitlab/</id>
            
            
            <published>2019-06-10T00:00:00+00:00</published>
            <updated>2019-06-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Prometheus is an open-source monitoring system with a built-in noSQL time-series database. It offers a multi-dimensional data model, a flexible query language, and diverse visualization possibilities. Prometheus collects metrics from http nedpoint. Most service dind’t have this endpoint so you need optional programs that generate additional metrics cald exporters.</p>

<h3 id="install-prometheus-from-package">Install prometheus from package</h3>

<pre><code>curl -s https://packagecloud.io/install/repositories/prometheus-rpm/release/script.rpm.sh | sudo bash

yum install prometheus2 alertmanager -y
</code></pre>

<h3 id="configurate-prometheus">Configurate Prometheus</h3>

<pre><code>nano /etc/prometheus/promethsu.yml
---
global:
  scrape_interval: 15s
  scrape_timeout: 15s
  evaluation_interval: 15s

alerting:
  alertmanagers:
  - static_configs:
    - targets:
       - localhost:9095

rule_files:
  - &quot;/etc/prometheus/alert.rules&quot;

scrape_configs:
- job_name: prometheus
  static_configs:
  - targets:
    - localhost:9090
- job_name: redis
  static_configs:
  - targets:
    - localhost:9121
- job_name: postgres
  static_configs:
  - targets:
    - localhost:9187
- job_name: node
  static_configs:
  - targets:
    - localhost:9100
- job_name: gitlab-workhorse
  static_configs:
  - targets:
    - localhost:9229
- job_name: gitlab-unicorn
  metrics_path: &quot;/-/metrics&quot;
  static_configs:
  - targets:
    - 127.0.0.1:8080
- job_name: gitlab-sidekiq
  static_configs:
  - targets:
    - 127.0.0.1:8082
- job_name: gitlab_monitor_database
  metrics_path: &quot;/database&quot;
  static_configs:
  - targets:
    - localhost:9168
- job_name: gitlab_monitor_sidekiq
  metrics_path: &quot;/sidekiq&quot;
  static_configs:
  - targets:
    - localhost:9168
- job_name: gitlab_monitor_process
  metrics_path: &quot;/process&quot;
  static_configs:
  - targets:
    - localhost:9168
- job_name: gitaly
  static_configs:
  - targets:
    - localhost:9236
- job_name: nginx
  static_configs:
  - targets:
    - localhost:9913
- job_name: 'playframework-app'
  scrape_interval: 5s
  metrics_path: '/metrics'
  static_configs:
  - targets: ['localhost:9000']
</code></pre>

<pre><code>nano /etc/prometheus/alert.rules
groups:
- name: host
  rules:
  - alert: low_connected_users
    expr: play_current_users &lt; 2
    for: 30s
    labels:
      severity: slack
    annotations:
      summary: &quot;Instance {{ $labels.instance }} under lower load&quot;
      description: &quot;{{ $labels.instance }} of job {{ $labels.job }} is under lower load.&quot;
</code></pre>

<p><img src="/img/include/mattermost_gitlab4.png" alt="Example image" /><br><br>
<img src="/img/include/mattermost_gitlab5.png" alt="Example image" /><br><br>
<img src="/img/include/mattermost_gitlab6.png" alt="Example image" /><br><br>
<img src="/img/include/mattermost_gitlab7.png" alt="Example image" /><br><br>
<img src="/img/include/mattermost_gitlab8.png" alt="Example image" /><br><br></p>

<pre><code>nano /etc/prometheus/alertmanager.yml
global:

templates:
- '/etc/prometheus/template/*.tmpl'

route:
 group_by: [alertname, job]
 # If an alert isn't caught by a route, send it to slack.
 receiver: slack_general
 routes:
  - match:
      severity: slack
    receiver: slack_general

receivers:
- name: slack_general
  slack_configs:
  - api_url: http://mattermost.devopstales.intra/hooks/9g4qwgpkzi898jzzeszzzzutmc
    channel: 'monitoring'
    username: &quot;prometheus&quot; #name ins mattermost
    text: &quot;&quot;
    send_resolved: true
</code></pre>

<pre><code>mkdir /etc/prometheus/template/
nano /etc/prometheus/template/alertmessage.tmpl
{{ define &quot;__slack_text&quot; }}
{{ range .Alerts }}{{ .Annotations.description}}{{ end }}
{{ end }}

{{ define &quot;__slack_title&quot; }}
{{ range .Alerts }} :scream: {{ .Annotations.summary}} :scream: {{ end }}
{{ end }}

{{ define &quot;slack.default.text&quot; }}{{ template &quot;__slack_text&quot; . }}{{ end }}
{{ define &quot;slack.default.title&quot; }}{{ template &quot;__slack_title&quot; . }}{{ end }}
</code></pre>

<h3 id="configurate-gitlab">Configurate Gitlab</h3>

<p><img src="/img/include/mattermost_gitlab1.png" alt="Example image" /><br><br>
<img src="/img/include/mattermost_gitlab2.png" alt="Example image" /><br><br>
<img src="/img/include/mattermost_gitlab3.png" alt="Example image" /><br><br></p>

<pre><code>nano /etc/gitlab/gitlab.rb
alertmanager['enable'] = false
prometheus['enable'] = false
node_exporter['enable'] = true
redis_exporter['enable'] = true
postgres_exporter['enable'] = true
gitlab_monitor['enable'] = true

gitlab-ctl reconfigure

systemctl start alertmanager.service
systemctl status alertmanager.service
systemctl start prometheus.service
systemctl status prometheus.service
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install mattermost for Gitlab]]></title>
            <link href="https://devopstales.github.io/home/install-mattermost-for-gitlab/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/gitlab-install/?utm_source=atom_feed" rel="related" type="text/html" title="Gitlab Install" />
                <link href="https://devopstales.github.io/home/openshift-letsencrypt/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Letsencrypt certificates" />
                <link href="https://devopstales.github.io/home/nextcloud-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Nextcloud SSO" />
                <link href="https://devopstales.github.io/home/nexus-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Sonatype Nexus SSO" />
                <link href="https://devopstales.github.io/home/hashicorp-sso/?utm_source=atom_feed" rel="related" type="text/html" title="SSO for hashicorp vault" />
            
                <id>https://devopstales.github.io/home/install-mattermost-for-gitlab/</id>
            
            
            <published>2019-06-09T00:00:00+00:00</published>
            <updated>2019-06-09T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Mattermost is an open source on premise alternative of Slack.</p>

<h3 id="install-mattermost-from-package">Install mattermost from package</h3>

<pre><code>sudo yum -y install https://harbottle.gitlab.io/harbottle-main/7/x86_64/harbottle-main-release.rpm

yum install mattermost-server -y
</code></pre>

<h3 id="configurate-mattermost">Configurate mattermost</h3>

<pre><code>nano /etc/mattermost/config.json
# postgresql
&quot;SiteURL&quot;: &quot;http://mattermost.devopstales.intra&quot;
&quot;DriverName&quot;: &quot;postgres&quot;,
&quot;DataSource&quot;: &quot;postgres://mmuser:Password1@localhost:5432/mattermost?sslmode=disable&amp;connect_timeout=10&quot;

# gitlab austh
&quot;AllowedUntrustedInternalConnections&quot;: &quot;gitlab.devopstales.intra&quot;
...
    &quot;GitLabSettings&quot;: {
        &quot;Enable&quot;: true,
        &quot;Secret&quot;: &quot;&lt;secret&gt;&quot;,
        &quot;Id&quot;: &quot;&lt;id&gt;&quot;,
        &quot;Scope&quot;: &quot;&quot;,
        &quot;AuthEndpoint&quot;: &quot;http://gitlab.devopstales.intra/oauth/authorize&quot;,
        &quot;TokenEndpoint&quot;: &quot;http://gitlab.devopstales.intra/oauth/token&quot;,
        &quot;UserApiEndpoint&quot;: &quot;http://gitlab.devopstales.intra/api/v4/user&quot;
    },
</code></pre>

<h3 id="edit-systemd-serice">Edit systemd serice</h3>

<pre><code>nano /usr/lib/systemd/system/mattermost.service
[Unit]
Description=Mattermost
After=syslog.target network.target
After=postgresql.service
Requires=postgresql-9.6.service

[Service]
Type=notify
NotifyAccess=main
WorkingDirectory=/usr/share/mattermost
User=mattermost
Group=mattermost
ExecStart=/usr/share/mattermost/bin/mattermost
TimeoutStartSec=3600
LimitNOFILE=49152

[Install]
WantedBy=multi-user.target

systemctl daemon-reload
systemctl start mattermost.service
</code></pre>

<h3 id="configurate-nginx-proxy">Configurate nginx proxy</h3>

<pre><code>nano /etc/nginx/conf.d/mattermost.conf
upstream backend {
   server localhost:8065;
   keepalive 32;
}

proxy_cache_path /var/cache/nginx levels=1:2 keys_zone=mattermost_cache:10m max_size=3g inactive=120m use_temp_path=off;

server {
   listen 80;
   server_name    mattermost.devopstales.intra;

   location ~ /api/v[0-9]+/(users/)?websocket$ {
       proxy_set_header Upgrade $http_upgrade;
       proxy_set_header Connection &quot;upgrade&quot;;
       client_max_body_size 50M;
       proxy_set_header Host $http_host;
       proxy_set_header X-Real-IP $remote_addr;
       proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
       proxy_set_header X-Forwarded-Proto $scheme;
       proxy_set_header X-Frame-Options SAMEORIGIN;
       proxy_buffers 256 16k;
       proxy_buffer_size 16k;
       client_body_timeout 60;
       send_timeout 300;
       lingering_timeout 5;
       proxy_connect_timeout 90;
       proxy_send_timeout 300;
       proxy_read_timeout 90s;
       proxy_pass http://backend;
   }

   location / {
       client_max_body_size 50M;
       proxy_set_header Connection &quot;&quot;;
       proxy_set_header Host $http_host;
       proxy_set_header X-Real-IP $remote_addr;
       proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
       proxy_set_header X-Forwarded-Proto $scheme;
       proxy_set_header X-Frame-Options SAMEORIGIN;
       proxy_buffers 256 16k;
       proxy_buffer_size 16k;
       proxy_read_timeout 600s;
       proxy_cache mattermost_cache;
       proxy_cache_revalidate on;
       proxy_cache_min_uses 2;
       proxy_cache_use_stale timeout;
       proxy_cache_lock on;
       proxy_http_version 1.1;
       proxy_pass http://backend;
   }
}
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Gitlab Install]]></title>
            <link href="https://devopstales.github.io/home/gitlab-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/gitlab-install/?utm_source=atom_feed" rel="related" type="text/html" title="Gitlab Install" />
                <link href="https://devopstales.github.io/home/gitlab-keycloak/?utm_source=atom_feed" rel="related" type="text/html" title="SSO login to Gitlab" />
                <link href="https://devopstales.github.io/home/openshift-letsencrypt/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Letsencrypt certificates" />
                <link href="https://devopstales.github.io/home/nextcloud-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Nextcloud SSO" />
                <link href="https://devopstales.github.io/home/nexus-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Sonatype Nexus SSO" />
            
                <id>https://devopstales.github.io/home/gitlab-install/</id>
            
            
            <published>2019-06-05T00:00:00+00:00</published>
            <updated>2019-06-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Install Gitab with custom postgresql and nginx proxy.</p>

<h3 id="install-ntpd">Install NTPD</h3>

<pre><code>yum install -y epel-release yum-utils
yum-config-manager --enable epel

sudo chkconfig ntpd on
sudo ntpdate 0.hu.pool.ntp.org
sudo service ntpd start
</code></pre>

<h3 id="install-postgresql">Install postgresql</h3>

<p>In a previous post I wrote about how to <a href="/linux/install-postgresql/#install-postgresql-10-on-centos-7">Install PostgreSQL 10</a></p>

<pre><code>su - postgres
createdb -U postgres gitlab
createdb -U postgres gitlab_ci
createdb -U postgres mattermost

createuser gituser
createuser ciuser
createuser mmuser

psql -U postgres gitlab
ALTER USER &quot;gituser&quot; WITH PASSWORD 'Password1';
GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO gituser;
ALTER ROLE gituser CREATEROLE SUPERUSER;
\q

psql -U postgres gitlab_ci
ALTER USER &quot;ciuser&quot; WITH PASSWORD 'Password1';
GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO ciuser;
\q

psql -U postgres mattermost
ALTER USER &quot;mmuser&quot; WITH PASSWORD 'Password1';
GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO mmuser;
\q
</code></pre>

<h3 id="install-nginx">install nginx</h3>

<pre><code>yum install -y nginx
mkdir /var/log/gitlab/nginx/

echo 'upstream gitlab-workhorse {
        server unix:/var/opt/gitlab/gitlab-workhorse/socket;
}

server {
        listen *:80;
#  listen *:443 ssl;
        server_name gitlab.devopstales.intra;
        server_tokens off;
        root /opt/gitlab/embedded/service/gitlab-rails/public;

        client_max_body_size 256m;

        real_ip_header X-Forwarded-For;

        access_log /var/log/gitlab/nginx/gitlab_access.log;
        error_log  /var/log/gitlab/nginx/gitlab_error.log;

 # ssl_certificate   /etc/nginx/ssl.d/gitlab.pem;
 # ssl_certificate_key   /etc/nginx/ssl.d/gitlab.key;


        location / {
                proxy_read_timeout	300;
                proxy_connect_timeout   300;
                proxy_redirect          off;

                proxy_buffering off;

                proxy_set_header    Host                $http_host;
                proxy_set_header    X-Real-IP           $remote_addr;
                proxy_set_header    X-Forwarded-For     $proxy_add_x_forwarded_for;
                proxy_set_header    X-Forwarded-Proto   $scheme;

                proxy_pass http://gitlab-workhorse;

    proxy_request_buffering off;
    proxy_http_version 1.1;
        }

        location ~ ^/(assets)/ {
                root /opt/gitlab/embedded/service/gitlab-rails/public;
                gzip_static on; # to serve pre-gzipped version
                        expires max;
               	add_header Cache-Control public;
        }

        error_page 502 /502.html;
}' &gt; /etc/nginx/conf.d/gitlab.conf

nano /etc/nginx/nginx.conf
log_format gitlab_access '$remote_addr - $remote_user [$time_local] &quot;$request&quot; $status $body_bytes_sent &quot;$http_referer&quot; &quot;$http_user_agent&quot;';
log_format gitlab_ci_access '$remote_addr - $remote_user [$time_local] &quot;$request&quot; $status $body_bytes_sent &quot;$http_referer&quot; &quot;$http_user_agent&quot;';
log_format gitlab_mattermost_access '$remote_addr - $remote_user [$time_local] &quot;$request&quot; $status $body_bytes_sent &quot;$http_referer&quot; &quot;$http_user_agent&quot;';

nginx -t
nginx -s reload

sudo usermod -aG gitlab-www nginx
</code></pre>

<h3 id="install-gitlab">install gitlab</h3>

<pre><code>curl -sS https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh | sudo bash
sudo yum install gitlab-ce -y

cd /opt/gitlab/embedded/bin
mv psql psql_moved
mv pg_dump pg_dump_moved

which pg_dump psql

ln -s /usr/bin/pg_dump /usr/bin/psql /opt/gitlab/embedded/bin/
</code></pre>

<pre><code>nano /etc/gitlab/gitlab.rb
external_url 'http://gitlab.devopstales.intra'
gitlab_rails['time_zone'] = 'Europe/Budapest'

gitlab_rails['db_adapter'] = &quot;postgresql&quot;
gitlab_rails['db_encoding'] = &quot;unicode&quot;
gitlab_rails['db_database'] = &quot;gitlab&quot;
gitlab_rails['db_pool'] = 10
gitlab_rails['db_username'] = &quot;gituser&quot;
gitlab_rails['db_password'] = &quot;Password1&quot;
gitlab_rails['db_host'] = '127.0.0.1'
gitlab_rails['db_port'] = 5432

gitlab_rails['redis_socket'] = &quot;/var/opt/gitlab/redis/redis.socket&quot;

unicorn['socket'] = '/var/opt/gitlab/gitlab-rails/sockets/gitlab.socket'
unicorn['log_directory'] = &quot;/var/log/gitlab/unicorn&quot;
unicorn['port'] = 8081

user['username'] = &quot;git&quot;
user['group'] = &quot;git&quot;

postgresql['enable'] = false

nginx['enable'] = false
web_server['external_users'] = ['nginx']
</code></pre>

<pre><code>gitlab-ctl reconfigure
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Migrate BIND to Windows DNS]]></title>
            <link href="https://devopstales.github.io/home/migrate-bind-to-windows-dns/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/migrate-bind-to-windows-dns/</id>
            
            
            <published>2019-06-04T00:00:00+00:00</published>
            <updated>2019-06-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Migrate dns Zones from bind to Windows DNS Server</p>

<h3 id="linux">Linux</h3>

<pre><code>nano /etc/bind/zones.active
allow-transfer { 192.168.0.8; };
</code></pre>

<h3 id="windows">Windows</h3>

<pre><code>Add-DnsServerSecondaryZone -Name &quot;devopstales.intra&quot; -ZoneFile &quot;devopstales.intra.dns&quot; -MasterServers 192.168.0.60
ConvertTo-DnsServerPrimaryZone -Name &quot;devopstales.intra&quot; -PassThru -Verbose -ZoneFile &quot;devopstales.intra.dns&quot; -Force
Set-DnsServerPrimaryZone -Name &quot;devopstales.intra&quot; –Notify Notifyservers –notifyservers &quot;192.168.0.5&quot; -SecondaryServers &quot;192.168.0.5&quot; –SecureSecondaries TransferToSecureServers
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Override a single external hostname with internal DNS-entry]]></title>
            <link href="https://devopstales.github.io/home/override-a-single-external-hostname-with-internal-dns-entry/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/override-a-single-external-hostname-with-internal-dns-entry/</id>
            
            
            <published>2019-06-04T00:00:00+00:00</published>
            <updated>2019-06-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Override a single external hostname with internal DNS-entry</p>

<h3 id="problem">Problem:</h3>

<p>Company.com has an exernal dns-record for service.company.com which should be resolved to an inernal IP by internal clients.</p>

<p>Let’s say that service.company.com resolves to 1.1.1.1 by the external DNS but when computers are connecting to this URL from inside the company network the internal DNS servers at ad.company.com needs to resolve service.company.com to 172.16.51.25.</p>

<p>Adding an entry to the hosts-file on each client computer to override service.company.com will not work when clients connect on exteral networks like from home or a coffeeshop.</p>

<h3 id="solution">Solution:</h3>

<p>The solution is to add a new Forward Lookup Zone named service.company.com and add a new Host-record, enter the internal IP-address but leave the Name blank.</p>

<p>On a DNS server running Windows Server 2012 this is of course achieved by using PowerShell!</p>

<p>First off, create a new DNS Forward Lookup Zone using PowerShell:</p>

<pre><code>Add-DnsServerPrimaryZone -Name service.company.com -ReplicationScope Forest

#Then add a host record to the zone:

Add-DnsServerResourceRecordA -IPv4Address 172.16.51.25 -ZoneName service.company.com -Name service.company.com
</code></pre>

<p>By specifying service.company.com as both ZoneName and Name a record with the name “(same as parent folder)” will be created.</p>

<p>This will only override DNS queries for the FQDN service.company.com and will not affect other records in company.com</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Sonarqube Install]]></title>
            <link href="https://devopstales.github.io/home/sonarkube-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/sonarkube-install/</id>
            
            
            <published>2019-05-30T00:00:00+00:00</published>
            <updated>2019-05-30T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Sonarqube Repository OSS is an artifact repository with universal support for popular formats.</p>

<pre><code>yum install epel-release -y
yum update -y
</code></pre>

<h3 id="install-postgresql">Install Postgresql</h3>

<p>In a previous post I wrote about how to <a href="/linux/install-postgresql/#install-postgresql-9-6-on-centos-7">Install PostgreSQL 10</a></p>

<pre><code>su - postgres
createuser -S sonar
createdb -O sonar sonar
psql
ALTER USER &quot;sonar&quot; WITH PASSWORD 'Password1';
\q
exit
</code></pre>

<h3 id="install-sonarqube">Install Sonarqube</h3>

<pre><code># https://binaries.sonarsource.com/Distribution/sonarqube/
cd /opt
wget https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-7.6.zip
unzip sonarqube-7.6.zip
ln -s /opt/sonarqube-7.6 /opt/sonarqube

nano /opt/sonarqube/conf/sonar.properties
sonar.jdbc.username=sonar
sonar.jdbc.password=Password1
sonar.jdbc.url=jdbc:postgresql://localhost/sonar

/opt/sonarqube/bin/linux-x86-64/sonar.sh
RUN_AS_USER=sonar

adduser -s /bin/false sonar
chown -R sonar:sonar /opt/sonarqube/

sysctl -w vm.max_map_count=262144
sysctl -w fs.file-max=65536
ulimit -n 65536
ulimit -u 4096
</code></pre>

<h3 id="create-sistemd-serice-for-sonarqube">Create sistemd serice for Sonarqube</h3>

<pre><code>echo '[Unit]
Description=Sonar
After=network.target network-online.target
Wants=network-online.target

[Service]
ExecStart=/opt/sonarqube/bin/linux-x86-64/sonar.sh start
ExecStop=/opt/sonarqube/bin/linux-x86-64/sonar.sh stop
ExecReload=/opt/sonarqube/bin/linux-x86-64/sonar.sh restart
PIDFile=/opt/sonarqube/bin/linux-x86-64/./SonarQube.pid
Type=forking
User=sonar
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target' &gt; /etc/systemd/system/sonar.service
</code></pre>

<h3 id="start-sonarqube">Start Sonarqube</h3>

<pre><code>sudo systemctl daemon-reload
sudo systemctl enable sonar.service
sudo systemctl start sonar.service

tailf /opt/sonarqube/logs/es.log
### To check, point your browser to http://localhost:9000. Default username is admin with password admin.
</code></pre>

<h3 id="apache-proxy">Apache proxy</h3>

<pre><code>echo 'ProxyRequests Off
ProxyPreserveHost On
&lt;VirtualHost *:80&gt;
  ServerName sonar.devopstales.intra
  ServerAdmin admin@somecompany.com
  ProxyPass / http://localhost:9000/
  ProxyPassReverse / http://localhost:9000/
  ErrorLog /var/log/sonar-error.log
  CustomLog /var/log/sonar-access.log common
&lt;/VirtualHost&gt;' &gt; /etc/httpd/conf.d/sonar.conf

systemctl start httpd
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift Letsencrypt certificates]]></title>
            <link href="https://devopstales.github.io/home/openshift-letsencrypt/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/openshift-letsencrypt/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Letsencrypt certificates" />
                <link href="https://devopstales.github.io/home/openshift-hostalreadyclaimed/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Error: HostAlreadyClaimed" />
                <link href="https://devopstales.github.io/home/openshift-gitlabrunner/?utm_source=atom_feed" rel="related" type="text/html" title="Install Gitlab runner on Openshift" />
                <link href="https://devopstales.github.io/home/openshift-extregistry/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: External registry" />
                <link href="https://devopstales.github.io/home/openshift-cert/?utm_source=atom_feed" rel="related" type="text/html" title="Change Certificates in Openshift" />
            
                <id>https://devopstales.github.io/home/openshift-letsencrypt/</id>
            
            
            <published>2019-05-28T00:00:00+00:00</published>
            <updated>2019-05-28T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Thanks to Tomáš Nožička developed openshift-acme as an ACME Controller for OpenShift and Kubernetes clusters. <br>
It automatically provision certficates</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<h3 id="environment">Environment</h3>

<pre><code>192.168.1.40    deployer
192.168.1.41    openshift01 # master node
192.168.1.42    openshift02 # infra node
192.168.1.43    openshift03 # worker node
</code></pre>

<h3 id="deploy-route">Deploy route</h3>

<pre><code>oc project default

oc label node openshift02.devopstales.intra &quot;router=letsencrypt&quot;
oc get node --show-labels

oc adm policy add-scc-to-user hostnetwork -z router
oc adm router router-letsencrypt --replicas=0 --ports=&quot;8080:8080,8443:8443&quot; --stats-port=1937 --selector=&quot;router=letsencrypt&quot; --labels=&quot;router=letsencrypt&quot;

oc set env dc/router-letsencrypt \
NAMESPACE_LABELS=&quot;router=letsencrypt&quot; \
ROUTER_ALLOW_WILDCARD_ROUTES=true \
ROUTER_SERVICE_HTTP_PORT=8080 \
ROUTER_SERVICE_HTTPS_PORT=8443 \
ROUTER_TCP_BALANCE_SCHEME=roundrobin

oc set env dc/router NAMESPACE_LABELS=&quot;router != letsencrypt&quot;

oc scale dc/router-letsencrypt --replicas=3
</code></pre>

<h3 id="deploy-letsencrypt">Deploy letsencrypt</h3>

<pre><code>GIT_REPO=https://raw.githubusercontent.com/devopstales/openshift-examples
GIT_PATH=/master/letsencrypt
oc new-project letsencrypt
oc create -f$GIT_REPO/$GIT_PATH/{clusterrole,serviceaccount,imagestream,deployment}.yaml
oc adm policy add-cluster-role-to-user openshift-acme -z openshift-acme
</code></pre>

<h1 id="demo">Demo</h1>

<pre><code>oc new-project test
oc label namespace test router=letsencrypt
oc new-app centos/ruby-25-centos7~https://github.com/sclorg/ruby-ex.git
oc expose svc/ruby-ex

oc patch route ruby-ex \
    -p '{&quot;metadata&quot;:{&quot;annotations&quot;:{  &quot;kubernetes.io/tls-acme&quot; : &quot;true&quot;   }}}'
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configure OpenVPN HA opnsense cluster]]></title>
            <link href="https://devopstales.github.io/home/opnsense-openvpn/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/opnsense-openvpn/</id>
            
            
            <published>2019-05-25T00:00:00+00:00</published>
            <updated>2019-05-25T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this LAB I&rsquo;ll be creating OpenVPN SSL Peer to Peer connection.</p>

<h3 id="the-architecture">The Architecture</h3>

<pre><code> ------ WAN ------
 |               |
PF1 -- sync -- PF2
 |               |
 ----- LAN -------  

WAN: 192.168.0.0/24 (Bridgelt)
LAN: 192.168.20.0/24
SYNC: 192.168.30.0/24
</code></pre>

<pre><code>opn01:
WAN 192.168.0.28
LAN: 192.168.20.28
SYNC:192.168.30.28

opn02:
WAN 192.168.0.29
LAN: 192.168.20.29
SYNC:192.168.30.29
</code></pre>

<h3 id="configurate-the-opevpn-service">Configurate the OpeVPN service</h3>

<p>Got to <code>VPN &gt; OpenVPN &gt; Wizards</code>
<img src="/img/include/opnsense_ovpn1.png" alt="Example image" /></p>

<p>If you ulodad your certificate seledt that in the drop doew menu or select Add new Certificate to generate a new one.
<img src="/img/include/opnsense_ovpn2.png" alt="Example image" /></p>

<p><img src="/img/include/opnsense_ovpn3.png" alt="Example image" /></p>

<p>Edit the Adwanced Configuration: <br>
<img src="/img/include/opnsense_ovpn4.png" alt="Example image" /></p>

<p><img src="/img/include/opnsense_ovpn5.png" alt="Example image" /></p>

<p><img src="/img/include/opnsense_ovpn6.png" alt="Example image" /></p>

<h3 id="configurate-nat-rules-to-ha">Configurate NAT Rules to HA</h3>

<p>Go to <code>Firewall &gt; NAT &gt; Outbound</code> and clone the manul LAN Rule
<img src="/img/include/opnsense_ovpn8.png" alt="Example image" /></p>

<h3 id="enable-connection-from-openvpn-to-master-and-slave">Enable Connection from OpenVPN to master and slave</h3>

<p>In default there in no rout to the salve nod. <br>
 Go to <code>Firewll &gt; Aliases &gt; Add</code> and create alias for CARP members: <br>
<img src="/img/include/opnsense_ovpn7.png" alt="Example image" /></p>

<p>Then go back to <code>Firewall &gt; NAT &gt; Outbound &gt; Settings</code> and create a new rule:
<img src="/img/include/opnsense_ovpn9.png" alt="Example image" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configure opnsense nextcloud backup]]></title>
            <link href="https://devopstales.github.io/home/opnsense-nextcloud/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/opnsense-nextcloud/</id>
            
            
            <published>2019-05-25T00:00:00+00:00</published>
            <updated>2019-05-25T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this LAB I&rsquo;ll be configurate the opnsense cloud backup solutuon for nextcloud.</p>

<h3 id="configurate-the-nextcloud">Configurate the nextcloud</h3>

<ul>
<li>login to your Nextcloud instance with the admin account</li>

<li><p>go to users
<img src="/img/include/nextcloud_sso1.png" alt="Example image" /></p></li>

<li><p>create a new user for opnsense
<img src="/img/include/opnsense_nextcloud2.png" alt="Example image" /></p></li>

<li><p>login with tehe new user and go to <code>profiele &gt; settings &gt; seurity</code></p></li>

<li><p>create token for user
<img src="/img/include/opnsense_nextcloud3.png" alt="Example image" /></p></li>
</ul>

<h3 id="configurate-opnsense-backup">Configurate opnsense backup</h3>

<ul>
<li>login to opnsense</li>
<li>go to <code>system &gt; config &gt; bckups</code></li>
<li>enable the nextclod config
<img src="/img/include/opnsense_nextcloud4.png" alt="Example image" /></li>
<li>click the <code>setup/test nextcloud</code> button</li>
</ul>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Nextcloud SSO]]></title>
            <link href="https://devopstales.github.io/home/nextcloud-sso/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/nexus-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Sonatype Nexus SSO" />
                <link href="https://devopstales.github.io/home/hashicorp-sso/?utm_source=atom_feed" rel="related" type="text/html" title="SSO for hashicorp vault" />
                <link href="https://devopstales.github.io/home/rundeck-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Rundeck SSO" />
                <link href="https://devopstales.github.io/home/mod-auth-openidc/?utm_source=atom_feed" rel="related" type="text/html" title="Apaceh2 oauth plugin" />
                <link href="https://devopstales.github.io/home/openshift-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift SSO authentication" />
            
                <id>https://devopstales.github.io/home/nextcloud-sso/</id>
            
            
            <published>2019-05-25T00:00:00+00:00</published>
            <updated>2019-05-25T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Nextcloud is a suite of client-server software for creating and using file hosting services. Nextcloud application functionally is similar to Dropbox.</p>

<h2 id="configuring-keycloak-and-nextcloud">Configuring Keycloak and Nextcloud</h2>

<h3 id="keycloak-side">Keycloak side</h3>

<ul>
<li>login to keycloak using the admin account</li>
<li>Under <code>Clients</code>, create a new client with <code>Client ID</code> &ldquo;nextcloud&rdquo; and <code>Root URL</code> &ldquo;cloud.devopstales.intra&rdquo;</li>
<li>On next screen, under the <code>Settings</code> tab, change <code>Access Type</code> from <code>public</code> to <code>confidential</code>, then Save</li>
<li>Go the the <code>Credentials</code> tab, note the <code>Secret</code></li>
<li>OPTIONAL: If there is no registered user yet you can create a test user: go to <code>Users</code>, click the <code>Add User</code> button, fill the <code>Username</code> with &ldquo;test&rdquo; and save. Then go to the <code>Credentials</code> tab, put the new password, toggle the <code>Temporary</code> option to <code>OFF</code>, press <code>Reset Password</code> and confirm</li>
</ul>

<p>Keycloak is now ready to be used for Nextcloud.</p>

<h3 id="nextcloud-side">NextCloud side</h3>

<ul>
<li>login to your Nextcloud instance with the admin account</li>
<li>Click on the user profile, then <code>Apps</code>
<img src="/img/include/nextcloud_sso1.png" alt="Example image" /><br><br></li>
<li>Go to <code>Social &amp; communication</code> and install the <code>Social Login</code> app</li>
<li>Go to <code>Settings</code> (in your user profile) the <code>Social Login</code><br><br>
<img src="/img/include/nextcloud_sso2.png" alt="Example image" /><br><br></li>
<li>Add a new <code>Custom OpenID Connect</code> by clicking on the <code>+</code> to its side</li>
<li>Fill the following:

<ul>
<li><code>Title</code> -&gt; &ldquo;keycloak&rdquo;</li>
<li><code>Authorize url</code> -&gt; <code>https://keycloak.devopstales.intra:8443/auth/realms/mydomain/protocol/openid-connect/auth</code></li>
<li><code>Token url</code> -&gt; <code>https:/keycloak.devopstales.intra:8443/auth/realms/mydomain/protocol/openid-connect/token</code></li>
<li><code>Client id</code> -&gt; &ldquo;nextcloud&rdquo;</li>
<li><code>Client Secret</code> -&gt; put the secret you noted down during the Keycloak configuration</li>
<li><code>Scope</code> -&gt; &ldquo;openid&rdquo;</li>
</ul></li>
<li>Press <code>Save</code></li>
</ul>

<p>Your Nextcloud instance is now configured. Log out and log back in using the <code>Alternative Logins -&gt; keycloak</code> method on the login page. It should redirect you to a keycloak auth form where you can log in with a registered keycloak user, then back to Nextcloud where you are now logged.
<img src="/img/include/nextcloud_sso3.png" alt="Example image" /><br><br></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configurate HA opnsense cluster]]></title>
            <link href="https://devopstales.github.io/home/opnsense-ha/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/opnsense-ha/</id>
            
            
            <published>2019-05-24T00:00:00+00:00</published>
            <updated>2019-05-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will configure 2 opnsense server to a HA cluster.</p>

<h3 id="the-architecture">The Architecture</h3>

<pre><code> ------ WAN ------
 |               |
PF1 -- sync -- PF2
 |               |
 ----- LAN -------  

WAN: 192.168.0.0/24 (Bridgelt)
LAN: 192.168.20.0/24
SYNC: 192.168.30.0/24
</code></pre>

<pre><code>opn01:
WAN 192.168.0.28
LAN: 192.168.20.28
SYNC:192.168.30.28

opn02:
WAN 192.168.0.29
LAN: 192.168.20.29
SYNC:192.168.30.29
</code></pre>

<h3 id="firewall-rules-for-sync">Firewall rules For sync</h3>

<p>On both firewalls add two rules to allow traffic on the SYNC interface: <br>
go to <code>Firewall &gt; Rules &gt; Sync</code> and click <code>Add</code>.</p>

<p>Rule 1:
<img src="/img/include/opnsense_carp1.png" alt="Example image" /></p>

<p>Rule 2:
<img src="/img/include/opnsense_carp2.png" alt="Example image" /></p>

<p>Rule 3:
<img src="/img/include/opnsense_carp3.png" alt="Example image" /></p>

<h3 id="synchronization-settings">Synchronization Settings</h3>

<p>Go to <code>System &gt; High Availalility &gt; Settings</code>. Configure the sections like on the pictures.</p>

<p>Master:
<img src="/img/include/opnsense_carp4.png" alt="Example image" /></p>

<p>Slave:
<img src="/img/include/opnsense_carp5.png" alt="Example image" /></p>

<p>Test the synchronisation. Go to <code>System &gt; User management</code> and createa new user on the master node. <br>
Then check on the slave node.</p>

<p>If it doesn&rsquo;t work, check:</p>

<ul>
<li>Are the firewall web interfaces running on the same protocols and ports?</li>
<li>Is the admin password set correctly? (User Manager &gt; Users &gt; admin.)</li>
<li>Are the firewall rules to allow synch set to use the correct interface (SYNC)?</li>
<li>If you&rsquo;re using VMs, are the firewalls on the same internal network?</li>
</ul>

<h3 id="create-virtual-ips">create virtual IPs</h3>

<p>On the master node go to<code>Firewall &gt; Virtual IPs &gt; Settings</code> and click Add. Create a new VIP adres for LAN and WAN interfaces.</p>

<p>WAN VIP on master:
<img src="/img/include/opnsense_carp6.png" alt="Example image" /></p>

<p>LAN VIP on master:
<img src="/img/include/opnsense_carp7.png" alt="Example image" /></p>

<h3 id="change-outbound-nat">Change outbound NAT</h3>

<p>Change the configuration of the outbound NAT to use the shared public IP (the WAN VIP) <br>
Go to <code>Firewall &gt; NAT &gt; Outbound</code> and set the mode to Hybrid Outbound NAT rule generation. <br> <br>
Create a new Outbound rule like this: <br>
<img src="/img/include/opnsense_carp8.png" alt="Example image" /></p>

<p>The translatino / target must be the WANIP IP. <br>
It should end up looking like this: <br></p>

<p><img src="/img/include/opnsense_carp8.png" alt="Example image" /></p>

<p>If you’ll be using your opnsense firewall as a DNS resolver you must change the settings of the DNS service (<code>Services &gt; DNS Resolver &gt; General Settings</code>) to lissen on the LAN VIP address. Then chnage the address of the DNS server in the DHCP configuration to us the LAN VIP adress.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Sonatype Nexus SSO]]></title>
            <link href="https://devopstales.github.io/home/nexus-sso/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/hashicorp-sso/?utm_source=atom_feed" rel="related" type="text/html" title="SSO for hashicorp vault" />
                <link href="https://devopstales.github.io/home/rundeck-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Rundeck SSO" />
                <link href="https://devopstales.github.io/home/mod-auth-openidc/?utm_source=atom_feed" rel="related" type="text/html" title="Apaceh2 oauth plugin" />
                <link href="https://devopstales.github.io/home/openshift-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift SSO authentication" />
                <link href="https://devopstales.github.io/home/openproject-sso/?utm_source=atom_feed" rel="related" type="text/html" title="OpenProject SSO" />
            
                <id>https://devopstales.github.io/home/nexus-sso/</id>
            
            
            <published>2019-05-23T00:00:00+00:00</published>
            <updated>2019-05-23T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Nexus Repository OSS is an artifact repository with universal support for popular formats.</p>

<h3 id="install-nexus">Install Nexus</h3>

<pre><code>cd /opt
wget https://download.sonatype.com/nexus/3/latest-unix.tar.gz
tar xvf latest-unix.tar.gz -C /opt
ln -s /opt/nexus-3.16.1-02/ /opt/nexus

adduser -s /bin/false nexus
chown -R nexus:nexus /opt/nexus
chown -R nexus:nexus /opt/sonatype-work/

echo 'run_as_user=&quot;nexus&quot;' &gt; /opt/nexus/bin/nexus.rc

nano /opt/nexus/bin/nexus
INSTALL4J_JAVA_HOME_OVERRIDE=/usr/lib/jvm/jre-1.8.0
</code></pre>

<h3 id="create-sistemd-serice-for-nexus">Create sistemd serice for Nexus</h3>

<pre><code>echo '[Unit]
Description=nexus service
After=network.target

[Service]
Type=forking
LimitNOFILE=65536
ExecStart=/opt/nexus/bin/nexus start
ExecStop=/opt/nexus/bin/nexus stop
User=nexus
Restart=on-abort

[Install]
WantedBy=multi-user.target' &gt; /etc/systemd/system/nexus.service
</code></pre>

<h3 id="start-nexus">Start Nexus</h3>

<pre><code>sudo systemctl daemon-reload
sudo systemctl enable nexus.service
sudo systemctl start nexus.service

tailf /opt/sonatype-work/nexus3/log/nexus.log
### To check, point your browser to http://localhost:8081. Default username is admin with password admin123.
</code></pre>

<h3 id="install-keycloak-authentication-plugin">Install Keycloak authentication plugin</h3>

<pre><code>NEXUS_PLUGINS=/opt/nexus/system
KEYCLOAK_PLUGIN_VERSION=0.3.3-SNAPSHOT
cd /opt
mkdir -p ${NEXUS_PLUGINS}/org/github/flytreeleft/nexus3-keycloak-plugin/${KEYCLOAK_PLUGIN_VERSION}/
cd ${NEXUS_PLUGINS}/org/github/flytreeleft/nexus3-keycloak-plugin/${KEYCLOAK_PLUGIN_VERSION}/
wget https://github.com/flytreeleft/nexus3-keycloak-plugin/releases/download/${KEYCLOAK_PLUGIN_VERSION}/nexus3-keycloak-plugin-${KEYCLOAK_PLUGIN_VERSION}.jar
chmod 644 ${NEXUS_PLUGINS}/org/github/flytreeleft/nexus3-keycloak-plugin/${KEYCLOAK_PLUGIN_VERSION}/nexus3-keycloak-plugin-${KEYCLOAK_PLUGIN_VERSION}.jar
echo &quot;mvn\\:org.github.flytreeleft/nexus3-keycloak-plugin/${KEYCLOAK_PLUGIN_VERSION} = 200&quot; &gt;&gt; /opt/nexus/etc/karaf/startup.properties
</code></pre>

<p>Login to your Keycloak, and navigate relm &gt; client
<img src="/img/include/nexus_sso1.png" alt="Example image" /><br><br>
Configurate Service Account Roles
<img src="/img/include/nexus_sso2.png" alt="Example image" /><br><br>
Configurate User Roles
<img src="/img/include/nexus_sso4.png" alt="Example image" /><br><br></p>

<pre><code>nano /opt/nexus/etc/keycloak.json
{
  &quot;realm&quot;: &quot;mydomain&quot;,
  &quot;auth-server-url&quot;: &quot;http://nexus.devopstales.intra:8080/auth&quot;,
  &quot;ssl-required&quot;: &quot;external&quot;,
  &quot;resource&quot;: &quot;web&quot;,
  &quot;credentials&quot;: {
    &quot;secret&quot;: &quot;41e39b6b-e23a-4fb1-be21-d30c02941ffc&quot;
  },
  &quot;confidential-port&quot;: 0
}

systemct restart nexus
</code></pre>

<p>After login to nexus you can navigate to the realm administration. Activate the Keycloak Authentication Realm plugin by dragging it to the right hand side.
<img src="/img/include/nexus_sso3.png" alt="Example image" /><br><br>
Mapp the Keycloak roles to nexus
<img src="/img/include/nexus_sso5.png" alt="Example image" /><br><br>
Go to server administration &gt; system &gt; capabilities &gt; add <br>
type: Ruth auth <br>
HTTP Header: X-Proxy-REMOTE-USER <br>
<img src="/img/include/nexus_sso6.png" alt="Example image" /><br><br></p>

<pre><code>yum install mod_auth_openidc httpd mod_ssl -y
nano /etc/httpd/conf.d/nexus-site.conf
ProxyRequests Off
ProxyPreserveHost On

&lt;VirtualHost *:80&gt;
    ServerName nexus.devopstales.intra
     Redirect permanent / https://nexus.devopstales.intra
    ErrorLog /var/log/httpd/error.log
    CustomLog /var/log/httpd/access.log common
&lt;/VirtualHost&gt;

&lt;VirtualHost *:443&gt;
    ServerAdmin webmaster@example.com
    ServerName nexus.devopstales.intra
    ServerAlias www.nexus.devopstales.intra
    DirectoryIndex index.html index.php

    SSLEngine on
    SSLCertificateFile /etc/httpd/ssl/domain.pem
    SSLCertificateKeyFile /etc/httpd/ssl/domain.pem
    SSLCertificateChainFile /etc/httpd/ssl/domain.pem

    AllowEncodedSlashes NoDecode
    AllowEncodedSlashes On
    RequestHeader set X-Forwarded-Proto &quot;https&quot;

    # keycloak
    OIDCProviderMetadataURL https://nexus.devopstales.intra:8443/auth/realms/mydomain/.well-known/openid-configuration
    OIDCSSLValidateServer Off
    OIDCClientID web
    OIDCClientSecret 41e39b6b-e23a-4fb1-be21-d30c02941ffc
    OIDCRedirectURI https://nexus.devopstales.intra/redirect_uri
    OIDCCryptoPassphrase passphrase
    OIDCJWKSRefreshInterval 3600
    OIDCScope &quot;openid email profile&quot;
    # maps the prefered_username claim to the REMOTE_USER environment variable
    OIDCRemoteUserClaim preferred_username

    &lt;Location /&gt;
      AuthType openid-connect
      Require valid-user
      RequestHeader set &quot;X-Proxy-REMOTE-USER&quot; %{REMOTE_USER}s
      ProxyPass http://localhost:8081/ nocanon
      ProxyPassReverse http://localhost:8081/
    &lt;/Location&gt;

    ErrorLog /var/log/httpd/error.log
    CustomLog /var/log/httpd/access.log common
&lt;/VirtualHost&gt;

# secure neus server
nano /opt/nexus/etc/nexus-default.properties
application-host=127.0.0.1
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[SSO for hashicorp vault]]></title>
            <link href="https://devopstales.github.io/home/hashicorp-sso/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/rundeck-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Rundeck SSO" />
                <link href="https://devopstales.github.io/home/mod-auth-openidc/?utm_source=atom_feed" rel="related" type="text/html" title="Apaceh2 oauth plugin" />
                <link href="https://devopstales.github.io/home/openshift-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift SSO authentication" />
                <link href="https://devopstales.github.io/home/openproject-sso/?utm_source=atom_feed" rel="related" type="text/html" title="OpenProject SSO" />
                <link href="https://devopstales.github.io/home/gitlab-keycloak/?utm_source=atom_feed" rel="related" type="text/html" title="SSO login to Gitlab" />
            
                <id>https://devopstales.github.io/home/hashicorp-sso/</id>
            
            
            <published>2019-05-20T00:00:00+00:00</published>
            <updated>2019-05-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I wil shiw you hiw to configure Hashicorp vault with Keycloak for SSO.</p>

<pre><code>vault auth enable oidc

vault write auth/oidc/config \
    oidc_discovery_url=&quot;https://sso.devopstales.intra/auth/realms/mydomain&quot; \
    oidc_client_id=&quot;web&quot; \
    oidc_client_secret=&quot;07d66ebd-1018-46c6-9c88-80aa3d4c2f68&quot; \
    default_role=&quot;reader&quot;
</code></pre>

<pre><code>vault write auth/oidc/role/reader \
        bound_audiences=&quot;web&quot; \
        allowed_redirect_uris=&quot;http://192.168.0.112:8200/ui/vault/auth/oidc/oidc/callback&quot; \
        allowed_redirect_uris=&quot;http://192.168.0.112:8250/oidc/callback&quot; \
        user_claim=&quot;sub&quot; \
        policies=&quot;reader&quot;
</code></pre>

<pre><code>nano reader.hcl
# Read permission on the k/v secrets
path &quot;/secret/*&quot; {
    capabilities = [&quot;read&quot;, &quot;list&quot;]
}

nano manager.hcl
# Manage k/v secrets
path &quot;/secret/*&quot; {
    capabilities = [&quot;create&quot;, &quot;read&quot;, &quot;update&quot;, &quot;delete&quot;, &quot;list&quot;]
}
</code></pre>

<pre><code>vault policy write reader reader.hcl
vault policy write manager manager.hcl

vault policy list
</code></pre>

<p><img src="/img/include/vault-oidc.png" alt="Example image" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install hashicorp vault]]></title>
            <link href="https://devopstales.github.io/home/hashicorp-vault/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/hashicorp-vault/?utm_source=atom_feed" rel="related" type="text/html" title="Install hashicorp vault" />
                <link href="https://devopstales.github.io/home/rundeck-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Rundeck SSO" />
                <link href="https://devopstales.github.io/home/mod-auth-openidc/?utm_source=atom_feed" rel="related" type="text/html" title="Apaceh2 oauth plugin" />
                <link href="https://devopstales.github.io/home/nagios-ncpa/?utm_source=atom_feed" rel="related" type="text/html" title="Nagios Cross Platform Agent" />
                <link href="https://devopstales.github.io/home/nagios-nrpe/?utm_source=atom_feed" rel="related" type="text/html" title="Nagios Remote Plugin Executor" />
            
                <id>https://devopstales.github.io/home/hashicorp-vault/</id>
            
            
            <published>2019-05-17T00:00:00+00:00</published>
            <updated>2019-05-17T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Hashicorp vault is a highly scalable, highly available, environment agnostic way to generate, manage, and store secrets.</p>

<h3 id="dowload-vault">Dowload  Vault</h3>

<pre><code># https://releases.hashicorp.com/vault/
cd /opt
VAULT_VERSION=&quot;1.1.2&quot;
curl -sO https://releases.hashicorp.com/vault/${VAULT_VERSION}/vault_${VAULT_VERSION}_linux_amd64.zip

unzip vault_${VAULT_VERSION}_linux_amd64.zip
mv vault /usr/bin/
vault --version
</code></pre>

<pre><code>vault -autocomplete-install
complete -C /usr/bin/vault vault
mkdir /etc/vault
mkdir -p /var/lib/vault/data

sudo useradd --system --home /etc/vault --shell /bin/false vault
sudo chown -R vault:vault /etc/vault /var/lib/vault/
</code></pre>

<h3 id="configure-vault-systemd-service">Configure Vault systemd service</h3>

<pre><code>nano /etc/systemd/system/vault.service
[Unit]
Description=vault server
Requires=network-online.target
After=network-online.target
ConditionFileNotEmpty=/etc/vault/config.hcl

[Service]
User=vault
Group=vault
Restart=on-failure
ExecStart=/usr/bin/vault server -config=/etc/vault
ExecStop=/usr/bin/vault step-down
#ExecReload=/bin/kill --signal HUP $MAINPID

[Install]
WantedBy=multi-user.target
</code></pre>

<h3 id="create-vault-config">Create vault config</h3>

<pre><code>nano /etc/vault/config.hcl
disable_cache = true
disable_mlock = true
ui = true
listener &quot;tcp&quot; {
   address          = &quot;0.0.0.0:8200&quot;
   tls_disable      = 1
}
storage &quot;file&quot; {
   path  = &quot;/var/lib/vault/data&quot;
 }
api_addr         = &quot;http://0.0.0.0:8200&quot;
max_lease_ttl         = &quot;10h&quot;
default_lease_ttl    = &quot;10h&quot;
cluster_name         = &quot;vault&quot;
raw_storage_endpoint     = true
disable_sealwrap     = true
disable_printable_check = true
</code></pre>

<pre><code>systemctl daemon-reload
systemctl enable --now vault
systemctl status vault
</code></pre>

<h3 id="configurate-client">Configurate Client</h3>

<pre><code>export VAULT_ADDR=http://127.0.0.1:8200
echo &quot;export VAULT_ADDR=http://127.0.0.1:8200&quot; &gt;&gt; ~/.bashrc

sudo rm -rf  /var/lib/vault/data/*
vault operator init &gt; /etc/vault/init.file

cat /etc/vault/init.file | grep &quot;Initial Root Token:&quot;
export VAULT_TOKEN=&quot;s.RcW0LuNIyCoTLWxrDPtUDkCw&quot;

### go to gou and un seal with 3 keys

vault status
</code></pre>

<h3 id="configurate-user-base-authentication">Configurate user-base authentication</h3>

<pre><code>vault auth enable userpass
vault write auth/userpass/users/devopstales \
    password=Password1 \
    policies=admins
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Rundeck SSO]]></title>
            <link href="https://devopstales.github.io/home/rundeck-sso/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/mod-auth-openidc/?utm_source=atom_feed" rel="related" type="text/html" title="Apaceh2 oauth plugin" />
                <link href="https://devopstales.github.io/home/openshift-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift SSO authentication" />
                <link href="https://devopstales.github.io/home/openproject-sso/?utm_source=atom_feed" rel="related" type="text/html" title="OpenProject SSO" />
                <link href="https://devopstales.github.io/home/gitlab-keycloak/?utm_source=atom_feed" rel="related" type="text/html" title="SSO login to Gitlab" />
                <link href="https://devopstales.github.io/home/keycloak1/?utm_source=atom_feed" rel="related" type="text/html" title="Install keycloak with mysql" />
            
                <id>https://devopstales.github.io/home/rundeck-sso/</id>
            
            
            <published>2019-05-14T00:00:00+00:00</published>
            <updated>2019-05-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will use Preauthenticated Mode for Rundeck with mod_auth_openidc and Keycloak</p>

<p>“Preauthenticated” means that the user name and role list are provided to Rundeck from another system, usually a reverse proxy set up “in front” of the Rundeck web application, such as Apache HTTPD.</p>

<h3 id="configurate-mapping-in-keycloak">Configurate mapping in Keycloak</h3>

<p>Login to Keycloak and create client for the app:
<img src="/img/include/gitlab-keycloak1.png" alt="Example image" /></p>

<p>At Mappers create mappers for user information:</p>

<ul>
<li>name: Audience

<ul>
<li>mapper type: Audience</li>
<li>Included Client Audience: web</li>
<li>Add to ID token: on</li>
</ul></li>
<li>name: groups

<ul>
<li>Mapper Type: Group Membership</li>
<li>Token Claim Name: groups</li>
<li>Full group path: off</li>
</ul></li>
</ul>

<h3 id="install-httpd">Install httpd</h3>

<pre><code>yum install mod_auth_openidc httpd php mod_ssl -y
</code></pre>

<h3 id="configurate-rundeck">Configurate Rundeck</h3>

<pre><code>nano /etc/rundeck/rundeck-config.properties
grails.serverURL=https://oauth.devopstales.intra
# Pre Auth mode settings
rundeck.security.authorization.preauthenticated.enabled=true
rundeck.security.authorization.preauthenticated.attributeName=REMOTE_USER_GROUPS
rundeck.security.authorization.preauthenticated.delimiter=;
# Header from which to obtain user name
rundeck.security.authorization.preauthenticated.userNameHeader=X-Forwarded-User
# Header from which to obtain list of roles
rundeck.security.authorization.preauthenticated.userRolesHeader=X-Forwarded-Roles
</code></pre>

<h3 id="create-vhost">Create vhost</h3>

<pre><code>nano /etc/httpd/conf.d/outh-site.conf
# NameVirtualHost *:80
&lt;VirtualHost *:80&gt;
   ServerName oauth.devopstales.intra
   DocumentRoot /var/www/oauth/
   Redirect permanent / https://oauth.devopstales.intra
&lt;/VirtualHost&gt;

&lt;VirtualHost *:443&gt;
    ServerAdmin webmaster@example.com
    ServerName oauth.devopstales.intra
    ServerAlias www.oauth.devopstales.intra
    DocumentRoot /var/www/oauth/
    DirectoryIndex index.html index.php
    ErrorLog /var/log/httpd/oauth-error.log
    CustomLog /var/log/httpd/oauth-access.log combined

    SSLEngine on
    SSLCertificateFile /etc/httpd/ssl/domain.pem
    SSLCertificateKeyFile /etc/httpd/ssl/domain.pem
    SSLCertificateChainFile /etc/httpd/ssl/domain.pem

    # keycloak
    OIDCProviderMetadataURL https://sso.devopstales.intra/auth/realms/mydomain/.well-known/openid-configuration
    OIDCSSLValidateServer Off
    OIDCClientID web
    OIDCClientSecret 6b6c68c3-ad51-4124-ac37-4784ed58797e
    OIDCRedirectURI https://oauth.devopstales.intra/redirect_uri
    OIDCCryptoPassphrase passphrase
    OIDCJWKSRefreshInterval 3600
    OIDCScope &quot;openid email profile&quot;
    # maps the prefered_username claim to the REMOTE_USER environment variable
    OIDCRemoteUserClaim preferred_username

    ProxyPreserveHost On

    &lt;Location /&gt;
        AuthType openid-connect
            Require valid-user
        RequestHeader set &quot;X-Forwarded-User&quot; %{REMOTE_USER}s
        RequestHeader set &quot;X-Forwarded-Roles&quot; %{OIDC_CLAIM_groups}e
        ProxyPass  http://localhost:4440/
        ProxyPassReverse http://localhost:4440/
    &lt;/Location&gt;

&lt;/VirtualHost&gt;
</code></pre>

<h3 id="start-apache">Start apache</h3>

<pre><code>systemct start httpd
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Apaceh2 oauth plugin]]></title>
            <link href="https://devopstales.github.io/home/mod-auth-openidc/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/sso/mod-auth-openidc/?utm_source=atom_feed" rel="related" type="text/html" title="Apaceh2 oauth plugin" />
                <link href="https://devopstales.github.io/home/openshift-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift SSO authentication" />
                <link href="https://devopstales.github.io/home/openproject-sso/?utm_source=atom_feed" rel="related" type="text/html" title="OpenProject SSO" />
                <link href="https://devopstales.github.io/home/gitlab-keycloak/?utm_source=atom_feed" rel="related" type="text/html" title="SSO login to Gitlab" />
                <link href="https://devopstales.github.io/home/keycloak1/?utm_source=atom_feed" rel="related" type="text/html" title="Install keycloak with mysql" />
            
                <id>https://devopstales.github.io/home/mod-auth-openidc/</id>
            
            
            <published>2019-05-13T00:00:00+00:00</published>
            <updated>2019-05-13T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Configure Apache plugin to use Keycloak as a user backend for login with OpenID and SSO.</p>

<p>mod_auth_openidc is an OpenID Connect Relying Party implementation for Apache HTTP Server 2.x</p>

<h3 id="install-the-plugin">Install the plugin</h3>

<pre><code>yum install mod_auth_openidc httpd php mod_ssl -y

mkdir -p /var/www/html/oauth/protected
echo &quot;index&quot; &gt; /var/www/html/oauth/index.htm
</code></pre>

<pre><code>nano /var/www/html/oauth/protected/index.php
&lt;!DOCTYPE html&gt;
&lt;html lang=&quot;en&quot;&gt;

&lt;head&gt;

   &lt;meta charset=&quot;utf-8&quot;&gt;
   &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot;&gt;
   &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot;&gt;
   &lt;meta name=&quot;description&quot; content=&quot;&quot;&gt;
   &lt;meta name=&quot;author&quot; content=&quot;&quot;&gt;

   &lt;title&gt;OpenID Connect: Received Claims&lt;/title&gt;

&lt;/head&gt;

&lt;body&gt;
         &lt;h3&gt;
            Claims sent back from OpenID Connect via the Apache module
         &lt;/h3&gt;
         &lt;br/&gt;


   &lt;!-- OpenAthens attribtues --&gt;
      &lt;?php session_start(); ?&gt;

         &lt;h2&gt;Claims&lt;/h2&gt;
         &lt;br/&gt;
         &lt;div class=&quot;row&quot;&gt;

               &lt;table class=&quot;table&quot; style=&quot;width:80%;&quot; border=&quot;1&quot;&gt;
                 &lt;?php foreach ($_SERVER as $key=&gt;$value): ?&gt;
                    &lt;?php if ( preg_match(&quot;/OIDC_/i&quot;, $key) ): ?&gt;
                       &lt;tr&gt;
                          &lt;td data-toggle=&quot;tooltip&quot; title=&lt;?php echo $key; ?&gt;&gt;&lt;?php echo $key; ?&gt;&lt;/td&gt;
                          &lt;td data-toggle=&quot;tooltip&quot; title=&lt;?php echo $value; ?&gt;&gt;&lt;?php echo $value; ?&gt;&lt;/td&gt;
                       &lt;/tr&gt;
                    &lt;?php endif; ?&gt;
                 &lt;?php endforeach; ?&gt;
               &lt;/table&gt;

&lt;/body&gt;&lt;/html&gt;
</code></pre>

<h3 id="create-vhost">Create vhost</h3>

<pre><code>nano /etc/httpd/conf.d/aouth-site.conf
# NameVirtualHost *:80
&lt;VirtualHost *:80&gt;
   ServerName oauth.devopstales.intra
   DocumentRoot /var/www/oauth/
   Redirect permanent / https://oauth.devopstales.intra
&lt;/VirtualHost&gt;

&lt;VirtualHost *:443&gt;
    ServerAdmin webmaster@example.com
    ServerName oauth.devopstales.intra
    ServerAlias www.oauth.devopstales.intra
    DocumentRoot /var/www/html/oauth/
    DirectoryIndex index.html index.php
    ErrorLog /var/log/httpd/oauth-error.log
    CustomLog /var/log/httpd/oauth-access.log combined

    SSLEngine on
    SSLCertificateFile /etc/httpd/ssl/domain.pem
    SSLCertificateKeyFile /etc/httpd/ssl/domain.pem
    SSLCertificateChainFile /etc/httpd/ssl/domain.pem

    # keycloak server
    OIDCProviderMetadataURL http://sso.devopstales.intra/auth/realms/mydomain/.well-known/openid-configuration
    # for self signed certificate
    OIDCSSLValidateServer Off
    OIDCClientID web
    OIDCClientSecret 5b721a2b-681f-402d-807c-b98c80672c16
    OIDCRedirectURI http://oauth.devopstales.intra/protected/redirect_uri
    OIDCCryptoPassphrase passphrase
    OIDCJWKSRefreshInterval 3600

    &lt;Location /protected/&gt;
       AuthType openid-connect
       Require valid-user
    &lt;/Location&gt;

&lt;/VirtualHost&gt;
</code></pre>

<h3 id="start-apache">Start apache</h3>

<pre><code>systemct start httpd
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Nagios Cross Platform Agent]]></title>
            <link href="https://devopstales.github.io/home/nagios-ncpa/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/nagios-nrpe/?utm_source=atom_feed" rel="related" type="text/html" title="Nagios Remote Plugin Executor" />
                <link href="https://devopstales.github.io/home/centreon-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Centreon on Centos 7" />
                <link href="https://devopstales.github.io/home/naemon-influxdb/?utm_source=atom_feed" rel="related" type="text/html" title="Install Nemon with Influxdb storage" />
                <link href="https://devopstales.github.io/monitoring/nagios-ncpa/?utm_source=atom_feed" rel="related" type="text/html" title="Nagios Cross Platform Agent" />
                <link href="https://devopstales.github.io/home/openshift-hostalreadyclaimed/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Error: HostAlreadyClaimed" />
            
                <id>https://devopstales.github.io/home/nagios-ncpa/</id>
            
            
            <published>2019-05-07T00:00:00+00:00</published>
            <updated>2019-05-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this article I will show you how to add Remote Linux machine and it’s services to Nagios Monitoring host using NCPA agent.</p>

<h3 id="what-is-ncpa">What is NCPA</h3>

<p>NCPA is written in Python and is able to run on almost any Operating System. IT build official binaries for Windows, Mac OS X, and various Linux flavors.</p>

<h3 id="ncpa-client">NCPA Client</h3>

<pre><code>rpm -Uvh https://repo.nagios.com/nagios/7/nagios-repo-7-3.el7.noarch.rpm
yum install ncpa -y

nano /usr/local/ncpa/etc/ncpa.cfg
# [listener]
# allowed_hosts = &lt;nagios host&gt;
[api]
community_string = Password1
[nrdp]
# hostname =
# [nrdp]
# parent =
# token =
[plugin directives]
plugin_path = /usr/lib64/nagios/plugins/

systemctl enable ncpa_listener
systemctl start ncpa_listener
systemctl status ncpa_listener

# https://192.168.0.100:5693/
</code></pre>

<h3 id="ncpa-server">NCPA Server</h3>

<pre><code>cd /tmp
wget https://assets.nagios.com/downloads/ncpa/check_ncpa.tar.gz
tar xvf check_ncpa.tar.gz
chown nagios:nagios check_ncpa.py
chmod 775 check_ncpa.py
mv check_ncpa.py /usr/lib64/nagios/plugins
</code></pre>

<p>Test the commands</p>

<pre><code>/usr/lib64/naemon/plugins/check_ncpa.py -H localhost -t 'Password1' -P 5693 -M system/agent_version
/usr/lib64/naemon/plugins/check_ncpa.py -H localhost -t 'Password1' -P 5693 -M cpu/percent -w 20 -c 40 -q 'aggregate=avg'
/usr/lib64/naemon/plugins/check_ncpa.py -H localhost -t 'Password1' -P 5693 -M memory/virtual -w 50 -c 80 -u G
/usr/lib64/naemon/plugins/check_ncpa.py -H localhost -t 'Password1' -P 5693 -M processes -w 150 -c 200

# Run custom plugin trouth NCPA
/usr/lib64/naemon/plugins/check_ncpa.py -H localhost -t 'Password1' -P 5693 -M plugins/check_users -q args=&quot;-w 5 -c 10&quot;
</code></pre>

<pre><code># store password in macro
nano /etc/nagios/resource.cfg
USER10 = Password1

# createcustom commands for ncpa
nano /etc/nagios/commands.cfg
define command {
    command_name    check_ncpa
    command_line    $USER1$/check_ncpa.py -H $HOSTADDRESS$ -t $USER10$ -P 5693 $ARG1$
}

define command {
    command_name    check_ncpa_cpu
    command_line    $USER1$/check_ncpa.py -H $HOSTADDRESS$ -t $USER10$ -P 5693 cpu/percent -w 20 -c 40 -q 'aggregate=avg'
}
</code></pre>

<pre><code>nano /etc/nagios/conf.d/ncpa-test.cfg
        define host{
        use                    generic-host
        host_name              devopstales
        address                192.168.0.20
}

define service{
        use                     generic-service
        host_name               devopstales
        service_description     NCPA Version
        check_command           check_ncpa|system/agent_version
        }

define service{
        use                     generic-service
        host_name               devopstales
        service_description     CPU Load
        check_command           check_ncpa_cpu
        }
</code></pre>

<h3 id="restart-nagios">Restart nagios</h3>

<pre><code>nagios -v /etc/nagios/nagios.cfg

Total Warnings: 0
Total Errors:   0

service nagios restart
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Nagios Remote Plugin Executor]]></title>
            <link href="https://devopstales.github.io/home/nagios-nrpe/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/centreon-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Centreon on Centos 7" />
                <link href="https://devopstales.github.io/home/naemon-influxdb/?utm_source=atom_feed" rel="related" type="text/html" title="Install Nemon with Influxdb storage" />
                <link href="https://devopstales.github.io/monitoring/nagios-nrpe/?utm_source=atom_feed" rel="related" type="text/html" title="Nagios Remote Plugin Executor" />
                <link href="https://devopstales.github.io/home/openshift-hostalreadyclaimed/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Error: HostAlreadyClaimed" />
                <link href="https://devopstales.github.io/home/k8s-copy-secret/?utm_source=atom_feed" rel="related" type="text/html" title="Copying Kubernetes Secrets Between Namespaces" />
            
                <id>https://devopstales.github.io/home/nagios-nrpe/</id>
            
            
            <published>2019-05-06T00:00:00+00:00</published>
            <updated>2019-05-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this article I will show you how to add Remote Linux machine and it’s services to Nagios Monitoring host using NRPE agent.</p>

<h3 id="what-is-nrpe">What is NRPE</h3>

<p>NRPE allows you to remotely execute Nagios plugins on other Linux/Unix machines.</p>

<p><img src="/img/include/nrpe.png" alt="Example image" /></p>

<h3 id="nrpe-client">NRPE Client</h3>

<pre><code>yum install nrpe nagios-plugins-all
# OR
apt-get install nagios-nrpe-server nagios-plugins
</code></pre>

<h3 id="nrpe-client-config">NRPE Client Config</h3>

<pre><code>nano /etc/nagios/nrpe.cfg
...
only_from = 127.0.0.1 localhost &lt;nagios_ip_address&gt;
...
command[check_users]=/usr/lib64/nagios/plugins/check_users -w 5 -c 10
command[check_load]=/usr/lib64/nagios/plugins/check_load -r -w 8.0,7.5,7.0 -c 11.0,10.0,9.0
command[check_disk]=/usr/lib64/nagios/plugins/check_disk -w 15% -c 10% /
command[check_mem]=/usr/lib64/nagios/plugins/check_mem -w 75% -c 90%
command[check_total_procs]=/usr/lib64/nagios/plugins/check_procs -w 300 -c 400
command[check_swap]=/usr/lib64/nagios/plugins/check_swap -w 10 -c 5
</code></pre>

<h3 id="nrpe-client-logging">NRPE Client Logging</h3>

<pre><code>nano /etc/nagios/nrpe.cfg
log_facility=local1
debug=1

nano  /etc/rsyslog.conf
local1.*                                                /var/log/nrpe.log
</code></pre>

<h3 id="start-nrpe-client">Start NRPE Client</h3>

<pre><code>systemctl start nrpe
systemctl enable nrp

ss -altn | grep 5666
LISTEN   0         5                   0.0.0.0:5666             0.0.0.0:*       
LISTEN   0         5                      [::]:5666                [::]:*

/usr/lib64/nagios/plugins/check_nrpe -H 127.0.0.1 -c check_total_procs
PROCS OK: 105 processes | procs=105;300;400;0;
</code></pre>

<h3 id="nrpe-server">NRPE Server</h3>

<pre><code>yum install nagios-plugins-nrpe
</code></pre>

<pre><code># createcustom commands for nrpe
nano /etc/nagios/commands.cfg
define command {
    command_name check_nrpe
    command_line $USER1$/check_nrpe -H $HOSTADDRESS$ -c $ARG1$
}
</code></pre>

<pre><code>nano /etc/nagios/conf.d/nrpe-test.cfg
        define host{
        use                    generic-host
        host_name              devopstales
        address                192.168.0.20
}


define service{
        use                     generic-service
        host_name               tecmint
        service_description     CPU Load
        check_command           check_nrpe!check_load
        }
</code></pre>

<h3 id="restart-nagios">Restart nagios</h3>

<pre><code>nagios -v /etc/nagios/nagios.cfg

Total Warnings: 0
Total Errors:   0

service nagios restart
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift Error: HostAlreadyClaimed]]></title>
            <link href="https://devopstales.github.io/home/openshift-hostalreadyclaimed/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-gitlabrunner/?utm_source=atom_feed" rel="related" type="text/html" title="Install Gitlab runner on Openshift" />
                <link href="https://devopstales.github.io/home/openshift-extregistry/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: External registry" />
                <link href="https://devopstales.github.io/home/openshift-cert/?utm_source=atom_feed" rel="related" type="text/html" title="Change Certificates in Openshift" />
                <link href="https://devopstales.github.io/home/openshift-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure Openshift vSphere Cloud Provider" />
                <link href="https://devopstales.github.io/home/openshift-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Helm" />
            
                <id>https://devopstales.github.io/home/openshift-hostalreadyclaimed/</id>
            
            
            <published>2019-05-05T00:00:00+00:00</published>
            <updated>2019-05-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>How to solvee Openshift Error: HostAlreadyClaimed</p>

<p>I created a new route for a service and the rout not created. Get this error:</p>

<pre><code>Name:			keycloak-gatekeeper
Namespace:		phpmyadmin
Created:		17 minutes ago
Labels:			app=keycloak-gatekeeper
Annotations:		&lt;none&gt;
Requested Host:		phpmyadmin.devopstales.intra
			  rejected by router router: HostAlreadyClaimed (36 seconds ago)
			    route phpmyadmin/phpmyadmin-phpmyadmin has host phpmyadmin.devopstales.intra
Path:			&lt;none&gt;
TLS Termination:	edge
Insecure Policy:	Redirect
Endpoint Port:		http

Service:	keycloak-gatekeeper
Weight:		100 (100%)
Endpoints:	10.130.2.149:3000
</code></pre>

<p>So I listed all the routes but I dod not found this route. In the end the force delete solved my problem.</p>

<pre><code>oc delete route --grace-period=0 --force=true --ignore-not-found=true -n phpmyadmin phpmyadmin-phpmyadmin
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Katello client]]></title>
            <link href="https://devopstales.github.io/home/katello-client/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/katello-client/</id>
            
            
            <published>2019-05-04T00:00:00+00:00</published>
            <updated>2019-05-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Katello brings the full power of content management alongside the provisioning and configuration capabilities of Foreman. Katello is the upstream community project from which the Red Hat Satellite product is derived after Red Hat Satellite Server 6.</p>

<h3 id="install-subscription-manager">Install subscription manager</h3>

<pre><code>yum install subscription-manager
rpm -ivh http://192.168.0.109/pub/katello-ca-consumer-latest.noarch.rpm
subscription-manager register --org=&quot;mydomain&quot; --activationkey=&quot;el7-key&quot;

subscription-manager repos --list

cd /etc/yum.repos.d/
mv CentOS-* epel* katello-client.repo /tmp/
yum clean all
yum repolist
</code></pre>

<h3 id="install-katello-client">Install Katello client</h3>

<pre><code>yum install katello-agent -y
systemctl start goferd
systemctl enable goferd
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Copying Kubernetes Secrets Between Namespaces]]></title>
            <link href="https://devopstales.github.io/home/k8s-copy-secret/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Helm" />
                <link href="https://devopstales.github.io/home/k8s-helm-rbac/?utm_source=atom_feed" rel="related" type="text/html" title="RBAC permissions for Helm" />
                <link href="https://devopstales.github.io/home/kubectl-multi-cluster-config/?utm_source=atom_feed" rel="related" type="text/html" title="Configure kubectl for multiple clusters" />
                <link href="https://devopstales.github.io/kubernetes/k8s-copy-secret/?utm_source=atom_feed" rel="related" type="text/html" title="Copying Kubernetes Secrets Between Namespaces" />
                <link href="https://devopstales.github.io/home/awx-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install AWX" />
            
                <id>https://devopstales.github.io/home/k8s-copy-secret/</id>
            
            
            <published>2019-05-03T00:00:00+00:00</published>
            <updated>2019-05-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>A simple way of copying common secret data between namespaces</p>

<pre><code>kubectl get secret private-registry --namespace=dev1 --export -o yaml |\
   kubectl apply --namespace=dev2 -f -
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Errata]]></title>
            <link href="https://devopstales.github.io/home/katello-errata/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/katello-errata/</id>
            
            
            <published>2019-05-02T00:00:00+00:00</published>
            <updated>2019-05-02T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Katello brings the full power of content management alongside the provisioning and configuration capabilities of Foreman. Katello is the upstream community project from which the Red Hat Satellite product is derived after Red Hat Satellite Server 6.</p>

<h3 id="errata">Errata</h3>

<pre><code>yum install -y git \
pulp-admin-client \
pulp-rpm-admin-extensions \
pulp-rpm-consumer-extensions \
pulp-rpm-handlers \
pulp-rpm-yumplugins \
pulp-rpm-admin-extensions \
pulp-consumer-client \
python-pulp-agent-lib \
perl-Text-Unidecode \
perl-XML-Simple \
perl-XML-Parser

cd /opt
git clone https://github.com/rdrgmnzs/pulp_centos_errata_import.git
cd ./pulp_centos_errata_import
wget -N https://cefs.steve-meier.de/errata.latest.xml.bz2
bunzip2 ./errata.latest.xml.bz2
mkdir -m0700 ~/.pulp
cat /etc/pki/katello/certs/pulp-client.crt /etc/pki/katello/private/pulp-client.key &gt; ~/.pulp/user-cert.pem
chmod 0400 ~/.pulp/user-cert.pem

# import Errata
perl ./errata_import.pl --errata=errata.latest.xml

pulp-admin repo list | less

add rarrata to rebo by repoid

perl ./errata_import.pl \
--errata=errata.latest.xml \
--include-repo=6356cfe9-766e-4f07-a85e-f287b25395e3 \
--include-repo=2-el7_content-v1_0-6356cfe9-766e-4f07-a85e-f287b25395e3 \
--include-repo=2-el7_content-Library-6356cfe9-766e-4f07-a85e-f287b25395e3 \
--include-repo=2-el7_content-stable-6356cfe9-766e-4f07-a85e-f287b25395e3 \
--include-repo=cb299646-b1a3-4225-9c78-986851a1725f \
--include-repo=2-el7_content-v1_0-cb299646-b1a3-4225-9c78-986851a1725f \
--include-repo=2-el7_content-Library-cb299646-b1a3-4225-9c78-986851a1725f \
--include-repo=2-el7_content-stable-cb299646-b1a3-4225-9c78-986851a1725f
</code></pre>

<h3 id="update-repo">Update repo</h3>

<pre><code>hammer repository synchronize \
--skip-metadata-check true \
--name &quot;base_x86_64&quot; \
--product &quot;el7_repos&quot;

hammer repository synchronize \
--skip-metadata-check true \
--name &quot;updates_x86_64&quot; \
--product &quot;el7_repos&quot;

hammer content-view publish \
--name &quot;el7_content&quot; \
--description &quot;Publishing repositories&quot;

hammer content-view version promote \
--content-view &quot;el7_content&quot; \
--version &quot;4.0&quot; \
--to-lifecycle-environment &quot;stable&quot;
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install AWX]]></title>
            <link href="https://devopstales.github.io/home/awx-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/awx-docker/?utm_source=atom_feed" rel="related" type="text/html" title="Install AWX in docker" />
                <link href="https://devopstales.github.io/linux/awx-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install AWX" />
                <link href="https://devopstales.github.io/linux/awx-docker/?utm_source=atom_feed" rel="related" type="text/html" title="Install AWX in docker" />
                <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
                <link href="https://devopstales.github.io/home/centreon-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Centreon on Centos 7" />
            
                <id>https://devopstales.github.io/home/awx-install/</id>
            
            
            <published>2019-04-30T00:00:00+00:00</published>
            <updated>2019-04-30T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>AWX is an open source web application that provides a user interface, REST API, and task engine for Ansible.</p>

<h3 id="install-postgresql">Install Postgresql</h3>

<pre><code>yum install epel-release -y
yum update -y
rpm -Uvh https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-centos10-10-2.noarch.rpm

yum -y install postgresql10-server postgresql10-contrib postgresql10
/usr/pgsql-10/bin/postgresql-10-setup initdb

sudo systemctl start postgresql-10
sudo systemctl enable postgresql-10

sudo -u postgres createuser -S awx
sudo -u postgres createdb -O awx awx
</code></pre>

<h3 id="install-requirements-and-awx">Install requirements and AWX</h3>

<pre><code># if selinux enabled
yum -y install policycoreutils-python
setsebool -P httpd_can_network_connect 1

# if firewall enabled
firewall-cmd --permanent --add-service=http
firewall-cmd --reload


yum -y install memcached ansible epel-release nginx
systemctl enable memcached
systemctl start memcached

yum install https://github.com/rabbitmq/erlang-rpm/releases/download/v20.1.7.1/erlang-20.1.7.1-1.el7.centos.x86_64.rpm
yum -y install https://dl.bintray.com/rabbitmq/all/rabbitmq-server/3.7.5/rabbitmq-server-3.7.5-1.el7.noarch.rpm
systemctl enable rabbitmq-server
systemctl start rabbitmq-server

cp -p /etc/nginx/nginx.conf{,.org}
wget -O /etc/nginx/nginx.conf https://raw.githubusercontent.com/sunilsankar/awx-build/master/nginx.conf
systemctl start nginx
systemctl enable nginx

yum -y install centos-release-scl centos-release-scl-rh
yum -y install rh-python36 rh-python36-Django rh-python36-django-split-settings \
rh-python36-django-qsstats-magic rh-python36-ansiconv rh-python36-prometheus_client \
rh-python36-python-memcached rh-python36-asn1crypto rh-python36-asgiref \
rh-python36-hyperlink rh-python36-Automat rh-python36-asgi_amqp rh-python36-uwsgi \
rh-python36-msgpack-python rh-python36-msgpack-python-debuginfo rh-python36-jsonpickle \
rh-python36-django-radius rh-python36-python-django-radius rh-python36-python-radius \
rh-python36-future rh-python36-pyrad rh-python36-netaddr rh-python36-tacacs_plus \
rh-python36-python3-saml rh-python36-xmlsec rh-python36-lxml rh-python36-defusedxml \
rh-python36-boto

wget -O /etc/yum.repos.d/ansible-awx.repo https://copr.fedorainfracloud.org/coprs/mrmeee/ansible-awx/repo/epel-7/mrmeee-ansible-awx-epel-7.repo
yum install -y ansible-awx


sudo -u awx scl enable rh-python36 rh-postgresql10 &quot;awx-manage migrate&quot;
echo &quot;from django.contrib.auth.models import User; User.objects.create_superuser('admin', 'root@localhost', 'Password1')&quot; \
| sudo -u awx scl enable rh-python36 rh-postgresql10 &quot;awx-manage shell&quot;

sudo -u awx scl enable rh-python36 rh-postgresql10 &quot;awx-manage create_preload_data&quot;
sudo -u awx scl enable rh-python36 rh-postgresql10 &quot;awx-manage provision_instance --hostname=$(hostname)&quot;
sudo -u awx scl enable rh-python36 rh-postgresql10 &quot;awx-manage register_queue --queuename=tower --hostnames=$(hostname)&quot;
</code></pre>

<h3 id="configure-database">Configure database</h3>

<pre><code>systemctl start awx-cbreceiver
systemctl start awx-dispatcher
systemctl start awx-channels-worker
systemctl start awx-daphne
systemctl start awx-web

systemctl status awx-cbreceiver
systemctl status awx-dispatcher
systemctl status awx-channels-worker
systemctl status awx-daphne
systemctl status awx-web

systemctl enable awx-cbreceiver
systemctl enable awx-dispatcher
systemctl enable awx-channels-worker
systemctl enable awx-daphne
systemctl enable awx-web
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install AWX in docker]]></title>
            <link href="https://devopstales.github.io/home/awx-docker/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/awx-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install AWX" />
                <link href="https://devopstales.github.io/linux/awx-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install AWX" />
                <link href="https://devopstales.github.io/linux/awx-docker/?utm_source=atom_feed" rel="related" type="text/html" title="Install AWX in docker" />
                <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
                <link href="https://devopstales.github.io/home/centreon-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Centreon on Centos 7" />
            
                <id>https://devopstales.github.io/home/awx-docker/</id>
            
            
            <published>2019-04-30T00:00:00+00:00</published>
            <updated>2019-04-30T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>AWX is an open source web application that provides a user interface, REST API, and task engine for Ansible.</p>

<h3 id="install-docker-and-docker-compose">Install Docker and docker-compose</h3>

<pre><code>sudo yum install -y yum-utils \
device-mapper-persistent-data \
lvm2

sudo yum-config-manager \
--add-repo \
https://download.docker.com/linux/centos/docker-ce.repo

sudo yum -y install docker-ce docker-ce-cli containerd.io

service docker start
systemctl enable docker

yum install epel-release
yum install python-pip -y
pip install docker-compose
</code></pre>

<pre><code>yum install git ansible -y

cd /opt
git clone https://github.com/ansible/awx.git
cd awx/installer/

nano inventory
postgres_data_dir=/opt/pgdocker
docker_compose_dir=/opt/awxcompose
pg_username=awx
pg_password=Password1
rabbitmq_password=Password1
admin_user=admin
admin_password=Password1
project_data_dir=/var/lib/awx/projects

ansible-playbook -i inventory install.yml

docker logs awx_task -f
</code></pre>

<p>dockerhub_base=ansible
dockerhub_version=latest</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configurate ipmitool]]></title>
            <link href="https://devopstales.github.io/home/ipmitool-config/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/ipmitool-config/</id>
            
            
            <published>2019-04-29T00:00:00+00:00</published>
            <updated>2019-04-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>The Intelligent Platform Management Interface (IPMI) is a set of computer interface specifications for an autonomous computer subsystem that provides management and monitoring capabilities independently of the host system&rsquo;s CPU, firmware (BIOS or UEFI) and operating system.</p>

<h3 id="ipmitool-on-pfsense">Ipmitool on pfsense</h3>

<pre><code>[2.3.3-RELEASE][root@fw.makz.me]/root: ipmitool
Could not open device at /dev/ipmi0 or /dev/ipmi/0 or /dev/ipmidev/0: No such file or directory

# solution
kldload ipmi
nano /boot/loader.conf
#Load ipmi.ko into the kernel
ipmi_load=&quot;YES&quot;
</code></pre>

<h3 id="ipmitool-on-linux">Ipmitool on Linux</h3>

<pre><code>modprobe ipmi_msghandler
modprobe ipmi_devintf
modprobe ipmi_si

nano /etc/modules
# OR
nano /etc/modprobe.d/*.conf
ipmi_msghandler
ipmi_devintf
ipmi_si
</code></pre>

<h3 id="ipmitool-configuration">Ipmitool Configuration</h3>

<pre><code>ipmitool lan set 1 ipsrc static
ipmitool lan set 1 ipaddr 192.168.0.211
ipmitool lan set 1 netmask 255.255.255.0
ipmitool lan set 1 defgw ipaddr 192.168.0.1
ipmitool lan set 1 arp respond on
ipmitool lan set 1 access on

ipmitool lan print 1
</code></pre>

<h3 id="monitor-ipmi-with-telegraf">Monitor ipmi with telegraf</h3>

<pre><code>nano /etc/telegraf/telegraf.conf
[[inputs.ipmi_sensor]]
        path = &quot;/usr/bin/ipmitool&quot;
        servers = [&quot;username:password@lan(192.168.0.211)&quot;]
        interval = &quot;30s&quot;
        timeout = &quot;20s&quot;
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configure spacewalk 2.9]]></title>
            <link href="https://devopstales.github.io/home/spacewalk-software-channels/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/spacewalk-software-channels/</id>
            
            
            <published>2019-04-29T00:00:00+00:00</published>
            <updated>2019-04-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how to create software channels on spacewalk server.</p>

<p>Login to the spacewalk web interface</p>

<h3 id="get-gpg-key">Get GPG key</h3>

<p>First, we need have an extracted GPG key information, To get the key information download it and extract it using GPG command.</p>

<pre><code>cd /etc/pki/rpm-gpg
wget http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-7
gpg --with-fingerprint RPM-GPG-KEY-CentOS-7
wget https://copr-be.cloud.fedoraproject.org/results/@spacewalkproject/spacewalk-2.9/pubkey.gpg -o RPM-GPG-KEY-spacewalk-2.9
gpg --with-fingerprint RPM-GPG-KEY-spacewalk-2.9

cp /etc/pki/rpm-gpg/RPM-GPG-KEY-* /var/www/html/pub/
</code></pre>

<h3 id="create-software-chanel">Create software chanel</h3>

<p>Channels (top) &gt; Manage Software Channels(Left side pane) &gt; Create Channel(Right side top corner).<br></p>

<p><img src="/img/include/spacewalk_1.png" alt="Example image" /></p>

<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>

<tbody>
<tr>
<td>Channel Name</td>
<td>centos 7 base - x86_64</td>
</tr>

<tr>
<td>Channel Label</td>
<td>centos7-base-x86_64</td>
</tr>

<tr>
<td>Parent Channel</td>
<td>none</td>
</tr>

<tr>
<td>Architecture</td>
<td>x86_64</td>
</tr>

<tr>
<td>Channel Summary</td>
<td>centos7-base-x86_64</td>
</tr>

<tr>
<td>GPG Key URL</td>
<td>file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7</td>
</tr>

<tr>
<td>GPG Key ID</td>
<td>F4A80EB5</td>
</tr>

<tr>
<td>GPG Fingerprint</td>
<td>6341 AB27 53D7 8A78 A7C2 7BB1 24C6 A8A7 F4A8 0EB5</td>
</tr>
</tbody>
</table>

<p><img src="/img/include/spacewalk_2.png" alt="Example image" /></p>

<h3 id="create-repositories">Create repositories</h3>

<p>Channels (Top) &gt; Manage Software Channels (Left side pane) &gt; Manage Repositories (Left side pane) &gt; Create Repository(Right side top corner).</p>

<table>
<thead>
<tr>
<th>Repository Label:</th>
<th>centos7_base_x86_64_repo</th>
</tr>
</thead>

<tbody>
<tr>
<td>Repository URL:</td>
<td><a href="http://mirror.centos.org/centos/7/os/x86_64/">http://mirror.centos.org/centos/7/os/x86_64/</a></td>
</tr>

<tr>
<td>Repository Type:</td>
<td>yum</td>
</tr>
</tbody>
</table>

<table>
<thead>
<tr>
<th>Repository Label:</th>
<th>centos7_update_x86_64_repo</th>
</tr>
</thead>

<tbody>
<tr>
<td>Repository URL:</td>
<td><a href="http://mirror.centos.org/centos/7/updates/x86_64/">http://mirror.centos.org/centos/7/updates/x86_64/</a></td>
</tr>

<tr>
<td>Repository Type:</td>
<td>yum</td>
</tr>
</tbody>
</table>

<table>
<thead>
<tr>
<th>Repository Label:</th>
<th>centos7_extra_x86_64_repo</th>
</tr>
</thead>

<tbody>
<tr>
<td>Repository URL:</td>
<td><a href="http://mirror.centos.org/centos/7/extras/x86_64/">http://mirror.centos.org/centos/7/extras/x86_64/</a></td>
</tr>

<tr>
<td>Repository Type:</td>
<td>yum</td>
</tr>
</tbody>
</table>

<table>
<thead>
<tr>
<th>Repository Label:</th>
<th>centos7_opnstk_x86_64_repo</th>
</tr>
</thead>

<tbody>
<tr>
<td>Repository URL:</td>
<td><a href="http://mirror.centos.org/centos/7/cloud/x86_64/">http://mirror.centos.org/centos/7/cloud/x86_64/</a></td>
</tr>

<tr>
<td>Repository Type:</td>
<td>yum</td>
</tr>
</tbody>
</table>

<h3 id="adding-repository-to-channel">Adding Repository to Channel</h3>

<p>Channels (Top) &gt; Manage Software Channels (Left side pane) &gt; Centos 7 Base x86_64  &gt; Repositories (Tab)  &gt; centos7_base_x86_64_repo (Check box)  &gt; Update Repositories (Bottom right corner). <br></p>

<p><img src="/img/include/spacewalk_3.png" alt="Example image" /></p>

<h3 id="creating-activation-key">Creating activation Key</h3>

<p>System  &gt; ( Top menu) Activation Keys (Left Side pane)  &gt; Create Key (Right side top corner) &gt; Fill description <br></p>

<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>

<tbody>
<tr>
<td>Description:</td>
<td>CentOS Linux 7 x86_64</td>
</tr>

<tr>
<td>Key:</td>
<td>centoslinux7-x86_64</td>
</tr>

<tr>
<td>Usage:</td>
<td></td>
</tr>

<tr>
<td>Base channels:</td>
<td>Centos 7 Base - x86_64</td>
</tr>

<tr>
<td>Add-On Entitlements:</td>
<td>Choose all available feature you about to use.</td>
</tr>
</tbody>
</table>

<p><img src="/img/include/spacewalk_4.png" alt="Example image" />
<img src="/img/include/spacewalk_5.png" alt="Example image" /></p>

<h3 id="start-syncing-repositories">Start Syncing repositories</h3>

<pre><code>spacewalk-repo-sync --channel centos-7-base-x86_64 --type yum
tail -f /var/log/rhn/reposync/centos-7-base-x86_64.log

df -hP /var/satellite
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Google Authenticator on pfSense]]></title>
            <link href="https://devopstales.github.io/home/pfsense-2fa/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/pfsense-2fa/</id>
            
            
            <published>2019-04-29T00:00:00+00:00</published>
            <updated>2019-04-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>This article explains how to set up OpenVPN with Google Authenticator on pfSense.</p>

<h3 id="set-up-the-freeradius">Set up the FreeRADIUS</h3>

<ul>
<li>Go to  <code>System &gt; Package Manager &gt; Available Packages</code> and install <code>FreeRADIUS</code> package.</li>
<li><code>Services &gt; FreeRADIUS &gt; Interfaces &gt; Add</code></li>
</ul>

<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>

<tbody>
<tr>
<td>Interface IP Address</td>
<td>127.0.0.1</td>
</tr>

<tr>
<td>Port</td>
<td>1812</td>
</tr>

<tr>
<td>Interface Type</td>
<td>Authentication</td>
</tr>

<tr>
<td>IP Version</td>
<td>IPv4</td>
</tr>

<tr>
<td>Description</td>
<td>Authentication</td>
</tr>
</tbody>
</table>

<p><img src="/img/include/pfsense_2fa_1.png" alt="Example image" /></p>

<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>

<tbody>
<tr>
<td>Interface IP Address</td>
<td>127.0.0.1</td>
</tr>

<tr>
<td>Port</td>
<td>1813</td>
</tr>

<tr>
<td>Interface Type</td>
<td>Authentication</td>
</tr>

<tr>
<td>IP Version</td>
<td>IPv4</td>
</tr>

<tr>
<td>Description</td>
<td>Accounting</td>
</tr>
</tbody>
</table>

<p><img src="/img/include/pfsense_2fa_2.png" alt="Example image" /></p>

<h3 id="add-a-nas-client">Add a NAS client</h3>

<ul>
<li><code>Services &gt; FreeRADIUS &gt; NAS/Clients &gt; Add</code></li>
</ul>

<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>

<tbody>
<tr>
<td>Client IP Address</td>
<td>127.0.0.1</td>
</tr>

<tr>
<td>Client IP Version</td>
<td>IPv4</td>
</tr>

<tr>
<td>Client Shortname</td>
<td>pfsenselocal</td>
</tr>

<tr>
<td>Client Shared Secret</td>
<td>Password1</td>
</tr>

<tr>
<td>Client Protocol</td>
<td>UDP</td>
</tr>

<tr>
<td>Client Type</td>
<td>other</td>
</tr>

<tr>
<td>Require Message Authenticator</td>
<td>No</td>
</tr>

<tr>
<td>Max Connections</td>
<td>16</td>
</tr>

<tr>
<td>Description</td>
<td>pfsenselocal</td>
</tr>
</tbody>
</table>

<p><img src="/img/include/pfsense_2fa_3.png" alt="Example image" /></p>

<h3 id="add-an-authentication-server-ro-pfsense">Add an authentication server ro pfSense</h3>

<ul>
<li><code>System &gt; User Manager &gt; Authentication Servers &gt; Add</code></li>
</ul>

<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>

<tbody>
<tr>
<td>Descriptive Name</td>
<td>localfreeradius</td>
</tr>

<tr>
<td>Type</td>
<td>RADIUS</td>
</tr>

<tr>
<td>Protocol</td>
<td>PAP</td>
</tr>

<tr>
<td>Hostname or IP address</td>
<td>127.0.0.1</td>
</tr>

<tr>
<td>Shared Secret</td>
<td>Password1</td>
</tr>

<tr>
<td>Services offered</td>
<td>Authentication and Accounting</td>
</tr>

<tr>
<td>Authentiocation port</td>
<td>1812</td>
</tr>

<tr>
<td>Accounting port</td>
<td>1813</td>
</tr>

<tr>
<td>Authentication Timeout</td>
<td>5</td>
</tr>

<tr>
<td>RADIUS NAS IP Attribute</td>
<td>LAN</td>
</tr>
</tbody>
</table>

<p><img src="/img/include/pfsense_2fa_4.png" alt="Example image" /></p>

<h3 id="configurate-otp-for-users">Configurate OTP for Users</h3>

<ul>
<li><code>Services &gt; FreeRADIUS &gt; Users &gt; Add</code></li>
</ul>

<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>

<tbody>
<tr>
<td>Username</td>
<td>tester</td>
</tr>

<tr>
<td>Password</td>
<td></td>
</tr>

<tr>
<td>Password Encryption</td>
<td>Cleartext-Password</td>
</tr>

<tr>
<td>One-Time Password</td>
<td>Enable One-Time Password (OTP) for this user</td>
</tr>

<tr>
<td>OTP Auth Method</td>
<td>Google-Authenticator</td>
</tr>

<tr>
<td>Init-Secret</td>
<td>click Generator OTP Secret</td>
</tr>

<tr>
<td>PIN</td>
<td>enter 4-8 numbers and remember them.</td>
</tr>

<tr>
<td>QR Code</td>
<td>click Generate QR Code.</td>
</tr>
</tbody>
</table>

<p>At this point open Google Authenticator on your phone and scan the QRCODE.</p>

<p><img src="/img/include/pfsense_2fa_5.png" alt="Example image" /></p>

<p>You can use One-Time Password (OTP) only for local FreeRadius users. FreeRadius users from diferent backenl like mysql or ldap did not work.</p>

<h3 id="configurate-openvpn">Configurate openvpn</h3>

<ul>
<li>Go to <code>VPN &gt; OpenVPN &gt; Servers &gt; Edit</code></li>
<li>Select localfreeradius for Backend for authentication</li>
</ul>

<p><img src="/img/include/pfsense_2fa_6.png" alt="Example image" /></p>

<ul>
<li>In the OpenVPN Server configuration, under <code>Advanced Configuration &gt; Custom options</code></li>
<li>add: <code>reneg-sec 0</code></li>
</ul>

<p>If you connect your OpenVPN client you must enter your username and the PIN + the Google Authenticator one-time code as your password. <br> If PIN is 1234 and the Google Authenticator code is 445 745 then the password is: 1234445745</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Katello]]></title>
            <link href="https://devopstales.github.io/home/katello-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/katello-install/</id>
            
            
            <published>2019-04-29T00:00:00+00:00</published>
            <updated>2019-04-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Katello brings the full power of content management alongside the provisioning and configuration capabilities of Foreman. Katello is the upstream community project from which the Red Hat Satellite product is derived after Red Hat Satellite Server 6.</p>

<h3 id="base-komponents">Base komponents</h3>

<ul>
<li>Foreman: provisioning on new clients.</li>
<li>Pulp: patch and content (package repository) management.</li>
<li>Candlepin: subscription and entitlement management.</li>
<li>Puppet: configuration management (actual running of modules assigned in Foreman).</li>
<li>Katello: unified workflow and WebUI for content (Pulp) and subscriptions (Candlepin).</li>
</ul>

<h3 id="hardware-requirements">Hardware Requirements</h3>

<ul>
<li>Two Logical CPUs</li>
<li>8 GB of memory (12 GB highly recommended)</li>
<li>The filesystem holding /var/lib/pulp needs to be large</li>
</ul>

<h3 id="required-repositories">Required Repositories</h3>

<pre><code># hostnevet beállítani !!!

yum -y localinstall https://fedorapeople.org/groups/katello/releases/yum/3.11/katello/el7/x86_64/katello-repos-latest.rpm
yum -y localinstall https://yum.theforeman.org/releases/1.21/el7/x86_64/foreman-release.rpm
yum -y localinstall https://yum.puppetlabs.com/puppetlabs-release-pc1-el-7.noarch.rpm
yum -y localinstall https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
</code></pre>

<h3 id="installation">Installation</h3>

<pre><code class="language-bash">yum -y install foreman-release-scl python-django
yum -y update
yum -y install katello


foreman-installer \
--scenario &quot;katello&quot; \
--foreman-initial-organization &quot;mydomain&quot; \
--foreman-initial-location &quot;office&quot; \
--enable-foreman-plugin-ansible \
--enable-foreman-proxy-plugin-ansible \
--enable-foreman-plugin-remote-execution \
--enable-foreman-proxy-plugin-remote-execution-ssh

# reset/gen Password
foreman-rake permissions:reset
</code></pre>

<h3 id="configure-hammer-cli">Configure hammer-cli</h3>

<pre><code>nano ~/.hammer/cli.modules.d/foreman.yml
:foreman:
 :host: 'https://katello.devopstales.intra/'
 :username: 'admin'
 :password: '**********'

hammer defaults add --param-name organization --param-value &quot;mydomain&quot;
hammer defaults add --param-name location --param-value &quot;office&quot;
hammer defaults list
</code></pre>

<h3 id="configure-gpg-keys">Configure gpg keys</h3>

<pre><code>hammer product create \
--name &quot;el7_repos&quot; \
--description &quot;Various repositories to use with CentOS 7&quot;

mkdir /etc/pki/rpm-gpg/import/
cd /etc/pki/rpm-gpg/import/
wget https://repo.mysql.com/RPM-GPG-KEY-mysql
wget http://mirror.centos.org/centos/7/os/x86_64/RPM-GPG-KEY-CentOS-7
wget https://archive.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-7Server
wget https://rpms.remirepo.net/RPM-GPG-KEY-remi
wget https://packages.cisofy.com/keys/cisofy-software-rpms-public.key

hammer gpg create \
--key &quot;RPM-GPG-KEY-CentOS-7&quot; \
--name &quot;RPM-GPG-KEY-CentOS-7&quot;

hammer gpg create \
--key &quot;RPM-GPG-KEY-mysql&quot; \
--name &quot;RPM-GPG-KEY-mysql&quot;

hammer gpg create \
--key &quot;RPM-GPG-KEY-EPEL-7Server&quot; \
--name &quot;RPM-GPG-KEY-EPEL-7Server&quot;

hammer gpg create \
--key &quot;RPM-GPG-KEY-remi&quot; \
--name &quot;RPM-GPG-KEY-remi&quot;

hammer gpg create \
--key &quot;cisofy-software-rpms-public.key&quot; \
--name &quot;RPM-GPG-KEY-cisofy&quot;
</code></pre>

<h3 id="create-yum-repositories">Create yum repositories</h3>

<pre><code>hammer gpg list

hammer repository create \
--product &quot;el7_repos&quot; \
--name &quot;base_x86_64&quot; \
--label &quot;base_x86_64&quot; \
--content-type &quot;yum&quot; \
--download-policy &quot;on_demand&quot; \
--gpg-key &quot;RPM-GPG-KEY-CentOS-7&quot; \
--url &quot;http://mirror.centos.org/centos/7/os/x86_64/&quot; \
--mirror-on-sync &quot;no&quot;

hammer repository create \
--product &quot;el7_repos&quot; \
--name &quot;extras_x86_64&quot; \
--label &quot;extras_x86_64&quot; \
--content-type &quot;yum&quot; \
--download-policy &quot;on_demand&quot; \
--gpg-key &quot;RPM-GPG-KEY-CentOS-7&quot; \
--url &quot;http://mirror.centos.org/centos/7/extras/x86_64/&quot; \
--mirror-on-sync &quot;no&quot;

hammer repository create \
--product &quot;el7_repos&quot; \
--name &quot;updates_x86_64&quot; \
--label &quot;updates_x86_64&quot; \
--content-type &quot;yum&quot; \
--download-policy &quot;on_demand&quot; \
--gpg-key &quot;RPM-GPG-KEY-CentOS-7&quot; \
--url &quot;http://mirror.centos.org/centos/7/updates/x86_64/&quot; \
--mirror-on-sync &quot;no&quot;

hammer repository create \
--product &quot;el7_repos&quot; \
--name &quot;epel_x86_64&quot; \
--label &quot;epel_x86_64&quot; \
--content-type &quot;yum&quot; \
--download-policy &quot;on_demand&quot; \
--gpg-key &quot;RPM-GPG-KEY-EPEL-7Server&quot; \
--url &quot;https://dl.fedoraproject.org/pub/epel/7Server/x86_64/&quot;

hammer repository create \
--product &quot;el7_repos&quot; \
--name &quot;lynis&quot; \
--label &quot;lynis&quot; \
--content-type &quot;yum&quot; \
--download-policy &quot;on_demand&quot; \
--gpg-key &quot;RPM-GPG-KEY-cisofy&quot; \
--url &quot;https://packages.cisofy.com/community/lynis/rpm/&quot;

hammer repository create \
--product &quot;el7_repos&quot; \
--name &quot;mysql_57_x86_64&quot; \
--label &quot;mysql_57_x86_64&quot; \
--content-type &quot;yum&quot; \
--download-policy &quot;on_demand&quot; \
--gpg-key &quot;RPM-GPG-KEY-mysql&quot; \
--url &quot;https://repo.mysql.com/yum/mysql-5.7-community/el/7/x86_64/&quot;

hammer repository create \
--product &quot;el7_repos&quot; \
--name &quot;katello_agent_x86_64&quot; \
--label &quot;katello_agent_x86_64&quot; \
--content-type &quot;yum&quot; \
--download-policy &quot;on_demand&quot; \
--url &quot;https://fedorapeople.org/groups/katello/releases/yum/latest/client/el7/x86_64/&quot;

hammer repository create \
--product &quot;el7_repos&quot; \
--name &quot;remi_php_56_x86_64&quot; \
--label &quot;remi_php_56_x86_64&quot; \
--content-type &quot;yum&quot; \
--download-policy &quot;on_demand&quot; \
--gpg-key &quot;RPM-GPG-KEY-remi&quot; \
--url &quot;https://mirrors.ukfast.co.uk/sites/remi/enterprise/7/php56/x86_64/&quot;

hammer repository create \
--product &quot;el7_repos&quot; \
--name &quot;remi_php_72_x86_64&quot; \
--label &quot;remi_php_72_x86_64&quot; \
--content-type &quot;yum&quot; \
--download-policy &quot;on_demand&quot; \
--gpg-key &quot;RPM-GPG-KEY-remi&quot; \
--url &quot;https://mirrors.ukfast.co.uk/sites/remi/enterprise/7/php72/x86_64/&quot;

hammer repository create \
--product &quot;el7_repos&quot; \
--name &quot;remi_safe_x86_64&quot; \
--label &quot;remi_safe_x86_64&quot; \
--content-type &quot;yum&quot; \
--download-policy &quot;on_demand&quot; \
--gpg-key &quot;RPM-GPG-KEY-remi&quot; \
--url &quot;https://mirrors.ukfast.co.uk/sites/remi/enterprise/7/safe/x86_64/&quot;
</code></pre>

<h3 id="sync-repos">Sync repos</h3>

<pre><code>hammer repository list

for i in $(seq 1 12); do \
hammer repository synchronize \
--product &quot;el7_repos&quot; \
--id &quot;$i&quot;; \
done

# Create a Content View
hammer content-view create \
--name &quot;el7_content&quot; \
--description &quot;Content view for CentOS 7&quot;

hammer product list

# Add Repositories to Content View
for i in $(seq 1 12); do \
hammer content-view add-repository \
--name &quot;el7_content&quot; \
--product &quot;el7_repos&quot; \
--repository-id &quot;$i&quot;; \
done

# Create a Lifecycle Environment
hammer lifecycle-environment create \
--name &quot;stable&quot; \
--label &quot;stable&quot; \
--prior &quot;Library&quot;

hammer lifecycle-environment list

# Publish a Content View
hammer content-view publish \
--name &quot;el7_content&quot; \
--description &quot;Publishing repositories&quot;

hammer content-view version list

# Promote Version to Lifecycle Environment
hammer content-view version promote \
--content-view &quot;el7_content&quot; \
--version &quot;1.0&quot; \
--to-lifecycle-environment &quot;stable&quot;

hammer content-view version list

# Create an Activation Key
hammer activation-key create \
--name &quot;el7-key&quot; \
--description &quot;Key to use with CentOS7&quot; \
--lifecycle-environment &quot;stable&quot; \
--content-view &quot;el7_content&quot; \
--unlimited-hosts

hammer activation-key list

# Add Subscription to Activation Key
hammer subscription list

hammer activation-key add-subscription \
--name &quot;el7-key&quot; \
--quantity &quot;1&quot; \
--subscription-id &quot;1&quot;

# Backup Katello Configuration
foreman-maintain backup snapshot -y /mnt/backup/
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install privacyIDEA]]></title>
            <link href="https://devopstales.github.io/home/privacyidea-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/privacyidea-install/</id>
            
            
            <published>2019-04-29T00:00:00+00:00</published>
            <updated>2019-04-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>privacyIDEA is a Two Factor Authentication System which is multi-tenency- and multi-instance-capable. It is opensource, written in Python and hosted at GitHub.</p>

<h3 id="configure-privacyidea-repo">Configure privacyidea repo</h3>

<pre><code># base os: Debian 9

apt update
apt install dirmngr -y

nano /etc/apt/sources.list.d/privacyidea.list
deb http://lancelot.netknights.it/community/xenial/stable xenial main

wget https://lancelot.netknights.it/NetKnights-Release.asc
apt-key add NetKnights-Release.asc
</code></pre>

<h3 id="install-privacyidea">Install privacyidea</h3>

<pre><code>apt update
apt install privacyidea-apache2

ln -s /etc/apache2/mods-available/rewrite.load /etc/apache2/mods-enabled/rewrite.load

nano nano sites-enabled/privacyidea.conf
# enable 80 to 443 redirection
systemctl restart apache2
</code></pre>

<h3 id="create-admin-user">Create admin user</h3>

<pre><code>pi-manage admin add admin -e admin@localhost
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install spacewalk 2.9]]></title>
            <link href="https://devopstales.github.io/home/spacewalk-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/spacewalk-install/</id>
            
            
            <published>2019-04-29T00:00:00+00:00</published>
            <updated>2019-04-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Spacewalk is an open source Linux systems management solution. Spacewalk is the upstream community project from which the Red Hat Satellite product is derived before Red Hat Satellite Server 6.</p>

<h3 id="create-answerfile">Create answerfile</h3>

<pre><code>cat &gt; /root/spacewalk_answers.txt &lt;&lt; EOF
admin-email = operation@devopstales.intra
ssl-set-cnames = spacewalk
ssl-set-org = Spacewalk Org
ssl-set-org-unit = spacewalk
ssl-set-city = Budapest
ssl-set-state = non
ssl-set-country = HU
ssl-password = Password1
ssl-set-email = operation@devopstales.intra
ssl-config-sslvhost = Y
enable-tftp=Y
EOF
</code></pre>

<h3 id="install-requirements">Install requirements</h3>

<pre><code>yum install ntp -y
service ntpd restart


rpm -Uvh https://copr-be.cloud.fedoraproject.org/results/@spacewalkproject/spacewalk-2.9/epel-7-x86_64/00830557-spacewalk-repo/spacewalk-repo-2.9-4.el7.noarch.rpm

rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm

yum clean all &amp;&amp; yum repolist
</code></pre>

<h3 id="install-spacewalk">Install Spacewalk</h3>

<pre><code>yum install -y spacewalk-setup-postgresql
yum install -y spacewalk-postgresql
spacewalk-setup --answer-file=/root/spacewalk_answers.txt
yum install spacecmd -y
</code></pre>

<p>Go to the WebUI and configure the admin user.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Update ILO firmware]]></title>
            <link href="https://devopstales.github.io/home/update-ilo/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/update-ilo/</id>
            
            
            <published>2019-04-29T00:00:00+00:00</published>
            <updated>2019-04-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Integrated Lights-Out, or iLO, is a proprietary embedded server management technology by Hewlett-Packard which provides out-of-band management facilities.</p>

<h3 id="update-from-linux-cli">Update from Linux CLI</h3>

<p>Download iLO firmware say CP032487.scexe</p>

<pre><code>cd /tmp
chnmod 755 CP032487.scexe
./CP032487.scexe
curl http:///xmldata?item=All
</code></pre>

<h3 id="update-from-ilo-cli">Update from ILO CLI</h3>

<pre><code>./CP032487.scexe --unpack=firmware
cd firmware
cp firmware/ilo2_NNN.bin $DocumentRoot (usually /var/www/html/)

# Login on iLO over ssh and upload firmware
ssh -l Administrator

iLO&gt; show
iLO&gt; cd /map1
iLO&gt; oemhp_ping
iLO&gt; load -source http:///firmware/ilo2_NNN.bin

curl http:///xmldata?item=All
</code></pre>

<h3 id="update-from-ilo-webui">Update from iLO WebUI</h3>

<ul>
<li>access iLO Web UI, go to Administration tab and upload bin file to upgrade iLO firmware</li>
</ul>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Proxmox Mail Gateway]]></title>
            <link href="https://devopstales.github.io/home/proxmox-mail-gateway/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/proxmox-mail-gateway/</id>
            
            
            <published>2019-04-28T00:00:00+00:00</published>
            <updated>2019-04-28T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Proxmox Mail Gateway is a full featured, open-source mail proxy and protects your mail server from spam, viruses, trojans and phishing emails.</p>

<h3 id="configurate-cluster">Configurate Cluster</h3>

<pre><code>pmg1.devopstales.intra 192.168.0.27
pmg2.devopstales.intra 192.168.0.28
</code></pre>

<p>After the base installation login the web interfate:</p>

<ul>
<li>192.168.0.27:8006</li>
<li>192.168.0.28:8006</li>
</ul>

<p>At Configuration &gt; Cluster create a new cluster
<img src="/img/include/pmg_1.png" alt="Example image" />
<img src="/img/include/pmg_2.png" alt="Example image" /></p>

<p>Copy the cluster info:
<img src="/img/include/pmg_3.png" alt="Example image" /></p>

<p>On the other host (pmg2) go to the same menu and click Join
<img src="/img/include/pmg_4.png" alt="Example image" />
Add the datat copyd from the master node (pmg1)
<img src="/img/include/pmg_5.png" alt="Example image" /></p>

<h3 id="basic-configuration">Basic Configuration</h3>

<p>On the master node&rsquo;s (pmg1) weg interface go to Configuration &gt; Mail Proxy
<img src="/img/include/pmg_6.png" alt="Example image" />
Edit the Default Relay and add your interbal mailservers ip:
<img src="/img/include/pmg_7.png" alt="Example image" />
On the pmg2 check the config is replicated</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Centreon on Centos 7]]></title>
            <link href="https://devopstales.github.io/home/centreon-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/naemon-influxdb/?utm_source=atom_feed" rel="related" type="text/html" title="Install Nemon with Influxdb storage" />
                <link href="https://devopstales.github.io/monitoring/centreon-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Centreon on Centos 7" />
                <link href="https://devopstales.github.io/home/pfsense-usg/?utm_source=atom_feed" rel="related" type="text/html" title="Pfsese USG S2S VPN" />
                <link href="https://devopstales.github.io/home/openshift-gitlabrunner/?utm_source=atom_feed" rel="related" type="text/html" title="Install Gitlab runner on Openshift" />
                <link href="https://devopstales.github.io/home/openshift-extregistry/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: External registry" />
            
                <id>https://devopstales.github.io/home/centreon-install/</id>
            
            
            <published>2019-04-27T00:00:00+00:00</published>
            <updated>2019-04-27T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Centreon is a free Open Source monitoring software which allows an administrator to easily configure alerts based on thresholds, generate email alerts, add systems to be monitored quickly without the need of configuring complicated configuration files.</p>

<h3 id="install-requisites">Install requisites</h3>

<pre><code>selinuxenabled &amp;&amp; echo enabled || echo disabled

nano /etc/selinux/config
SELINUX=disabled

yum install -y epel-release mariadb-server
mkdir /etc/systemd/system/mariadb.service.d/
echo -ne &quot;[Service]\nLimitNOFILE=32000\n&quot; | tee /etc/systemd/system/mariadb.service.d/limits.conf

systemctl daemon-reload
systemctl enable mariadb
systemctl start mariadb
systemctl status mariadb

yum install centos-release-scl
</code></pre>

<h3 id="install-centreon">Install Centreon</h3>

<pre><code>cd /opt
wget http://yum.centreon.com/standard/19.04/el7/stable/noarch/RPMS/centreon-release-19.04-1.el7.centos.noarch.rpm
yum install --nogpgcheck centreon-release-19.04-1.el7.centos.noarch.rpm
yum install -y centreon-base-config-centreon-engine centreon

echo &quot;date.timezone = Europe/Budapest&quot; &gt; /etc/opt/rh/rh-php71/php.d/php-timezone.ini
systemctl restart rh-php71-php-fpm

systemctl enable httpd24-httpd
systemctl enable snmpd
systemctl enable snmptrapd
systemctl enable rh-php71-php-fpm
systemctl enable centcore
systemctl enable centreontrapd
systemctl enable cbd
systemctl enable centengine
systemctl enable centreon

systemctl start rh-php71-php-fpm
systemctl start httpd24-httpd
systemctl start mysqld
systemctl start cbd
systemctl start snmpd
systemctl start snmptrapd
</code></pre>

<h3 id="configurate-centreon">Configurate centreon</h3>

<p><img src="/img/include/centreon_1.png" alt="Example image" />
<img src="/img/include/centreon_2.png" alt="Example image" />
<img src="/img/include/centreon_3.png" alt="Example image" />
<img src="/img/include/centreon_4.png" alt="Example image" />
<img src="/img/include/centreon_5.png" alt="Example image" />
<img src="/img/include/centreon_6.png" alt="Example image" />
<img src="/img/include/centreon_7.png" alt="Example image" />
<img src="/img/include/centreon_8.png" alt="Example image" />
<img src="/img/include/centreon_9.png" alt="Example image" /></p>

<pre><code>systemctl start cbd
systemctl start centcore
systemctl start centreontrapd
yum install centreon-widget* -y
</code></pre>

<p><img src="/img/include/centreon_10.png" alt="Example image" />
Select Central and Click Export Configuration. <br>
Then the poller will ativated.
<img src="/img/include/centreon_11.png" alt="Example image" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Pfsese USG S2S VPN]]></title>
            <link href="https://devopstales.github.io/home/pfsense-usg/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/pfsense-usg/?utm_source=atom_feed" rel="related" type="text/html" title="Pfsese USG S2S VPN" />
                <link href="https://devopstales.github.io/home/pfsense-telegraf/?utm_source=atom_feed" rel="related" type="text/html" title="Install telegraf on pfsense" />
                <link href="https://devopstales.github.io/home/centreon-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Centreon on Centos 7" />
                <link href="https://devopstales.github.io/home/openshift-gitlabrunner/?utm_source=atom_feed" rel="related" type="text/html" title="Install Gitlab runner on Openshift" />
                <link href="https://devopstales.github.io/home/openshift-extregistry/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: External registry" />
            
                <id>https://devopstales.github.io/home/pfsense-usg/</id>
            
            
            <published>2019-04-27T00:00:00+00:00</published>
            <updated>2019-04-27T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I will show you how to create a site-to-site VPN for pfSense and unifi usg.</p>

<h3 id="creating-a-new-ipsec-vpn-on-pfsense">Creating a new IPsec VPN on pfsense</h3>

<p>At <code>VPN &gt; IPsec &gt; Add</code><br></p>

<p><img src="/img/include/usg-pfsense-1.png" alt="Example image" />
<img src="/img/include/usg-pfsense-2.png" alt="Example image" />
<img src="/img/include/usg-pfsense-3.png" alt="Example image" /></p>

<p>At <code>Firewall &gt; Roles &gt; IPsec &gt; Add</code><br>
<img src="/img/include/usg-pfsense-4.png" alt="Example image" /></p>

<h3 id="configure-usg">Configure USG</h3>

<p><img src="/img/include/usg-pfsense-5.png" alt="Example image" />
<img src="/img/include/usg-pfsense-6.png" alt="Example image" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Gitlab runner on Openshift]]></title>
            <link href="https://devopstales.github.io/home/openshift-gitlabrunner/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/openshift-gitlabrunner/?utm_source=atom_feed" rel="related" type="text/html" title="Install Gitlab runner on Openshift" />
                <link href="https://devopstales.github.io/home/openshift-extregistry/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: External registry" />
                <link href="https://devopstales.github.io/home/openshift-cert/?utm_source=atom_feed" rel="related" type="text/html" title="Change Certificates in Openshift" />
                <link href="https://devopstales.github.io/home/openshift-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure Openshift vSphere Cloud Provider" />
                <link href="https://devopstales.github.io/home/openshift-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Helm" />
            
                <id>https://devopstales.github.io/home/openshift-gitlabrunner/</id>
            
            
            <published>2019-04-20T00:00:00+00:00</published>
            <updated>2019-04-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will configure a gtlab rubber for Openshift.</p>

<h3 id="creating-a-service-account">Creating a Service Account</h3>

<pre><code>oc new-project gitlab-rubber
oc create sa gitlab-ci
oc policy add-role-to-user edit system:serviceaccount:gitlab-rubber:gitlab-ci

oc get sa
NAME         SECRETS   AGE
builder      2         2d
default      2         2d
deployer     2         2d
gitlab-ci    2         2d

oc describe sa gitlab-ci
Name:           gitlab-ci
Namespace:      constellation
Labels:         &lt;none&gt;
Annotations:    &lt;none&gt;

Image pull secrets:     gitlab-ci-dockercfg-q5mj9

Mountable secrets:      gitlab-ci-token-gvvkv
                        gitlab-ci-dockercfg-q5mj9

Tokens:                 gitlab-ci-token-gvvkv
                        gitlab-ci-token-tfsf7

oc describe secret gitlab-ci-token-gvvkv
...
token:          eyJ...&lt;very-long-token&gt;...-cw

oc login --token=eyJ...&lt;very-long-token&gt;...-cw
</code></pre>

<h3 id="edit-gitlab-ci-config">Edit Gitlab-ci config</h3>

<pre><code>nano  .gitlab-ci.yml
image: ebits/openshift-client

stages:
  - deployToOpenShift

variables:
  OPENSHIFT_SERVER: https://master.openshift.devopstales.intra:443
  OPENSHIFT_DOMAIN: openshift.devopstales.intra
  # Configure this variable in Secure Variables:
  OPENSHIFT_TOKEN: eyJ...&lt;very-long-token&gt;...-cw

.deploy: &amp;deploy
  before_script:
    - oc login &quot;$OPENSHIFT_SERVER&quot; --token=&quot;$OPENSHIFT_TOKEN&quot; --insecure-skip-tls-verify
  # login with the service account
    - oc project &quot;slides-openshift&quot;
  # enter into our slides project on OpenShift
  script:
    - &quot;oc get services $APP 2&gt; /dev/null || oc new-app . --name=$APP&quot;
  # create a new application from the image in the OpenShift registry
    - &quot;oc start-build $APP --from-dir=. --follow || sleep 3s&quot;
  # start a new build
    - &quot;oc get routes $APP 2&gt; /dev/null || oc expose service $APP --hostname=$APP_HOST&quot;
  # expose our application

develop:
  &lt;&lt;: *deploy
  stage: deployToOpenShift
  tags:
    - docker
  variables:
    APP: slides-openshift
    APP_HOST: demo-slides.$OPENSHIFT_DOMAIN
  environment:
    name: develop
    url: http://demo-slides.$OPENSHIFT_DOMAIN
  except:
    - master
</code></pre>

<h3 id="create-a-kubernetes-runner-in-openshift-from-template">Create a kubernetes runner in Openshift from template:</h3>

<pre><code>wget https://raw.githubusercontent.com/devopstales/openshift-examples/master/template/gitlab-runner-template.yml
oc deploy gitlab-runner-template.yml
</code></pre>

<p>Deploy the template from the gui:</p>

<pre><code>oc adm policy add-scc-to-user privileged system:serviceaccount:gitlab-rubber:&lt;application-name&gt;-user
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift: External registry]]></title>
            <link href="https://devopstales.github.io/home/openshift-extregistry/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-cert/?utm_source=atom_feed" rel="related" type="text/html" title="Change Certificates in Openshift" />
                <link href="https://devopstales.github.io/home/openshift-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure Openshift vSphere Cloud Provider" />
                <link href="https://devopstales.github.io/home/openshift-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Helm" />
                <link href="https://devopstales.github.io/home/openshift-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/openshift-ldap/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift LDAP authentication" />
            
                <id>https://devopstales.github.io/home/openshift-extregistry/</id>
            
            
            <published>2019-04-19T00:00:00+00:00</published>
            <updated>2019-04-19T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I this post I will demonstrate howyou can use an external registry in Openshift.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<h3 id="configuring-openshift">Configuring Openshift</h3>

<p>From your client machine, create a Kubernetes secret object for Harbor.:</p>

<pre><code>oc new-proyect registrytest

oc create secret docker-registry harbor \
--docker-server=https://harbor.devopstales.intra \
--docker-username=admin \
--docker-email=admin@devopstales.intra \
--docker-password='[your_admin_harbor_password]'
</code></pre>

<p>If you want you can add this secret to the deafult template of the project creation.</p>

<h3 id="deploy-the-private-image-on-the-openshift-cluster">Deploy the private image on the Openshift cluster</h3>

<pre><code>nano registrytest-deployment.yaml
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  labels:
    run: nginx
  name: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      run: nginx
  template:
    metadata:
      labels:
        run: nginx
    spec:
      containers:
      - image: harbor.devopstales.intra/test/nginx:V2
        name: nginx
      imagePullSecrets:
      - name: harbor

oc apply -f kuard-deployment.yaml
oc get pods
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install vMWare Harbor]]></title>
            <link href="https://devopstales.github.io/home/harbor-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/harbor-install/</id>
            
            
            <published>2019-04-18T00:00:00+00:00</published>
            <updated>2019-04-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Vmware harbor ia an open source trusted cloud native registry project that stores, signs, and scans content.</p>

<p>Why harbor? Opeshift and Gitlab has its own docker regytry but nether can intgrate with clair Vulnerability scanner.</p>

<h3 id="install-docker-and-docker-compose">Install Docker and Docker-Compose</h3>

<pre><code>yum install epel-release wget -y
yum install -y yum-utils
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum install -y docker-ce

sudo yum install -y python-pip
pip install docker-compose

sudo systemctl start docker
sudo systemctl enable docker
</code></pre>

<h3 id="generate-your-own-ssl-certificate">Generate your own SSL certificate</h3>

<pre><code>nano certgen.sh
#!/bin/sh

export PASSPHRASE=$(head -c 500 /dev/urandom | tr -dc a-z0-9A-Z | head -c 128; echo)
DOMAIN=devopstales.intra

subj=&quot;
C=HU
ST=Pest
O=My Company
localityName=Budapest
commonName=*.$DOMAIN
organizationalUnitName=OU
emailAddress=root@$DOMAIN
&quot;

openssl genrsa -des3 -out domain.key -passout env:PASSPHRASE 2048

openssl req \
    -new \
    -batch \
    -subj &quot;$(echo -n &quot;$subj&quot; | tr &quot;\n&quot; &quot;/&quot;)&quot; \
    -key domain.key \
    -out domain.csr \
    -passin env:PASSPHRASE

cp domain.key domain.key.org

openssl rsa -in domain.key.org -out domain.key -passin env:PASSPHRASE

openssl x509 -req -days 3650 -in domain.csr -signkey domain.key -out domain.crt
cat domain.crt domain.key &gt; domain.pem
</code></pre>

<pre><code>chmod +x certgen.sh
./certgen.sh

mkdir -p /etc/docker/certs.d/harbor.devopstales.intra
cp domain.crt domain.key /etc/docker/certs.d/harbor.devopstales.intra/
cp domain.crt /etc/docker/certs.d/harbor.devopstales.intra/domain.cert
sudo systemctl restart docker
</code></pre>

<h3 id="install-notary">Install notary</h3>

<pre><code>curl -L https://github.com/theupdateframework/notary/releases/download/v0.6.1/notary-$(uname -s)-amd64 -o /usr/local/bin/notary
chmod +x /usr/local/bin/notary

mkdir -p ~/.docker/tls/harbor.devopstales.intra:4443/
cp ~/domain.crt ~/.docker/tls/harbor.devopstales.intra:4443/
cp ~/domain.key ~/.docker/tls/harbor.devopstales.intra:4443/
cp ~/domain.crt ~/.docker/tls/harbor.devopstales.intra:4443/domain.cert
</code></pre>

<h3 id="install-harbor">Install Harbor</h3>

<pre><code># https://github.com/vmware/harbor/releases/
wget https://storage.googleapis.com/harbor-releases/release-1.7.0/harbor-online-installer-v1.7.5.tgz
tar -xzf harbor-online-installer-v1.7.5.tgz

cd harbor
nano harbor.cfg
hostname = harbor.devopstales.intra
ui_url_protocol = https
ssl_cert = /root/domain.crt
ssl_cert_key = /root/domain.key

./prepare
./install.sh --with-notary --with-clair

docker login harbor.devopstales.intra
</code></pre>

<p>Access the Harbor UI with the username &ldquo;admin&rdquo; and password &ldquo;Harbor12345&rdquo;
<img src="/img/include/harbor_1.png" alt="Example image" /></p>

<p>Create a nwe project.
<img src="/img/include/harbor_2.png" alt="Example image" /></p>

<p>Configure automatic Vulnerability scan for project.
<img src="/img/include/harbor_3.png" alt="Example image" /></p>

<pre><code>docker pull nginx
docker tag nginx:latest harbor.devopstales.intra/test/nginx:V1
docker push harbor.devopstales.intra/test/nginx:V1
</code></pre>

<p><img src="/img/include/harbor_4.png" alt="Example image" /></p>

<pre><code>docker tag nginx:latest harbor.devopstales.intra/test/nginx:V2
export DOCKER_CONTENT_TRUST_SERVER=https://harbor.devopstales.intra:4443
export DOCKER_CONTENT_TRUST=1
docker push harbor.devopstales.intra/test/nginx:V2
</code></pre>

<p><img src="/img/include/harbor_5.png" alt="Example image" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Change Certificates in Openshift]]></title>
            <link href="https://devopstales.github.io/home/openshift-cert/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure Openshift vSphere Cloud Provider" />
                <link href="https://devopstales.github.io/home/openshift-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Helm" />
                <link href="https://devopstales.github.io/home/openshift-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/openshift-ldap/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift LDAP authentication" />
                <link href="https://devopstales.github.io/kubernetes/openshift-cert/?utm_source=atom_feed" rel="related" type="text/html" title="Change Certificates in Openshift" />
            
                <id>https://devopstales.github.io/home/openshift-cert/</id>
            
            
            <published>2019-04-17T00:00:00+00:00</published>
            <updated>2019-04-17T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how can you chnage certificate in Openshift.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<h3 id="configure-certs">Configure certs:</h3>

<p>If you want to configure your Openshift cluster to use your own certificate you can do that wit this configuration.<br>
In my case the certificate files is MyCert.crt MyCert.key and the root CA is ccca.pem.</p>

<pre><code>nano /ec/ansible/hosts
openshift_master_overwrite_named_certificates=true
openshift_hosted_router_certificate={&quot;certfile&quot;: &quot;/root/cert/MyCert.crt&quot;, &quot;keyfile&quot;: &quot;/root/cert/MyCert.key&quot;, &quot;cafile&quot;: &quot;/root/cert/ccca.pem&quot;}
openshift_master_named_certificates=[{&quot;names&quot;: [&quot;master.openshit.devopstales.intra&quot;],&quot;certfile&quot;: &quot;/root/cert/MyCert.crt&quot;, &quot;keyfile&quot;: &quot;/root/cert/MyCert.key&quot;, &quot;cafile&quot;: &quot;/root/cert/ccca.pem&quot;}]

openshift_redeploy_openshift_ca=true
openshift_certificate_expiry_fail_on_warn=false

# registry
openshift_hosted_registry_routecertificates={&quot;certfile&quot;: &quot;/root/cert/MyCert.crt&quot;, &quot;keyfile&quot;: &quot;/root/cert/MyCert.key&quot;, &quot;cafile&quot;: &quot;/root/cert/ccca.pem&quot;}
openshift_hosted_registry_routetermination=reencrypt
</code></pre>

<h3 id="run-the-installer">Run the Installer</h3>

<p>If your certificate is renewd you can cahge the certificate in the cluster with this playbooks.</p>

<pre><code>oc get csr | grep Pending | awk '{print $1}' | xargs oc adm certificate approve

ansible-playbook -i hosts /usr/share/ansible/openshift-ansible/playbooks/redeploy-certificates.yml

ansible-playbook -i hosts /usr/share/ansible/openshift-ansible/playbooks/openshift-master/redeploy-openshift-ca.yml
ansible-playbook -i hosts /usr/share/ansible/openshift-ansible/playbooks/openshift-etcd/redeploy-ca.yml
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configure Openshift vSphere Cloud Provider]]></title>
            <link href="https://devopstales.github.io/home/openshift-vmware/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Helm" />
                <link href="https://devopstales.github.io/home/openshift-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/openshift-ldap/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift LDAP authentication" />
                <link href="https://devopstales.github.io/kubernetes/openshift-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure Openshift vSphere Cloud Provider" />
                <link href="https://devopstales.github.io/kubernetes/openshift-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Helm" />
            
                <id>https://devopstales.github.io/home/openshift-vmware/</id>
            
            
            <published>2019-04-16T00:00:00+00:00</published>
            <updated>2019-04-16T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how can you use vmware for persistent storagi on Openshift.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<h3 id="vsphere-configuration">vSphere Configuration</h3>

<ul>
<li>Create a folder for all the VMs in vCenter

<ul>
<li>In the navigator, select the data center</li>
<li>Right-click and select the menu option to create the folder.</li>
<li>Select All vCenter Actions &gt; New VM and Template Folder.</li>
<li>Move Openshift vms to this folder</li>
</ul></li>
<li>The name of the virtual machine must match the name of the nodes for the OpenShift cluster.</li>
</ul>

<p><img src="/img/include/k8s-vmware.png" alt="Example image" /></p>

<h3 id="set-up-the-govc-environment">Set up the GOVC environment:</h3>

<pre><code># on deployer
curl -LO https://github.com/vmware/govmomi/releases/download/v0.20.0/govc_linux_amd64.gz
gunzip govc_linux_amd64.gz
chmod +x govc_linux_amd64
cp govc_linux_amd64 /usr/bin/govc
echo &quot;export GOVC_URL='vCenter IP OR FQDN'&quot; &gt;&gt; /etc/profile
echo &quot;export GOVC_USERNAME='vCenter User'&quot; &gt;&gt; /etc/profile
echo &quot;export GOVC_PASSWORD='vCenter Password'&quot; &gt;&gt; /etc/profile
echo &quot;export GOVC_INSECURE=1&quot; &gt;&gt; /etc/profile
source /etc/profile
</code></pre>

<p>Add <code>disk.enableUUID=1</code> for all VM:</p>

<pre><code>govc vm.info &lt;vm&gt;
govc ls /Datacenter/kubernetes/&lt;vm-folder-name&gt;
# example:
govc ls /Datacenter/kubernetes/okd-01

govc vm.change -e=&quot;disk.enableUUID=1&quot; -vm='VM Path'
# example:
govc vm.change -e=&quot;disk.enableUUID=1&quot; -vm='/datacenter/kubernetes/okd-01/okd-m01'
</code></pre>

<p>VM Hardware should be at version 15 or higher. Upgrade if needed:</p>

<pre><code>govc vm.option.info '/datacenter/kubernetes/okd-01/okd-m01' | grep HwVersion

govc vm.upgrade -version=15 -vm '/datacenter/kubernetes/okd-01/okd-m01'
</code></pre>

<h3 id="create-the-required-roles">Create the required Roles</h3>

<ul>
<li>Navigate in the vSphere Client - Menu &gt; Administration &gt; Roles</li>
<li>Add a new Role and select the permissions required. Repeat for each role.</li>
</ul>

<table>
<thead>
<tr>
<th align="center">Roles</th>
<th align="center">Privileges</th>
<th align="center">Entities</th>
<th align="center">Propagate to Children</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">vcp-manage-okd-node-vms</td>
<td align="center">Resource.AssignVMToPoolVirtualMachine.Config.AddExistingDisk, VirtualMachine.Config.AddNewDisk, VirtualMachine.Config.AddRemoveDevice, VirtualMachine.Config.RemoveDisk, VirtualMachine.Config.SettingsVirtualMachine.Inventory.Create, VirtualMachine.Inventory.Delete</td>
<td align="center">Cluster, Hosts, VM Folder</td>
<td align="center">Yes</td>
</tr>

<tr>
<td align="center">vcp-manage-okd-volumes</td>
<td align="center">Datastore.AllocateSpace, Datastore.FileManagement (Low level file operations)</td>
<td align="center">Datastore</td>
<td align="center">No</td>
</tr>

<tr>
<td align="center">vcp-view-okd-spbm-profile</td>
<td align="center">StorageProfile.View (Profile-driven storage view)</td>
<td align="center">vCenter</td>
<td align="center">No</td>
</tr>

<tr>
<td align="center">Read-only (pre-existing default role)</td>
<td align="center">System.Anonymous, System.Read, System.View</td>
<td align="center">Datacenter, Datastore Cluster, Datastore Storage Folder</td>
<td align="center">No</td>
</tr>
</tbody>
</table>

<h3 id="create-a-service-account">Create a service account</h3>

<ul>
<li>Create a vsphere user, or add a domain user, to provide access and assign the new roles to.</li>
</ul>

<h3 id="configure-ansible-installer">Configure ansible installer</h3>

<pre><code>nano /etc/hosts
openshift_master_dynamic_provisioning_enabled=true
openshift_cloudprovider_kind=vsphere
openshift_cloudprovider_vsphere_username=&lt;vCenter User&gt;
openshift_cloudprovider_vsphere_password=&lt;vCenter Password&gt;
openshift_cloudprovider_vsphere_host=&lt;vCenter IP OR FQDN&gt;
openshift_cloudprovider_vsphere_datacenter=&lt;Datacenter&gt;
openshift_cloudprovider_vsphere_datastore=&lt;Datastore&gt;
openshift_cloudprovider_vsphere_folder=&lt;vm-folder-name&gt;
</code></pre>

<h3 id="add-providerid">Add providerID</h3>

<pre><code>nano openshift-vmware-pacher.sh
DATACENTER='&lt;Datacenter&gt;'
FOLDER='&lt;vm-folder-name&gt;'
for vm in $(govc ls /$DATACENTER/vm/$FOLDER ); do
  MACHINE_INFO=$(govc vm.info -json -dc=$DATACENTER -vm.ipath=&quot;$vm&quot; -e=true)
  # My VMs are created on vmware with upper case names, so I need to edit the names with awk
  VM_NAME=$(jq -r ' .VirtualMachines[] | .Name' &lt;&lt;&lt; $MACHINE_INFO | awk '{print tolower($0)}')
  # UUIDs come in lowercase, upper case then
  VM_UUID=$( jq -r ' .VirtualMachines[] | .Config.Uuid' &lt;&lt;&lt; $MACHINE_INFO | awk '{print toupper($0)}')
  echo &quot;Patching $VM_NAME with UUID:$VM_UUID&quot;
  # This is done using dry-run to avoid possible mistakes, remove when you are confident you got everything right.
  kubectl patch node $VM_NAME -p &quot;{\&quot;spec\&quot;:{\&quot;providerID\&quot;:\&quot;vsphere://$VM_UUID\&quot;}}&quot;
done

chmod +x openshift-vmware-pacher.sh
./openshift-vmware-pacher.sh
</code></pre>

<h3 id="run-the-installer">Run the Installer</h3>

<pre><code># deployer
cd /usr/share/ansible/openshift-ansible/
sudo ansible-playbook -i inventory/hosts.localhost playbooks/prerequisites.yml
sudo ansible-playbook -i inventory/hosts.localhost playbooks/deploy_cluster.yml

# If installastion failed or went wrong, the following uninstallation script can be run, and running installation can be tried again:
sudo ansible-playbook -i inventory/hosts.localhost playbooks/adhoc/uninstall.yml
</code></pre>

<h3 id="create-vsphere-storage-class">Create vSphere storage-class</h3>

<pre><code>nano vmware-sc.yml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: &quot;true&quot;
  name: &quot;vsphere-standard&quot;
provisioner: kubernetes.io/vsphere-volume
parameters:
    diskformat: zeroedthick
    datastore: &quot;NFS&quot;
reclaimPolicy: Delete

oc aplay -f vmware-sc.yml
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift Helm]]></title>
            <link href="https://devopstales.github.io/home/openshift-helm/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/openshift-ldap/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift LDAP authentication" />
                <link href="https://devopstales.github.io/kubernetes/openshift-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Helm" />
                <link href="https://devopstales.github.io/kubernetes/openshift-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/kubernetes/openshift-ldap/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift LDAP authentication" />
            
                <id>https://devopstales.github.io/home/openshift-helm/</id>
            
            
            <published>2019-04-15T00:00:00+00:00</published>
            <updated>2019-04-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I this post I will demonstrate the basic configuration of Helm on Openshift.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<h3 id="helm">Helm</h3>

<p>Helm is a package manager and teplating engine for Kubernetes. It based on tree main components:</p>

<ul>
<li>the helm cli client</li>
<li>the helm server called tiller</li>
<li>the template pcakage called halm chart</li>
</ul>

<h3 id="install-helm-cli">Install helm cli</h3>

<pre><code># https://github.com/helm/helm/releases
curl -s https://storage.googleapis.com/kubernetes-helm/helm-v2.9.1-linux-amd64.tar.gz | tar xz
cd linux-amd64
cp helm /usr/bin
</code></pre>

<h3 id="helm-with-cluster-admin-permissions">Helm with cluster-admin permissions</h3>

<pre><code>nano helm-cluster-admin.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller-admin
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tiller-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: tiller-admin
  namespace: kube-system
</code></pre>

<h3 id="init-helm">Init helm</h3>

<pre><code>oc login master.openshift.devopstales.intra:443
kubectl apply -f helm-cluster-admin.yaml
helm init --service-account tiller-admin
</code></pre>

<h3 id="test-hem">Test hem</h3>

<pre><code>oc new-project myapp
helm install stable/ghost -n blog

oc get pods -n myapp
export APP_HOST=$(kubectl get svc --namespace myapp blog-ghost --template &quot;{{ range (index .status.loadBalancer.ingress 0) }}{{ . }}{{ end }}&quot;)
export APP_PASSWORD=$(kubectl get secret --namespace myapp blog-ghost -o jsonpath=&quot;{.data.ghost-password}&quot; | base64 --decode)
export APP_DATABASE_PASSWORD=$(kubectl get secret --namespace myapp blog-mariadb -o jsonpath=&quot;{.data.mariadb-password}&quot; | base64 --decode)
helm upgrade blog stable/ghost --set service.type=LoadBalancer,ghostHost=$APP_HOST,ghostPassword=$APP_PASSWORD,mariadb.db.password=$APP_DATABASE_PASSWORD

oc get pods -n myapp
echo Password: $(kubectl get secret --namespace myapp blog-ghost -o jsonpath=&quot;{.data.ghost-password}&quot; | base64 --decode)
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Pfsese https]]></title>
            <link href="https://devopstales.github.io/home/pfsense-cert/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/pfsense-cert/</id>
            
            
            <published>2019-04-15T00:00:00+00:00</published>
            <updated>2019-04-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I will show you how to Enable SSL for pfSense.</p>

<h3 id="creating-a-new-certificate">Creating a new Certificate</h3>

<p>At <code>System &gt; Certificate Manager &gt; Certificates &gt; Add</code><br>
Make sure you choose &ldquo;Import an existing Certificate&rdquo; under Method and enter Descriptive name so you know what the certificate is.
<img src="/img/include/pfsense_cert_1.png" alt="Example image" /></p>

<p>At System &gt; Advanced &gt; Admin Access<br>
Make sure HTTPS is selected as Protocol and now change the SSL Certificate to the one you have created. Scroll down and click on Save. Now, when you restart your Web Browser, you should see a Secure Connection to pfSense when accessing it.
<img src="/img/include/pfsense_cert_2.png" alt="Example image" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[RBAC permissions for Helm]]></title>
            <link href="https://devopstales.github.io/home/k8s-helm-rbac/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/kubectl-multi-cluster-config/?utm_source=atom_feed" rel="related" type="text/html" title="Configure kubectl for multiple clusters" />
                <link href="https://devopstales.github.io/home/openshift-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift SSO authentication" />
                <link href="https://devopstales.github.io/home/openshift-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/openshift-ldap/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift LDAP authentication" />
                <link href="https://devopstales.github.io/home/openproject-sso/?utm_source=atom_feed" rel="related" type="text/html" title="OpenProject SSO" />
            
                <id>https://devopstales.github.io/home/k8s-helm-rbac/</id>
            
            
            <published>2019-04-14T00:00:00+00:00</published>
            <updated>2019-04-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I this post I will demonstrate the basic mechanism of helm and Role-based access control (RBAC).</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../kubernetes/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../kubernetes/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../kubernetes/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part2: <a href="../../kubernetes/k8s-metallb-bgp-pfsense/">Intall metal-lb with BGP</a></li>
<li>Part3: <a href="../../kubernetes/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-contour-ingress --></li>
<li>Part4: <a href="../../kubernetes/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../kubernetes/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../kubernetes/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../kubernetes/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../kubernetes/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a></li>
<li>Part5e: <a href="../../kubernetes/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a></li>
<li>Part5f: <a href="../../kubernetes/kk8s-vmware/">vSphere persistent storage for K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes netwoking caliko vs ... --></li>
<li>Part6a: <a href="../../kubernetes/k8s-ipvs/">Install k8s with IPVS mode</a></li>
<li>Part6b: <a href="../../kubernetes/k8s-calico-ebpf/">Install k8s with IPVS mode</a></li>
<li>Part7: <a href="../../kubernetes/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part8: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part9: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part10: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part11: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part12: <a href="../../kubernetes/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- TICSK stack monitoring -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<p>I whant to use helm on Openshift but firt I startid with the basics of helm and Role-based access control (RBAC) on a simple Kubernestes cluster. Most people seem to be running Helm with their own credentials or a dedicated service account with cluster-admin permissions. This isn’t very good from a security perspective, especially so if it’s being run within CI/CD.</p>

<h3 id="helm">Helm</h3>

<p>Helm is a package manager and teplating engine for Kubernetes. It based on tree main components:</p>

<ul>
<li>the helm cli client</li>
<li>the helm server called tiller</li>
<li>the template pcakage called halm chart</li>
</ul>

<h3 id="helm-with-cluster-admin-permissions">Helm with cluster-admin permissions</h3>

<pre><code>nano helm-cluster-admin.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller-admin
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tiller-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: tiller-admin
  namespace: kube-system
</code></pre>

<h3 id="helm-with-namespace-permissions">Helm with namespace permissions</h3>

<p>We are granting permissions on only the API groups and resources that Tiller needs to deploy and manage releases in its namespace.</p>

<pre><code>nano helm-dev-namespace.yaml
kind: Namespace
apiVersion: v1
metadata:
  name: dev
---
kind: ServiceAccount
apiVersion: v1
metadata:
  name: tiller
  namespace: dev
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: tiller-manager
  namespace: dev
rules:
- apiGroups: [&quot;&quot;, &quot;batch&quot;, &quot;extensions&quot;, &quot;apps&quot;]
  resources: [&quot;*&quot;]
  verbs: [&quot;*&quot;]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: tiller-binding
  namespace: dev
subjects:
- kind: ServiceAccount
  name: tiller
  namespace: dev
roleRef:
  kind: Role
  name: tiller-manager
  apiGroup: rbac.authorization.k8s.io
</code></pre>

<pre><code>nano helm-prod-namespace.yaml
kind: Namespace
apiVersion: v1
metadata:
  name: prod
---
kind: ServiceAccount
apiVersion: v1
metadata:
  name: tiller
  namespace: prod
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: tiller-manager
  namespace: prod
rules:
- apiGroups: [&quot;&quot;, &quot;batch&quot;, &quot;extensions&quot;, &quot;apps&quot;]
  resources: [&quot;*&quot;]
  verbs: [&quot;*&quot;]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: tiller-binding
  namespace: prod
subjects:
- kind: ServiceAccount
  name: tiller
  namespace: prod
roleRef:
  kind: Role
  name: tiller-manager
  apiGroup: rbac.authorization.k8s.io
</code></pre>

<pre><code>kubectl create -f helm-dev-namespace.yaml
kubectl create -f helm-prod-namespace.yaml

kubectl -n dev get sa
kubectl -n prod get sa

helm init --service-account tiller --tiller-namespace dev
helm init --service-account tiller --tiller-namespace prod
</code></pre>

<h3 id="helm-with-minimal-cluster-permissions">Helm with minimal cluster permissions</h3>

<pre><code>nano helm-cluster-role.yml
kind: Namespace
apiVersion: v1
metadata:
  name: helm
---
kind: ServiceAccount
apiVersion: v1
metadata:
  name: helm
  namespace: helm
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: helm-clusterrole
rules:
  - apiGroups: [&quot;&quot;]
    resources: [&quot;pods/portforward&quot;]
    verbs: [&quot;create&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;pods&quot;]
    verbs: [&quot;list&quot;, &quot;get&quot;]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: helm-clusterrolebinding
roleRef:
  kind: ClusterRole
  apiGroup: rbac.authorization.k8s.io
  name: helm-clusterrole
subjects:
  - kind: ServiceAccount
    name: helm
    namespace: helm
</code></pre>

<h3 id="generate-a-kubeconfig-file-from-the-helm-service-account">Generate a Kubeconfig file from the Helm Service Account</h3>

<p>Credit to Ami Mahloof for this script.</p>

<pre><code>NAMESPACE=helm
# Find the secret associated with the Service Account
SECRET=$(kubectl -n $NAMESPACE get sa helm -o jsonpath='{.secrets[].name}')
# Get the token from the secret
TOKEN=$(kubectl get secrets -n $NAMESPACE $SECRET -o jsonpath='{.data.token}' | base64 -D)
# Get the CA from the secret
kubectl get secrets -n $NAMESPACE $SECRET -o jsonpath='{.data.ca\.crt}' | base64 -D &gt; ca.crt

CONTEXT=$(kubectl config current-context)
CLUSTER_NAME=$(kubectl config get-contexts $CONTEXT --no-headers=true | awk '{print $3}')
SERVER=$(kubectl config view -o jsonpath=&quot;{.clusters[?(@.name == \&quot;${CLUSTER_NAME}\&quot;)].cluster.server}&quot;)
KUBECONFIG_FILE=config
USER=helm
CA=ca.crt

# Set up config
kubectl config set-cluster $CLUSTER_NAME \
--kubeconfig=$KUBECONFIG_FILE \
--server=$SERVER \
--certificate-authority=$CA \
--embed-certs=true

kubectl config set-credentials $USER \
--kubeconfig=$KUBECONFIG_FILE \
--token=$TOKEN

kubectl config set-context $USER \
--kubeconfig=$KUBECONFIG_FILE \
--cluster=$CLUSTER_NAME \
--user=$USER

kubectl config use-context $USER \
--kubeconfig=$KUBECONFIG_FILE
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift SSO authentication]]></title>
            <link href="https://devopstales.github.io/home/openshift-sso/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openproject-sso/?utm_source=atom_feed" rel="related" type="text/html" title="OpenProject SSO" />
                <link href="https://devopstales.github.io/home/gitlab-keycloak/?utm_source=atom_feed" rel="related" type="text/html" title="SSO login to Gitlab" />
                <link href="https://devopstales.github.io/home/keycloak1/?utm_source=atom_feed" rel="related" type="text/html" title="Install keycloak with mysql" />
                <link href="https://devopstales.github.io/home/keycloak2/?utm_source=atom_feed" rel="related" type="text/html" title="Install keycloak with postgresql" />
                <link href="https://devopstales.github.io/sso/openshift-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift SSO authentication" />
            
                <id>https://devopstales.github.io/home/openshift-sso/</id>
            
            
            <published>2019-04-13T00:00:00+00:00</published>
            <updated>2019-04-13T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Configure Openshift Cluster to use Keycloak as a user backend for login with oauth2 and SSO.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<p>With Ansible-openshift you can not change the authetication method after Install !! If you installed the cluster with htpasswd, then change to LDAP the playbook trys to add a second authentication methot for the config. It is forbidden to add a second type of identity provider in the version 3.11 of Ansible-openshift. To solv this problem we must change the configuration manually.</p>

<h3 id="environment">Environment</h3>

<pre><code>192.168.1.40    deployer
192.168.1.41    openshift01 # master node
192.168.1.42    openshift02 # infra node
192.168.1.43    openshift03 # worker node
</code></pre>

<h3 id="configuration-on-keycloak">Configuration on Keycloak</h3>

<pre><code>create new client on keycloak in relm mydomain
Client ID: openshift
Clyent Protocol: openid-connect
Access type: confidential
Valid Redirect URIs: https://master.openshift.mydomain.itra/*
delete other urls

# On Credentials tap copy the secret to clientSecrethez in config.
</code></pre>

<h3 id="configurate-the-cluster">Configurate The cluster</h3>

<pre><code># on all openshift hosts
nano /etc/origin/master/master-config.yaml
...
  identityProviders:
  - name: keycloak
    challenge: false
    login: true
    provider:
      apiVersion: v1
      kind: OpenIDIdentityProvider
      clientID: openshift
      clientSecret: ef03ffe6-854a-48b4-a26d-190c2861e3c8
      claims:
        id:
        - sub
        preferredUsername:
        - preferred_username
        name:
        - name
        email:
        - email
      urls:
        authorize: https://sso.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/auth
        token: https://sso.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/token
        logoutURL: &quot;https://sso.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/logout?redirect_uri=https://master.openshift.mydomain.itra/console&quot;
  - challenge: true
</code></pre>

<h3 id="reconfigurate-the-cluster">Reconfigurate the cluster</h3>

<pre><code># on all openshift hosts
master-restart api
master-restart controllers
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift Ceph RBD for dynamic provisioning]]></title>
            <link href="https://devopstales.github.io/home/openshift-ceph/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-ldap/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift LDAP authentication" />
                <link href="https://devopstales.github.io/kubernetes/openshift-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/kubernetes/openshift-ldap/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift LDAP authentication" />
                <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
                <link href="https://devopstales.github.io/kubernetes/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
            
                <id>https://devopstales.github.io/home/openshift-ceph/</id>
            
            
            <published>2019-04-12T00:00:00+00:00</published>
            <updated>2019-04-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how can you use CEPH RBD for persistent storagi on Openshift.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<h3 id="environment">Environment</h3>

<pre><code># openshift cluster
192.168.1.41    openshift01 # master node
192.168.1.42    openshift02 # infra node
192.168.1.43    openshift03 # worker node

# ceph cluster
192.168.1.31    ceph01
192.168.1.32    ceph02
192.168.1.33    ceph03
</code></pre>

<h3 id="prerequirement">Prerequirement</h3>

<p>RBD volume provisioner needs admin key from Ceph to provision storage. To get the admin key from Ceph cluster use this command:</p>

<pre><code>sudo ceph --cluster ceph auth get-key client.admin | base64
QVFBOFF2SlZheUJQRVJBQWgvS2cwT1laQUhPQno3akZwekxxdGc9PQ==

nano ceph-admin-secret.yaml
apiVersion: v1
data:
  key: QVFBOFF2SlZheUJQRVJBQWgvS2cwT1laQUhPQno3akZwekxxdGc9PQ==
kind: Secret
metadata:
  name: ceph-admin-secret
  namespace: kube-system
type: kubernetes.io/rbd

</code></pre>

<p>I will also create a separate Ceph pool for</p>

<pre><code>sudo ceph --cluster ceph osd pool create k8s 1024 1024
sudo ceph --cluster ceph auth get-or-create client.k8s mon 'allow r' osd 'allow rwx pool=k8s'
sudo ceph --cluster ceph auth get-key client.k8s | base64
QVFBOFF2SlZheUJQRVJBQWgvS2ctS2htOFNSZnRvclJPRk1jdXc9PQ==

nano ceph-secret-k8s.yaml
apiVersion: v1
data:
  key: QVFBOFF2SlZheUJQRVJBQWgvS2ctS2htOFNSZnRvclJPRk1jdXc9PQ==
kind: Secret
metadata:
  name: ceph-secret-k8s
  namespace: kube-system
type: kubernetes.io/rbd

</code></pre>

<pre><code># on all openshift node
yum install -y ceph-common

# on one openshift master node
nano  k8s-storage.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: &quot;true&quot;
  name: k8s
parameters:
  adminId: admin
  adminSecretName: ceph-admin-secret
  adminSecretNamespace: kube-system
  imageFeatures: layering
  imageFormat: &quot;2&quot;
  monitors: 192.168.1.31:6789, 192.168.1.32:6789, 192.168.1.33:6789
  pool: k8s
  userId: k8s
  userSecretName: ceph-secret-k8s
provisioner: kubernetes.io/rbd
reclaimPolicy: Delete
volumeBindingMode: Immediate


oc create -f ceph-admin-secret.yaml
oc create -f ceph-secret-k8s.yaml
oc create -f k8s-storage.yaml
</code></pre>

<h3 id="add-secrets-to-existng-namespaces">Add secrets to existng namespaces</h3>

<pre><code># on one openshift master node
oc project default
oc apply -f ceph-secret-k8s.yaml

oc project management-infra
oc apply -f ceph-secret-k8s.yaml

oc project openshift-infra
oc apply -f ceph-secret-k8s.yaml

oc project openshift-logging
oc apply -f ceph-secret-k8s.yaml

oc project openshift-metrics-server
oc apply -f ceph-secret-k8s.yaml

oc project openshift-monitoring
oc apply -f ceph-secret-k8s.yaml
</code></pre>

<h3 id="add-secret-to-template">Add secret to template</h3>

<p>If we add the secret to the template iw will be present in all of the newly created namespaces.</p>

<pre><code># on one openshift master node
su - origin
oc adm create-bootstrap-project-template -o yaml &gt; template.yaml
# add secrets to  the yml without namespace
nano template.yaml
...
- apiVersion: v1
  data:
    key: QVFBOFF2SlZheUJQRVJBQWgvS2ctS2htOFNSZnRvclJPRk1jdXc9PQ==
  kind: Secret
  metadata:
    name: ceph-secret-k8s
  type: kubernetes.io/rbd
...
oc create -f template.yaml -n default

# on all the openshift master nodes
nano /etc/origin/master/master-config.yaml
...
projectConfig:
  projectRequestTemplate: &quot;default/project-request&quot;
...

master-restart api
master-restart controllers
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift LDAP authentication]]></title>
            <link href="https://devopstales.github.io/home/openshift-ldap/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openshift-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/kubernetes/openshift-ldap/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift LDAP authentication" />
                <link href="https://devopstales.github.io/kubernetes/openshift-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
                <link href="https://devopstales.github.io/kubernetes/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
            
                <id>https://devopstales.github.io/home/openshift-ldap/</id>
            
            
            <published>2019-04-12T00:00:00+00:00</published>
            <updated>2019-04-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Configure Openshift Cluster to use LDAP as a user backend for login with Ansible-openshift</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<p>In the last post I used the basic htpasswd authentication method for the installatipn.<br>
But I can use Ansible-openshift to configure an LDAP backed at the install for the authentication.</p>

<h3 id="environment">Environment</h3>

<pre><code>192.168.1.40    deployer
192.168.1.41    openshift01 # master node
192.168.1.42    openshift02 # infra node
192.168.1.43    openshift03 # worker node
</code></pre>

<p>With Ansible-openshift you can not change the authetication method after Install !! If you installed the cluster with htpasswd, then change to LDAP the playbook trys to add a second authentication methot for the config. It is forbidden to add a second type of identity provider in the version 3.11 of Ansible-openshift so choose wisely.</p>

<h3 id="configurate-installer">Configurate Installer</h3>

<pre><code># deployer
nano /etc/ansible/ansible.cfg
# use HTPasswd for authentication
#openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'}]

# LDAP
openshift_master_identity_providers=[{'name': 'email_jira_ldap', 'challenge': 'true', 'login': 'true', 'kind': 'LDAPPasswordIdentityProvider', 'attributes': {'id': ['mail'], 'email': ['mail'], 'name': ['displayName'], 'preferredUsername': ['mail']}, 'bindDN': 'CN=ldapbrowser,DC=mydomain,DC=myintra', 'bindPassword': '*******', 'insecure': 'true', 'url': 'ldap://ldap01.mydomain.myintra/dc=mydomain,dc=myintra?mail?sub?(objectClass=*)'}]
</code></pre>

<h3 id="run-the-installer">Run the Installer</h3>

<pre><code># deployer
cd /usr/share/ansible/openshift-ansible/
sudo ansible-playbook -i inventory/hosts.localhost playbooks/prerequisites.yml
sudo ansible-playbook -i inventory/hosts.localhost playbooks/deploy_cluster.yml
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[WAN failower on pfsense]]></title>
            <link href="https://devopstales.github.io/home/pfsense-wlan/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/pfsense-wlan/</id>
            
            
            <published>2019-04-12T00:00:00+00:00</published>
            <updated>2019-04-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this pos I&rsquo;ll create a WAN failower configuration.</p>

<h3 id="the-architecture">The Architecture</h3>

<pre><code>
------- WAN1 ------
| ----- WAN2 ---- |
| |             | |
PF1 -- sync -- PF2
 |               |
 ----- LAN -------  


WAN1: 192.168.0.0/24 (Bridgelt)
LAN: 10.0.1.0/24
SYNC: 10.0.2.0/24
WAN2: 10.0.4.0/24
</code></pre>

<h3 id="configurate-wip-for-wan2">Configurate WIP for WAN2</h3>

<p>At <code>Firewall &gt; Virtual IPs &gt; Add</code>
<img src="/img/include/pfsenseWAN_1.jpg" alt="Example image" /></p>

<h3 id="add-gateway-for-wan-interfaces">Add Gateway for WAN interfaces</h3>

<p>At <code>System &gt; Routing &gt; Add</code>
<img src="/img/include/pfsenseWAN_2.jpg" alt="Example image" /></p>

<h3 id="configuring-monitor-ip">Configuring Monitor IP</h3>

<p>At<code>System &gt; Routing &gt; Edit gateways</code> and add google dns ad monitoring ip
<img src="/img/include/pfsenseWAN_3.jpg" alt="Example image" /></p>

<p><img src="/img/include/pfsenseWAN_4.jpg" alt="Example image" /></p>

<h3 id="configuring-gateway-group">Configuring Gateway Group</h3>

<p>At<code>System &gt; Routing &gt; Gateway Groups</code> Create 3 Groups
<img src="/img/include/pfsenseWAN_5.jpg" alt="Example image" /></p>

<p><img src="/img/include/pfsenseWAN_6.jpg" alt="Example image" /></p>

<p><img src="/img/include/pfsenseWAN_7.jpg" alt="Example image" /></p>

<h3 id="configuring-firewall-rules">Configuring Firewall Rules</h3>

<p>Got to <code>Firewall &gt; Rules &gt; LAN</code> and edit the IPv4 rule. Chane the Gateway</p>

<p><img src="/img/include/pfsenseWAN_8.jpg" alt="Example image" /></p>

<p><img src="/img/include/pfsenseWAN_9.jpg" alt="Example image" /></p>

<p>Clone the changed roles to two other rules and change the Gateway to the other Gateway Groups.
<img src="/img/include/pfsenseWAN_10.jpg" alt="Example image" /></p>

<h3 id="configurate-nat">Configurate NAT</h3>

<p>Go to<code>Firewall &gt; NAT &gt; Outbound</code> <br>
Clone WAN1 rules and edit them to WLAN2
<img src="/img/include/pfsenseWAN_11.jpg" alt="Example image" /></p>

<p><img src="/img/include/pfsenseWAN_12.jpg" alt="Example image" /></p>

<h2 id="pfsense-email-notification-when-wan-connection-goes-down">pfSense email notification when WAN connection goes down</h2>

<p>Go to <code>System &gt; Advanced &gt; Notifications</code></p>

<h3 id="example-with-google-gmail-smtp">Example with Google Gmail SMTP</h3>

<p><img src="/img/include/pfsenseWAN_13.jpg" alt="Example image" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configure OpenVPN HA pfsense cluster]]></title>
            <link href="https://devopstales.github.io/home/pfsense-openvpn/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/pfsense-openvpn/</id>
            
            
            <published>2019-04-11T00:00:00+00:00</published>
            <updated>2019-04-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this LAB I&rsquo;ll be creating OpenVPN SSL Peer to Peer connection.</p>

<h3 id="generating-ca-certificate">Generating CA Certificate</h3>

<p>At <code>System &gt; Cert.Manager &gt; CAs &gt; Add</code>
<img src="/img/include/OpenVPN_1.jpg" alt="Example image" /></p>

<p><img src="/img/include/OpenVPN_2.jpg" alt="Example image" /></p>

<h3 id="generate-server-certificate">Generate Server Certificate</h3>

<p>At <code>System &gt; Cert.Manager &gt; Certificates &gt; Add</code>
<img src="/img/include/OpenVPN_3.jpg" alt="Example image" /></p>

<h3 id="generate-user-certificate">Generate User Certificate</h3>

<p>For this demo I will&rsquo;create one certificate for all users, but in live you should create a separate certificate for all users.</p>

<p>At <code>System &gt; Cert.Manager &gt; Certificates &gt; Add</code>
<img src="/img/include/OpenVPN_4.jpg" alt="Example image" /></p>

<p>At <code>SystemUser &gt; ManagerUsers</code> add the User certificate for the users.
<img src="/img/include/OpenVPN_5.jpg" alt="Example image" /></p>

<h3 id="intall-openvpn-package-exporter">Intall Openvpn package exporter</h3>

<p>Got to<code>System &gt; Package Manager &gt; Available Packages</code> and install <code>openvpn-client-export</code> plugin.</p>

<h3 id="configurate-the-opevpn-service">Configurate the OpeVPN service</h3>

<p>Got to <code>VPN &gt; OpenVPN &gt; Wizards</code>
<img src="/img/include/OpenVPN_6.jpg" alt="Example image" /></p>

<p><img src="/img/include/OpenVPN_7.jpg" alt="Example image" /></p>

<p><img src="/img/include/OpenVPN_8.jpg" alt="Example image" /></p>

<p><img src="/img/include/OpenVPN_9.jpg" alt="Example image" /></p>

<p>Edit the Adwanced Configuration:
<img src="/img/include/OpenVPN_18.jpg" alt="Example image" /></p>

<p><img src="/img/include/OpenVPN_10.jpg" alt="Example image" /></p>

<p><img src="/img/include/OpenVPN_11.jpg" alt="Example image" /></p>

<h3 id="configurate-nat-rules-to-ha">Configurate NAT Rules to HA</h3>

<p>Go to <code>Firewall &gt; NAT &gt; Outbound</code> and clone the LAN Rules?
<img src="/img/include/OpenVPN_12.jpg" alt="Example image" /></p>

<p><img src="/img/include/OpenVPN_13.jpg" alt="Example image" /></p>

<p><img src="/img/include/OpenVPN_14.jpg" alt="Example image" /></p>

<p><img src="/img/include/OpenVPN_15.jpg" alt="Example image" /></p>

<h3 id="enable-connection-from-openvpn-to-master-and-slave">Enable Connection from OpenVPN to master and slave</h3>

<p>In default there in no rout to the salve nod. Go to <code>Firewll &gt; Aliases &gt; Add</code> and create alias for CARP members:
<img src="/img/include/OpenVPN_16.png" alt="Example image" /></p>

<p>Then go back to <code>Firewall &gt; NAT &gt; Outbound</code> and create a new rule:
<img src="/img/include/OpenVPN_17.png" alt="Example image" /></p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configurate HA pfsense cluster]]></title>
            <link href="https://devopstales.github.io/home/pfsense-ha/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/pfsense-ha/</id>
            
            
            <published>2019-04-10T00:00:00+00:00</published>
            <updated>2019-04-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will configure 2 pfsense server to a HA cluster.</p>

<h3 id="the-architecture">The Architecture</h3>

<pre><code> ------ WAN ------
 |               |
PF1 -- sync -- PF2
 |               |
 ----- LAN -------  

WAN: 192.168.0.0/24 (Bridgelt)
LAN: 10.0.1.0/24
SYNC: 10.0.2.0/24
</code></pre>

<pre><code>pf1:
WAN 192.168.0.21
LAN: 10.0.1.21
SYNC:10.0.2.21

pf2:
WAN 192.168.0.22
LAN: 10.0.1.22
SYNC:10.0.2.22
</code></pre>

<p><img src="/img/include/carp_1.png" alt="Example image" /></p>

<p><img src="/img/include/carp_2.png" alt="Example image" /></p>

<p><img src="/img/include/carp_3.png" alt="Example image" /></p>

<h3 id="firewall-rules-for-sync">Firewall rules For sync</h3>

<p>On both firewalls add two rules to allow traffic on the SYNC interface: <br>
go to <code>Firewall &gt; Rules &gt; Sync</code> and click <code>Add</code>.</p>

<p>Rule 1:
<img src="/img/include/carp_4.png" alt="Example image" /></p>

<p>Rule 2:
<img src="/img/include/carp_5.png" alt="Example image" /></p>

<p>Rule 3:
<img src="/img/include/carp_6.png" alt="Example image" /></p>

<h3 id="synchronization-settings">Synchronization Settings</h3>

<p>Go to <code>System &gt; High Availability Sync</code> and configure the sections like on the pictures.</p>

<p>Master:
<img src="/img/include/carp_7.png" alt="Example image" /></p>

<p>Slave:
<img src="/img/include/carp_8.png" alt="Example image" /></p>

<p>Test the synchronisation. Go to <code>System &gt; User management</code> and createa new user on the master node. <br>
Then check on the slave node.</p>

<p>If it doesn&rsquo;t work, check:</p>

<ul>
<li>Are the firewall web interfaces running on the same protocols and ports?</li>
<li>Is the admin password set correctly? (<code>User Manager &gt; Users &gt; admin</code>.)</li>
<li>Are the firewall rules to allow synch set to use the correct interface (SYNC)?</li>
<li>If you&rsquo;re using VMs, are the firewalls on the same internal network?</li>
</ul>

<h3 id="create-virtual-ips">create virtual IPs</h3>

<p>On the master node go to <code>Firewall &gt; Virtual IPs</code> and click <code>Add</code>. Create a new VIP adres for LAN and WAN interfaces.</p>

<p>WAN VIP on master:
<img src="/img/include/carp_9.png" alt="Example image" /></p>

<p>WAN VIP on salave:
<img src="/img/include/carp_10.png" alt="Example image" /></p>

<p>LAN VIP on master:
<img src="/img/include/carp_11.png" alt="Example image" /></p>

<p>LAN VIP on slave:
<img src="/img/include/carp_12.png" alt="Example image" /></p>

<h3 id="change-outbound-nat">Change outbound NAT</h3>

<p>Change the configuration of the outbound NAT to use the shared public IP (the WAN VIP) <br>
Go to <code>Firewall &gt; NAT &gt; Outbound</code> and set the mode to <code>Hybrid Outbound NAT</code> rule generation.</p>

<p><img src="/img/include/carp_13.png" alt="Example image" /></p>

<p><img src="/img/include/carp_14.png" alt="Example image" /></p>

<p>Find your LAN IP ranges (there should be two) and click the edit icon and change the Translation Address to the WAN VIP address.</p>

<p><img src="/img/include/carp_15.png" alt="Example image" /></p>

<p>Do the same for the other LAN network mapping. It should end up looking like this:</p>

<p><img src="/img/include/carp_16.png" alt="Example image" /></p>

<p>If you’ll be using your pfSense firewall as a DNS resolver you must change the settings of the DNS service (<code>Services &gt; DNS Resolver &gt; General Settings</code>) to lissen on the LAN VIP address. Then chnage the address of the DNS server in the DHCP configuration to us the LAN VIP adress.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Rundeck]]></title>
            <link href="https://devopstales.github.io/home/rundeck/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/rundeck/</id>
            
            
            <published>2019-04-09T00:00:00+00:00</published>
            <updated>2019-04-09T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Rundeck is open source software that powers self-service operations.</p>

<h3 id="install-mysql">Install MySQL</h3>

<pre><code>echo &quot;[mariadb]
name = MariaDB
baseurl = http://yum.mariadb.org/10.0/centos7-amd64
gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDB
gpgcheck=1&quot; &gt; /etc/yum.repos.d/MariaDB.repo

yum -y install httpd MariaDB-Galera-server.x86_64

systemctl enable httpd
systemctl enable mysql
systemctl start httpd
systemctl start mysql

mysql_secure_installation
mysql -u root -p
create database rundeck;
grant all on rundeck.* to 'rundeck'@'localhost' identified by 'Password1';
quit
</code></pre>

<h3 id="install-rundeck">Install rundeck</h3>

<pre><code>yum install java-1.8.0 httpd -y
rpm -Uvh http://repo.rundeck.org/latest.rpm
yum install rundeck
</code></pre>

<h3 id="configure-rundeck">Configure rundeck</h3>

<pre><code>nano /etc/rundeck/rundeck-config.properties
# change hostname here
# grails.serverURL=http://localhost:4440
grails.serverURL=http://rundeck.devopstales.intra
#dataSource.url = jdbc:h2:file:/var/lib/rundeck/data/rundeckdb
dataSource.url = jdbc:mysql://localhost/rundeck?autoReconnect=true&amp;useSSL=false
dataSource.username=rundeck
dataSource.password=Password1
dataSource.driverClassName=com.mysql.jdbc.Driver
# mail server
grails.mail.host=localhost
grails.mail.port=25

service rundeckd start
</code></pre>

<h3 id="configure-httpd">Configure httpd</h3>

<pre><code>nano /etc/httpd/conf.d/rundeck_proxy.conf
&lt;virtualhost *:80&gt;
        ServerName rundeck.devopstales.intra
        ServerAlias www.rundeck.devopstales.intra
        ServerAdmin admin@rundeck.devopstales.intra

        ProxyRequests Off
        ProxyPass / http://localhost:4440/
        ProxyPassReverse / http://localhost:4440/
&lt;/virtualhost&gt;

service httpd restart
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Nextcloud]]></title>
            <link href="https://devopstales.github.io/home/nextcloud/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/nextcloud/</id>
            
            
            <published>2019-04-08T00:00:00+00:00</published>
            <updated>2019-04-08T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Nextcloud is a suite of client-server software for creating and using file hosting services. Nextcloud application functionally is similar to Dropbox.</p>

<h3 id="install-postgresql">Install Postgresql</h3>

<pre><code>wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
sudo sh -c 'echo &quot;deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -sc)-pgdg main&quot; &gt; /etc/apt/sources.list.d/PostgreSQL.list'

apt update
apt-get install postgresql-10
apt-get install pgadmin4

systemctl enable postgresql.service
</code></pre>

<h3 id="configure-database">Configure database</h3>

<pre><code>createuser cloud
psql
ALTER USER cloud WITH ENCRYPTED password 'Password1';
CREATE DATABASE cloud WITH ENCODING='UTF8' OWNER=cloud;
\q
</code></pre>

<h3 id="install-requirements">Install requirements</h3>

<pre><code>apt-get install -y libapache2-mod-php php7.0 php7.0-xml php7.0-curl php7.0-gd php7.0 php7.0-cgi php7.0-cli php7.0-zip php7.0-mbstring wget unzip php7.0-pgsql
</code></pre>

<h3 id="configurate-php">Configurate php</h3>

<pre><code>nano /etc/php/7.0/apache2/php.ini
file_uploads = On
allow_url_fopen = On
short_open_tag = On
memory_limit = 256M
upload_max_filesize = 100M
max_execution_time = 360
date.timezone = Europe/Budapest
</code></pre>

<h3 id="install-them">Install Them</h3>

<pre><code>cd /usr/share/redmine/public/themes
# https://github.com/akabekobeko/redmine-theme-minimalflat2/releases
wget https://github.com/akabekobeko/redmine-theme-minimalflat2/releases/download/v1.5.0/minimalflat2-1.5.0.zip
unzip minimalflat2-1.5.0.zip
</code></pre>

<h3 id="configurate-apache">Configurate Apache</h3>

<pre><code>mkdir /var/www/nextcloud
chown www-data:www-data /var/www/nextcloud
chmod 750 /var/www/nextcloud

mkdir -p /var/nextcloud/data
chown www-data:www-data /var/nextcloud/data
chmod 750 /var/nextcloud/data

cd  /var/www/nextcloud
wget https://download.nextcloud.com/server/installer/setup-nextcloud.php
chown www-data:www-data setup-nextcloud.php
</code></pre>

<h3 id="create-vhostfile">Create vhostfile</h3>

<pre><code>echo '&lt;VirtualHost *:80&gt;
ServerAdmin admin@example.com
DocumentRoot &quot;/var/www/nextcloud&quot;
ServerName cloud.devopstales.intra
&lt;Directory &quot;/var/www/nextcloud/&quot;&gt;
Options MultiViews FollowSymlinks

AllowOverride All
Order allow,deny
Allow from all
&lt;/Directory&gt;
TransferLog /var/log/apache2/nextcloud_access.log
ErrorLog /var/log/apache2/nextcloud_error.log
&lt;/VirtualHost&gt;' &gt; /etc/apache2/sites-available/nextcloud.conf

a2dissite 000-default
a2ensite nextcloud
a2enmod rewrite
a2enmod headers
a2enmod env
a2enmod dir
a2enmod mime
service apache2 reload
</code></pre>

<h3 id="install-nextcloud">Install nextcloud</h3>

<p>Go to <a href="http://cloud.devopstales.intra/setup-nextcloud.php">http://cloud.devopstales.intra/setup-nextcloud.php</a> and add the db configuration to install the aplication.</p>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Redmine]]></title>
            <link href="https://devopstales.github.io/home/redmine/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/redmine/</id>
            
            
            <published>2019-04-06T00:00:00+00:00</published>
            <updated>2019-04-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Redmine is a free and open source, web-based project management and issue tracking tool. I will install it on Ubuntu becous on CetOS there in no pre build package for redmine.</p>

<h3 id="install-and-configure-postgresql">Install and configure Postgresql</h3>

<p>In a previous post I wrote about how to <a href="/linux/install-postgresql/#install-postgresql-10-on-debian">Install PostgreSQL 10</a></p>

<pre><code>su - postgres
createuser redmine
psql
ALTER USER redmine WITH ENCRYPTED password 'Password1';
CREATE DATABASE redmine WITH ENCODING='UTF8' OWNER=redmine;
\q
</code></pre>

<h3 id="install-redmine">Install Redmine</h3>

<pre><code>apt install git redmine redmine-pgsql
chmod 777 /usr/share/redmine/instances/default/tmp/cache/
cd /usr/share/redmine
ruby bin/rails server webrick –e production
</code></pre>

<pre><code>echo '[Unit]
Description=Redmine server
After=syslog.target
After=network.target

[Service]
Type=simple
User=redmine
Group=redmine
WorkingDirectory=/usr/share/redmine
ExecStart=/usr/bin/ruby /usr/share/redmine/bin/rails server webrick –e production

# Give a reasonable amount of time for the server to start up/shut down
TimeoutSec=300

[Install]
WantedBy=multi-user.target' &gt; /etc/systemd/system/redmine.service
</code></pre>

<pre><code>apt install apache2 libapache2-mod-passenger
cp /usr/share/doc/redmine/examples/apache2-passenger-host.conf /etc/apache2/sites-available/redmine.conf
nano /etc/apache2/sites-available/redmine.conf

a2enmod passenger
a2enmod proxy
a2enmod rewrite
a2ensite redmine.conf
a2dissite 000-default
service apache2 reload
</code></pre>

<h3 id="install-them">Install Them</h3>

<pre><code>cd /usr/share/redmine/public/themes
# https://github.com/akabekobeko/redmine-theme-minimalflat2/releases
wget https://github.com/akabekobeko/redmine-theme-minimalflat2/releases/download/v1.5.0/minimalflat2-1.5.0.zip
unzip minimalflat2-1.5.0.zip
</code></pre>

<h3 id="install-plugin">Install plugin</h3>

<pre><code>ln -s /usr/share/redmine/bin /usr/share/rubygems-integration/all/specifications/

mkdir /usr/share/redmine/plugins
cd /usr/share/redmine/plugins

git clone https://github.com/applewu/redmine_omniauth_gitlab
cd /usr/share/redmine
bundle install --without development test
bundle exec rake redmine:plugins NAME=redmine_omniauth_gitlab RAILS_ENV=production
ll /usr/share/redmine/public/plugin_assets/
service apache2 restart
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[OpenProject SSO]]></title>
            <link href="https://devopstales.github.io/home/openproject-sso/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/gitlab-keycloak/?utm_source=atom_feed" rel="related" type="text/html" title="SSO login to Gitlab" />
                <link href="https://devopstales.github.io/home/keycloak1/?utm_source=atom_feed" rel="related" type="text/html" title="Install keycloak with mysql" />
                <link href="https://devopstales.github.io/home/keycloak2/?utm_source=atom_feed" rel="related" type="text/html" title="Install keycloak with postgresql" />
                <link href="https://devopstales.github.io/sso/openproject-sso/?utm_source=atom_feed" rel="related" type="text/html" title="OpenProject SSO" />
                <link href="https://devopstales.github.io/sso/gitlab-keycloak/?utm_source=atom_feed" rel="related" type="text/html" title="SSO login to Gitlab" />
            
                <id>https://devopstales.github.io/home/openproject-sso/</id>
            
            
            <published>2019-04-06T00:00:00+00:00</published>
            <updated>2019-04-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Configurate openproject to use Keycloak as sso Identity Provider.</p>

<h3 id="configure-openproject">Configure OpenProject</h3>

<p>Login to openproject with admin and change the config of Self-registrtion to automatic account activation:
Administraion &gt; System Settings &gt; Authentication &gt; Self-registration</p>

<pre><code>nano /opt/openproject/config/configuration.yml
default:
  omniauth_direct_login_provider: openid
  openid_connect:
    openid:
      host: &quot;sso.devopstales.intra&quot;
      identifier: &quot;project&quot;
      secret: &quot;57583084-b54b-4b32-935b-73776f27b89f&quot;
      icon: &quot;openid_connect/auth_provider-google.png&quot;
      display_name: &quot;SSO&quot;
      authorization_endpoint: &quot;http://sso.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/auth&quot;
      token_endpoint: 'http://sso.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/token'
      userinfo_endpoint: 'http://sso.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/userinfo'
      end_session_endpoint: 'http://sso.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/logout'
      check_session_iframe: 'http://sso.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/login-status-iframe.html'
      sso: true
      issuer: 'http://project.devopstales.intra/login'
      discovery: false

service openproject restart
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[SSO login to Gitlab]]></title>
            <link href="https://devopstales.github.io/home/gitlab-keycloak/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/openproject-sso/?utm_source=atom_feed" rel="related" type="text/html" title="OpenProject SSO" />
                <link href="https://devopstales.github.io/home/keycloak1/?utm_source=atom_feed" rel="related" type="text/html" title="Install keycloak with mysql" />
                <link href="https://devopstales.github.io/home/keycloak2/?utm_source=atom_feed" rel="related" type="text/html" title="Install keycloak with postgresql" />
                <link href="https://devopstales.github.io/sso/gitlab-keycloak/?utm_source=atom_feed" rel="related" type="text/html" title="SSO login to Gitlab" />
                <link href="https://devopstales.github.io/sso/openproject-sso/?utm_source=atom_feed" rel="related" type="text/html" title="OpenProject SSO" />
            
                <id>https://devopstales.github.io/home/gitlab-keycloak/</id>
            
            
            <published>2019-04-06T00:00:00+00:00</published>
            <updated>2019-04-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Configurate Gitab to use Keycloak as SSO Identity Proider.</p>

<h3 id="configurate-keycloak">Configurate Keycloak</h3>

<p>Login to Keycloak and create client for Gitlab:
<img src="/img/include/gitlab-keycloak1.png" alt="Example image" /></p>

<p>At Mappers create mappers for all user information to GitLab:</p>

<ul>
<li>Name: name

<ul>
<li>Mapper Type: User Property</li>
<li>Property: Username</li>
</ul></li>
<li>Name: email

<ul>
<li>Mapper Type: User Property</li>
<li>Property: Email</li>
</ul></li>
<li>Name: first_name

<ul>
<li>Mapper Type: User Property</li>
<li>Property: FirstName</li>
</ul></li>
<li>Name: last_name

<ul>
<li>Mapper Type: User Property</li>
<li>Property: LastName</li>
</ul></li>
</ul>

<h3 id="configurate-gitlab">Configurate Gitlab</h3>

<pre><code>nano /etc/gitlab/gitlab.rb
gitlab_rails['omniauth_enabled'] = true
gitlab_rails['omniauth_block_auto_created_users'] = false
gitlab_rails['omniauth_allow_single_sign_on'] = ['oauth2_generic']
# gitlab_rails['omniauth_auto_sign_in_with_provider'] = 'oauth2_generic'

gitlab_rails['omniauth_providers'] = [
{
        'name' =&gt; 'oauth2_generic',
        'app_id' =&gt; 'gitlab',
        'app_secret' =&gt; 'KEYCLOAK SECRET GOES HERE',
        'args' =&gt; {
        client_options: {
                'site' =&gt; 'http://sso.devopstales.intra', # including port if necessary
                'user_info_url' =&gt; '/auth/realms/devopstales/protocol/openid-connect/userinfo',
                'authorize_url' =&gt; '/auth/realms/devopstales/protocol/openid-connect/auth',
                'token_url' =&gt; '/auth/realms/devopstales/protocol/openid-connect/token',
        },
        user_response_structure: {
        #root_path: ['user'], # i.e. if attributes are returned in JsonAPI format (in a 'user' node nested under a 'data' node)
        attributes: { email:'email', first_name:'given_name', last_name:'family_name', name:'name', nickname:'preferred_username' }, # if the nickname attribute of a user is called 'username'
        id_path: 'preferred_username'
        },
        }
}
]

gitlab-ctl reconfigure
</code></pre>

<h3 id="gitlab-mattermost-config">Gitlab Mattermost config</h3>

<pre><code># on gitlab gui:
login: admin area / Applications / new
Redirect URI use:
http://mattermost.devopstales.intra/login/gitlab/complete
http://mattermost.devopstales.intra/signup/gitlab/complete

# configfile
nano /etc/gitlab/gitlab.rb

mattermost_external_url 'http://mattermost.devopstales.intra'
mattermost['enable'] = true
mattermost['service_address'] = &quot;127.0.0.1&quot;
mattermost['service_port'] = &quot;8065&quot;
mattermost['sql_driver_name'] = 'postgres'
mattermost['sql_data_source'] = &quot;postgres://mmuser:Password1@127.0.0.1:5432/mattermost?sslmode=disable&amp;connect_timeout=10&quot;
mattermost['log_file_directory'] = '/var/log/gitlab/mattermost/'
mattermost_nginx['enable'] = false

mattermost['gitlab_enable'] = true
mattermost['gitlab_id'] = &quot;&lt;ID&gt;&quot; # oauth id drom gitlab gui
mattermost['gitlab_secret'] = &quot;&lt;token&gt;&quot; # oauth token drom gitlab gui
mattermost['gitlab_scope'] = &quot;&quot;
mattermost['gitlab_auth_endpoint'] = &quot;http://gitlab.devopstales.intra/oauth/authorize&quot;
mattermost['gitlab_token_endpoint'] = &quot;http://gitlab.devopstales.intra/oauth/token&quot;
mattermost['gitlab_user_api_endpoint'] = &quot;http://gitlab.devopstales.intra/api/v4/user&quot;

gitlab-ctl reconfigure
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install keycloak with mysql]]></title>
            <link href="https://devopstales.github.io/home/keycloak1/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/keycloak2/?utm_source=atom_feed" rel="related" type="text/html" title="Install keycloak with postgresql" />
                <link href="https://devopstales.github.io/sso/keycloak1/?utm_source=atom_feed" rel="related" type="text/html" title="Install keycloak with mysql" />
                <link href="https://devopstales.github.io/sso/keycloak2/?utm_source=atom_feed" rel="related" type="text/html" title="Install keycloak with postgresql" />
                <link href="https://devopstales.github.io/home/install-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ceph cluster" />
                <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
            
                <id>https://devopstales.github.io/home/keycloak1/</id>
            
            
            <published>2019-04-05T00:00:00+00:00</published>
            <updated>2019-04-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Keycloak is an open source identity and access management solution.</p>

<h3 id="install-dependencies">Install dependencies</h3>

<pre><code>yum install -y epel-release
yum install -y java-1.8.0-openjdk-headless tmux nano mariadb-server unzip nginx

cd /opt/
wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.47.zip
unzip mysql-connector-java-5.1.47.zip
</code></pre>

<h3 id="configure-database">Configure database</h3>

<pre><code>service mariadb start

mysql -uroot
CREATE DATABASE keycloak CHARACTER SET utf8 COLLATE utf8_unicode_ci;
GRANT ALL PRIVILEGES ON keycloak.* TO 'keycloak'@'%' identified by 'Password1';
GRANT ALL PRIVILEGES ON keycloak.* TO 'keycloak'@'localhost' identified by 'Password1';
FLUSH privileges;
exit;
</code></pre>

<h3 id="install-keycloak">Install keycloak</h3>

<pre><code>groupadd -r keycloak
useradd -m -d /var/lib/keycloak -s /sbin/nologin -r -g keycloak keycloak

mkdir -p /opt/keycloak/
cd /opt/keycloak/

# https://www.keycloak.org/downloads.html
wget https://downloads.jboss.org/keycloak/4.8.2.Final/keycloak-4.8.2.Final.tar.gz

tar -xzf keycloak-4.8.2.Final.tar.gz
ln -s /opt/keycloak/keycloak-4.8.2.Final /opt/keycloak/current
chown keycloak: -R /opt/keycloak
sudo -u keycloak chmod 700 /opt/keycloak/current/standalone

mkdir /var/log/keycloak
chown keycloak: -R /var/log/keycloak

chown keycloak: -R /opt/keycloak
sudo -u keycloak chmod 700 /opt/keycloak/current/standalone
</code></pre>

<pre><code>echo '[Unit]
Description=Keycloak
After=network.target syslog.target

[Service]
Type=idle
User=keycloak
Group=keycloak
ExecStart=/opt/keycloak/current/bin/standalone.sh -b 0.0.0.0
TimeoutStartSec=600
TimeoutStopSec=600

StandardOutput=syslog
StandardError=syslog
SyslogIdentifier=keycloak

[Install]
WantedBy=multi-user.target
' &gt; /etc/systemd/system/keycloak.service
</code></pre>

<pre><code>echo 'if $programname == &quot;keycloak&quot; then /var/log/keycloak/jboss.log
&amp; stop
'&gt;/etc/rsyslog.d/keycloak.conf

systemctl daemon-reload
service rsyslog restart
systemctl start keycloak.service
</code></pre>

<h3 id="configure-wildfly">Configure wildfly</h3>

<pre><code>cd /opt/keycloak/current/

./bin/jboss-cli.sh -c 'module add --name=org.mysql  --dependencies=javax.api,javax.transaction.api --resources=/opt/mysql-connector-java-5.1.47/mysql-connector-java-5.1.47.jar'

./bin/jboss-cli.sh 'embed-server,/subsystem=datasources/jdbc-driver=org.mysql:add(driver-name=org.mysql,driver-module-name=org.mysql,driver-class-name=com.mysql.jdbc.Driver)'

./bin/jboss-cli.sh 'embed-server,/subsystem=datasources/data-source=KeycloakDS:remove'

./bin/jboss-cli.sh 'embed-server,/subsystem=datasources/data-source=KeycloakDS:add(driver-name=org.mysql,enabled=true,use-java-context=true,connection-url=&quot;jdbc:mysql://localhost:3306/keycloak?useSSL=false&amp;amp;useLegacyDatetimeCode=false&amp;amp;serverTimezone=Europe/Budapest&amp;amp;characterEncoding=UTF-8&quot;,jndi-name=&quot;java:/jboss/datasources/KeycloakDS&quot;,user-name=keycloak,password=&quot;Password1&quot;,valid-connection-checker-class-name=org.jboss.jca.adapters.jdbc.extensions.mysql.MySQLValidConnectionChecker,validate-on-match=true,exception-sorter-class-name=org.jboss.jca.adapters.jdbc.extensions.mysql.MySQLValidConnectionChecker)'

./bin/add-user-keycloak.sh -u admin -p Password1 -r master

# for nginx proxy
./bin/jboss-cli.sh 'embed-server,/subsystem=undertow/server=default-server/http-listener=default:write-attribute(name=proxy-address-forwarding,value=true)'

./bin/jboss-cli.sh 'embed-server,/socket-binding-group=standard-sockets/socket-binding=proxy-https:add(port=443)'

./bin/jboss-cli.sh 'embed-server,/subsystem=undertow/server=default-server/http-listener=default:write-attribute(name=redirect-socket,value=proxy-https)'

# disabla color in log
./bin/jboss-cli.sh -c '/subsystem=logging/console-handler=CONSOLE:write-attribute(name=named-formatter, value=PATTERN)'
</code></pre>

<h3 id="configurate-proxy">Configurate proxy</h3>

<pre><code>systemctl restart keycloak.service

echo 'upstream keycloak {
    # Use IP Hash for session persistence
    ip_hash;

    # List of Keycloak servers
    server 127.0.0.1:8080;
}


server {
    listen 80;
    server_name sso.devopstales.intra;

    # Redirect all HTTP to HTTPS
    location / {
      return 301 https://$server_name$request_uri;
    }
}

server {
    listen 443 ssl http2;
    server_name sso.devopstales.intra;

    ssl_certificate /etc/nginx/ssl/domain.pem;
    ssl_certificate_key /etc/nginx/ssl/domain.pem;
    ssl_session_cache shared:SSL:1m;
    ssl_prefer_server_ciphers on;

    location / {
      proxy_set_header Host $host;
      proxy_set_header X-Real-IP $remote_addr;
      proxy_set_header X-Forwarded-For  $proxy_add_x_forwarded_for;
      proxy_set_header X-Forwarded-Proto  $scheme;
      proxy_pass http://keycloak;
    }
}
' &gt; /etc/nginx/conf.d/keycloak.conf

mkdir /etc/nginx/ssl

systemctl restart nginx

# go to sso.devopstales.intra
# login admin / Password1
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install keycloak with postgresql]]></title>
            <link href="https://devopstales.github.io/home/keycloak2/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/keycloak1/?utm_source=atom_feed" rel="related" type="text/html" title="Install keycloak with mysql" />
                <link href="https://devopstales.github.io/sso/keycloak1/?utm_source=atom_feed" rel="related" type="text/html" title="Install keycloak with mysql" />
                <link href="https://devopstales.github.io/sso/keycloak2/?utm_source=atom_feed" rel="related" type="text/html" title="Install keycloak with postgresql" />
                <link href="https://devopstales.github.io/home/install-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ceph cluster" />
                <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
            
                <id>https://devopstales.github.io/home/keycloak2/</id>
            
            
            <published>2019-04-05T00:00:00+00:00</published>
            <updated>2019-04-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Keycloak is an open source identity and access management solution.</p>

<h3 id="install-dependencies">Install dependencies</h3>

<pre><code>yum install -y epel-release
yum install -y java-1.8.0-openjdk-headless tmux nano mariadb-server unzip httpd

cd /opt
# https://jdbc.postgresql.org/download.html
wget https://jdbc.postgresql.org/download/postgresql-42.2.5.jar
</code></pre>

<h3 id="install-and-configure-database">Install and configure database</h3>

<p>In a previous post I wrote about how to <a href="/linux/install-postgresql/#install-postgresql-10-on-centos-7">Install PostgreSQL 10</a></p>

<pre><code>nano /var/lib/pgsql/10/data/pg_hba.conf
# TYPE  DATABASE        USER            ADDRESS                 METHOD
local   all             keycloak                                md5

su - postgres
createuser keycloak
psql
ALTER USER keycloak WITH ENCRYPTED password 'Password1';
CREATE DATABASE keycloak WITH ENCODING='UTF8' OWNER=keycloak;
\q
</code></pre>

<h3 id="install-keycloak">Install keycloak</h3>

<pre><code>groupadd -r keycloak
useradd -m -d /var/lib/keycloak -s /sbin/nologin -r -g keycloak keycloak

mkdir -p /opt/keycloak/
cd /opt/keycloak/

# https://www.keycloak.org/downloads.html
wget https://downloads.jboss.org/keycloak/4.8.2.Final/keycloak-4.8.2.Final.tar.gz

tar -xzf keycloak-4.8.2.Final.tar.gz
ln -s /opt/keycloak/keycloak-4.8.2.Final /opt/keycloak/current
chown keycloak: -R /opt/keycloak
sudo -u keycloak chmod 700 /opt/keycloak/current/standalone

mkdir /var/log/keycloak
chown keycloak: -R /var/log/keycloak

chown keycloak: -R /opt/keycloak
sudo -u keycloak chmod 700 /opt/keycloak/current/standalone
</code></pre>

<pre><code>echo '[Unit]
Description=Keycloak
After=network.target syslog.target

[Service]
Type=idle
User=keycloak
Group=keycloak
ExecStart=/opt/keycloak/current/bin/standalone.sh -b 0.0.0.0
TimeoutStartSec=600
TimeoutStopSec=600

StandardOutput=syslog
StandardError=syslog
SyslogIdentifier=keycloak

[Install]
WantedBy=multi-user.target
' &gt; /etc/systemd/system/keycloak.service
</code></pre>

<pre><code>echo 'if $programname == &quot;keycloak&quot; then /var/log/keycloak/jboss.log
&amp; stop
'&gt;/etc/rsyslog.d/keycloak.conf

systemctl daemon-reload
service rsyslog restart
systemctl start keycloak.service
</code></pre>

<h3 id="configure-wildfly">Configure wildfly</h3>

<pre><code>cd /opt/keycloak/current/modules
mkdir -p org/postgresql/main
cp /opt/postgresql-42.2.5.jar .

echo '&lt;?xml version=&quot;1.0&quot; ?&gt;
&lt;module xmlns=&quot;urn:jboss:module:1.3&quot; name=&quot;org.postgresql&quot;&gt;

    &lt;resources&gt;
        &lt;resource-root path=&quot;postgresql-42.2.5.jar&quot;/&gt;
	&lt;/resources&gt;

	&lt;dependencies&gt;
		&lt;module name=&quot;javax.api&quot;/&gt;
		&lt;module name=&quot;javax.transaction.api&quot;/&gt;
	&lt;/dependencies&gt;
&lt;/module&gt;' &gt; org/postgresql/main/module.xml
</code></pre>

<pre><code>cd /opt/keycloak/current/standalone/configuration/
nano standalone.xml
...
        &lt;datasources&gt;
				&lt;datasource jndi-name=&quot;java:jboss/datasources/KeycloakDS&quot; pool-name=&quot;KeycloakDS&quot; enabled=&quot;true&quot; use-java-context=&quot;true&quot;&gt;
					&lt;connection-url&gt;jdbc:postgresql://localhost:5432/keycloak&lt;/connection-url&gt;
					&lt;driver&gt;postgresql&lt;/driver&gt;
					&lt;pool&gt;
						&lt;max-pool-size&gt;20&lt;/max-pool-size&gt;
					&lt;/pool&gt;
					&lt;security&gt;
						&lt;user-name&gt;keycloak&lt;/user-name&gt;
						&lt;password&gt;Password1&lt;/password&gt;
					&lt;/security&gt;
				&lt;/datasource&gt;
...
        &lt;drivers&gt;
					&lt;driver name=&quot;postgresql&quot; module=&quot;org.postgresql&quot;&gt;
						&lt;xa-datasource-class&gt;org.postgresql.xa.PGXADataSource&lt;/xa-datasource-class&gt;
				&lt;/driver&gt;
...
&lt;default-bindings context-service=&quot;java:jboss/ee/concurrency/context/default&quot; datasource=&quot;java:jboss/datasources/KeycloakDS&quot;
</code></pre>

<pre><code>cd /opt/keycloak/current
./bin/add-user-keycloak.sh -u admin -p Password1 -r master
systemctl restart keycloak.service
</code></pre>

<h3 id="configurate-proxy">Configurate proxy</h3>

<pre><code>echo '&lt;VirtualHost *:80&gt;
    ServerName sso.devopstales.intra

    ProxyPreserveHost On
#    SSLProxyEngine On
#    SSLProxyCheckPeerCN on
#    SSLProxyCheckPeerExpire on
    RequestHeader set X-Forwarded-Proto &quot;https&quot;
    RequestHeader set X-Forwarded-Port &quot;80&quot; #443
    ProxyPass / http://127.0.0.1:8080/
    ProxyPassReverse / http://127.0.0.1:8080/
&lt;/VirtualHost&gt;' &gt; /etc/apache2/sites-available/keycloak.conf

sudo a2enmod headers
a2enmod proxy
a2enmod rewrite
a2ensite keycloak.confcd
service httpd restart

# go to sso.devopstales.intra
# login admin / Password1
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Openproject]]></title>
            <link href="https://devopstales.github.io/home/openproject/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/openproject/</id>
            
            
            <published>2019-04-05T00:00:00+00:00</published>
            <updated>2019-04-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Openproject is a free and open source, web-based project management and issue tracking tool.</p>

<h3 id="install-openproject">Install OpenProject</h3>

<pre><code>sudo wget -O /etc/yum.repos.d/openproject-ce.repo https://dl.packager.io/srv/opf/openproject-ce/stable/8/installer/el/7.repo

yum install openproject memcached -y
service memcached start
</code></pre>

<h3 id="install-postgresql">Install Postgresql</h3>

<p>In a previous post I wrote about how to <a href="/linux/install-postgresql/#install-postgresql-9-6-on-centos-7">Install PostgreSQL 10</a></p>

<pre><code>su - postgres
createuser project
psql
ALTER USER project WITH ENCRYPTED password 'Password1';
CREATE DATABASE project WITH ENCODING='UTF8' OWNER=project;
\q
</code></pre>

<h3 id="configure-openproject">Configure OpenProject</h3>

<pre><code>openproject configure
</code></pre>

<h3 id="reset-password">Reset Password</h3>

<pre><code>openproject run console

admin = User.find_by(login: 'admin')
admin.password = 'Password11' # minimum 10 characters
admin.password_confirmation = 'Password11'

admin.save! # Watch the output for errors
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Ceph cluster]]></title>
            <link href="https://devopstales.github.io/home/install-ceph/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/ceph-block-device/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph Block Device" />
                <link href="https://devopstales.github.io/home/ceph-cephfs/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph CephFS" />
                <link href="https://devopstales.github.io/linux/install-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ceph cluster" />
                <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
                <link href="https://devopstales.github.io/linux/ceph-block-device/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph Block Device" />
            
                <id>https://devopstales.github.io/home/install-ceph/</id>
            
            
            <published>2019-03-12T00:00:00+00:00</published>
            <updated>2019-03-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Ceph is free and open source distributed objectstorage solution. With Ceph we can easily provide and manage block storage, object storage and file storage.</p>

<h3 id="base-components">Base Components</h3>

<ul>
<li>Monitors (ceph-mon) : As the name suggests a ceph monitor nodes keep an eye on cluster state, OSD Map and Crush map</li>
<li>OSD ( Ceph-osd): These are the nodes which are part of cluster and provides data store, data replication and recovery functionalities. OSD also provides information to monitor nodes.</li>
<li>MDS (Ceph-mds) : It is a ceph meta-data server and stores the meta data of ceph file systems like block storage.</li>
<li>Ceph Deployment Node : It is used to deploy the Ceph cluster, it is also called as Ceph-admin or Ceph-utility node.</li>
</ul>

<h3 id="environment">Environment</h3>

<pre><code>192.168.1.31    ceph01
192.168.1.32    ceph02
192.168.1.33    ceph03
</code></pre>

<h3 id="install-requirements">Install Requirements</h3>

<pre><code># all hosts
yum install ntp ntpdate ntp-doc epel-release -y
ntpdate europe.pool.ntp.org
systemctl start ntpd
systemctl enable ntpd

useradd cephuser &amp;&amp; echo &quot;Password1&quot; | passwd --stdin cephuser
echo &quot;cephuser ALL = (root) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/cephuser
chmod 0440 /etc/sudoers.d/cephuser

sed -i --follow-symlinks 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux
systemctl mask firewalld

reboot
</code></pre>

<pre><code># cep01
ssh-keygen
ssh-copy-it ceph02
ssh-copy-it ceph03

nano ~/.ssh/config
Host ceph01
   Hostname ceph01
   User cephuser
Host ceph02
   Hostname ceph02
   User cephuser
Host ceph03
   Hostname ceph03
   User cephuser

chmod 644 ~/.ssh/config
</code></pre>

<h3 id="install-ceph-deployer">Install ceph-deployer</h3>

<pre><code># cep01
sudo rpm -Uvh https://download.ceph.com/rpm-mimic/el7/noarch/ceph-release-1-1.el7.noarch.rpm

yum update
yum install -y ceph-deploy

mkdir /home/ceph/cluster1
cd ~/cluster1
</code></pre>

<h3 id="delete-ceph-config-if-exists">Delete ceph config if exists</h3>

<pre><code># cep01
ceph-deploy purge ceph01 ceph02 ceph03
# unmount if not working
ceph-deploy purgedata ceph01 ceph02 ceph03
ceph-deploy forgetkeys
</code></pre>

<h3 id="install-ceph">Install ceph</h3>

<pre><code>ceph-deploy install ceph01 ceph02 ceph03
ceph-deploy --cluster &lt;cluster-name&gt; new ceph01 ceph02 ceph03

# edit before pupulate config
nano &lt;cluster-name&gt;.conf
osd_max_object_name_len = 256
osd_max_object_namespace_len = 64

ceph-deploy --cluster &lt;cluster-name&gt; mon create ceph01 ceph02 ceph03
ceph-deploy --cluster &lt;cluster-name&gt; gatherkeys ceph01 ceph02 ceph03
</code></pre>

<pre><code>ceph-deploy disk list ceph01
ceph-deploy disk list ceph02
ceph-deploy disk list ceph03


ceph-deploy disk zap ceph01:sdb
ceph-deploy disk zap ceph02:sdb
ceph-deploy disk zap ceph03:sdb

ceph-deploy osd create ceph01:sdb
ceph-deploy osd create ceph02:sdb
ceph-deploy osd create ceph03:sdb

ceph-deploy osd create ceph01:sdc
ceph-deploy osd create ceph02:sdc
ceph-deploy osd create ceph03:sdc
</code></pre>

<h3 id="test-cluster">Test cluster</h3>

<pre><code>sudo ceph health
sudo ceph -s
sudo ceph osd tree

# install adminkey-ring
ceph-deploy admin ceph01 ceph02 ceph03
ssh ceph node01 sudo ceph osd lspools
ssh ceph node01 sudo ceph osd create mycorp 128
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Openshift]]></title>
            <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/kubernetes/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
                <link href="https://devopstales.github.io/home/install-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ceph cluster" />
                <link href="https://devopstales.github.io/home/ceph-block-device/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph Block Device" />
                <link href="https://devopstales.github.io/home/ceph-cephfs/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph CephFS" />
                <link href="https://devopstales.github.io/home/kubectl-multi-cluster-config/?utm_source=atom_feed" rel="related" type="text/html" title="Configure kubectl for multiple clusters" />
            
                <id>https://devopstales.github.io/home/ansible-openshift-install/</id>
            
            
            <published>2019-03-12T00:00:00+00:00</published>
            <updated>2019-03-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Ansible-openshift is a pre made ansible playbook for Openshift installation. In this Post I will show you how to use to install a new Openshift cluster.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../kubernetes/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../kubernetes/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../kubernetes/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../kubernetes/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../kubernetes/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../kubernetes/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../kubernetes/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../kubernetes/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../kubernetes/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../kubernetes/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../kubernetes/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../kubernetes/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../kubernetes/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../kubernetes/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../kubernetes/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<h3 id="environment">Environment</h3>

<pre><code>192.168.1.40    deployer
192.168.1.41    openshift01 # master node
192.168.1.42    openshift02 # infra node
192.168.1.43    openshift03 # worker node

# hardware requirement
4 CPU
16G RAM
</code></pre>

<h3 id="dns-config">DNS config</h3>

<pre><code>master.openshift     300 IN  A 192.168.1.41
openshift            300 IN  A 192.168.1.42
*.openshift            300 IN  A 192.168.1.42
</code></pre>

<h3 id="prerequirement">Prerequirement</h3>

<pre><code># deployer
yum install epel-release centos-release-openshift-origin311
yum --disablerepo=* --enablerepo=centos-ansible26 install ansible
yum install openshift-ansible nano

echo &quot;exclude=ansible&quot; &gt;&gt; /etc/yum.conf

nano ~/.ssh/config
Host openshift01
    Hostname openshift01.devopstales.intra
    User origin

Host openshift02
    Hostname openshift02.devopstales.intra
    User origin

Host openshift03
    Hostname openshift03.devopstales.intra
    User origin
</code></pre>

<pre><code># on all openshift hosts
hostnamectl set-hostname openshift01
yum -y update
yum -y install centos-release-openshift-origin311 epel-release docker git pyOpenSSL

useradd origin
passwd origin
echo -e 'Defaults:origin !requiretty\norigin ALL = (root) NOPASSWD:ALL' | tee /etc/sudoers.d/origin
chmod 440 /etc/sudoers.d/origin
reboot

# Disable swap permanently
nano /etc/fstab
#/dev/mapper/centos_openshift01-swap swap                    swap    defaults        0 0

sudo swapoff -a

sudo lvremove -Ay /dev/centos/swap
sudo lvextend -l +100%FREE centos/root
sudo xfs_growfs /

sudo nano /etc/default/grub
GRUB_TIMEOUT=5
GRUB_DEFAULT=saved
GRUB_DISABLE_SUBMENU=true
GRUB_TERMINAL_OUTPUT=&quot;console&quot;
# GRUB_CMDLINE_LINUX=&quot;rd.lvm.lv=centos/root rd.lvm.lv=centos/swap crashkernel=auto rhgb quiet&quot;
GRUB_CMDLINE_LINUX=&quot;rd.lvm.lv=centos/root crashkernel=auto rhgb quiet&quot;
GRUB_DISABLE_RECOVERY=&quot;true&quot;

dracut --regenerate-all -f
grub2-mkconfig -o /boot/grub2/grub.cfg
</code></pre>

<h3 id="configurate-installer">Configurate Installer</h3>

<pre><code># deployer

nano /etc/ansible/hosts
[OSEv3:children]
masters
nodes
etcd

[OSEv3:vars]
# admin user created in previous section
ansible_ssh_user=origin
ansible_become=true
openshift_deployment_type=origin
os_firewall_use_firewalld=True
openshift_clock_enabled=true

# use HTPasswd for authentication
openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'}]

# define default sub-domain for Master node
openshift_master_default_subdomain=openshift.devopstales.intra
osm_default_subdomain=openshift.devopstales.intra

# allow unencrypted connection within cluster
openshift_docker_insecure_registries=172.30.0.0/16

openshift_master_cluster_hostname=master.openshift.devopstales.intra
openshift_master_cluster_public_hostname=master.openshift.devopstales.intra
openshift_public_hostname=master.openshift.devopstales.intra

openshift_master_api_port=443
openshift_master_console_port=443

[masters]
openshift01 containerized=true openshift_public_hostname=master.openshift.devopstales.intra

[etcd]
openshift01 containerized=true

[nodes]
# defined values for [openshift_node_group_name] in the file below
# [/usr/share/ansible/openshift-ansible/roles/openshift_facts/defaults/main.yml]
openshift01 openshift_node_group_name='node-config-master'
openshift02 openshift_node_group_name='node-config-infra'
openshift03 openshift_node_group_name='node-config-compute'
</code></pre>

<h3 id="run-the-installer">Run the Installer</h3>

<pre><code># deployer
cd /usr/share/ansible/openshift-ansible/
sudo ansible-playbook playbooks/prerequisites.yml
sudo ansible-playbook playbooks/deploy_cluster.yml

# If installastion failed or went wrong, the following uninstallation script can be run, and running installation can be tried again:
sudo ansible-playbook playbooks/adhoc/uninstall.yml
</code></pre>

<h3 id="user-management">User management</h3>

<pre><code># on openshift master

cd /etc/origin/master/
# add user
htpasswd [/path/to/users.htpasswd] [user_name]
htpasswd htpasswd devopstales

# delete user
htpasswd -D [htpasswd/file/path/]  [user-name] [password]
htpasswd -D htpasswd devopstales Password1

# it will remove only the username from the htpasswd file by default it won’t remove user identity
oc delete  identity htpasswd_auth:user
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Use Ceph Block Device]]></title>
            <link href="https://devopstales.github.io/home/ceph-block-device/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/install-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ceph cluster" />
                <link href="https://devopstales.github.io/home/ceph-cephfs/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph CephFS" />
                <link href="https://devopstales.github.io/linux/ceph-block-device/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph Block Device" />
                <link href="https://devopstales.github.io/linux/install-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ceph cluster" />
                <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
            
                <id>https://devopstales.github.io/home/ceph-block-device/</id>
            
            
            <published>2019-03-12T00:00:00+00:00</published>
            <updated>2019-03-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Ceph block devices are thin-provisioned, resizable and store data striped over multiple OSDs in a Ceph cluster. Ceph block devices leverage RADOS capabilities such as snapshotting, replication and consistency. Ceph’s RADOS Block Devices (RBD) interact with OSDs using kernel modules or the librbd library.</p>

<h3 id="environment">Environment</h3>

<pre><code>192.168.1.31    ceph01
192.168.1.32    ceph02
192.168.1.33    ceph03
</code></pre>

<h3 id="install-client-cli">Install Client cli</h3>

<pre><code>ceph-deploy install client
ceph-deploy admin client

sudo chmod 644 /etc/ceph/ceph.client.admin.keyring
</code></pre>

<h3 id="basic-usage">Basic Usage</h3>

<pre><code># create pool
ceph osd pool create stack 64 64

# createdisk to pool
rbd create disk01 --size 10G --image-feature layering --pool stack --allow-shrink

# show list
rbd ls -l

# show info
rbd --image disk01 -p stack info

# resize
rbd resize --image disk01 -p stack --size 6G

# remove
rbd rm disk01 -p stack

# map the image to device
sudo rbd map disk01

# show mapping
rbd showmapped

# format with XFS
sudo mkfs.xfs /dev/rbd0

# mount device
sudo mount /dev/rbd0 /mnt
df -hT
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Use Ceph CephFS]]></title>
            <link href="https://devopstales.github.io/home/ceph-cephfs/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/install-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ceph cluster" />
                <link href="https://devopstales.github.io/home/ceph-block-device/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph Block Device" />
                <link href="https://devopstales.github.io/linux/install-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ceph cluster" />
                <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
                <link href="https://devopstales.github.io/linux/ceph-block-device/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph Block Device" />
            
                <id>https://devopstales.github.io/home/ceph-cephfs/</id>
            
            
            <published>2019-03-12T00:00:00+00:00</published>
            <updated>2019-03-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>The Ceph Filesystem (CephFS) is a POSIX-compliant filesystem that uses a Ceph Storage Cluster to store its data. The Ceph filesystem uses the same Ceph Storage Cluster system as Ceph Block Devices, Ceph Object Storage with its S3 and Swift APIs, or native bindings (librados).</p>

<h3 id="environment">Environment</h3>

<pre><code>192.168.1.31    ceph01
192.168.1.32    ceph02
192.168.1.33    ceph03
</code></pre>

<h3 id="prerequirements">Prerequirements</h3>

<pre><code># Create MDS (MetaData Server)
ceph-deploy --overwrite-conf mds create ceph02

# create pools
sudo ceph osd pool create cephfs_data 128
sudo ceph osd pool create cephfs_metadata 128

# enable pools
sudo ceph fs new cephfs cephfs_metadata cephfs_data

# show pools
sudo ceph osd lspools
sudo ceph fs ls
sudo ceph mds stat
</code></pre>

<h3 id="basic-usage">Basic Usage</h3>

<pre><code># Mount CephFS on a Client.

yum -y install ceph-fuse
ssh ceph@ceph01 &quot;sudo ceph-authtool -p /etc/ceph/ceph.client.admin.keyring&quot; &gt; admin.key
chmod 600 admin.key

mount -t ceph ceph01:6789:/ /mnt -o name=admin,secretfile=admin.key
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configure kubectl for multiple clusters]]></title>
            <link href="https://devopstales.github.io/home/kubectl-multi-cluster-config/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/proxmox-backup-pve-zsync/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with pve-zsync" />
                <link href="https://devopstales.github.io/home/proxmox-backup-sanoid/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with sanoid" />
                <link href="https://devopstales.github.io/home/proxmox-backup-znapzend/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with znapzend" />
                <link href="https://devopstales.github.io/home/proxmox-node-remove/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox node removal" />
                <link href="https://devopstales.github.io/home/pfsense-telegraf/?utm_source=atom_feed" rel="related" type="text/html" title="Install telegraf on pfsense" />
            
                <id>https://devopstales.github.io/home/kubectl-multi-cluster-config/</id>
            
            
            <published>2019-03-11T00:00:00+00:00</published>
            <updated>2019-03-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I use a multiple Kubernetes clusters on a daily basis, so keeping my configs organized is important to don’t confuse myself.</p>

<p>kubectl looks at an environment variable called KUBECONFIG to hold a colon-separated list of paths to configuration files, so I can use multiple cluster config files.</p>

<h3 id="download-kubernetes-config">Download kubernetes config</h3>

<pre><code>scp root@DEV_SERVER:/etc/kubernetes/admin.conf ~/.kube/dev-config
scp root@TST_SERVER:/etc/kubernetes/admin.conf ~/.kube/tst-config
scp root@UAT_SERVER:/etc/kubernetes/admin.conf ~/.kube/uat-config
scp root@PROD_SERVER:/etc/kubernetes/admin.conf ~/.kube/prod-config
</code></pre>

<h3 id="edit-config-files">Edit config files</h3>

<pre><code>nano ~/.kube/dev-config
...
- cluster:
    server: https://1.1.1.1:6443
  name: dev-config
...
contexts:
- context:
    cluster: dev-config
    user: dev-admin
  name: dev-config
...
users:
- name: dev-admin
...
</code></pre>

<h3 id="use-config-files-in-kubeconfig-variable">Use config files in KUBECONFIG variable</h3>

<pre><code>nano ~/.bashrc
export KUBECONFIG=$HOME/.kube/dev-config:$HOME/.kube/tst-config:$HOME/.kube/uat-config:$HOME/.kube/prod-config

echo $KUBECONFIG

/home/ME/.kube/dev-config:/home/ME/.kube/tst-config:/home/ME/.kube/uat-config:/home/ME/.kube/prod-config

source ~/.bashrc
</code></pre>

<h3 id="use-clusters-with-kubectl">Use clusters with kubectl</h3>

<pre><code># get the current context
kubectl config current-context

# use a different context
kubectl config use-context work-dev
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Proxmox backup with pve-zsync]]></title>
            <link href="https://devopstales.github.io/home/proxmox-backup-pve-zsync/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/proxmox-backup-sanoid/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with sanoid" />
                <link href="https://devopstales.github.io/home/proxmox-backup-znapzend/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with znapzend" />
                <link href="https://devopstales.github.io/home/proxmox-node-remove/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox node removal" />
                <link href="https://devopstales.github.io/home/kubectl-multi-cluster-config/?utm_source=atom_feed" rel="related" type="text/html" title="Configure kubectl for multiple clusters" />
                <link href="https://devopstales.github.io/linux/proxmox-backup-pve-zsync/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with pve-zsync" />
            
                <id>https://devopstales.github.io/home/proxmox-backup-pve-zsync/</id>
            
            
            <published>2019-03-11T00:00:00+00:00</published>
            <updated>2019-03-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Backup your machines on Pfsense with pve-zsync tool.</p>

<h3 id="the-servers">The servers</h3>

<pre><code>proxmox-node1 192.168.10.20
omv-nas1 192.168.10.50
</code></pre>

<h3 id="install-pve-zsync-on-servers">Install pve-zsync on servers</h3>

<pre><code>apt-get install pve-zsync
</code></pre>

<h3 id="configure-pve-zsync">Configure pve-zsync</h3>

<pre><code>pve-zsync create --source 107 --dest 192.168.10.50:tank --verbose --maxsnap 14 --name Backup_ZFS_srv_107

# test the config
cat /etc/cron.d/pve-zsync
* 8 * * * root pve-zsync sync --source 107 --dest 192.168.10.50:tank --name Backup_ZFS_srv_107 --maxsnap 14 --method ssh

# send diff
pve-zsync sync --source 107 --dest 192.168.10.50:tank --verbose --maxsnap 14 --name Backup_ZFS_srv_107

# the tool send the vm config to the /var/lib/pve-zsync/
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Proxmox backup with sanoid]]></title>
            <link href="https://devopstales.github.io/home/proxmox-backup-sanoid/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/proxmox-backup-pve-zsync/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with pve-zsync" />
                <link href="https://devopstales.github.io/home/proxmox-backup-znapzend/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with znapzend" />
                <link href="https://devopstales.github.io/home/proxmox-node-remove/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox node removal" />
                <link href="https://devopstales.github.io/home/kubectl-multi-cluster-config/?utm_source=atom_feed" rel="related" type="text/html" title="Configure kubectl for multiple clusters" />
                <link href="https://devopstales.github.io/linux/proxmox-backup-pve-zsync/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with pve-zsync" />
            
                <id>https://devopstales.github.io/home/proxmox-backup-sanoid/</id>
            
            
            <published>2019-03-11T00:00:00+00:00</published>
            <updated>2019-03-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Backup your machines on Pfsense with sanoid tool.</p>

<h3 id="the-servers">The servers</h3>

<pre><code>proxmox-node1 192.168.10.20
omv-nas1 192.168.10.50
</code></pre>

<h3 id="build-sanoid-on-servers">Build sanoid on servers</h3>

<pre><code>apt-get install libcapture-tiny-perl libconfig-inifiles-perl git

cd /opt
git clone https://github.com/jimsalterjrs/sanoid

ln /opt/sanoid/sanoid /usr/sbin/
</code></pre>

<p>Or you can build deb package:</p>

<h3 id="build-and-install-sanoid-deb-package">Build and install sanoid deb package</h3>

<pre><code>sudo apt-get install devscripts debhelper dh-systemd
git clone https://github.com/jimsalterjrs/sanoid.git
cd sanoid
debuild -us -uc

cd ..
sudo apt-get install libconfig-inifiles-perl
sudo dpkg -i sanoid_2.0.1_all.deb
</code></pre>

<h3 id="configure-sanoid">Configure sanoid</h3>

<pre><code>mkdir -p /etc/sanoid
cp /opt/sanoid/sanoid.conf /etc/sanoid/sanoid.conf
cp /opt/sanoid/sanoid.defaults.conf /etc/sanoid/sanoid.defaults.conf

nano /etc/crontab
* 2 * * * root /usr/sbin/sanoid --cron
* 3 * * * root /usr/sbin/syncoid --recursive tank root@192.168.10.50:tank

</code></pre>

<pre><code>    ####################
    # sanoid.conf file #
    ####################
    [local-zfs]
            use_template = production
    #############################
    # templates below this line #
    #############################
    [template_production]
            # store hourly snapshots 36h
            # hourly = 36
            # store 14 days of daily snaps
            daily = 14
            # store back 6 months of monthly
            # monthly = 6
            # store back 3 yearly (remove manually if to large)
            # yearly = 3
            # create new snapshots
            autosnap = yes
            # clean old snapshot
            autoprune = yes
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Proxmox backup with znapzend]]></title>
            <link href="https://devopstales.github.io/home/proxmox-backup-znapzend/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/proxmox-backup-znapzend/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with znapzend" />
                <link href="https://devopstales.github.io/home/proxmox-backup-pve-zsync/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with pve-zsync" />
                <link href="https://devopstales.github.io/home/proxmox-backup-sanoid/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with sanoid" />
                <link href="https://devopstales.github.io/home/proxmox-node-remove/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox node removal" />
                <link href="https://devopstales.github.io/home/kubectl-multi-cluster-config/?utm_source=atom_feed" rel="related" type="text/html" title="Configure kubectl for multiple clusters" />
            
                <id>https://devopstales.github.io/home/proxmox-backup-znapzend/</id>
            
            
            <published>2019-03-11T00:00:00+00:00</published>
            <updated>2019-03-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Backup your machines on Pfsense with znapzend tool.</p>

<h3 id="the-servers">The servers</h3>

<pre><code>proxmox-node1 192.168.10.20
omv-nas1 192.168.10.50
</code></pre>

<h3 id="create-sub-volume">Create sub volume</h3>

<p>Note that while recursive configurations are well supported to set up backup and retention policies for a whole dataset subtree under the dataset to which you have applied explicit configuration, at this time pruning of such trees (&ldquo;I want every dataset under var except var/tmp&rdquo;) is not supported.</p>

<pre><code>zfs create local-zfs/vm-data
pvesm add zfspool local-zfs --pool local-zfs/vm-data
</code></pre>

<h3 id="build-znapzend-on-servers">Build znapzend on servers</h3>

<pre><code>apt-get install perl unzip git mbuffer build-essential git

cd /root
git clone https://github.com/oetiker/znapzend
cd /root/znapzend
./configure --prefix=/opt/znapzend

make
make install

ln -s /opt/znapzend/bin/znapzend /usr/local/bin/znapzend
ln -s /opt/znapzend/bin/znapzendzetup /usr/local/bin/znapzendzetup
ln -s /opt/znapzend/bin/znapzendztatz /usr/local/bin/znapzendztatz

znapzend --version
</code></pre>

<p>Or you can download a the deb package from here:
<a href="https://github.com/devopstales/znapzend-debian/releases">https://github.com/devopstales/znapzend-debian/releases</a></p>

<h3 id="install-znapzend-on-servers">Install znapzend on servers</h3>

<pre><code>dpkg -i znapzend_0.19.1_amd64_stretch.deb
</code></pre>

<h3 id="configure-znapzend">Configure znapzend</h3>

<pre><code>znapzendzetup create --recursive\
--mbuffer=/usr/bin/mbuffer \
--mbuffersize=1G \
SRC '2d=&gt;1d' local-zfs/vmdata \
DST:a '14d=&gt;1d' root@192.168.10.50:tank

# test
znapzend --debug --noaction --runonce=local-zfs
znapzendzetup list
</code></pre>

<h3 id="create-znapzend-service">Create znapzend service</h3>

<pre><code>nano /etc/default/znapzend
ZNAPZENDOPTIONS=&quot;--logto=/var/log/znapzend.log&quot;

systemctl enable znapzend.service
systemctl restart znapzend.service
systemctl status znapzend.service
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Proxmox node removal]]></title>
            <link href="https://devopstales.github.io/home/proxmox-node-remove/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/proxmox-node-remove/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox node removal" />
                <link href="https://devopstales.github.io/home/pfsense-telegraf/?utm_source=atom_feed" rel="related" type="text/html" title="Install telegraf on pfsense" />
                <link href="https://devopstales.github.io/home/prometheus-influxdb/?utm_source=atom_feed" rel="related" type="text/html" title="Install Prometheus with Influxdb storage" />
                <link href="https://devopstales.github.io/home/install-postgresql/?utm_source=atom_feed" rel="related" type="text/html" title="Install PostgreSQL" />
                <link href="https://devopstales.github.io/home/naemon-influxdb/?utm_source=atom_feed" rel="related" type="text/html" title="Install Nemon with Influxdb storage" />
            
                <id>https://devopstales.github.io/home/proxmox-node-remove/</id>
            
            
            <published>2019-03-07T00:00:00+00:00</published>
            <updated>2019-03-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>The correct way to remove nod from proxmox cluster.</p>

<h3 id="display-all-active-nodes">Display all active nodes</h3>

<pre><code>root@proxmox-node2:~# pvecm nodes
Membership information
----------------------
Nodeid Votes Name
1 1 proxmox-node1 (local)
2 1 proxmox-node2
3 1 proxmox-node3
4 1 proxmox-node4
</code></pre>

<h3 id="shutdown-node-and-remove">Shutdown node and remove</h3>

<pre><code>root@proxmox-node2:~# pvecm delnode proxmox-node3

root@proxmox-node2:~# ls -l /etc/pve/nodes/
proxmox-node1 proxmox-node2 proxmox-node3 proxmox-node4

root@proxmox-node2:~# rm -rf /etc/pve/nodes/proxmox-node3
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install telegraf on pfsense]]></title>
            <link href="https://devopstales.github.io/home/pfsense-telegraf/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/monitoring/pfsense-telegraf/?utm_source=atom_feed" rel="related" type="text/html" title="Install telegraf on pfsense" />
                <link href="https://devopstales.github.io/home/prometheus-alertmanagger/?utm_source=atom_feed" rel="related" type="text/html" title="Install Alertmanagger" />
                <link href="https://devopstales.github.io/home/prometheus-influxdb/?utm_source=atom_feed" rel="related" type="text/html" title="Install Prometheus with Influxdb storage" />
                <link href="https://devopstales.github.io/home/install-postgresql/?utm_source=atom_feed" rel="related" type="text/html" title="Install PostgreSQL" />
                <link href="https://devopstales.github.io/home/naemon-influxdb/?utm_source=atom_feed" rel="related" type="text/html" title="Install Nemon with Influxdb storage" />
            
                <id>https://devopstales.github.io/home/pfsense-telegraf/</id>
            
            
            <published>2019-03-06T00:00:00+00:00</published>
            <updated>2019-03-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Install and configure telegraf on pfsense to provides system information to prometheus.</p>

<h3 id="install-telegraf">Install telegraf</h3>

<ul>
<li>At the System / Package menu install the telegraf service to the pfsense.</li>
<li>ssh to the pfsense server and open a shell</li>
</ul>

<p><img src="/img/include/pfsense-telegraf.png" alt="Example image" /></p>

<h3 id="install-nano">Install nano</h3>

<pre><code>pkg
pkg update
pkg install nano
</code></pre>

<h3 id="configure-telegraf">Configure telegraf</h3>

<pre><code>cd /usr/local/etc

nano telegraf.conf
[[outputs.prometheus_client]]
 listen = &quot;:9273&quot;

echo &quot;telegraf_enable=&quot;YES&quot;&quot; &gt;&gt; /etc/rc.conf

cd /usr/local/etc/rc.d
./telegraf restart
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Prometheus with Influxdb storage]]></title>
            <link href="https://devopstales.github.io/home/prometheus-influxdb/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/prometheus-node-exporter/?utm_source=atom_feed" rel="related" type="text/html" title="Install Node-exporter" />
                <link href="https://devopstales.github.io/home/prometheus-install/?utm_source=atom_feed" rel="related" type="text/html" title="Prometheus Install" />
                <link href="https://devopstales.github.io/monitoring/prometheus-influxdb/?utm_source=atom_feed" rel="related" type="text/html" title="Install Prometheus with Influxdb storage" />
                <link href="https://devopstales.github.io/home/naemon-influxdb/?utm_source=atom_feed" rel="related" type="text/html" title="Install Nemon with Influxdb storage" />
                <link href="https://devopstales.github.io/home/install-postgresql/?utm_source=atom_feed" rel="related" type="text/html" title="Install PostgreSQL" />
            
                <id>https://devopstales.github.io/home/prometheus-influxdb/</id>
            
            
            <published>2019-02-02T00:00:00+00:00</published>
            <updated>2019-02-02T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Example article description</blockquote><p>Use Influxdb to as storage for Prometheus.</p>

<h3 id="install-infludxb">Install Infludxb</h3>

<pre><code>cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/influxdb.repo
[influxdb]
name = InfluxDB Repository - RHEL \$releasever
baseurl = https://repos.influxdata.com/rhel/\$releasever/\$basearch/stable
enabled = 1
gpgcheck = 1
gpgkey = https://repos.influxdata.com/influxdb.key
EOF

yum install influxdb -y
</code></pre>

<h3 id="configure-influxdb">Configure Influxdb</h3>

<pre><code>nano /etc/influxdb/influxdb.conf
[http]
   enabled = true
   bind-address = &quot;localhost:8086&quot;
   auth-enabled = false
</code></pre>

<h3 id="configure-prometheus">Configure Prometheus</h3>

<pre><code>nano /etc/prometheus/prometheus.yml
remote_write:
  - url: &quot;http://localhost:8086/api/v1/prom/write?db=prometheus&quot;

remote_read:
  - url: &quot;http://localhost:8086/api/v1/prom/read?db=prometheus&quot;

# with authentication
#remote_write:
#  - url: &quot;http://localhost:8086/api/v1/prom/write?db=prometheus&amp;u=username&amp;p=password&quot;

#remote_read:
#  - url: &quot;http://localhost:8086/api/v1/prom/read?db=prometheus&amp;u=username&amp;p=password&quot;
</code></pre>

<pre><code>systemctl start influxdb
systemctl enable influxdb

echo 'CREATE DATABASE &quot;prometheus&quot;' | influx

systemctl start prometheus
systemctl status prometheus

influx
USE prometheus
select * from /.*/ limit 1
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install PostgreSQL]]></title>
            <link href="https://devopstales.github.io/home/install-postgresql/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/install-postgresql/?utm_source=atom_feed" rel="related" type="text/html" title="Install PostgreSQL" />
                <link href="https://devopstales.github.io/home/naemon-influxdb/?utm_source=atom_feed" rel="related" type="text/html" title="Install Nemon with Influxdb storage" />
                <link href="https://devopstales.github.io/home/prometheus-alertmanagger/?utm_source=atom_feed" rel="related" type="text/html" title="Install Alertmanagger" />
                <link href="https://devopstales.github.io/home/prometheus-node-exporter/?utm_source=atom_feed" rel="related" type="text/html" title="Install Node-exporter" />
                <link href="https://devopstales.github.io/home/prometheus-install/?utm_source=atom_feed" rel="related" type="text/html" title="Prometheus Install" />
            
                <id>https://devopstales.github.io/home/install-postgresql/</id>
            
            
            <published>2019-01-10T00:00:00+00:00</published>
            <updated>2019-01-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this pos I will show you how to install Postgresql on difrent Linux distributions.</p>

<p>PostgreSQL, also known as Postgres, is a free and open-source relational database management system (RDBMS).</p>

<h3 id="install-postgresql-9-6-on-centos-7">Install PostgreSQL 9.6 on CentOS 7</h3>

<pre><code>yum install epel-release -y
yum update -y
rpm -Uvh https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-centos10-10-2.noarch.rpm

yum install -y postgresql96-server postgresql96 postgresql96-contrib unzip
/usr/pgsql-9.6/bin/postgresql96-setup initdb

nano /var/lib/pgsql/9.6/data/pg_hba.conf
local   all             all                                      trust

sudo systemctl start postgresql-9.6
sudo systemctl enable postgresql-9.6
</code></pre>

<h3 id="install-postgresql-10-on-centos-7">Install PostgreSQL 10 on CentOS 7</h3>

<pre><code>yum install epel-release -y
yum update -y
rpm -Uvh https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-centos10-10-2.noarch.rpm

yum -y install postgresql10-server postgresql10-contrib postgresql10
/usr/pgsql-10/bin/postgresql-10-setup initdb

nano /var/lib/pgsql/10/data/pg_hba.conf
host    all             all             127.0.0.1/32            md5
host    all             all             ::1/128                 md5
local   all             postgres                                peer

sudo systemctl start postgresql-10
sudo systemctl enable postgresql-10
</code></pre>

<h3 id="install-postgresql-10-on-centos-8">Install PostgreSQL 10 on CentOS 8</h3>

<pre><code>yum install postgresql postgresql-server postgresql-contrib -y

postgresql-setup --initdb

nano /var/lib/pgsql/data/pg_hba.conf
host    all             all             127.0.0.1/32            md5
host    all             all             ::1/128                 md5
local   all             postgres                                peer

systemctl start postgresql
systemctl enable postgresql
</code></pre>

<h3 id="install-postgresql-10-on-debian">Install PostgreSQL 10 on Debian</h3>

<pre><code>wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
sudo sh -c 'echo &quot;deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -sc)-pgdg main&quot; &gt; /etc/apt/sources.list.d/PostgreSQL.list'

apt update
apt-get install postgresql-10
apt-get install pgadmin4

systemctl enable postgresql.service
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Nemon with Influxdb storage]]></title>
            <link href="https://devopstales.github.io/home/naemon-influxdb/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/monitoring/naemon-influxdb/?utm_source=atom_feed" rel="related" type="text/html" title="Install Nemon with Influxdb storage" />
                <link href="https://devopstales.github.io/home/prometheus-alertmanagger/?utm_source=atom_feed" rel="related" type="text/html" title="Install Alertmanagger" />
                <link href="https://devopstales.github.io/home/prometheus-node-exporter/?utm_source=atom_feed" rel="related" type="text/html" title="Install Node-exporter" />
                <link href="https://devopstales.github.io/home/prometheus-install/?utm_source=atom_feed" rel="related" type="text/html" title="Prometheus Install" />
                <link href="https://devopstales.github.io/monitoring/prometheus-alertmanagger/?utm_source=atom_feed" rel="related" type="text/html" title="Install Alertmanagger" />
            
                <id>https://devopstales.github.io/home/naemon-influxdb/</id>
            
            
            <published>2019-01-01T00:00:00+00:00</published>
            <updated>2019-01-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I prefer to use Naemon (a fork of nagos) with Influxdb as a storage for graphical data.</p>

<h3 id="install-naemon">Install Naemon</h3>

<pre><code>yum install epel-release nano -y
yum install httpd php php-gd -y

rpm -Uvh &quot;https://labs.consol.de/repo/stable/rhel7/x86_64/labs-consol-stable.rhel7.noarch.rpm&quot;

yum install naemon* -y
yum install nagios-plugins nagios-plugins-all nagios-plugins-nrpe nrpe -y

nano /etc/php.ini
date.timezone = Europe/Budapest


systemctl enable httpd
systemctl enable naemon
systemctl start httpd
systemctl start naemon

htpasswd /etc/thruk/htpasswd thrukadmin
# http://SERVER-IP/naemon
</code></pre>

<h3 id="install-infludxb">Install Infludxb</h3>

<pre><code>cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/influxdb.repo
[influxdb]
name = InfluxDB Repository - RHEL \$releasever
baseurl = https://repos.influxdata.com/rhel/\$releasever/\$basearch/stable
enabled = 1
gpgcheck = 1
gpgkey = https://repos.influxdata.com/influxdb.key
EOF

yum install influxdb -y

nano /etc/influxdb/influxdb.conf
[http]
   enabled = true
   bind-address = &quot;localhost:8086&quot;
   auth-enabled = false

systemctl start influxdb
systemctl enable influxdb
</code></pre>

<h3 id="configurate-naemon">Configurate Naemon</h3>

<pre><code>sed -i &quot;s@^process_performance_data=0@#process_performance_data=0@&quot; /etc/naemon/naemon.cfg
# config nagios
nano /etc/naemon/module-conf.d/nagios_nagflux.cfg
process_performance_data=1

host_perfdata_file=/var/naemon/host-perfdata
host_perfdata_file_template=DATATYPE::HOSTPERFDATA\tTIMET::$TIMET$\tHOSTNAME::$HOSTNAME$\tHOSTPERFDATA::$HOSTPERFDATA$\tHOSTCHECKCOMMAND::$HOSTCHECKCOMMAND$
host_perfdata_file_mode=a
host_perfdata_file_processing_interval=15
host_perfdata_file_processing_command=process-host-perfdata-file-nagflux

service_perfdata_file=/var/naemon/service-perfdata
service_perfdata_file_template=DATATYPE::SERVICEPERFDATA\tTIMET::$TIMET$\tHOSTNAME::$HOSTNAME$\tSERVICEDESC::$SERVICEDESC$\tSERVICEPERFDATA::$SERVICEPERFDATA$\tSERVICECHECKCOMMAND::$SERVICECHECKCOMMAND$
service_perfdata_file_mode=a
service_perfdata_file_processing_interval=15
service_perfdata_file_processing_command=process-service-perfdata-file-nagflux

chown naemon:naemon /etc/naemon/module-conf.d/nagios_nagflux.cfg

nano /etc/naemon/conf.d/histou.cfg
define command {
    command_name    process-host-perfdata-file-nagflux
    command_line    /bin/mv /var/naemon/host-perfdata /var/nagflux/perfdata/$TIMET$.perfdata.host
    }

define command {
    command_name    process-service-perfdata-file-nagflux
    command_line    /bin/mv /var/naemon/service-perfdata /var/nagflux/perfdata/$TIMET$.perfdata.service
    }

define host {
   name       host-grafana
   action_url http://192.168.10.112/grafana/dashboard/script/histou.js?host=$HOSTNAME$&amp;theme=light&amp;annotations=true
   notes_url   http://192.168.10.112/dokuwiki/doku.php?id=inventory:$HOSTNAME$
   register   0
}

define service {
   name       service-grafana
   action_url http://192.168.10.112/grafana/dashboard/script/histou.js?host=$HOSTNAME$&amp;service=$SERVICEDESC$&amp;theme=light&amp;annotations=true
   register   0
}

mkdir /var/naemon/
chown -R naemon:naemon /var/naemon/

cd /etc/thruk/ssi/
cp extinfo-header.ssi.example extinfo-header.ssi
cp status-header.ssi.example status-header.ssi

systemctl restart naemon

ll /var/naemon/
ll /var/nagflux/perfdata/
</code></pre>

<h3 id="install-nagflux">Install nagflux</h3>

<pre><code>cd /usr/bin/
wget https://github.com/Griesbacher/nagflux/releases/download/v0.4.1/nagflux
chmod +x nagflux

mkdir -p /var/nagflux/perfdata
mkdir -p /var/nagflux/spool
chown -R naemon:apache /var/nagflux

mkdir /etc/nagflux
cat &lt;&lt;EOF | sudo tee /etc/nagflux/config.gcfg
[main]
NagiosSpoolfileFolder = &quot;/var/nagflux/perfdata&quot;
NagiosSpoolfileWorker = 1
InfluxWorker = 2
MaxInfluxWorker = 5
DumpFile = &quot;/var/log/nagflux/nagflux.dump&quot;
NagfluxSpoolfileFolder = &quot;/var/nagflux/spool&quot;
FieldSeparator = &quot;&amp;&quot;
BufferSize = 1000
FileBufferSize = 65536
DefaultTarget = &quot;Influxdb&quot;

[Log]
LogFile = &quot;/var/log/nagflux/nagflux.log&quot;
MinSeverity = &quot;INFO&quot;

[InfluxDBGlobal]
CreateDatabaseIfNotExists = true
NastyString = &quot;&quot;
NastyStringToReplace = &quot;&quot;
HostcheckAlias = &quot;hostcheck&quot;

[InfluxDB &quot;nagflux&quot;]
Enabled = true
Version = 1.0
Address = &quot;http://localhost:8086&quot;
Arguments = &quot;precision=ms&amp;db=nagflux&amp;u=admin&amp;p=Password1&quot;
StopPullingDataIfDown = true

[Livestatus]
#tcp or file
Type = &quot;file&quot;
#tcp: 127.0.0.1:6557 or file /var/run/live
Address = &quot;/var/cache/naemon/live&quot;
MinutesToWait = 3
Version = &quot;&quot;
EOF

mkdir /var/log/nagflux
mkdir /var/nagflux

cat &lt;&lt;EOF | sudo tee /etc/systemd/system/nagflux.service
[Unit]
Description=A connector which transforms performancedata from Nagios/Icinga(2)/Naemon to InfluxDB/Elasticsearch
Documentation=https://github.com/Griesbacher/nagflux
After=network-online.target

[Service]
User=root
Group=root
ExecStart=/usr/bin/nagflux -configPath /etc/nagflux/config.gcfg
Restart=on-failure

[Install]
WantedBy=multi-user.target
Alias=nagflux.service
EOF

systemctl daemon-reload
systemctl start nagflux
systemctl enable nagflux

tailf /var/log/nagflux/nagflux.log
</code></pre>

<h3 id="install-grafana">Install grafana</h3>

<pre><code>curl -s https://packagecloud.io/install/repositories/grafana/stable/script.rpm.sh | sudo bash
yum install grafana -y

cp /etc/grafana/grafana.ini /etc/grafana/grafana.ini.bak
echo &quot;&quot; &gt; /etc/grafana/grafana.ini
nano /etc/grafana/grafana.ini
[paths]
logs = /var/log/grafana

[log]
mode = file
[log.file]
level =  Info
daily_rotate = true

[server]
http_port = 3000
http_addr = 0.0.0.0
domain = localhost
root_url = %(protocol)s://%(domain)s/grafana/
enable_gzip = false

[snapshots]
external_enabled = false

[security]
disable_gravatar = true
# same username and password for thruk
admin_user = thrukadmin
admin_password = Password1

[users]
allow_sign_up = false
default_theme = light

[auth.basic]
enabled = false

[auth.proxy]
enabled = true
auto_sign_up = true

[alerting]
enabled = true
execute_alerts = true

nano /etc/httpd/conf.d/grafana.conf
&lt;IfModule !mod_proxy.c&gt;
    LoadModule proxy_module /usr/lib64/httpd/modules/mod_proxy.so
&lt;/IfModule&gt;
&lt;IfModule !mod_proxy_http.c&gt;
    LoadModule proxy_http_module /usr/lib64/httpd/modules/mod_proxy_http.so
&lt;/IfModule&gt;

&lt;Location /grafana&gt;
    ProxyPass http://127.0.0.1:3000 retry=0 disablereuse=On
    ProxyPassReverse http://127.0.0.1:3000/grafana
    RewriteEngine On
    RewriteRule .* - [E=PROXY_USER:%{LA-U:REMOTE_USER},NS]
    SetEnvIf Request_Protocol ^HTTPS.* IS_HTTPS=1
    SetEnvIf Authorization &quot;^.+$&quot; IS_BASIC_AUTH=1
    # without thruk cookie auth, use the proxy user from the rewrite rule above
    RequestHeader set X-WEBAUTH-USER &quot;%{PROXY_USER}s&quot;  env=IS_HTTPS
    RequestHeader set X-WEBAUTH-USER &quot;%{PROXY_USER}e&quot;  env=!IS_HTTPS
    # when thruk cookie auth is used, fallback to remote user directly
    RequestHeader set X-WEBAUTH-USER &quot;%{REMOTE_USER}e&quot; env=!IS_BASIC_AUTH
    RequestHeader unset Authorization
&lt;/Location&gt;

echo &quot;
apiVersion: 1

deleteDatasources:
  - name: nagflux

datasources:
- name: nagflux
  type: influxdb
  url: http://localhost:8086
  access: proxy
  database: nagflux
  isDefault: true
  version: 1
  editable: true
&quot; &gt; /etc/grafana/provisioning/datasources/nagflux.yaml

systemctl start grafana-server
systemctl enable grafana-server
systemctl restart httpd


# http://SERVER-IP:3000
# admin/admin

# datasource:
nagflux
influxdb
http://localhost:8086
</code></pre>

<h3 id="install-histou">Install histou</h3>

<pre><code>cd /tmp
wget -O histou.tar.gz https://github.com/Griesbacher/histou/archive/v0.4.3.tar.gz
mkdir -p /var/www/html/histou
cd /var/www/html/histou
tar xzf /tmp/histou.tar.gz --strip-components 1
cp histou.ini.example histou.ini
cp histou.js /usr/share/grafana/public/dashboards/

nano /usr/share/grafana/public/dashboards/histou.js
var url = 'http://192.168.10.112/histou/';

systemctl restart httpd
systemctl restart grafana-server

# http://192.168.10.112/histou/?host=localhost&amp;service=PING
# http://192.168.10.112:3000/dashboard/script/histou.js?host=localhost&amp;service=PING

# nagios config

sed -i '/name.*generic-host/a\        use                             host-grafana' /etc/naemon/conf.d/templates/hosts.cfg
sed -i '/name.*generic-service/a\        use                             service-grafana' /etc/naemon/conf.d/templates/services.cfg

systemctl restart naemon
</code></pre>

<h3 id="inpluxdb-commands">Inpluxdb commands</h3>

<pre><code>influx
create database nagflux;
CREATE USER &quot;admin&quot; WITH PASSWORD 'Password1' WITH ALL PRIVILEGES;
show DATABASES;
USE nagflux;
select * from /.*/ limit 1;
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Alertmanagger]]></title>
            <link href="https://devopstales.github.io/home/prometheus-alertmanagger/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/monitoring/prometheus-alertmanagger/?utm_source=atom_feed" rel="related" type="text/html" title="Install Alertmanagger" />
                <link href="https://devopstales.github.io/home/prometheus-node-exporter/?utm_source=atom_feed" rel="related" type="text/html" title="Install Node-exporter" />
                <link href="https://devopstales.github.io/home/prometheus-install/?utm_source=atom_feed" rel="related" type="text/html" title="Prometheus Install" />
            
                <id>https://devopstales.github.io/home/prometheus-alertmanagger/</id>
            
            
            <published>2018-10-18T00:00:00+00:00</published>
            <updated>2018-10-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Example article description</blockquote><p>The Alertmanager handles alerts sent by client applications such as the Prometheus server.</p>

<h3 id="download-alertmanager">Download Alertmanager</h3>

<pre><code>wget https://github.com/prometheus/alertmanager/releases/download/v0.15.0-rc.1/alertmanager-0.15.0-rc.1.linux-amd64.tar.gz
tar -xzf alertmanager-0.15.0-rc.1.linux-amd64.tar.gz
</code></pre>

<h3 id="install-binaris">Install binaris</h3>

<pre><code>useradd --no-create-home --shell /bin/false alertmanager

mkdir /etc/alertmanager
mkdir /etc/alertmanager/template
mkdir -p /var/lib/alertmanager/data
touch /etc/alertmanager/alertmanager.yml

chown -R alertmanager:alertmanager /etc/alertmanager
chown -R alertmanager:alertmanager /var/lib/alertmanager

cp alertmanager-*linux-amd64/alertmanager /usr/local/bin/
cp alertmanager-*linux-amd64/amtool /usr/local/bin/

chown alertmanager:alertmanager /usr/local/bin/alertmanager
chown alertmanager:alertmanager /usr/local/bin/amtool
</code></pre>

<h3 id="create-servis-for-alertmanager">Create servis for Alertmanager</h3>

<pre><code>nano /etc/systemd/system/alertmanager.service
[Unit]
Description=Prometheus Alertmanager Service
Wants=network-online.target
After=network.target

[Service]
User=alertmanager
Group=alertmanager
Type=simple
ExecStart=/usr/local/bin/alertmanager \
    --config.file /etc/alertmanager/alertmanager.yml \
    --storage.path /var/lib/alertmanager/data
Restart=always

[Install]
WantedBy=multi-user.target
</code></pre>

<h3 id="configure-alertmanager">Configure Alertmanager</h3>

<pre><code>nano /etc/alertmanager/alertmanager.yml
global:
  smtp_smarthost: 'localhost:25'
  smtp_from: 'alertmanager@devopstales.intra'
#  smtp_auth_username: 'alertmanager'
#  smtp_auth_password: 'password'

templates:
- '/etc/alertmanager/template/*.tmpl'

route:
  repeat_interval: 3h
  receiver: mails

receivers:
- name: 'mails'
  email_configs:
  - to: 'admin@devopstales.intra'
</code></pre>

<h3 id="configure-prometheus">Configure Prometheus</h3>

<pre><code>nano /etc/prometheus/prometheus.yml
global:
  scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.

alerting:
  alertmanagers:
  - static_configs:
    - targets:
       - localhost:9093
</code></pre>

<pre><code>sudo systemctl daemon-reload
sudo systemctl enable alertmanager
sudo systemctl start alertmanager
sudo systemctl ststus alertmanager
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Node-exporter]]></title>
            <link href="https://devopstales.github.io/home/prometheus-node-exporter/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/prometheus-install/?utm_source=atom_feed" rel="related" type="text/html" title="Prometheus Install" />
                <link href="https://devopstales.github.io/monitoring/prometheus-node-exporter/?utm_source=atom_feed" rel="related" type="text/html" title="Install Node-exporter" />
                <link href="https://devopstales.github.io/monitoring/prometheus-install/?utm_source=atom_feed" rel="related" type="text/html" title="Prometheus Install" />
            
                <id>https://devopstales.github.io/home/prometheus-node-exporter/</id>
            
            
            <published>2018-10-17T00:00:00+00:00</published>
            <updated>2018-10-17T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Example article description</blockquote><p>Install node-exporter to provides detailed information about the system, including CPU, disk, and memory usage.</p>

<h3 id="download-node-exporter">Download Node-exporter</h3>

<pre><code>cd /tmp
wget https://github.com/prometheus/node_exporter/releases/download/v0.16.0/node_exporter-0.16.0.linux-amd64.tar.gz
tar -xzf node_exporter-0.16.0.linux-amd64.tar.gz
</code></pre>

<h3 id="install-binaris">Install binaris</h3>

<pre><code>sudo useradd -rs /bin/false node_exporter

sudo mv node_exporter*linux-amd64/node_exporter /usr/local/bin
mkdir -p /etc/node_exporter/data

chown -R node_exporter:node_exporter /etc/node_exporter

# host role based teg
cat &lt;&lt;EOF &gt; /etc/node_exporter/data/roles.prom
machine_role{role=&quot;postfix&quot;} 1
machine_role{role=&quot;apache&quot;} 1
EOF
</code></pre>

<h3 id="create-servis-for-node-exporter">Create servis for Node-exporter</h3>

<pre><code>cat &lt;&lt;EOF &gt; /etc/systemd/system/node_exporter.service
[Unit]
Description=Node Exporter
Wants=network-online.target
After=network-online.target

[Service]
User=node_exporter
Group=node_exporter
Type=simple
ExecStart=/usr/local/bin/node_exporter --collector.textfile.directory /etc/node_exporter/data/

[Install]
WantedBy=multi-user.target
EOF
</code></pre>

<pre><code>systemctl daemon-reload
systemctl enable node_exporter
systemctl start node_exporter
systemctl status node_exporter
</code></pre>

<h3 id="configure-prometheus">Configure Prometheus</h3>

<pre><code>nano /etc/prometheus/prometheus.yml
global:
  scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.

alerting:
  alertmanagers:
  - static_configs:
    - targets:
       - localhost:9093

rule_files:
  # - &quot;first_rules.yml&quot;
  # - &quot;second_rules.yml&quot;

scrape_configs:
  - job_name: 'prometheus_metrics'
    scrape_interval: 5s
    static_configs:
      - targets: ['prometheus01.devopstales.intra:9090']

  - job_name: 'node_exporter_metrics'
    scrape_interval: 5s
    static_configs:
      - targets: ['prometheus01.devopstales.intra:9100']
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Prometheus Install]]></title>
            <link href="https://devopstales.github.io/home/prometheus-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/monitoring/prometheus-install/?utm_source=atom_feed" rel="related" type="text/html" title="Prometheus Install" />
            
                <id>https://devopstales.github.io/home/prometheus-install/</id>
            
            
            <published>2018-08-21T00:00:00+00:00</published>
            <updated>2018-08-21T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Example article description</blockquote><p>Prometheus is an open-source monitoring system with a built-in noSQL time-series database. It offers a multi-dimensional data model, a flexible query language, and diverse visualization possibilities. Prometheus collects metrics from http nedpoint. Most service dind&rsquo;t have this endpoint so you need optional programs that generate additional metrics cald exporters.</p>

<p>In this tutorial, you&rsquo;ll install, configure, and secure Prometheus and Node Exporter to generate metrics about your server&rsquo;s performance.</p>

<h3 id="download-prometheus">Download Prometheus</h3>

<pre><code>curl -LO &quot;https://github.com/prometheus/prometheus/releases/download/v2.2.1/prometheus-2.2.1.linux-amd64.tar.gz&quot;
tar -xzf prometheus-2.2.1.linux-amd64.tar.gz
</code></pre>

<h3 id="install-binaris">Install binaris</h3>

<pre><code>cp prometheus-*linux-amd64/prometheus /usr/local/bin/
cp prometheus-*linux-amd64/promtool /usr/local/bin/

useradd --no-create-home --shell /bin/false prometheus

mkdir /etc/prometheus
mkdir /var/lib/prometheus

chown prometheus:prometheus /var/lib/prometheus
chown prometheus:prometheus /usr/local/bin/prometheus
chown prometheus:prometheus /usr/local/bin/promtool

cp -r prometheus-*linux-amd64/consoles /etc/prometheus
cp -r prometheus-*linux-amd64/console_libraries /etc/prometheus

chown -R prometheus:prometheus /etc/prometheus/consoles
chown -R prometheus:prometheus /etc/prometheus/console_libraries

cp prometheus-*linux-amd64/prometheus.yml /etc/prometheus/
chown -R prometheus:prometheus /etc/prometheus/prometheus.yml
</code></pre>

<h3 id="create-servis-for-prometheus">Create servis for prometheus</h3>

<pre><code>nano /etc/systemd/system/prometheus.service
[Unit]
Description=Prometheus
Wants=network-online.target
After=network-online.target

[Service]
User=prometheus
Group=prometheus
Type=simple
ExecStart=/usr/local/bin/prometheus \
    --config.file /etc/prometheus/prometheus.yml \
    --storage.tsdb.path /var/lib/prometheus/ \
    --web.console.templates=/etc/prometheus/consoles \
    --web.console.libraries=/etc/prometheus/console_libraries

[Install]
WantedBy=multi-user.target
</code></pre>

<h3 id="configure-prometheus">Configure Prometheus</h3>

<pre><code>nano /etc/prometheus/prometheus.yml

global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'prometheus'
    scrape_interval: 5s
    static_configs:
      - targets: ['localhost:9090']
</code></pre>

<pre><code>systemctl daemon-reload
systemctl start prometheus
systemctl status prometheus
</code></pre>]]></content>
            
                 
                    
                 
                    
                 
                    
                
            
        </entry>
    
</feed>
