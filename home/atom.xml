<?xml version="1.0" encoding="utf-8"?> 
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <generator uri="https://gohugo.io/" version="0.59.1">Hugo</generator><title type="html"><![CDATA[Homes on devopstales]]></title>
    
        <subtitle type="html"><![CDATA[Blog about dev and ops stuff]]></subtitle>
    
    
    
            <link href="https://devopstales.github.io/home/" rel="alternate" type="text/html" title="HTML" />
            <link href="https://devopstales.github.io/home/index.xml" rel="alternate" type="application/rss+xml" title="RSS" />
            <link href="https://devopstales.github.io/home/atom.xml" rel="self" type="application/atom+xml" title="Atom" />
    <updated>2020-06-16T16:18:51+00:00</updated>
    
    
    <author>
            <name>Blaiserman</name>
            </author>
    
        <id>https://devopstales.github.io/home/</id>
    
        
        <entry>
            <title type="html"><![CDATA[Install cert-manager to Openshift]]></title>
            <link href="https://devopstales.github.io/home/openshift-cert-manager/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/openshift-cert-manager/?utm_source=atom_feed" rel="related" type="text/html" title="Install cert-manager to Openshift" />
                <link href="https://devopstales.github.io/cloud/openshift-auto-approval-csr/?utm_source=atom_feed" rel="related" type="text/html" title="How to Enable Auto Approval of CSR in Openshift v3.11" />
                <link href="https://devopstales.github.io/home/openshift-auto-approval-csr/?utm_source=atom_feed" rel="related" type="text/html" title="How to Enable Auto Approval of CSR in Openshift v3.11" />
                <link href="https://devopstales.github.io/home/openshift-secondary-router/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift secondary route" />
                <link href="https://devopstales.github.io/cloud/openshift-secondary-router/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift secondary route" />
            
                <id>https://devopstales.github.io/home/openshift-cert-manager/</id>
            
            
            <published>2020-06-10T00:00:00+00:00</published>
            <updated>2020-06-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p><code>cert-manager</code> is a service that automatically creates certificate requests and sign certificate based on annotations. The created certificate will be stored in a secret.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../cloud/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../cloud/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../cloud/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../cloud/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../cloud/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../cloud/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../cloud/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../cloud/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../cloud/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../cloud/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../cloud/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../cloud/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../cloud/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../cloud/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../cloud/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<p>Normally in kubernetes you can use a secret for TLS in an ingress cinfiguration but in Openshift there is no way to get the certificate from a secret for a route. So we will use <code>cert-utils-operator</code> for recreating routs with the propriety certificate based on annotations.</p>

<h3 id="install-cert-managger">Install cert-managger</h3>

<pre><code>oc create namespace cert-manager
oc apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.15.1/cert-manager-legacy.yaml
</code></pre>

<p>Create <code>ClusterIssuer</code> to create certs. For this demo I will use a Self-signed root CA, what is trustin in my browser. <code>cert-manager</code> can handle Let&rsquo;s encrypt as an issuer both with http and dns challenges so yu can use Let&rsquo;s encrypt certs in a private network without publication your route.</p>

<pre><code>nano issuer.yaml
---
apiVersion: v1
data:
  tls.crt: LS0tLS1C...
  tls.key: LS0tLSGF...
kind: Secret
metadata:
  name: ca-key-pair
  namespace: cert-manager
---
apiVersion: cert-manager.io/v1alpha2
kind: ClusterIssuer
metadata:
  name: ca-issuer
  namespace: cert-manager
spec:
  ca:
    secretName: ca-key-pair
</code></pre>

<h3 id="install-cert-utils-operator">Install cert-utils-operator</h3>

<p>I usethe v0.1.0 and not the latest one (at the moment v0.1.1) besause athe v0.1.1 has a bug on OKD 3.11:
- <a href="https://github.com/redhat-cop/cert-utils-operator/issues/51">https://github.com/redhat-cop/cert-utils-operator/issues/51</a></p>

<pre><code>helm repo add cert-utils-operator https://redhat-cop.github.io/cert-utils-operator
helm update
# export CERT_UTILS_CHART_VERSION=$(helm search cert-utils-operator/cert-utils-operator | grep cert-utils-operator/cert-utils-operator | awk '{print $2}')

helm fetch cert-utils-operator/cert-utils-operator --version v0.1.0
helm template cert-utils-operator-v0.1.0.tgz --namespace cert-manager | oc apply -f - -n cert-manager
</code></pre>

<pre><code>apiVersion: apps.openshift.io/v1
kind: DeploymentConfig
metadata:
  annotations:
    openshift.io/generated-by: OpenShiftWebConsole
  labels:
    app: nginx2
  name: nginx2
spec:
  replicas: 1
  selector:
    app: nginx2
    deploymentconfig: nginx2
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx2
        deploymentconfig: nginx2
    spec:
      containers:
        - image: &gt;-
            bitnami/nginx@sha256:2bff7d085671a8b0f9ec296cf57fba995d06c1b5fb350575dd429c361520f0a4
          imagePullPolicy: Always
          name: nginx2
          ports:
            - containerPort: 8080
              protocol: TCP
            - containerPort: 8443
              protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
  test: false
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    openshift.io/generated-by: OpenShiftWebConsole
  labels:
    app: nginx2
  name: nginx2
spec:
  clusterIP: 172.30.17.64
  ports:
    - name: 8080-tcp
      port: 8080
      protocol: TCP
      targetPort: 8080
    - name: 8443-tcp
      port: 8443
      protocol: TCP
      targetPort: 8443
  selector:
    deploymentconfig: nginx2
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
---
apiVersion: cert-manager.io/v1alpha2
kind: Certificate
metadata:
  name: nginx2-route-tls
  namespace: default
spec:
  secretName: nginx2-route-tls
  duration: 24h
  renewBefore: 12h
  commonName: nginx.openshift.mydomain.intra
  dnsNames:
  - nginx.openshift.mydomain.intra
  issuerRef:
    name: ca-issuer
    kind: ClusterIssuer
    group: cert-manager.io
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    cert-utils-operator.redhat-cop.io/certs-from-secret=nginx2-route-tls
  labels:
    app: nginx2
  name: nginx2
spec:
  host: nginx.openshift.mydomain.intra
  port:
    targetPort: 8080-tcp
  tls:
    insecureEdgeTerminationPolicy: Redirect
    termination: edge
  to:
    kind: Service
    name: nginx2
    weight: 100
  wildcardPolicy: None

---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    cert-utils-operator.redhat-cop.io/certs-from-secret: nginx2-route-tls
  labels:
    app: nginx2
  name: nginx2
spec:
  host: nginx.openshift.mydomain.intra
  port:
    targetPort: 8080-tcp
  tls:
    insecureEdgeTerminationPolicy: Redirect
    termination: edge
  to:
    kind: Service
    name: nginx2
    weight: 100
  wildcardPolicy: None
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to Enable Auto Approval of CSR in Openshift v3.11]]></title>
            <link href="https://devopstales.github.io/home/openshift-auto-approval-csr/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/openshift-auto-approval-csr/?utm_source=atom_feed" rel="related" type="text/html" title="How to Enable Auto Approval of CSR in Openshift v3.11" />
                <link href="https://devopstales.github.io/home/openshift-secondary-router/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift secondary route" />
                <link href="https://devopstales.github.io/cloud/openshift-secondary-router/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift secondary route" />
                <link href="https://devopstales.github.io/home/openshift-gitlab-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Installing GitLab on OpenShift" />
                <link href="https://devopstales.github.io/cloud/openshift-gitlab-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Installing GitLab on OpenShift" />
            
                <id>https://devopstales.github.io/home/openshift-auto-approval-csr/</id>
            
            
            <published>2020-05-27T00:00:00+00:00</published>
            <updated>2020-05-27T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Nodes certificates are not Completely redeployed through playbook but through a different mechanism.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../cloud/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../cloud/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../cloud/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../cloud/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../cloud/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../cloud/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../cloud/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../cloud/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../cloud/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../cloud/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../cloud/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../cloud/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../cloud/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../cloud/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../cloud/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<p>SSL Certificates will be valid for the period of 1 year and at 85% of the certificate the node will trigger a CSR that would have to be approved for the certificate to be redeployed.</p>

<p>The Only certificates that are renewed/redeployed through CSR’s mechanism are the kubelet/nodes certificates. Any other certificates e.g, router, master, api certs, etcd, docker-registry, etc are still redeployed through the usual playbooks.</p>

<p>If triggered CSR is not approved either manually or in automated way then after one year all nodes will go to NotReady State.</p>

<h3 id="check-and-approve-csr-s-manually">Check and approve csr&rsquo;s manually</h3>

<pre><code>oc get csr
oc describe csr &lt;csr_name&gt;
oc adm certificate &lt;approve csr_name&gt;

oc get csr -o name | xargs oc adm certificate approve
</code></pre>

<h3 id="approve-csr-s-automaticle">Approve csr&rsquo;s automaticle</h3>

<p>At install time you can add this option to your ansible hosts fiel:</p>

<pre><code>openshift_master_bootstrap_auto_approve=true
</code></pre>

<p>If you installed the cluster and want to change this option run this playbook:</p>

<pre><code>ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/openshift-master/enable_bootstrap.yml \
-e openshift_master_bootstrap_auto_approve=true
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install OpenEBS for Kubernetes]]></title>
            <link href="https://devopstales.github.io/home/k8s-install-openebs/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/k8s-install-openebs/?utm_source=atom_feed" rel="related" type="text/html" title="Install OpenEBS for Kubernetes" />
                <link href="https://devopstales.github.io/cloud/k8s-kubeadm-ha/?utm_source=atom_feed" rel="related" type="text/html" title="How to install kubernetes with kubeadm in HA mode" />
                <link href="https://devopstales.github.io/home/k8s-kubeadm-ha/?utm_source=atom_feed" rel="related" type="text/html" title="How to install kubernetes with kubeadm in HA mode" />
                <link href="https://devopstales.github.io/home/k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install kubernetes with kubeadm" />
                <link href="https://devopstales.github.io/cloud/k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install kubernetes with kubeadm" />
            
                <id>https://devopstales.github.io/home/k8s-install-openebs/</id>
            
            
            <published>2020-05-20T00:00:00+00:00</published>
            <updated>2020-05-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>OpenEBS is an open-source project for container-attached and container-native storage on Kubernetes.</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../cloud/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../cloud/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../cloud/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../cloud/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part3: <a href="../../cloud/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-traefik-ingress --></li>
<li>Part4: <a href="../../cloud/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../cloud/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../cloud/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../cloud/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../cloud/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a>
<!-- CEPH in Kubernetes with rook --></li>
<li>Part5d: <a href="../../cloud/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes vMware --></li>
<li>Part6: <a href="../../cloud/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part7: <a href="../../cloud/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part8: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part9: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part10: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part11: <a href="../../cloud/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- prometheus operator + grafana operator -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<p>On all host we have an unused unpartitioned disk called sdb.</p>

<pre><code>lsblk

NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda               8:0    0   80G  0 disk
├─sda1            8:1    0    1G  0 part /boot
└─sda2            8:2    0   79G  0 part
  ├─centos-root 253:0    0   50G  0 lvm  /
  ├─centos-swap 253:1    0    1G  0 lvm
  └─centos-home 253:3    0   28G  0 lvm  /home
sdb               8:16   0  100G  0 disk
</code></pre>

<h3 id="install-requirements">Install requirements</h3>

<p>OpenEBS use iscsi for persisten volume sharing so we need  <code>iscsid</code>.</p>

<h4 id="debian-ubuntu">Debian / Ubuntu</h4>

<pre><code>apt-get install open-iscsi
service open-iscsi enable
service open-iscsi restart

</code></pre>

<h4 id="centos">CentOS</h4>

<pre><code>yum install iscsi-initiator-utils -y
systemctl enable iscsid
systemctl start iscsid
</code></pre>

<h3 id="deploy-openebs-with-helm">Deploy OpenEBS with helm</h3>

<p>OpenEBS is in the stable repo so we didn&rsquo;t need to add a separate helm repository for installing it. In my case I will use teh <code>openebs-system</code> namespace for the install.</p>

<pre><code>kubectl create ns openebs-system
helm upgrade --install openebs stable/openebs --version 1.7.0 --namespace=openebs-system
</code></pre>

<p>Wait for all the  pods are started.</p>

<pre><code>kubectl get pods -n openebs-system

NAME                                           READY     STATUS    RESTARTS   AGE
openebs-admission-server-7b4859ccd5-bz4zt      1/1       Running   0          14m
openebs-apiserver-556ffff45c-nk9x9             1/1       Running   5          15m
openebs-localpv-provisioner-76b466d4b8-5tj4w   1/1       Running   0          15m
openebs-ndm-f6cqz                              1/1       Running   0          15m
openebs-ndm-operator-5f6c5497d7-chf6t          1/1       Running   1          15m
openebs-ndm-qrmp9                              1/1       Running   0          15m
openebs-ndm-stgml                              1/1       Running   0          15m
openebs-provisioner-c9c7f9ff8-hn4bl            1/1       Running   0          15m
openebs-snapshot-operator-6578d74b7-2wc97      2/2       Running   0          15m
</code></pre>

<p>Now verify if OpenEBS is installed successfully.</p>

<pre><code>kubectl get blockdevice -n openebs-system

NAME                                           NODENAME    SIZE         CLAIMSTATE   STATUS    AGE
blockdevice-0c4e03f9e39a4092108215f19eca9da8   k8s-node1   1048576000   Unclaimed    Active    16m
blockdevice-1aaa1142a7b9c65dfa32dec88fe1749b   k8s-node2   1048576000   Unclaimed    Active    16m
blockdevice-5f728d1068c72337609fc1f88855b9bb   k8s-node3   1048576000   Unclaimed    Active    16m
</code></pre>

<pre><code>kubectl describe blockdevice blockdevice-0c4e03f9e39a4092108215f19eca9da8
...
  Devlinks:
    Kind:  by-id
    Links:
      /dev/disk/by-id/ata-VBOX_HARDDISK_VBd4679835-eb798f2c
      /dev/disk/by-id/lvm-pv-uuid-PWnLFv-b0jS-7CLZ-Cmym-0dia-RQkI-w0Hkam
    Kind:  by-path
    Links:
      /dev/disk/by-path/pci-0000:00:01.1-ata-2.0
  Filesystem:
    Fs Type:  LVM2_member
  Node Attributes:
    Node Name:  k8s-node1
  Partitioned:  No
  Path:         /dev/sdb
...
</code></pre>

<h4 id="verify-storageclasses">Verify StorageClasses:</h4>

<pre><code>kubectl get sc

NAME                        PROVISIONER                                                AGE
openebs-device              openebs.io/local                                           64s
openebs-hostpath            openebs.io/local                                           64s
openebs-jiva-default        openebs.io/provisioner-iscsi                               64s
openebs-snapshot-promoter   volumesnapshot.external-storage.k8s.io/snapshot-promoter   64s
</code></pre>

<h3 id="storage-engines">Storage engines</h3>

<p>OpenEBS offers three storage engines:
* Jiva
* cStor
* LocalPV</p>

<p>Jiva is a light weight storage engine that is recommended to use for low capacity workloads. It is actually based on the same technology that powers Longhorn.</p>

<pre><code>---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: demo-vol1-claim
spec:
  storageClassName: openebs-jiva-default
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 4G
</code></pre>

<p>cStor requires raw disks. The snapshot and storage management features of the other cStor engine are more advanced than Jiva. For provisioning a cStor Volume, it requires a cStor Storage Pool and a StorageClass. cStor provides iSCSI targets, which are appropriate for RWO (ReadWriteOnce) access mode and is suitable for all types of databases. cStor supports thin provisioning by default.</p>

<pre><code>cat stor-pool1-config.yaml
---
apiVersion: openebs.io/v1alpha1
kind: StoragePoolClaim
metadata:
  name: cstor-disk-pool
  annotations:
    cas.openebs.io/config: |
      - name: PoolResourceRequests
        value: |-
            memory: 500Mb
      - name: PoolResourceLimits
        value: |-
            memory: 500Mb
spec:
  name: cstor-disk-pool
  type: disk
  poolSpec:
    poolType: striped
  blockDevices:
    blockDeviceList:
    - blockdevice-0c4e03f9e39a4092108215f19eca9da8
    - blockdevice-1aaa1142a7b9c65dfa32dec88fe1749b
    - blockdevice-5f728d1068c72337609fc1f88855b9bb
</code></pre>

<pre><code>kubectl apply -f stor-pool1-config.yaml

kubectl get spc

NAME              AGE
cstor-disk-pool   20s

kubectl get csp

NAME                   ALLOCATED   FREE      CAPACITY   STATUS    TYPE      AGE
cstor-disk-pool-cxm8   294K        100G      100G       Healthy   striped   27m
cstor-disk-pool-r1hl   270K        100G      100G       Healthy   striped   27m
cstor-disk-pool-t05z   92K         100G      100G       Healthy   striped   27m
</code></pre>

<pre><code>cat openebs-sc-rep3.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: openebs-cstore-default
  annotations:
    openebs.io/cas-type: cstor
    cas.openebs.io/config: |
      - name: StoragePoolClaim
        value: &quot;cstor-disk-pool&quot;
      - name: ReplicaCount
        value: &quot;3&quot;
    storageclass.kubernetes.io/is-default-class: &quot;true&quot;
provisioner: openebs.io/provisioner-iscsi
</code></pre>

<pre><code>cat test-cs-pcv.yaml
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: test-cs-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3G
</code></pre>

<pre><code>kubectl apply -f openebs-sc-rep3.yaml
kubectl apply -f test-cs-pcv.yaml
</code></pre>

<p>Local PV is based on Kubernetes local persistent volumes but it has a dynamic provisioner. It can store data either in a directory, or use disks; in the first case the hostpath can be shared by multiple persistent volumes, while when using disks each persistent volume requires a separate device. Local PV offers extremely high performance close to what you get by reading from and writing to the disk directly, but it doesn’t offer features such as replication, which are built in Jiva and cStor.</p>

<pre><code>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    cas.openebs.io/config: |
      - name: StorageType
        value: &quot;hostpath&quot;
      - name: BasePath
        value: &quot;/mnt/openebs&quot;
    openebs.io/cas-type: local
  name: openebs-hostpath-mount
provisioner: openebs.io/local
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
</code></pre>

<h2 id="sources">Sources:</h2>

<ul>
<li><a href="https://vitobotta.com/2019/07/03/openebs-tips/">https://vitobotta.com/2019/07/03/openebs-tips/</a></li>
</ul>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Foreman openidc SSO with keycloak]]></title>
            <link href="https://devopstales.github.io/home/foreman-sso/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/sso/foreman-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Foreman openidc SSO with keycloak" />
            
                <id>https://devopstales.github.io/home/foreman-sso/</id>
            
            
            <published>2020-05-15T00:00:00+00:00</published>
            <updated>2020-05-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I this post I will show you how you can configure Foreman to use Keycloak asz an OIDC SSO authentication provider.</p>

<p>I use a Self-signed certificate for keycloak so my first step is to add the root CA of this certificate as a trusted certificate on the foreman server:</p>

<pre><code>cp rootca.crt /etc/pki/ca-trust/source/anchors/
update-ca-trust
</code></pre>

<h3 id="install-requirements">install requirements</h3>

<pre><code>yum install mod_auth_openidc keycloak-httpd-client-install
</code></pre>

<p>The <code>keycloak-httpd-client-install</code> is a commandline tool thet helps to configure the apache2&rsquo;s <code>mod_auth_openidc</code> plugin with Keycloak. This feature is not yet supported by foreman-installer. As a result, re-running the foreman-installer command can purge the changes in Apache files added by the keycloak-httpd-client-install.</p>

<pre><code>keycloak-httpd-client-install \
--app-name foreman-openidc \
--keycloak-server-url &quot;https://keycloak.mydomain.intra&quot; \
--keycloak-admin-usernam &quot;admin&quot; \
--keycloak-realm &quot;ssl-realm&quot; \
--keycloak-admin-realm master \
--keycloak-auth-role root-admin \
-t openidc -l /users/extlogin

systemct restart httpd
</code></pre>

<h3 id="configure-keycloak">Configure Keycloak</h3>

<p>We need to create two mappers for the new client in keycloak:</p>

<p><img src="/img/include/foreman-sso-1.png" alt="Example image" /><br/>
<img src="/img/include/foreman-sso-2.png" alt="Example image" /><br/></p>

<p>Then in Keycloak create a group called <code>foreman-admin</code> and add the test user for it.</p>

<h3 id="configure-foreman">Configure Foreman:</h3>

<pre><code>Administer / Settings / Authentication
Authorize login delegation = yes
Authorize login delegation auth source user autocreate = External
OIDC Algorithm = RS256
# the client id from keycloak
OIDC Audience = foreman.mydomain.intra-foreman-openidc
OIDC Issuer = https://keycloak.mydomain.intra/auth/realms/ssl-realm
OIDC JWKs URL = https://keycloak.mydomain.intra/auth/realms/ssl-realm/protocol/openid-connect/certs
</code></pre>

<p>Create an user groupe in foreman called <code>foreman-admin</code> and map with the external group called <code>foreman-admin</code>.
<img src="/img/include/foreman-sso-3.png" alt="Example image" /><br/>
<img src="/img/include/foreman-sso-4.png" alt="Example image" /><br/>
<img src="/img/include/foreman-sso-5.png" alt="Example image" /><br/></p>

<p>The log out and go the he url of the foreman. It will redirect to Keycloak to login. Add the test users credentials the login wit it.</p>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install  Chef server]]></title>
            <link href="https://devopstales.github.io/home/chef-first-cookbook/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/chef-first-cookbook/?utm_source=atom_feed" rel="related" type="text/html" title="Install  Chef server" />
                <link href="https://devopstales.github.io/home/chef-server-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install  Chef server" />
                <link href="https://devopstales.github.io/linux/chef-server-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install  Chef server" />
                <link href="https://devopstales.github.io/home/ansible-k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Kubernetes" />
                <link href="https://devopstales.github.io/cloud/ansible-k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Kubernetes" />
            
                <id>https://devopstales.github.io/home/chef-first-cookbook/</id>
            
            
            <published>2020-05-07T00:00:00+00:00</published>
            <updated>2020-05-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I this post I will show you how you can create a basic chef structure.</p>

<h3 id="create-environments">Create environments</h3>

<pre><code>cd ~/chef-repo

mkdir environments
nano environments/production.json
{
  &quot;name&quot;: &quot;Production&quot;,
  &quot;description&quot;: &quot;&quot;,
  &quot;cookbook_versions&quot;: {

  },
  &quot;json_class&quot;: &quot;Chef::Environment&quot;,
  &quot;chef_type&quot;: &quot;environment&quot;,
  &quot;default_attributes&quot;: {

  },
  &quot;override_attributes&quot;: {

  }
}

nano environments/development.json
{
  &quot;name&quot;: &quot;Development&quot;,
  &quot;description&quot;: &quot;&quot;,
  &quot;cookbook_versions&quot;: {

  },
  &quot;json_class&quot;: &quot;Chef::Environment&quot;,
  &quot;chef_type&quot;: &quot;environment&quot;,
  &quot;default_attributes&quot;: {

  },
  &quot;override_attributes&quot;: {

  }
}
</code></pre>

<pre><code>knife environment from file environments/production.json
knife environment from file environments/development.json
</code></pre>

<h3 id="create-roles">Create roles</h3>

<pre><code>mkdir roles
nano roles/base.json
{

  &quot;name&quot;: &quot;base&quot;,
  &quot;description&quot;: &quot;Default operation role&quot;,
  &quot;json_class&quot;: &quot;Chef::Role&quot;,

  &quot;override_attributes&quot;: {
    &quot;chef_client&quot;: {
      &quot;config&quot;: {
        &quot;interval&quot;: 900,
        &quot;splay&quot;: 30
      }
    }
  },

  &quot;chef_type&quot;: &quot;role&quot;,

  &quot;run_list&quot;: [
    &quot;recipe[operation]&quot;
  ],

  &quot;env_run_lists&quot;: {
  }

}
</code></pre>

<pre><code>knife role from file roles/base.json
</code></pre>

<h3 id="create-node-config">Create node config</h3>

<pre><code>mkdir nodes
nano nodes/test.mydomain.intra.json
{
  &quot;name&quot;: &quot;test.mydomain.intra&quot;,
  &quot;chef_environment&quot;: &quot;Production&quot;,
  &quot;normal&quot;: {
  	&quot;tags&quot;: [

    ]
  },
  &quot;run_list&quot;: [
	&quot;role[base]&quot;
]

}
</code></pre>

<pre><code>knife node from file nodes/test.mydomain.intra.json
</code></pre>

<h3 id="create-cookbook">Create cookbook</h3>

<pre><code>cd chef-repo/cookbooks
chef generate cookbook operation
nano operation/recipes/default.rb
include_recipe 'operation::packages'

nano operation/recipes/packages.rb
package &quot;nano&quot; do
  action :install
end
</code></pre>

<pre><code>cd ..
knife cookbook upload operation
knife cookbook list
</code></pre>

<h3 id="test-cookbook">Test cookbook</h3>

<pre><code>knife ssh 'name:test.mydomain.intra' 'sudo chef-client' -x root
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install  Chef server]]></title>
            <link href="https://devopstales.github.io/home/chef-server-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/chef-server-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install  Chef server" />
                <link href="https://devopstales.github.io/home/ansible-k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Kubernetes" />
                <link href="https://devopstales.github.io/cloud/ansible-k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Kubernetes" />
                <link href="https://devopstales.github.io/cloud/ansible-operator-overview/?utm_source=atom_feed" rel="related" type="text/html" title="Ansible Operator Overview" />
                <link href="https://devopstales.github.io/home/ansible-operator-overview/?utm_source=atom_feed" rel="related" type="text/html" title="Ansible Operator Overview" />
            
                <id>https://devopstales.github.io/home/chef-server-install/</id>
            
            
            <published>2020-04-30T00:00:00+00:00</published>
            <updated>2020-04-30T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Chef is a powerful configuration management utility writy in ruby. This post will help you to setup a chef 13 on CentOS 7</p>

<ul>
<li>Chef Server: This is the central hub server that stores the cookbooks and recipes uploaded from workstations.</li>
<li>Chef Workstations: This where recipes, cookbooks, and other chef configuration details are created or edited.</li>
<li>Chef Client: This the target node where the configurations are deployed by the chef-client.</li>
</ul>

<h2 id="chef-server-install">Chef Server Install:</h2>

<pre><code>cd /opt
wget https://packages.chef.io/files/stable/chef-server/13.2.0/el/7/chef-server-core-13.2.0-1.el7.x86_64.rpm
yum install chef-server-core-13.2.0-1.el7.x86_64.rpm -y

chef-server-ctl reconfigure
chef-server-ctl status
</code></pre>

<p>Create admin user for chef server:</p>

<pre><code># chef-server-ctl user-create USER_NAME FIRST_NAME LAST_NAME EMAIL 'PASSWORD' -f PATH_FILE_NAME
chef-server-ctl user-create admin admin admin admin@devopstales.intra Password1 -f /etc/chef/admin.pem
</code></pre>

<p>Now create an organization to hold the chef configurations.</p>

<pre><code># chef-server-ctl org-create short_name 'full_organization_name' --association_user user_name --filename ORGANIZATION-validator.pem

chef-server-ctl org-create devopstales &quot;DevOpsTales, Inc&quot; --association_user admin -f /etc/chef/devopstales-validator.pem
</code></pre>

<h2 id="install-chef-workstation">Install Chef workstation:</h2>

<p>Fot this demo I will install the workstation on the same server as the Chef server, but in a pruduction enviroment it is your laptop or pc.</p>

<pre><code>wget https://packages.chef.io/files/stable/chefdk/4.7.73/el/7/chefdk-4.7.73-1.el7.x86_64.rpm
yum install -y chefdk-4.7.73-1.el7.x86_64.rpm
chef verify
</code></pre>

<pre><code>which ruby
echo 'eval &quot;$(chef shell-init bash)&quot;' &gt;&gt; ~/.bash_profile
. ~/.bash_profile
which ruby
</code></pre>

<pre><code>cd ~
chef generate repo chef-repo
mkdir -p ~/chef-repo/.chef
cp /etc/chef/admin.pem ~/chef-repo/.chef/
cp /etc/chef/devopstales-validator.pem ~/chef-repo/.chef/
</code></pre>

<pre><code>nano ~/chef-repo/.chef/knife.rb
current_dir = File.dirname(__FILE__)
log_level                :info
log_location             STDOUT
node_name                &quot;admin&quot;
client_key               &quot;#{current_dir}/admin.pem&quot;
validation_client_name   &quot;devopstzales-validator&quot;
validation_key           &quot;#{current_dir}/itzgeek-validator.pem&quot;
chef_server_url          &quot;https://cchef.mydomain.intra/organizations/devopstales&quot;
syntax_check_cache_path  &quot;#{ENV['HOME']}/.chef/syntaxcache&quot;
cookbook_path            [&quot;#{current_dir}/../cookbooks&quot;]
</code></pre>

<p>test kinife client:</p>

<pre><code>cd ~/chef-repo/
knife ssl fetch
knife client list
</code></pre>

<h2 id="install-chef-client">Install chef client:</h2>

<p>Before we can bootstrap a chef client on a server we need valid DNS resolution for both.</p>

<pre><code>knife bootstrap -N test.mydomain.intra test.mydomain.intra -y root -P vagrant
</code></pre>

<hr />

<ul>
<li><a href="https://downloads.chef.io/chef-server/stable/">https://downloads.chef.io/chef-server/stable/</a></li>
<li><a href="https://downloads.chef.io/chefdk/stable/">https://downloads.chef.io/chefdk/stable/</a></li>
</ul>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to install kubernetes with kubeadm in HA mode]]></title>
            <link href="https://devopstales.github.io/home/k8s-kubeadm-ha/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/k8s-kubeadm-ha/?utm_source=atom_feed" rel="related" type="text/html" title="How to install kubernetes with kubeadm in HA mode" />
                <link href="https://devopstales.github.io/home/k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install kubernetes with kubeadm" />
                <link href="https://devopstales.github.io/cloud/k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install kubernetes with kubeadm" />
                <link href="https://devopstales.github.io/cloud/k8s-local-pv/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
                <link href="https://devopstales.github.io/home/k8s-local-pv/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
            
                <id>https://devopstales.github.io/home/k8s-kubeadm-ha/</id>
            
            
            <published>2020-04-02T00:00:00+00:00</published>
            <updated>2020-04-02T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I&rsquo;will show you how to install kubernetes in HA mode with kubeadm, keepaliwed and envoyproxy.</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../cloud/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../cloud/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../cloud/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../cloud/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part3: <a href="../../cloud/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-traefik-ingress --></li>
<li>Part4: <a href="../../cloud/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../cloud/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../cloud/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../cloud/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../cloud/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a>
<!-- CEPH in Kubernetes with rook --></li>
<li>Part5d: <a href="../../cloud/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes vMware --></li>
<li>Part6: <a href="../../cloud/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part7: <a href="../../cloud/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part8: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part9: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part10: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part11: <a href="../../cloud/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- prometheus operator + grafana operator -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<pre><code>172.17.8.100  # kubernetes cluster ip
172.17.8.101  master01 # master node
172.17.8.102  master02 # frontend node
172.17.8.103  master03 # worker node

# hardware requirement
2 CPU
4G RAM
</code></pre>

<h3 id="install-docker">Install Docker</h3>

<pre><code>yum install -y -q yum-utils device-mapper-persistent-data lvm2
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum install -y -q docker-ce docker-compose

echo '{
  &quot;log-driver&quot;: &quot;json-file&quot;,
  &quot;log-opts&quot;: {
    &quot;max-size&quot;: &quot;100m&quot;
  },
  &quot;storage-driver&quot;: &quot;overlay2&quot;,
  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],
  &quot;storage-opts&quot;: [
    &quot;overlay2.override_kernel_check=true&quot;
  ]
}' &gt; /etc/docker/daemon.json

systemctl enable docker
systemctl start docker
</code></pre>

<h3 id="disable-swap">Disable swap</h3>

<pre><code>free -h
swapoff -a
swapoff -a
sed -i.bak -r 's/(.+ swap .+)/#\1/' /etc/fstab
free -h
</code></pre>

<h3 id="configuuration">Configuuration</h3>

<pre><code>cat &gt;&gt;/etc/sysctl.d/kubernetes.conf&lt;&lt;EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.ipv6.conf.all.disable_ipv6      = 1
net.ipv6.conf.default.disable_ipv6  = 1
EOF
cat &gt;&gt;/etc/sysctl.d/ipv6.conf&lt;&lt;EOF
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1
net.ipv6.conf.eth0.disable_ipv6 = 1
EOF
sysctl --system
</code></pre>

<h3 id="install-kubeadm">Install kubeadm</h3>

<pre><code>[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
        https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF


yum install epel-release -y
yum install -y kubeadm kubelet kubectl keepalived
</code></pre>

<h3 id="configure-keepalived-on-first-master">Configure keepalived on first master</h3>

<pre><code>touch /etc/keepalived/check_apiserver.sh
chmod +x /etc/keepalived/check_apiserver.sh

cat &lt;&lt;EOF &gt; /etc/keepalived/keepalived.conf
! Configuration File for keepalived
global_defs {
    router_id k8s-node1
    enable_script_security
    script_user root
}
vrrp_script check_apiserver {
    script &quot;/etc/keepalived/check_apiserver.sh&quot;
    interval 5
    weight -10
    fall 2
    rise 2
}
vrrp_instance VI_1 {
    state BACKUP
    interface enp0s8
    mcast_src_ip 172.17.8.101
    virtual_router_id 51
    priority 150
    advert_int 5
    authentication {
        auth_type PASS
        auth_pass Password1
    }
    virtual_ipaddress {
        172.17.8.100/24 brd 172.17.8.255 dev enp0s8
    }
    track_script {
       check_apiserver
    }
}
EOF

systemctl start keepalived
systemctl enable keepalived
</code></pre>

<h3 id="configure-envoy-on-first-master">Configure envoy on first master</h3>

<pre><code>cat &lt;&lt;EOF &gt; /etc/kubernetes/envoy.yaml
static_resources:
  listeners:
  - name: main
    address:
      socket_address:
        address: 0.0.0.0
        port_value: 16443
    filter_chains:
    - filters:
      - name: envoy.tcp_proxy
        config:
          stat_prefix: ingress_tcp
          cluster: k8s

  clusters:
  - name: k8s
    connect_timeout: 0.25s
    type: strict_dns
    lb_policy: round_robin
    hosts:
    - socket_address:
        address: 172.17.8.101
        port_value: 6443
    - socket_address:
        address: 172.17.8.102
        port_value: 6443
    - socket_address:
        address: 172.17.8.103
        port_value: 6443
    health_checks:
    - timeout: 1s
      interval: 5s
      unhealthy_threshold: 1
      healthy_threshold: 1
      http_health_check:
        path: &quot;/healthz&quot;

admin:
  access_log_path: &quot;/dev/null&quot;
  address:
    socket_address:
      address: 0.0.0.0
      port_value: 8001
EOF
</code></pre>

<h3 id="create-loadbalancer-on-all-masters">Create loadbalancer on all masters</h3>

<pre><code>
cat&lt;&lt;EOF &gt; /etc/kubernetes/envoy.yaml
static_resources:
  listeners:
  - name: main
    address:
      socket_address:
        address: 0.0.0.0
        port_value: 16443
    filter_chains:
    - filters:
      - name: envoy.tcp_proxy
        config:
          stat_prefix: ingress_tcp
          cluster: k8s

  clusters:
  - name: k8s
    connect_timeout: 0.25s
    type: strict_dns # static
    lb_policy: round_robin
    hosts:
    - socket_address:
        address: 172.17.8.101
        port_value: 6443
    - socket_address:
        address: 172.17.8.102
        port_value: 6443
    - socket_address:
        address: 172.17.8.103
        port_value: 6443
    health_checks:
    - timeout: 1s
      interval: 5s
      unhealthy_threshold: 1
      healthy_threshold: 1
      http_health_check:
        path: &quot;/healthz&quot;

admin:
  access_log_path: &quot;/dev/null&quot;
  address:
    socket_address:
      address: 0.0.0.0
      port_value: 8001
EOF

cat&lt;&lt;EOF &gt; /etc/kubernetes/docker-compose.yaml
version: '3'
services:
    api-lb:
        image: envoyproxy/envoy:latest
        restart: always
        network_mode: &quot;host&quot;
        ports:
            - 16443:16443
            - 8001:8001
        volumes:
            - /etc/kubernetes/envoy.yaml:/etc/envoy/envoy.yaml
EOF

cd /etc/kubernetes/
docker-compose pull
docker-compose up -d
docker-compose ps

netstat -tulpn | grep 6443
</code></pre>

<h3 id="initialize-kubernetes-in-the-first-master">Initialize kubernetes in the first master</h3>

<p>I have multiple interfaces in my masters so to use the correct one I need to add <code>--apiserver-advertise-address &quot;172.17.8.101&quot;</code> to my kubeadm commands and add <code>KUBELET_EXTRA_ARGS</code> for kubelet config.</p>

<pre><code>echo 'KUBELET_EXTRA_ARGS=&quot;--node-ip=172.17.8.101&quot;' &gt; /etc/sysconfig/kubelet

kubeadm config images pull --kubernetes-version 1.16.8
kubeadm init --control-plane-endpoint &quot;172.17.8.100:16443&quot; --apiserver-advertise-address &quot;172.17.8.101&quot; --upload-certs --kubernetes-version 1.16.8 --pod-network-cidr=10.244.0.0/16

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

kubectl
</code></pre>

<h3 id="join-other-masters">Join other masters</h3>

<pre><code>
kubeadm config images pull --kubernetes-version 1.16.8

echo 'KUBELET_EXTRA_ARGS=&quot;--node-ip=172.17.8.102&quot;' &gt; /etc/sysconfig/kubelet

kubeadm join 172.17.8.100:16443 --token 3vqtop.z2kbok4o0wchu4ed \
  --discovery-token-ca-cert-hash sha256:5840ee4de07bb296e2639669c17df7e3240271a1880115336ebc5b91fb8a3555 \
  --control-plane --certificate-key dc99dc10a0269d1a3edfc2e318a78c6bbebdee8081b460535f699d210cec5dcb \
  --apiserver-advertise-address &quot;172.17.8.102&quot;

echo 'KUBELET_EXTRA_ARGS=&quot;--node-ip=172.17.8.103&quot;' &gt; /etc/sysconfig/kubelet

kubeadm join 172.17.8.100:16443 --token 3vqtop.z2kbok4o0wchu4ed \
  --discovery-token-ca-cert-hash sha256:5840ee4de07bb296e2639669c17df7e3240271a1880115336ebc5b91fb8a3555 \
  --control-plane --certificate-key dc99dc10a0269d1a3edfc2e318a78c6bbebdee8081b460535f699d210cec5dcb \
  --apiserver-advertise-address &quot;172.17.8.103&quot;
</code></pre>

<h3 id="fix-keepalibed-check-script-on-first-master">Fix keepalibed check script on first master</h3>

<pre><code>echo '#!/bin/bash

# if check error then repeat check for 12 times, else exit
err=0
for k in $(seq 1 12)
do
    check_code=$(curl -sk https://localhost:16443)
    if [[ $check_code == &quot;&quot; ]]; then
        err=$(expr $err + 1)
        sleep 5
        continue
    else
        err=0
        break
    fi
done

if [[ $err != &quot;0&quot; ]]; then
    # if apiserver is down send SIG=1
    echo 'apiserver error!'
    exit 1
else
    # if apiserver is up send SIG=0
    echo 'apiserver normal!'
    exit 0
fi' &gt; /etc/keepalived/check_apiserver.sh
</code></pre>

<h3 id="configure-keepalived-on-other-masters">Configure keepalived on other masters</h3>

<pre><code>echo '#!/bin/bash

# if check error then repeat check for 12 times, else exit
err=0
for k in $(seq 1 12)
do
    check_code=$(curl -sk https://localhost:16443)
    if [[ $check_code == &quot;&quot; ]]; then
        err=$(expr $err + 1)
        sleep 5
        continue
    else
        err=0
        break
    fi
done

if [[ $err != &quot;0&quot; ]]; then
    # if apiserver is down send SIG=1
    echo 'apiserver error!'
    exit 1
else
    # if apiserver is up send SIG=0
    echo 'apiserver normal!'
    exit 0
fi' &gt; /etc/keepalived/check_apiserver.sh

chmod +x /etc/keepalived/check_apiserver.sh
</code></pre>

<pre><code>cat &lt;&lt;EOF &gt; /etc/keepalived/keepalived.conf
! Configuration File for keepalived
global_defs {
    router_id k8s-node2
    enable_script_security
    script_user root
}
vrrp_script check_apiserver {
    script &quot;/etc/keepalived/check_apiserver.sh&quot;
    interval 5
    weight -10
    fall 2
    rise 2
}
vrrp_instance VI_1 {
    state BACKUP
    interface enp0s8
    mcast_src_ip 172.17.8.102
    virtual_router_id 51
    priority 100
    advert_int 5
    authentication {
        auth_type PASS
        auth_pass Password1
    }
    virtual_ipaddress {
        172.17.8.100/24 brd 172.17.8.255 dev enp0s8
    }
    track_script {
       check_apiserver
    }
}
EOF

systemctl start keepalived
systemctl enable keepalived
</code></pre>

<pre><code>cat &lt;&lt;EOF &gt; /etc/keepalived/keepalived.conf
! Configuration File for keepalived
global_defs {
    router_id k8s-node3
    enable_script_security
    script_user root
}
vrrp_script check_apiserver {
    script &quot;/etc/keepalived/check_apiserver.sh&quot;
    interval 5
    weight -10
    fall 2
    rise 2
}
vrrp_instance VI_1 {
    state BACKUP
    interface enp0s8
    mcast_src_ip 172.17.8.103
    virtual_router_id 51
    priority 50
    advert_int 5
    authentication {
        auth_type PASS
        auth_pass Password1
    }
    virtual_ipaddress {
        172.17.8.100/24 brd 172.17.8.255 dev enp0s8
    }
    track_script {
       check_apiserver
    }
}
EOF

systemctl start keepalived
systemctl enable keepalived
</code></pre>

<pre><code>kubectl scale deploy/coredns  --replicas=3 -n kube-system
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[K8s ERROR at kubectl logs]]></title>
            <link href="https://devopstales.github.io/home/k8s-error-at-kubectl-logs/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/k8s-error-at-kubectl-logs/?utm_source=atom_feed" rel="related" type="text/html" title="K8s ERROR at kubectl logs" />
            
                <id>https://devopstales.github.io/home/k8s-error-at-kubectl-logs/</id>
            
            
            <published>2020-03-06T00:00:00+00:00</published>
            <updated>2020-03-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I installed a kubernetes cluster in a Vagran environment. First everything was fine, but wen I try to add the command <code>kubectl logs</code> I got this error:</p>

<pre><code>$ kubectl logs busybox-7c9687585b-12d75
Error from server (NotFound): the server could not find the requested resource ( pods/log busybox-7c9687585b-12d75)
</code></pre>

<p>and another for port-forward and exec:</p>

<pre><code>$ kubectl exec -it busybox-7c9687585b-12d75 -- /bin/sh
error: unable to upgrade connection: pod does not exist
</code></pre>

<p>When I eun with <code>-v=9</code> I hot HTTP Error Code 404 all around. I wondered if the connection with the kubelet is wrong because of the multiple interfaces of the VMs? So I tested it:</p>

<pre><code>$ kubectl get nodes worker1 -o yaml
apiVersion: v1
kind: Node
...
status:
  addresses:
  - address: 10.0.2.15
...
</code></pre>

<p>The was the problem, the address is came from the NAT interface of the VM not from the bridged. But why? On the master I explicitly set the <code>--apiserver-advertise-address</code> for the bridged interface&rsquo;s IP. I needed to add an explicit IP address of the bridged interface on the workers too. The problem is there is no option for that in kubeadm.</p>

<p>I looked at the man page of the kubelet and I found the following option:</p>

<pre><code>--node-ip string    IP address of the node. If set, kubelet will use this IP address for the node
</code></pre>

<p>That is wat I need to set. So I added a no job to the end of my bootstrap scripts for master and worker nodes too.</p>

<pre><code>echo KUBELET_EXTRA_ARGS=\&quot;--node-ip=`ip addr show enp0s8 | grep inet | grep -E -o &quot;([0-9]{1,3}[\.]){3}[0-9]{1,3}/&quot; | tr -d '/'`\&quot; &gt; /etc/sysconfig/kubelet
systemctl restart kubelet
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[PhotonOS Basics]]></title>
            <link href="https://devopstales.github.io/home/photon_basics/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/photon_basics/?utm_source=atom_feed" rel="related" type="text/html" title="PhotonOS Basics" />
                <link href="https://devopstales.github.io/home/protonos_vagrant_box/?utm_source=atom_feed" rel="related" type="text/html" title="Jow to create Vagrant box?" />
                <link href="https://devopstales.github.io/cloud/protonos_vagrant_box/?utm_source=atom_feed" rel="related" type="text/html" title="Jow to create Vagrant box?" />
            
                <id>https://devopstales.github.io/home/photon_basics/</id>
            
            
            <published>2020-03-04T00:00:00+00:00</published>
            <updated>2020-03-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Project Photon OS is an open source, minimal Linux container host that is optimized for cloud-native applications, cloud platforms, and VMware infrastructure.</p>

<h3 id="star-in-vagrant">Star in Vagrant</h3>

<pre><code>nano Vagrantfile
Vagrant.configure(&quot;2&quot;) do |config|
  config.vm.box = &quot;devopstales/photon3&quot;
  config.vm.synced_folder &quot;.&quot;, &quot;/vagrant&quot;, type: &quot;virtualbox&quot;

  config.timezone.value = :host
  config.vm.provider &quot;virtualbox&quot; do |vb|
    vb.name = &quot;photon01&quot;
      vb.memory = 4096

      vb.cpus = 2
      vb.linked_clone = true
      vb.customize [&quot;modifyvm&quot;, :id, &quot;--vram&quot;, &quot;8&quot;]
  end

  config.vm.define &quot;pothon01&quot; do |vbox|
    vbox.vm.network :public_network, ip: &quot;192.168.0.112&quot; , bridge: &quot;wlan0&quot; #, adapter: &quot;1&quot;
    vbox.vm.hostname = &quot;pothon01.mydomain.intra&quot;
    vbox.hostsupdater.remove_on_suspend = false
    vbox.vbguest.auto_update = false
  end
end
</code></pre>

<pre><code>vagrant up
vagrant ssh
</code></pre>

<p>When you try to login the server wants you to change the passowrd of the <code>vagrant</code> user. The base password is <code>vagrant</code> as usual.</p>

<pre><code>vagrant ssh
You are required to change your password immediately (password expired)
Last login: Sun Apr 14 20:30:22 2019 from 10.0.2.2
WARNING: Your password has expired.
You must change your password now and login again!
Changing password for vagrant.
Current password:
New password:
Retype new password:
passwd: password updated successfully
Connection to 127.0.0.1 closed.
</code></pre>

<p>Vagrant can&rsquo;t configure the ip, hostname and mount the <code>/vagrant</code> fonder.</p>

<h3 id="package-management">Package management</h3>

<p>On Photon OS, <code>tdnf</code> is the default package manager for installing new packages. It is a C implementation of the <code>DNF</code> package manager without Python dependencies. <code>DNF</code> is the next upcoming major version of yum.</p>

<p>Let&rsquo;s install packages for the next steps.</p>

<pre><code>tdnf install nano awk tar build-essential linux-devel less -y
</code></pre>

<h3 id="install-virtualbox-guest-additions">Install virtualbox guest additions</h3>

<p><img src="/img/include/photon_base_1.png" alt="Example image" /><br><br></p>

<pre><code>mount /dev/cdrom /mnt/cdrom
cd /mnt/cdrom
./VBoxLinuxAdditions.run
</code></pre>

<h3 id="configure-static-ip">Configure static ip</h3>

<p>PhotonOS use systemd-networkd to manage network configurations. systemd-networkd configorations is located under <code>/etc/systemd/network/</code>.</p>

<pre><code>cat /etc/systemd/network/99-dhcp-en.network
[Match]
Name=e*

[Network]
DHCP=yes
IPv6AcceptRA=no
</code></pre>

<pre><code>cat &gt; /etc/systemd/network/20-static-eth1.network &lt;&lt; &quot;EOF&quot;
[Match]
Name=eth1

[Network]
DHCP=no
Address=192.168.0.112/24
Gateway=192.168.0.1
DNS=8.8.8.8
Domains=mydomain.intra
NTP=0.pool.ntp.org
EOF
</code></pre>

<pre><code>chmod 644 /etc/systemd/network/20-static-eth1.network
systemctl restart systemd-networkd
systemctl status systemd-networkd -l
</code></pre>

<h3 id="configure-hostname">Configure hostname</h3>

<pre><code>hostnamectl set-hostname &quot;pothon01.mydomain.intra&quot;
</code></pre>

<pre><code>hostnamectl status
   Static hostname: pothon01.mydomain.intra
         Icon name: computer-vm
           Chassis: vm
        Machine ID: 2c230d2255834f75bff4872bff234df4
           Boot ID: 2708cffec4cf4c6e91053cc11825b590
    Virtualization: oracle
  Operating System: VMware Photon OS/Linux
            Kernel: Linux 4.19.32-3.ph3
      Architecture: x86-64
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Jow to create Vagrant box?]]></title>
            <link href="https://devopstales.github.io/home/protonos_vagrant_box/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/protonos_vagrant_box/?utm_source=atom_feed" rel="related" type="text/html" title="Jow to create Vagrant box?" />
            
                <id>https://devopstales.github.io/home/protonos_vagrant_box/</id>
            
            
            <published>2020-03-03T00:00:00+00:00</published>
            <updated>2020-03-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how to create a vagrant box from pothonos ISO.</p>

<h3 id="base-vm">Base VM</h3>

<p>Fist we need to create a virtualbox vm and install the <a href="https://github.com/vmware/photon/wiki/Downloading-Photon-OS">latest PothonOS ISO</a>.</p>

<ul>
<li>Name: Photon-base</li>
<li>Type: Linux</li>
<li>Version: Other Linux (64-bit)</li>
<li>Memory Size: 1024MB</li>
<li>New Virtual Disk: [Type: VDI, Size: 40 GB]</li>
</ul>

<p>Modify the hardware settings of the virtual machine:</p>

<ul>
<li>Disable audio</li>
<li>Disable USB</li>
<li>Ensure Network Adapter 1 is set to NAT</li>
<li>Add this port-forwarding rule:

<ul>
<li>Name: SSH</li>
<li>Protocol: TCP</li>
<li>Host IP: blank</li>
<li>Host Port: 2222</li>
<li>Guest IP: blank</li>
<li>Guest Port: 22</li>
</ul></li>
</ul>

<h3 id="configure-system">Configure System</h3>

<p>Change the root users password to <code>vagrant</code></p>

<pre><code>passwd
chage -I -1 -m 0 -M 99999 -E -1 root
</code></pre>

<p>Upgrade system and install packages:</p>

<pre><code>tdnf upgrade -y
tdnf install sudo nano wget awk tar build-essential linux-devel less -y
</code></pre>

<h3 id="create-the-vagrant-account">Create the vagrant account</h3>

<p>Next you need to create the default vagrant user account:</p>

<pre><code>useradd -m -G sudo vagrant
passwd vagrant
chage -I -1 -m 0 -M 99999 -E -1 vagrant

cat &gt; /etc/sudoers.d/vagrant &lt; EOF
# add vagrant user
vagrant ALL=(ALL) NOPASSWD:ALL
EOF
</code></pre>

<h3 id="change-ssh-config">Change ssh config</h3>

<pre><code>nano /etc/ssh/sshd_config
AuthorizedKeysFile     %h/.ssh/authorized_keys
</code></pre>

<pre><code>systemctl restart sshd
</code></pre>

<h3 id="add-vagrant-ssh-keys-to-vagrant-user">Add vagrant ssh keys to vagrant user</h3>

<pre><code>su - vagrant

mkdir -p /home/vagrant/.ssh
chmod 0700 /home/vagrant/.ssh
</code></pre>

<pre><code>wget --no-check-certificate \
    https://raw.github.com/mitchellh/vagrant/master/keys/vagrant.pub \
    -O /home/vagrant/.ssh/authorized_keys
</code></pre>

<pre><code>chmod 0600 /home/vagrant/.ssh/authorized_keys
chown –R vagrant /home/vagrant/.ssh
</code></pre>

<h3 id="add-virtualboxadditions">Add VirtualBoxadditions</h3>

<p>Go to your virtualbox menu for the VM and select <code>Devices / Insert Guest Additions CD Image</code></p>

<p><img src="/img/include/photon_base_1.png" alt="Example image" /><br><br></p>

<pre><code>mount /dev/cdrom /mnt/cdrom
cd /mnt/cdrom
./VBoxLinuxAdditions.run
</code></pre>

<h3 id="compress-vm">Compress vm</h3>

<pre><code>sudo dd if=/dev/zero of=/EMPTY bs=1M
sudo rm -f /EMPTY
</code></pre>

<h3 id="package-box">Package box</h3>

<pre><code>vagrant package --base &quot;Photon-base&quot;

vagrant box add &lt;user-name&gt;/photon3 package.box
vagrant box add devopstales/photon3 package.box
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[OpenShift 4.2 with Red Hat CodeReady Containers]]></title>
            <link href="https://devopstales.github.io/home/openshift_4/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/openshift_4/?utm_source=atom_feed" rel="related" type="text/html" title="OpenShift 4.2 with Red Hat CodeReady Containers" />
                <link href="https://devopstales.github.io/home/openshift-secondary-router/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift secondary route" />
                <link href="https://devopstales.github.io/cloud/openshift-secondary-router/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift secondary route" />
                <link href="https://devopstales.github.io/home/openshift-gitlab-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Installing GitLab on OpenShift" />
                <link href="https://devopstales.github.io/cloud/openshift-gitlab-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Installing GitLab on OpenShift" />
            
                <id>https://devopstales.github.io/home/openshift_4/</id>
            
            
            <published>2020-03-02T00:00:00+00:00</published>
            <updated>2020-03-02T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>The Red Hat CodeReady Containers enables you to run a minimal OpenShift 4.2 or newer cluster on your local laptop or desktop computer.</p>

<h3 id="download-crc">Download CRC</h3>

<pre><code>wget https://mirror.openshift.com/pub/openshift-v4/clients/crc/latest/crc-linux-amd64.tar.xz
tar xvf crc-linux-amd64.tar.xz
sudo cp crc-linux-*-amd64/crc /usr/local/sbin/
crc version
</code></pre>

<h3 id="setup-crs">Setup CRS</h3>

<pre><code>crc setup
</code></pre>

<pre><code>INFO Checking if running as non-root
INFO Caching oc binary
INFO Setting up virtualization
INFO Setting up KVM
INFO Installing libvirt service and dependencies
INFO Adding user to libvirt group
INFO Enabling libvirt
INFO Starting libvirt service
INFO Will use root access: start libvirtd service
[sudo] password for devopstales:
INFO Checking if a supported libvirt version is installed
INFO Installing crc-driver-libvirt
INFO Removing older system-wide crc-driver-libvirt
INFO Setting up libvirt 'crc' network
INFO Starting libvirt 'crc' network
INFO Checking if NetworkManager is installed
INFO Checking if NetworkManager service is running
INFO Writing Network Manager config for crc
INFO Will use root access: write NetworkManager config in /etc/NetworkManager/conf.d/crc-nm-dnsmasq.conf
INFO Will use root access: execute systemctl daemon-reload command
INFO Will use root access: execute systemctl stop/start command
INFO Writing dnsmasq config for crc
INFO Will use root access: write dnsmasq configuration in /etc/NetworkManager/dnsmasq.d/crc.conf
INFO Will use root access: execute systemctl daemon-reload command
INFO Will use root access: execute systemctl stop/start command
INFO Unpacking bundle from the CRC binary
Setup is complete, you can now run 'crc start' to start the OpenShift cluster
</code></pre>

<p>Thec configuration creats two dnsmasq config:</p>

<pre><code>cat /etc/NetworkManager/conf.d/crc-nm-dnsmasq.conf
[main]
dns=dnsmasq

cat /etc/NetworkManager/dnsmasq.d/crc.conf
server=/apps-crc.testing/192.168.130.11
server=/crc.testing/192.168.130.11
</code></pre>

<p>The first enable the NetworkManager&rsquo;s dnsmasq plugin to be used as a dns server and the second points two dns zone the <code>*.apps-crc.testing</code> and the <code>*.crc.testing</code> to the ip of the new vm&rsquo;s ip. The crc creats a kvm network for the vm whit this ip range.</p>

<pre><code>sudo virsh net-list
 Name      State    Autostart   Persistent
--------------------------------------------
 crc       active   yes         yes
 default   active   yes         yes
</code></pre>

<pre><code>sudo virsh net-edit
 &lt;network&gt;
  &lt;name&gt;crc&lt;/name&gt;
  &lt;uuid&gt;49eee855-d342-46c3-9ed3-b8d1758814cd&lt;/uuid&gt;
  &lt;forward mode='nat'&gt;
    &lt;nat&gt;
      &lt;port start='1024' end='65535'/&gt;
    &lt;/nat&gt;
  &lt;/forward&gt;
  &lt;bridge name='crc' stp='on' delay='0'/&gt;

  &lt;mac address='52:54:00:fd:be:d0'/&gt;
  &lt;ip family='ipv4' address='192.168.130.1' prefix='24'&gt;
    &lt;dhcp&gt;
      &lt;host mac='52:fd:fc:07:21:82' ip='192.168.130.11'/&gt;
    &lt;/dhcp&gt;
  &lt;/ip&gt;
&lt;/network&gt;
</code></pre>

<h3 id="start-crs">Start CRS</h3>

<pre><code>crc start
</code></pre>

<pre><code>INFO Checking if running as non-root
INFO Checking if oc binary is cached
INFO Checking if Virtualization is enabled
INFO Checking if KVM is enabled
INFO Checking if libvirt is installed
INFO Checking if user is part of libvirt group
INFO Checking if libvirt is enabled
INFO Checking if libvirt daemon is running
INFO Checking if a supported libvirt version is installed
INFO Checking if crc-driver-libvirt is installed
INFO Checking if libvirt 'crc' network is available
INFO Checking if libvirt 'crc' network is active
INFO Checking if NetworkManager is installed
INFO Checking if NetworkManager service is running
INFO Checking if /etc/NetworkManager/conf.d/crc-nm-dnsmasq.conf exists
INFO Checking if /etc/NetworkManager/dnsmasq.d/crc.conf exists
? Image pull secret [? for help]
</code></pre>

<p>Please note that a valid OpenShift user pull secret is required during installation. The pull secret can be copied or downloaded from the Pull Secret section of the <a href="https://cloud.redhat.com/openshift/install/crc/installer-provisioned">Install on Laptop: Red Hat CodeReady Containers</a> page on cloud.redhat.com.</p>

<pre><code>INFO To access the cluster, first set up your environment by following 'crc oc-env' instructions
INFO Then you can access it by running 'oc login -u developer -p developer https://api.crc.testing:6443'
INFO To login as an admin, run 'oc login -u kubeadmin -p 7z6T5-qmTth-oxaoD-p3xQF https://api.crc.testing:6443'
INFO
INFO You can now run 'crc console' and use these credentials to access the OpenShift web console
</code></pre>

<p>Go to the console:</p>

<pre><code>crc console
</code></pre>

<p><img src="/img/include/okd4.png" alt="Example image" /></p>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[CEPH backup with Benji]]></title>
            <link href="https://devopstales.github.io/home/ceph_backup_benji/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/ceph_backup_benji/?utm_source=atom_feed" rel="related" type="text/html" title="CEPH backup with Benji" />
                <link href="https://devopstales.github.io/home/k8s-ceph-storage-with-csi-driver/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD volume with CSI driver" />
                <link href="https://devopstales.github.io/cloud/k8s-ceph-storage-with-csi-driver/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD volume with CSI driver" />
                <link href="https://devopstales.github.io/cloud/who-mapping-rbd-device/?utm_source=atom_feed" rel="related" type="text/html" title="Ceph: who&#39;s mapping a RBD device" />
                <link href="https://devopstales.github.io/home/who-mapping-rbd-device/?utm_source=atom_feed" rel="related" type="text/html" title="Ceph: who&#39;s mapping a RBD device" />
            
                <id>https://devopstales.github.io/home/ceph_backup_benji/</id>
            
            
            <published>2020-02-24T00:00:00+00:00</published>
            <updated>2020-02-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this article I will show you how to use benji to backup CEPH RBD incrementally.</p>

<p>Benji Backup is a block based deduplicating backup software. It builds on the excellent foundations and concepts of backy² by Daniel Kraft.</p>

<p>Benji requires <code>Python 3.6.5</code> or newer because older Python versions have some shortcomings in the <code>concurrent.futures</code> implementation which lead to an excessive memory usage. So in this example I will use docker to make easier to implement.</p>

<pre><code>mkdir -p /opt/benji/config/
mkdir -p /opt/benji/postgresql/data

nano /opt/benji/docker-compose.yaml
version: '3.5'

services:
  benji:
    image: elementalnet/benji-k8s:0.5.0
    restart: always
    volumes:
      - /etc/ceph:/etc/ceph
      - /backup/benji-ceph:/data
      - /opt/benji/config/benji.yaml:/benji/etc/benji.yaml
      - /opt/benji/config/backup-ceph:/benji/scripts/backup-ceph
      - /opt/benji/config/crontab:/benji/etc/crontab

  postgres:
    image: postgres:11
    restart: always
    environment:
      - &quot;POSTGRES_DB=benji-prod&quot;
      - &quot;POSTGRES_USER=benji-prod&quot;
      - &quot;POSTGRES_PASSWORD=Password1&quot;
    volumes:
      - /opt/benji/postgresql/data:/var/lib/postgresql/data
</code></pre>

<h3 id="create-benji-config">Create benji config</h3>

<pre><code>nano /opt/benji/config/benji.yaml
configurationVersion: '1'
databaseEngine: postgresql://benji-prod:Password1@postgres:5432/benji-prod
defaultStorage: storage-1
storages:
  - name: storage-1
    storageId: 1
    module: file
    configuration:
      path: /data
ios:
  - name: rbd
    module: rbd
</code></pre>

<h3 id="create-backup-script">Create backup script</h3>

<pre><code>nano /opt/benji/config/backup-ceph
#!/bin/bash

. common.sh
. prometheus.sh
. ceph.sh
. hooks.sh

FSFREEZE=no
BENJI_INSTANCE:devopstales
POOL=testpool
EXCLUDE=&quot;test1&quot;

# benji::backup::ceph test/test test test test/test

echo &quot;POOL :&quot; $POOL;
for IMAGE in `rbd ls $POOL | grep -v @ | grep -v $EXCLUDE`;
do
#   echo $POOL/$IMAGE
   benji::backup::ceph $POOL/$IMAGE $POOL $IMAGE $POOL/$IMAGE
done
</code></pre>

<h3 id="create-crontab">Create crontab</h3>

<pre><code>nano /opt/benji/config/crontab
BENJI_INSTANCE:devopstales
00 00 * * * root backup-ceph
00 04 * * * root benji-command enforce days8,weeks3,months2
00 05 * * * root benji-command cleanup
30 05 * * * root benji-versions-status
00 06 * * * root benji-command batch-deep-scrub
</code></pre>

<h3 id="perform-a-backup">Perform a backup</h3>

<pre><code>cd /opt/benji/
docker-compose up -d
docker exec -it benji_benji_1 bash
benji database-init
benji backup rbd:testpool/test2 devopstales
benji ls
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Using the NetworkManager’s DNSMasq plugin]]></title>
            <link href="https://devopstales.github.io/home/networkmanagger-dnsmasq/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/networkmanagger-dnsmasq/?utm_source=atom_feed" rel="related" type="text/html" title="Using the NetworkManager’s DNSMasq plugin" />
            
                <id>https://devopstales.github.io/home/networkmanagger-dnsmasq/</id>
            
            
            <published>2020-02-24T00:00:00+00:00</published>
            <updated>2020-02-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Imagine you want to test something in a demo setup with 5 machines. You create the necessary VMs in your local environment – but you cannot address them properly by name. With 5 machines you also need to write down the appropriate IP addresses – that’s hardly practical.</p>

<p>Luckily, there is an elegant solution: The dnsmasq plugin is a hidden gem of NetworkManager.</p>

<h3 id="enable-networkmanager-s-dnsmasq">Enable NetworkManager&rsquo;s dnsmasq</h3>

<pre><code># /etc/NetworkManager/conf.d/00-use-dnsmasq.conf
#
# This enabled the dnsmasq plugin.
[main]
dns=dnsmasq
</code></pre>

<pre><code># /etc/NetworkManager/dnsmasq.d/00-homelab.conf
# This file sets up the local lablab domain and
# defines some aliases and a wildcard.
local=/homelab/

# The below defines a Wildcard DNS Entry.
address=/.ose.homelab/192.168.101.125

# Below I define some host names.  I also pull in   
address=/openshift.homelab/192.168.101.120
address=/openshift-int.homelab/192.168.101.120
</code></pre>

<pre><code># /etc/NetworkManager/dnsmasq.d/02-add-hosts.conf
# By default, the plugin does not read from /etc/hosts.  
# This forces the plugin to slurp in the file.
#
# If you didn't want to write to the /etc/hosts file.  This could
# be pointed to another file.
#
addn-hosts=/etc/hosts
</code></pre>

<p>Restart your network managger <code>systemctl restart NetworkManager</code>. If everything is working right, you should see that your resolv.conf points to 127.0.0.1 and a new dnsmasq process spawned.</p>

<pre><code>cat /etc/resolv.conf
# Generated by NetworkManager
nameserver 127.0.0.1
</code></pre>

<h3 id="configurate-networkmanager-fo-libvirt-s-domain">Configurate NetworkManager fo libvirt&rsquo;s domain</h3>

<p>Libvirt comes with its own in-build DNS server, dnsmasq to serve DHCP and DNS to servers for vms.
Additionally, NetworkManager can be configured to use its dnsmasq plugin to forwarding DNS requests to the libvirt instance if needed.</p>

<pre><code># /etc/NetworkManager/dnsmasq.d/01-libvirt_dnsmasq.conf
server=/qxyz.intra/192.168.122.1
</code></pre>

<h3 id="configuring-libvirt">Configuring libvirt</h3>

<p>First of all, libvirt needs to be configured. Given that the network &ldquo;default&rdquo; is assigned to the relevant VMs, the configuration should look like this:</p>

<pre><code>sudo virsh net-edit default
&lt;network connections='1'&gt;
  &lt;name&gt;default&lt;/name&gt;
  &lt;uuid&gt;158880c3-9adb-4a44-ab51-d0bc1c18cddc&lt;/uuid&gt;
  &lt;forward mode='nat'&gt;
    &lt;nat&gt;
      &lt;port start='1024' end='65535'/&gt;
    &lt;/nat&gt;
  &lt;/forward&gt;
  &lt;bridge name='virbr0' stp='on' delay='0'/&gt;
  &lt;mac address='52:54:00:fa:cb:e5'/&gt;
  &lt;domain name='qxyz.de' localOnly='yes'/&gt;
  &lt;ip address='192.168.122.1' netmask='255.255.255.0'&gt;
    &lt;dhcp&gt;
      &lt;range start='192.168.122.128' end='192.168.122.254'/&gt;
    &lt;/dhcp&gt;
  &lt;/ip&gt;
&lt;/network&gt;
</code></pre>

<h3 id="configuring-the-vm-guests">Configuring the VM guests</h3>

<pre><code>sudo hostnamectl set-hostname neon.qxyz.intra
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[pgBackRest Backup to S3]]></title>
            <link href="https://devopstales.github.io/home/pgbackrest_backup_to_s3/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/pgbackrest_backup_to_s3/?utm_source=atom_feed" rel="related" type="text/html" title="pgBackRest Backup to S3" />
                <link href="https://devopstales.github.io/home/pgbackrest_backup_server/?utm_source=atom_feed" rel="related" type="text/html" title="pgBackRest Backup server" />
                <link href="https://devopstales.github.io/linux/pgbackrest_backup_server/?utm_source=atom_feed" rel="related" type="text/html" title="pgBackRest Backup server" />
                <link href="https://devopstales.github.io/linux/postgresql_pgbackrest/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL backup with pgBackRest" />
                <link href="https://devopstales.github.io/home/postgresql_pgbackrest/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL backup with pgBackRest" />
            
                <id>https://devopstales.github.io/home/pgbackrest_backup_to_s3/</id>
            
            
            <published>2020-02-21T00:00:00+00:00</published>
            <updated>2020-02-21T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this article I will show you how to use pgBackRest to backup PostgreSQL servers to S3 buckets.</p>

<p>For the purpose of this demo setup, we’ll use MinIO server, which is an Amazon S3 Compatible Object Storage.</p>

<h3 id="s3cmd">s3cmd</h3>

<pre><code>yum install -y epel-release
yum --enablerepo epel-testing install -y s3cmd
</code></pre>

<pre><code>nano ~/.s3cfg
host_base = minio.mydomain.intra:9000
host_bucket = minio.mydomain.intra:9000
bucket_location = us-east-1
use_https = false
access_key = &lt;minop_access_key&gt;
secret_key = &lt;minio_secret_key&gt;
signature_v2 = False
</code></pre>

<p>Create bucket and folders</p>

<pre><code>s3cmd mb --no-check-certificate s3://pgbackrest
mkdir postgresql1
s3cmd cp postgresql1 --no-check-certificate s3://pgbackrest
s3cmd ls --no-check-certificate s3://pgbackrest/postgresql1
                       DIR   s3://pgbackrest/postgresql1/
</code></pre>

<h3 id="installation">Installation</h3>

<pre><code>yum install epel-release -y
yum update -y
rpm -Uvh https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-centos10-10-2.noarch.rpm
yum install -y pgbackrest
</code></pre>

<h3 id="configure-pgbackrest">Configure pgBackRest</h3>

<pre><code>nano /etc/pgbackrest.conf

[global]
repo1-path=/postgresql1
repo1-type=s3
repo1-s3-endpoint=minio.mydomain.intra
repo1-s3-port=9000
repo1-s3-bucket=pgbackrest
repo1-s3-verify-tls=n
repo1-s3-key=&lt;minop_access_key&gt;
repo1-s3-key-secret=&lt;minio_secret_key&gt;
repo1-s3-region=us-east-1

repo1-retention-full=1
process-max=2
log-level-console=info
log-level-file=debug
start-fast=y
delta=y

[postgresql1]
pg1-path=/var/lib/pgsql/12/data
</code></pre>

<h3 id="configure-postgresql-on-postgresql1">Configure PostgreSQL on postgresql1</h3>

<pre><code>nano /var/lib/pgsql/12/data/postgresql.conf
...
archive_mode = on
archive_command = 'pgbackrest --stanza=postgresql1 archive-push %p'
...

systemctl restart postgresql12
</code></pre>

<h3 id="perform-a-backup">Perform a backup</h3>

<p>Before we can start a backup we need to initialize the backup repository.</p>

<pre><code>sudo -iu postgres pgbackrest --stanza=postgresql1 stanza-create
sudo -iu postgres pgbackrest --stanza=postgresql1 check
</code></pre>

<pre><code>sudo -iu postgres pgbackrest --stanza=postgresql1 --type=full backup
</code></pre>

<h3 id="show-backup-information">Show backup information</h3>

<pre><code>postgresql1 -iu postgres pgbackrest info
stanza: postgresql1
    status: ok
    cipher: none

    db (current)
        wal archive min/max (11-1): 000000010000000000000005/000000010000000000000005

        full backup: 20200219-091209F
            timestamp start/stop: 2020-02-19 09:12:09 / 2020-02-19 09:12:21
            wal start/stop: 000000010000000000000005 / 000000010000000000000005
            database size: 23.5MB, backup size: 23.5MB
            repository size: 2.8MB, repository backup size: 2.8MB
</code></pre>

<h3 id="restore-a-backup">Restore a backup</h3>

<p>The restore command can then be used on the postgresql1 host.</p>

<pre><code>sudo systemctl stop postgresql12

sudo -iu postgres pgbackrest --stanza=postgresql1 restore
</code></pre>

<p>Restore only test database:</p>

<pre><code>sudo -iu postgres pgbackrest --stanza=postgresql1 --delta --db-include=test restore
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[pgBackRest Backup server]]></title>
            <link href="https://devopstales.github.io/home/pgbackrest_backup_server/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/pgbackrest_backup_server/?utm_source=atom_feed" rel="related" type="text/html" title="pgBackRest Backup server" />
                <link href="https://devopstales.github.io/linux/postgresql_pgbackrest/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL backup with pgBackRest" />
                <link href="https://devopstales.github.io/home/postgresql_pgbackrest/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL backup with pgBackRest" />
                <link href="https://devopstales.github.io/linux/postgresql_replication/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL Streamin replication" />
                <link href="https://devopstales.github.io/home/postgresql_replication/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL Streamin replication" />
            
                <id>https://devopstales.github.io/home/pgbackrest_backup_server/</id>
            
            
            <published>2020-02-20T00:00:00+00:00</published>
            <updated>2020-02-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this article I will show you how to use pgBackRest in a dedicated backup server to backup remote PostgreSQL servers.</p>

<h3 id="installation">Installation</h3>

<pre><code>yum install epel-release -y
yum update -y
rpm -Uvh https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-centos10-10-2.noarch.rpm
yum install -y pgbackrest
</code></pre>

<h3 id="allow-passwordles-ssh-for-hosts">Allow passwordles ssh for hosts</h3>

<p>Generate ssh key on all hosts and copy ssh keys</p>

<pre><code>sudo -u postgres ssh-keygen -f /var/lib/pgsql/.ssh/id_rsa
sudo -u postgres restorecon -R /var/lib/pgsql/.ssh
</code></pre>

<pre><code>sudo -u postgres ssh-copy-id -i /var/lib/pgsql/.ssh/id_rsa postgres@backup-server

sudo -u postgres ssh-copy-id -i /var/lib/pgsql/.ssh/id_rsa postgres@postgresql1
</code></pre>

<h3 id="configure-pgbackrest-on-postgresql1">Configure pgBackRest on postgresql1</h3>

<pre><code>nano /etc/pgbackrest.conf

[global]
repo1-host=backup-server
repo1-host-user=postgres
process-max=2
log-level-console=info
log-level-file=debug

[postgresql1]
pg1-path=/var/lib/pgsql/12/data
</code></pre>

<h3 id="configure-postgresql-on-postgresql1">Configure PostgreSQL on postgresql1</h3>

<pre><code>nano /var/lib/pgsql/12/data/postgresql.conf
...
archive_mode = on
archive_command = 'pgbackrest --stanza=postgresql1 archive-push %p'
...

systemctl restart postgresql12
</code></pre>

<h3 id="configure-pgbackrest-on-backup-server">Configure pgBackRest on backup-server</h3>

<pre><code>nano /etc/pgbackrest.conf

[global]
repo1-path=/var/lib/pgbackrest
repo1-retention-full=1
process-max=2
log-level-console=info
log-level-file=debug
start-fast=y
stop-auto=y

[postgresql1]
pg1-path=/var/lib/pgsql/11/data
pg1-host=postgresql1
pg1-host-user=postgres
</code></pre>

<h3 id="perform-a-backup">Perform a backup</h3>

<p>Before we can start a backup we need to initialize the backup repository on backup-server.</p>

<pre><code>sudo -iu postgres pgbackrest --stanza=postgresql1 stanza-create
</code></pre>

<p>Finally, check the configuration on all hosts.</p>

<pre><code>sudo -iu postgres pgbackrest --stanza=postgresql1 check
</code></pre>

<pre><code>sudo -iu postgres pgbackrest --stanza=postgresql1 --type=full backup
</code></pre>

<h3 id="show-backup-information">Show backup information</h3>

<pre><code>postgresql1 -iu postgres pgbackrest info
stanza: postgresql1
    status: ok
    cipher: none

    db (current)
        wal archive min/max (11-1): 000000010000000000000005/000000010000000000000005

        full backup: 20200219-091209F
            timestamp start/stop: 2020-02-19 09:12:09 / 2020-02-19 09:12:21
            wal start/stop: 000000010000000000000005 / 000000010000000000000005
            database size: 23.5MB, backup size: 23.5MB
            repository size: 2.8MB, repository backup size: 2.8MB
</code></pre>

<h3 id="restore-a-backup">Restore a backup</h3>

<p>The restore command can then be used on the postgresql1 host.</p>

<pre><code>sudo systemctl stop postgresql12

sudo -iu postgres pgbackrest --stanza=postgresql1 restore
</code></pre>

<p>Restore only test database:</p>

<pre><code>sudo -iu postgres pgbackrest --stanza=postgresql1 --delta --db-include=test restore
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[PostgreSQL backup with pgBackRest]]></title>
            <link href="https://devopstales.github.io/home/postgresql_pgbackrest/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/postgresql_pgbackrest/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL backup with pgBackRest" />
                <link href="https://devopstales.github.io/linux/postgresql_replication/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL Streamin replication" />
                <link href="https://devopstales.github.io/home/postgresql_replication/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL Streamin replication" />
                <link href="https://devopstales.github.io/home/install-postgresql/?utm_source=atom_feed" rel="related" type="text/html" title="Install PostgreSQL" />
                <link href="https://devopstales.github.io/linux/install-postgresql/?utm_source=atom_feed" rel="related" type="text/html" title="Install PostgreSQL" />
            
                <id>https://devopstales.github.io/home/postgresql_pgbackrest/</id>
            
            
            <published>2020-02-19T00:00:00+00:00</published>
            <updated>2020-02-19T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this article I will show you how to use pgBackRest as a backup program for PostgreSQL.</p>

<p>pgBackRest aims to be a simple, reliable backup and restore solution that can seamlessly scale up to the largest databases and workloads by utilizing algorithms that are optimized for database-specific requirements.</p>

<h3 id="installation">Installation</h3>

<pre><code>yum install epel-release -y
yum update -y
rpm -Uvh https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-centos10-10-2.noarch.rpm
yum install -y pgbackrest
</code></pre>

<h3 id="configure-pgbackrest">Configure pgBackRest</h3>

<pre><code>cp /etc/pgbackrest.conf /etc/pgbackrest.conf.bck

nano /etc/pgbackrest.conf
[global]
repo1-path=/var/lib/pgsql/12/backups
log-level-console=info
log-level-file=debug
start-fast=y

[backup_stanza]
pg1-path=/var/lib/pgsql/12/data
repo1-retention-full=1
</code></pre>

<p>The configuration is based on two section the <code>global</code> what is specify the repository where you stores the backups and WAL segments archives and a <code>stanza</code>, in my case <code>backup_stanza</code>.</p>

<p>A stanza defines the backup configuration for a specific PostgreSQL database cluster. The stanza section must define the database cluster path and host/user if the database cluster is remote. Also, any global configuration sections can be overridden to define stanza-specific settings.</p>

<h3 id="configure-postgresql">Configure PostgreSQL</h3>

<pre><code>nano /var/lib/pgsql/12/data/postgresql.conf
...
archive_mode = on
archive_command = 'pgbackrest --stanza=backup_stanza archive-push %p'
...

systemctl restart postgresql12
</code></pre>

<h3 id="perform-a-backup">Perform a backup</h3>

<p>Before we can start a backup we need to initialize the backup repository.</p>

<pre><code>$ sudo -iu postgres pgbackrest --stanza=backup_stanza stanza-create
P00   INFO: stanza-create command begin 2.23: ...
P00   INFO: stanza-create command end: completed successfully

$ sudo -iu postgres pgbackrest --stanza=backup_stanza check
P00   INFO: check command begin 2.23: ...
P00   INFO: WAL segment ... successfully stored in the archive at ...
P00   INFO: check command end: completed successfully
</code></pre>

<pre><code>$ sudo -iu postgres pgbackrest --stanza=backup_stanza --type=full backup
P00   INFO: backup command begin 2.23: ...
P00   INFO: execute non-exclusive pg_start_backup() with label
        &quot;pgBackRest backup started at ...&quot;: backup begins after the requested immediate checkpoint completes
P00   INFO: backup start archive = 000000010000000000000005, lsn = 0/5000028
P00   INFO: full backup size = 23.5MB
P00   INFO: execute non-exclusive pg_stop_backup() and wait for all WAL segments to archive
P00   INFO: backup stop archive = 000000010000000000000005, lsn = 0/5000130
P00   INFO: new backup label = 20200219-091209F
P00   INFO: backup command end: completed successfully
P00   INFO: expire command begin
P00   INFO: expire full backup 20200219-090152F
P00   INFO: remove expired backup 20200219-090152F
P00   INFO: expire command end: completed successfully
</code></pre>

<h3 id="show-backup-information">Show backup information</h3>

<pre><code>$ sudo -iu postgres pgbackrest info
stanza: backup_stanza
    status: ok
    cipher: none

    db (current)
        wal archive min/max (11-1): 000000010000000000000005/000000010000000000000005

        full backup: 20200219-091209F
            timestamp start/stop: 2020-02-19 09:12:09 / 2020-02-19 09:12:21
            wal start/stop: 000000010000000000000005 / 000000010000000000000005
            database size: 23.5MB, backup size: 23.5MB
            repository size: 2.8MB, repository backup size: 2.8MB
</code></pre>

<h3 id="restore-a-backup">Restore a backup</h3>

<pre><code>sudo systemctl stop postgresql12

sudo -iu postgres pgbackrest --stanza=backup_stanza restore
P00   INFO: restore command begin 2.15: ...
P00   INFO: restore backup set 20200219-091209F
P00   INFO: write /var/lib/pgsql/11/data/recovery.conf
P00   INFO: restore global/pg_control (performed last to ensure aborted restores cannot be started)
P00   INFO: restore command end: completed successfully
</code></pre>

<p>Restore only test database:</p>

<pre><code>sudo -iu postgres pgbackrest --stanza=backup_stanza --delta --db-include=test restore
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Free sso for Mattermost Teams Edition]]></title>
            <link href="https://devopstales.github.io/home/mattermost-keycloak-sso/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/sso/mattermost-keycloak-sso/?utm_source=atom_feed" rel="related" type="text/html" title="Free sso for Mattermost Teams Edition" />
                <link href="https://devopstales.github.io/linux/install-mattermost-for-gitlab/?utm_source=atom_feed" rel="related" type="text/html" title="Install mattermost for Gitlab" />
                <link href="https://devopstales.github.io/home/install-mattermost-for-gitlab/?utm_source=atom_feed" rel="related" type="text/html" title="Install mattermost for Gitlab" />
            
                <id>https://devopstales.github.io/home/mattermost-keycloak-sso/</id>
            
            
            <published>2020-02-16T00:00:00+00:00</published>
            <updated>2020-02-16T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this article I will show you how to use Keycloak as an authentication provider for  Mattermost Teams Edition.</p>

<p>Slack and Mattermost are very similar products. With Mattermost Teams Edition you can get almost all the functionality of the payed Slack. The only problem is sso. In Mattermost you can use google sso login in the E20 licensing what is more costly the Slack. (Slack: 8$/user Mattermost E20: 8.5$/user) In  Mattermost Teams Edition (the free edition of Mattermost) the only authentication provider you can use is gitlab, but thanks to <a href="https://qiita.com/wadahiro/items/8b118c34aae904353865">wadahiro</a> we can use Keycloak instead of gitlab.</p>

<h3 id="configurate-mattermost">Configurate MAttermost</h3>

<pre><code>nano /etc/mattermost/config.json
...
&quot;GitLabSettings&quot;: {
    &quot;Enable&quot;: false,
    &quot;Secret&quot;: &quot;&lt;secret&gt;&quot;,
    &quot;Id&quot;: &quot;mattermost&quot;,
    &quot;Scope&quot;: &quot;&quot;,
    &quot;AuthEndpoint&quot;: &quot;https://keycloak.mydomain.intra/auth/realms/mydomain/protocol/openid-connect/auth&quot;,
    &quot;TokenEndpoint&quot;: &quot;https://keycloak.mydomain.intra/auth/realms/mydomain/protocol/openid-connect/token&quot;,
    &quot;UserApiEndpoint&quot;: &quot;https://keycloak.mydomain.intra/auth/realms/mydomain/protocol/openid-connect/userinfo&quot;
},
</code></pre>

<h3 id="create-client-on-keycloak">Create Client on Keycloak</h3>

<p>In keycloak go to <code>Configure &gt; Clients</code> and create a new client for Mattermostw With the fallowing data:</p>

<ul>
<li>Standard Flow Enabled: <code>ON</code></li>
<li>Access Type: <code>confidential</code></li>
<li>Valid Redirect URIs: <code>http://&lt;Mattermost-FQDN&gt;/signup/gitlab/complete</code></li>
</ul>

<p><img src="/img/include/mattermost_sso1.png" alt="image" /> <br></p>

<h3 id="create-mapper-for-correct-data">Create mapper for correct data</h3>

<p>Mattermost <a href="https://github.com/mattermost/mattermost-server/blob/v4.10.0/model/gitlab/gitlab.go#L19-L25">want</a> the fallowing data from the authentication provider:</p>

<pre><code>type GitLabUser struct {
	Id       int64  `json:&quot;id&quot;`
	Username string `json:&quot;username&quot;`
	Login    string `json:&quot;login&quot;`
	Email    string `json:&quot;email&quot;`
	Name     string `json:&quot;name&quot;`
}
</code></pre>

<p>Create mapping for username:</p>

<p><img src="/img/include/mattermost_sso2.png" alt="image" /> <br></p>

<p>The only problematic data is the ID. For the test run you can use a self generated id like this:</p>

<p><img src="/img/include/mattermost_sso3.png" alt="image" /> <br></p>

<h3 id="create-user-for-test">Create user for test</h3>

<p>For testing purposes we’re going to create a local user in Keycloak.</p>

<p><img src="/img/include/mattermost_sso4.png" alt="image" /> <br></p>

<p>Now we need to create an attribute for this user with the mattermost id and the value should be an integer between 1 and 9999999999999999999.</p>

<p><img src="/img/include/mattermost_sso5.png" alt="image" /> <br></p>

<p>If you have many users you didn&rsquo;t want to create id for them manually. If you use LDAP for users the best fit for this need was the employeeNumber or EmployeeId ldap attribute.</p>

<p><img src="/img/include/mattermost_sso6.png" alt="image" /> <br></p>

<p>If you use ActiveDirectory this attribute maybe null so we need to generate it with a powershell script.</p>

<pre><code>ForEach ($User in ((Get-ADUser -Filter * -Properties SamAccountName,EmployeeId)))
{
if ( ([string]::IsNullOrEmpty($User.EmployeeId)))
{
$DATE = (Get-ADuser $User.SamAccountName -Properties whencreated).whencreated.ToString('yyMMddHHmmss')
$RANDOM = (Get-Random -Maximum 9999)
$DATA = -join ($DATE, $RANDOM)
$User.SamAccountName
$DATA
Set-ADUser $User.SamAccountName -employeeID $DATA
}
}
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[PostgreSQL: pg_rewind]]></title>
            <link href="https://devopstales.github.io/home/postgresql_pg_rewind/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/postgresql_pg_rewind/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL: pg_rewind" />
                <link href="https://devopstales.github.io/linux/postgresql_replication/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL Streamin replication" />
                <link href="https://devopstales.github.io/home/postgresql_replication/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL Streamin replication" />
                <link href="https://devopstales.github.io/home/install-postgresql/?utm_source=atom_feed" rel="related" type="text/html" title="Install PostgreSQL" />
                <link href="https://devopstales.github.io/linux/install-postgresql/?utm_source=atom_feed" rel="related" type="text/html" title="Install PostgreSQL" />
            
                <id>https://devopstales.github.io/home/postgresql_pg_rewind/</id>
            
            
            <published>2020-02-05T00:00:00+00:00</published>
            <updated>2020-02-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this pos I will show you how to perform a rewind on a broken Streamin replication.</p>

<h3 id="create-test-db">create test db</h3>

<pre><code># master:
sudo -iu postgres psql -c &quot;create database test1;&quot;
</code></pre>

<h3 id="break-the-replication">break the replication</h3>

<pre><code># slaeve:
sudo -u postgres /usr/pgsql-12/bin/pg_ctl stop -m fast -D /var/lib/pgsql/12/data/
</code></pre>

<p>generate data in master</p>

<pre><code># master:
sudo -iu postgres /usr/pgsql-12/bin/pgbench -i -s 100 -d test1
</code></pre>

<h3 id="perform-the-rewind">perform the rewind</h3>

<pre><code># slaeve:
sudo -u postgres /usr/pgsql-12/bin/pg_rewind  --target-pgdata=/var/lib/pgsql/12/data/ --source-server=&quot;host=192.168.0.110 user=admin password=Password1 dbname=test1&quot; -P
systemctl start postgresql-12
systemctl status postgresql-12
</code></pre>

<h3 id="test-the-replication-status">test the replication status</h3>

<pre><code>/usr/pgsql-12/bin/pg_controldata -D /var/lib/pgsql/12/data/ | grep cluster
Database cluster state:               in archive recovery
</code></pre>

<pre><code># master:
sudo -u postgres psql -x -c &quot;select * from pg_stat_replication&quot;
could not change directory to &quot;/root&quot;: Permission denied
-[ RECORD 1 ]----+------------------------------
pid              | 11548
usesysid         | 16384
usename          | replica_user
application_name | walreceiver
client_addr      | 192.168.0.111
client_hostname  |
client_port      | 48592
backend_start    | 2020-02-08 10:49:45.449591+01
backend_xmin     |
state            | streaming
sent_lsn         | 1/448638D0
write_lsn        | 1/448638D0
flush_lsn        | 1/448638D0
replay_lsn       | 1/448638D0
write_lag        |
flush_lag        |
replay_lag       |
sync_priority    | 0
sync_state       | async
reply_time       | 2020-02-08 10:50:03.026266+01
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[PostgreSQL Streamin replication]]></title>
            <link href="https://devopstales.github.io/home/postgresql_replication/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/postgresql_replication/?utm_source=atom_feed" rel="related" type="text/html" title="PostgreSQL Streamin replication" />
                <link href="https://devopstales.github.io/linux/install-postgresql/?utm_source=atom_feed" rel="related" type="text/html" title="Install PostgreSQL" />
                <link href="https://devopstales.github.io/home/install-postgresql/?utm_source=atom_feed" rel="related" type="text/html" title="Install PostgreSQL" />
            
                <id>https://devopstales.github.io/home/postgresql_replication/</id>
            
            
            <published>2020-02-03T00:00:00+00:00</published>
            <updated>2020-02-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this pos I will show you how to configure Postgresql Streamin replication.</p>

<h3 id="wal-segments">WAL segments</h3>

<p>At all times, PostgreSQL maintains a write ahead log (WAL) in the pg_wal/ subdirectory of the cluster&rsquo;s data directory. The log describes every change made to the database&rsquo;s data files. This log exists primarily for crash-safety purposes: if the system crashes, the database can be restored to consistency by “replaying&rdquo; the log entries made since the last checkpoint. This wal segments san be used to repliyate the transactions to a second read database server.</p>

<h3 id="requirements">requirements</h3>

<pre><code>yum install https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm -y
yum install postgresql12 postgresql12-server postgresql12-contrib -y
</code></pre>

<pre><code>/usr/pgsql-12/bin/postgresql-12-setup initdb
</code></pre>

<p>Set up the streaming replication related parameters on the two servers</p>

<pre><code>$ nano /var/lib/pgsql/12/data/postgresql.conf
--------------------------------------------------------
listen_addresses = '*'

wal_level = hot_standby

max_wal_senders = 6

#16 Mb * 32 = 512 Mb
wal_keep_segments = 32

max_wal_size = 2GB

# for pg dump on slave
hot_standby_feedback = on

# for pg_rewind
full_page_writes = on
wal_log_hints = on
</code></pre>

<h3 id="master-config">master config</h3>

<pre><code>$ nano /var/lib/pgsql/12/data/postgresql.conf
--------------------------------------------------------
archive_mode = on
archive_command = 'cp %p /var/lib/pgsql/archive/%f'
</code></pre>

<pre><code>systemctl enable postgresql-12
systemctl start postgresql-12
</code></pre>

<pre><code>su - postgres
pgsql
ALTER SYSTEM SET listen_addresses TO '*';
CREATE ROLE replica_user WITH REPLICATION LOGIN;
\du
\q
exit
</code></pre>

<pre><code>echo &quot;host    replication     replica_user    192.168.0.111/32      trust&quot;&gt;&gt;/var/lib/pgsql/12/data/pg_hba.conf
systemctl restart postgresql-12
</code></pre>

<h3 id="slave-config">slave config</h3>

<pre><code>rm -rf /var/lib/pgsql/12/data/*

su - postgres
/usr/pgsql-12/bin/pg_basebackup --host=192.168.0.110 --pgdata=/var/lib/pgsql/12/data/ --username=replica_user --verbose --progress --wal-method=stream --write-recovery-conf --checkpoint=fast

--create-slot --slot=Slot_name

ll /var/lib/pgsql/12/data/standby.signal
cat /var/lib/pgsql/12/data/postgresql.auto.conf

echo &quot;restore_command = 'cp /var/lib/pgsql/archive/%f %p'&quot; &gt;&gt; /var/lib/pgsql/12/data/postgresql.auto.conf

systemctl start postgresql-12
</code></pre>

<h3 id="slave-test">slave  test</h3>

<pre><code>SELECT * FROM pg_stat_wal_receiver;
sudo -u postgres psql -x -c &quot;select * from pg_stat_wal_receiver&quot;
</code></pre>

<h3 id="master-test">master test</h3>

<pre><code>SELECT * FROM pg_stat_replication;
sudo -u postgres psql -x -c &quot;select * from pg_stat_replication&quot;
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes project longhorn]]></title>
            <link href="https://devopstales.github.io/home/k8s-longhorn/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/k8s-longhorn/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes project longhorn" />
            
                <id>https://devopstales.github.io/home/k8s-longhorn/</id>
            
            
            <published>2020-01-18T00:00:00+00:00</published>
            <updated>2020-01-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Longhorn is lightweight, reliable, and powerful distributed block storage system for Kubernetes..</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../cloud/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../cloud/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../cloud/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../cloud/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part3: <a href="../../cloud/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-traefik-ingress --></li>
<li>Part4: <a href="../../cloud/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../cloud/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../cloud/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../cloud/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../cloud/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a>
<!-- CEPH in Kubernetes with rook --></li>
<li>Part5d: <a href="../../cloud/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes vMware --></li>
<li>Part6: <a href="../../cloud/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part7: <a href="../../cloud/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part8: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part9: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part10: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part11: <a href="../../cloud/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- prometheus operator + grafana operator -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<p>You can install Longhorn on an existing Kubernetes cluster with one <code>kubectl apply</code> command or using Helm charts. Once Longhorn is installed, it adds persistent volume support to the Kubernetes cluster.</p>

<h3 id="install-dependency">Install dependency</h3>

<pre><code>yum install iscsi-initiator-utils

modprobe iscsi_tcp
echo &quot;iscsi_tcp&quot; &gt;/etc/modules-load.d/iscsi-tcp.conf
</code></pre>

<h3 id="deploy-longhorn-and-storageclass">Deploy Longhorn and storageclass</h3>

<pre><code>kubectl apply -f https://raw.githubusercontent.com/rancher/longhorn/master/deploy/longhorn.yaml
kubectl apply -f https://raw.githubusercontent.com/rancher/longhorn/master/examples/storageclass.yaml

kubectl get storageclass
NAME       PROVISIONER          RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
longhorn   driver.longhorn.io   Delete          Immediate           false                  11m
</code></pre>

<p>Patch longhorn storageclass to be the default storageclass.</p>

<pre><code>kubectl patch storageclass longhorn -p \
  '{&quot;metadata&quot;: {&quot;annotations&quot;:{&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;true&quot;}}}'

kubectl get storageclass
NAME                 PROVISIONER          RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
longhorn (default)   driver.longhorn.io   Delete          Immediate           false                  12m
</code></pre>

<h3 id="deploy-admin-gui">Deploy admin gui</h3>

<pre><code>kubectl -n longhorn-system get svc

cat minio-sec.yaml
---
apiVersion: v1
kind: Secret
metadata:
  namespace: longhorn-system
  name: longhorn-minio
type: Opaque
data:
  AWS_ACCESS_KEY_ID: bWluaW8=
  AWS_SECRET_ACCESS_KEY: bWluaW8xMjM=
  AWS_ENDPOINTS: aHR0cDovL21pbmlvLmxvbmdob3JuLXN5c3RlbTo5MDAw
</code></pre>

<p><img src="/img/include/longhorn0.png" alt="Example image" /></br>
<img src="/img/include/longhorn2.png" alt="Example image" /></br>
<img src="/img/include/longhorn1.png" alt="Example image" /></br></p>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to speed up zfs resilver?]]></title>
            <link href="https://devopstales.github.io/home/speed_up_zfs/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/speed_up_zfs/?utm_source=atom_feed" rel="related" type="text/html" title="How to speed up zfs resilver?" />
            
                <id>https://devopstales.github.io/home/speed_up_zfs/</id>
            
            
            <published>2020-01-12T00:00:00+00:00</published>
            <updated>2020-01-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this article I will show you how speed up zfs.</p>

<p>First we need to understand there is two type of zfs the FreeBSD/Solaris based and Linux based cald Zfs On Linux or ZOL.</p>

<p>When a device is replaced, a resilvering operation is initiated to move data from the good copies to the new device. This action is a form of disk scrubbing. RAIDz resilvering is very slow in OpenZFS-based zpools. Basically, it starts with every transaction that’s ever happened in the pool and plays them back one-by-one to the new drive. This is very IO-intensive. Some say if you’re using hard drives larger than 1TB and you are using OpenZFS, use mirror, not RAIDz* or use only SSD for RAIDz*.</p>

<p>In ZOL 0.8.0 they changed to a new resilvering solution. The previous resilvering algorithm repairs blocks from oldest to newest, which can degrade into a lot of small random I/O. The new resilvering algorithm uses a two-step process to sort and resilver blocks in LBA order. If you have an older version of ZOL or want even better performance you can tweak with the zfs configuration.</p>

<h3 id="speed-up-resilvering">Speed up resilvering</h3>

<pre><code>echo 0 &gt; /sys/module/zfs/parameters/zfs_resilver_delay
echo 0 &gt; /sys/module/zfs/parameters/zfs_scrub_delay
echo 512 &gt; /sys/module/zfs/parameters/zfs_top_maxinflight
echo 8192 &gt; /sys/module/zfs/parameters/zfs_top_maxinflight
echo 8000 &gt; /sys/module/zfs/parameters/zfs_resilver_min_time_ms
</code></pre>

<h3 id="speed-up-on-pool">Speed up on pool</h3>

<pre><code>zpool set autoreplace=on zpool

echo 0 &gt; /sys/module/zfs/parameters/zfs_scan_idle
echo 24 &gt; /sys/module/zfs/parameters/zfs_vdev_scrub_min_active
echo 64 &gt; /sys/module/zfs/parameters/zfs_vdev_scrub_max_active
echo 64 &gt; /sys/module/zfs/parameters/zfs_vdev_sync_write_min_active
echo 32 &gt; /sys/module/zfs/parameters/zfs_vdev_sync_write_max_active
echo 8 &gt; /sys/module/zfs/parameters/zfs_vdev_sync_read_min_active
echo 32 &gt; /sys/module/zfs/parameters/zfs_vdev_sync_read_max_active
echo 8 &gt; /sys/module/zfs/parameters/zfs_vdev_async_read_min_active
echo 32 &gt; /sys/module/zfs/parameters/zfs_vdev_async_read_max_active
echo 8 &gt; /sys/module/zfs/parameters/zfs_vdev_async_write_min_active
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes nginx ingress with helm]]></title>
            <link href="https://devopstales.github.io/home/k8s-local-pv/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/k8s-local-pv/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
                <link href="https://devopstales.github.io/home/k8s-velero-backup/?utm_source=atom_feed" rel="related" type="text/html" title="Backup your Kubernetes Cluster with Velero" />
                <link href="https://devopstales.github.io/cloud/k8s-velero-backup/?utm_source=atom_feed" rel="related" type="text/html" title="Backup your Kubernetes Cluster with Velero" />
                <link href="https://devopstales.github.io/cloud/k8s-cert-manager/?utm_source=atom_feed" rel="related" type="text/html" title="Install cert-manager for Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-cert-manager/?utm_source=atom_feed" rel="related" type="text/html" title="Install cert-manager for Kubernetes" />
            
                <id>https://devopstales.github.io/home/k8s-local-pv/</id>
            
            
            <published>2020-01-08T00:00:00+00:00</published>
            <updated>2020-01-08T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how to use a local folder as a persistent volume in Kubernetes.</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../cloud/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../cloud/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../cloud/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../cloud/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part3: <a href="../../cloud/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-traefik-ingress --></li>
<li>Part4: <a href="../../cloud/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../cloud/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../cloud/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../cloud/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../cloud/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a>
<!-- CEPH in Kubernetes with rook --></li>
<li>Part5d: <a href="../../cloud/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes vMware --></li>
<li>Part6: <a href="../../cloud/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part7: <a href="../../cloud/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part8: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part9: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part10: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part11: <a href="../../cloud/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- prometheus operator + grafana operator -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<p>For a production environment this is not an ideal structure because if you store the data on a single host if the host dies your data will be lost. For this Demo I will use a separate disk for storing the PV&rsquo;s folders. So you can backup or replicate this disk separately.</p>

<h3 id="configure-the-disk">Configure the disk</h3>

<pre><code>vgcreate local-vg /dev/sdd
lvcreate -l 100%FREE -n local-lv local-vg /dev/sdd
mkfs.xfs -f /dev/local-vg/local-lv
mkdir -p /mnt/local-storage/
mount /dev/local-vg/local-lv /mnt/local-storage
echo &quot;/dev/local-vg/local-lv        /mnt/local-storage              xfs defaults 0 0&quot; &gt;&gt; /etc/fstab
rm -rf /mnt/local-storage/lost+found
</code></pre>

<p>Now you can create every PV and PVC manually.</p>

<pre><code>mkdir /mnt/local-storage/pv-tst

cat pv-tst.yaml
---
kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv-tst
spec:
  capacity:
    storage: 1Gi
  local:
    path: /mnt/local-storage/pv-tst
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local
  nodeAffinity:
    required:
      nodeSelectorTerms:
        - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
                - kubernetes03.devopstales.intra
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pv-tst
  namespace: tst
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  volumeName: pv-tst
  storageClassName: local
</code></pre>

<h3 id="add-automated-hostpath-provisioner">Add automated hostpath-provisioner</h3>

<p>This is a Persistent Volume Claim (PVC) provisioner for Kubernetes. It dynamically provisions hostPath volumes to provide storage for PVCs.</p>

<pre><code>git clone https://github.com/torchbox/k8s-hostpath-provisioner
cd k8s-hostpath-provisioner
kubectl apply -f deployment.yaml

nano local-sc.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: auto-local
  annotations:
    storageclass.kubernetes.io/is-default-class: &quot;true&quot;
provisioner: torchbox.com/hostpath
parameters:
  pvDir: /mnt/local-storage
</code></pre>

<p>Test the provisioner by creating a new PVC:</p>

<pre><code>cat testpvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: testpvc
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 50Gi

kubectl create -f testpvc.yaml
kubectl get pvc
NAME      STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
testpvc   Bound     pvc-145c785e-ab83-11e7-9432-4201ac1fd019   50Gi       RWX            auto-local     10s
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubectl authentication with Kuberos]]></title>
            <link href="https://devopstales.github.io/home/k8s-kuberos/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/k8s-kuberos/</id>
            
            
            <published>2020-01-07T00:00:00+00:00</published>
            <updated>2020-01-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Kuberos is an OIDC authentication helper for Kubernetes&rsquo; kubectl</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../cloud/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../cloud/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../cloud/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../cloud/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part3: <a href="../../cloud/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-traefik-ingress --></li>
<li>Part4: <a href="../../cloud/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../cloud/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../cloud/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../cloud/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../cloud/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a>
<!-- CEPH in Kubernetes with rook --></li>
<li>Part5d: <a href="../../cloud/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes vMware --></li>
<li>Part6: <a href="../../cloud/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part7: <a href="../../cloud/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part8: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part9: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part10: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part11: <a href="../../cloud/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- prometheus operator + grafana operator -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<pre><code>nano /etc/kubernetes/manifests/kube-apiserver.yaml
...
    command:
    - /hyperkube
    - apiserver
    - --advertise-address=10.10.40.30
...

    - --oidc-issuer-url=https://keycloak.devopstales.intra/auth/realms/mydomain
    - --oidc-client-id=k8s
    - --oidc-username-claim=email
    - --oidc-groups-claim=groups
...

systemctl restart docker kubelet
</code></pre>

<pre><code>cat &gt;&gt;EOF&lt; values.yaml
replicaCount: 1

kuberos:
  oidcClientURL: https://keycloak.devopstales.intra/auth/realms/mydomain
  oidcClientID: k8s
  oidcSecret: 43219919-0904-4338-bc0f-c986e1891a7a
  clusters:
  - name: openshift
    apiServer: https://192.168.0.106:6443
    # `apiServer` is the url for kubectl
    #   This is typically  https://api.fqdn
    caCrt: |-
      -----BEGIN CERTIFICATE-----
      MIIDZDCCAkygAwIBAgIIe/R9sc8oJiAwDQYJKoZIhvcNAQELBQAwFTETMBEGA1UE
      AxMKa3ViZXJuZXRlczAeFw0xOTEyMjcxNzM3MzlaFw0yMDEyMjYxNzM3MzlaMBkx
      FzAVBgNVBAMTDmt1YmUtYXBpc2VydmVyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8A
      MIIBCgKCAQEAzAg7MflA/HVTVcZPsGJH71cfcJ/U1CtEYbXfu/AQbhGg09XKmeK9
      aGEK3kSgi/Hyoi7M+e/ntx1+Gp/jwc8kanMFRLgxdKCxxi4MOswZF/q2loUdNoE/
      OQVPWQi8Hgznubw/0gINUkIq8mRx9Bb+RcRnJEfD3CXkxDhUNeCvvjeTrujguF0h
      pgfzrLoc2kGdJYpHiLqow8jRq7XXk0RzZaqCQjAEZgqWamwbTTqFZh3v+1gF/2s0
      EbFVVL2Ctu1dOGe1FkZxte7/Po1XBkPLQuRXbH3QRiJkPfyOW16T1nWk1QTcpCdH
      HO/l+CY2nLPFZL1BM83QuVmPgR1T1p+5tQIDAQABo4GzMIGwMA4GA1UdDwEB/wQE
      AwIFoDATBgNVHSUEDDAKBggrBgEFBQcDATCBiAYDVR0RBIGAMH6CEms4cy5teWRv
      bWFpbi5pbnRyYYIKa3ViZXJuZXRlc4ISa3ViZXJuZXRlcy5kZWZhdWx0ghZrdWJl
      cm5ldGVzLmRlZmF1bHQuc3ZjgiRrdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0
      ZXIubG9jYWyHBApgAAGHBMCoAGowDQYJKoZIhvcNAQELBQADggEBAJZ7jxPR72V6
      PYL3SKCWaS+RgTuGuSm0pYu26cBmPOjsugd8DUrJ7+iAnKDHUmmw22sWheLLCokc
      YU/AIfdbbsz0+f+/qthkO7zJmAJgdIAOMJ5MQCbxMBt+6L813r1R3QI7kAGxHvzV
      loKJVIIHq/6K3gFEZDfo0myvNvtOIpBCeMnZRK+8hx3UNcHckZbhkan1Z1j9t9iw
      b6Vv5jY1+9t2Iltd2wuNaUvHicx+3X6JPAqVR6H0jI3i+QSyT1EHXtBtbQBBpP4T
      5WDz+9uDa1mIDHtww7DTnJwY+hGI7fVF2H7XQaM4xwhGnwIwbkSh45JWVtUEHMou
      Q7T4bTyrwuQ=
      -----END CERTIFICATE-----
    # `caCrt` is the public / CA cert for the cluster
    # cat /etc/kubernetes/pki/apiserver.crt

ingress:
  enabled: true
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/proxy-buffer-size: &quot;64k&quot;
    cert-manager.io/cluster-issuer: ca-issuer
    ingress.kubernetes.io/force-ssl-redirect: &quot;true&quot;
  path: /
  hosts:
    - kubectl.devopstales.intra
  tls:
    - secretName: default-cert
      hosts:
        - kubectl.devopstales.intra

image:
  repository: negz/kuberos
  tag: ede4085
  pullPolicy: IfNotPresent

service:
  type: ClusterIP
  port: 80
  annotations: {}
  # Add your service annotations here.

resources: {}
EOF
</code></pre>

<pre><code>helm upgrade --install kuberos stable/kuberos --namespace kuberos -f values.yaml
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Dashboard authentication with Keycloak]]></title>
            <link href="https://devopstales.github.io/home/k8s-gangway/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/k8s-gangway/</id>
            
            
            <published>2020-01-06T00:00:00+00:00</published>
            <updated>2020-01-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Kuberos is an OIDC authentication helper for Kubernetes&rsquo; kubectl</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../cloud/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../cloud/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../cloud/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../cloud/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part3: <a href="../../cloud/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-traefik-ingress --></li>
<li>Part4: <a href="../../cloud/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../cloud/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../cloud/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../cloud/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../cloud/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a>
<!-- CEPH in Kubernetes with rook --></li>
<li>Part5d: <a href="../../cloud/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes vMware --></li>
<li>Part6: <a href="../../cloud/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part7: <a href="../../cloud/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part8: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part9: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part10: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part11: <a href="../../cloud/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- prometheus operator + grafana operator -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<pre><code>nano /etc/kubernetes/manifests/kube-apiserver.yaml
...
    command:
    - /hyperkube
    - apiserver
    - --advertise-address=10.10.40.30
...

    - --oidc-issuer-url=https://keycloak.devopstales.intra/auth/realms/mydomain
    - --oidc-client-id=k8s
    - --oidc-username-claim=email
    - --oidc-groups-claim=groups
...

systemctl restart docker kubelet
</code></pre>

<pre><code>nano gangway.yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: gangway
---
apiVersion: v1
kind: Secret
metadata:
  name: gangway-key
  labels:
    app.kubernetes.io/name: gangway
type: Opaque
data:
  sessionkey: &quot;ZTZNYlJUbDdHcHlSeXVFU0J6ZDZmbUs5Mks5a21NWEo=&quot;
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: gangway
  namespace: gangway
data:
  gangway.yaml: |
    clusterName: &quot;minikube&quot;
    authorizeURL: &quot;https://keycloak.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/auth&quot;
    tokenURL: &quot;https://keycloak.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/token&quot;
    audience: &quot;https://keycloak.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/userinfo&quot;
    # Used to specify the scope of the requested Oauth authorization.
    # scopes: [&quot;openid&quot;, &quot;profile&quot;, &quot;email&quot;, &quot;offline_access&quot;]
    # scopes: [&quot;groups&quot;]
    # scopes: [&quot;openid&quot;, &quot;profile&quot;, &quot;email&quot;, &quot;offline_access&quot;, &quot;groups&quot;]
    # Where to redirect back to. This should be a URL where gangway is reachable.
    # Typically this also needs to be registered as part of the oauth application
    # with the oAuth provider.
    # Env var: GANGWAY_REDIRECT_URL
    redirectURL: &quot;https://gangway.devopstales.intra/callback&quot;
    clientID: &quot;k8s&quot;
    clientSecret: &quot;43219919-0904-4338-bc0f-c986e1891a7a&quot;
    # The JWT claim to use as the username. This is used in UI.
    # Default is &quot;nickname&quot;. This is combined with the clusterName
    # for the &quot;user&quot; portion of the kubeconfig.
    # Env var: GANGWAY_USERNAME_CLAIM
    # usernameClaim: &quot;sub&quot;
    usernameClaim: &quot;preferred_username&quot;
    emailClaim: &quot;email&quot;
    # The API server endpoint used to configure kubectl
    # Env var: GANGWAY_APISERVER_URL
    # apiServerURL: &quot;https://kube.codeformuenster.org:6443&quot;
    apiServerURL: &quot;https://192.168.0.106:8443&quot;
    # The path to find the CA bundle for the API server. Used to configure kubectl.
    # This is typically mounted into the default location for workloads running on
    # a Kubernetes cluster and doesn't need to be set.
    # Env var: GANGWAY_CLUSTER_CA_PATH
    # cluster_ca_path: &quot;/var/run/secrets/kubernetes.io/serviceaccount/ca.crt&quot;
    # The path to a root CA to trust for self signed certificates at the Oauth2 URLs
    # Env var: GANGWAY_TRUSTED_CA_PATH
    # for self signd certificate:
    trustedCAPath: /gangway/rootca.crt
    # The path gangway uses to create urls (defaults to &quot;&quot;)
    # Env var: GANGWAY_HTTP_PATH
    # httpPath: &quot;https://${GANGWAY_HTTP_PATH}&quot;
    # for self signd certificate:
  rootca.crt: |
    -----BEGIN CERTIFICATE-----
    MIIEATCCAumgAwIBAgIUXHOQsGW+UHpCphHViHSvyBny8BwwDQYJKoZIhvcNAQEL
    BQAwgY4xCzAJBgNVBAYTAkhVMQ0wCwYDVQQIDARQZXN0MREwDwYDVQQHDAhCdWRh
    cGVzdDETMBEGA1UECgwKTXkgQ29tcGFueTELMAkGA1UECwwCT1UxFzAVBgNVBAMM
    Dm15ZG9tYWluLmludHJhMSIwIAYJKoZIhvcNAQkBFhNyb290QG15ZG9tYWluLmlu
    dHJhMCAXDTE5MTIyNzE3MTk0OVoYDzIxMTkxMjAzMTcxOTQ5WjCBjjELMAkGA1UE
    BhMCSFUxDTALBgNVBAgMBFBlc3QxETAPBgNVBAcMCEJ1ZGFwZXN0MRMwEQYDVQQK
    DApNeSBDb21wYW55MQswCQYDVQQLDAJPVTEXMBUGA1UEAwwObXlkb21haW4uaW50
    cmExIjAgBgkqhkiG9w0BCQEWE3Jvb3RAbXlkb21haW4uaW50cmEwggEiMA0GCSqG
    SIb3DQEBAQUAA4IBDwAwggEKAoIBAQDevUZouW101hAu68qojvfmnC3fUIA9L5nj
    J+OTbgwDhxYduHVcmwFcrZJzBn/udm72sAUsjmoc34ZoEQVr1mCjrnDdb3NtOWBI
    XYmN7/RySzu5DSFLFv8Sj+27VvGLpYTXgDEt+IQpV4EgosX6DzjYK7BtmqaWCY3t
    aClGnzxEotlxMakTCt9eALD+l/ffV4NbiS6sPNaOFHbG8CKRnfzDzqh78qYaSH8d
    wWxGLGAvciNm1wv1G3NIkjMIZlkMqAv6uTzEtfOPQrHigG8sbb4hHAg8a9RtH2Sk
    nXjZRb3Wfo+XJ2eUCZyC6pwvZfEuZBuRAAo12Ycp/Ve2FC3kvpzLAgMBAAGjUzBR
    MB0GA1UdDgQWBBTRPLiCReojvBQXna2zkBBTsKdsnzAfBgNVHSMEGDAWgBTRPLiC
    ReojvBQXna2zkBBTsKdsnzAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUA
    A4IBAQBXp676KZ7VTPc/RPI+QIyb0ugPE+w6fkpw1PIzG+y0h53AsOu15tXxKX5L
    SwotjXoDuTnqQqLZ5wTFSiolscay+MpEDnoIdo+Pw7u3q3bpn6GmDjae1BaIL/En
    wvxvvJQsOJrXfEUQeC6M75i/MrYPSwhWNDAbqJTY2qEuRXcj/AALGrnlF5DEEd+O
    RYw79sj+xU88/kCOVWI35LwiH+/0QWFyKcPQvY8nER69nt5evFGqUQPE6qlOJKg/
    YD8dK+OF26Ta/qz0iKNAfh3WDgYU4lHAawKtwAbpBVBLlzLl+bD11BQvn6zDWVWA
    rfrwKUIO+dwSL3ZKS0kA0OlN3dyy
    -----END CERTIFICATE-----
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gangway
  namespace: gangway
  labels:
    app: gangway
spec:
  replicas: 1
  selector:
    matchLabels:
      app: gangway
  strategy:
  template:
    metadata:
      labels:
        app: gangway
    spec:
      containers:
      - name: gangway
        image: gcr.io/heptio-images/gangway:v3.2.0
        command: [&quot;gangway&quot;, &quot;-config&quot;, &quot;/gangway/gangway.yaml&quot;]
        env:
        - name: GANGWAY_SESSION_SECURITY_KEY
          valueFrom:
            secretKeyRef:
              key: sessionkey
              name: gangway-key
        - name: GANGWAY_PORT
          value: &quot;8080&quot;
        ports:
        - name: http
          containerPort: 8080
          protocol: TCP
        resources:
          requests:
            cpu: &quot;100m&quot;
            memory: &quot;128Mi&quot;
          limits:
            cpu: &quot;200m&quot;
            memory: &quot;512Mi&quot;
        volumeMounts:
        - name: gangway
          mountPath: /gangway/
        livenessProbe:
          httpGet:
            path: /
            port: 8080
          initialDelaySeconds: 20
          timeoutSeconds: 1
          periodSeconds: 60
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /
            port: 8080
          timeoutSeconds: 1
          periodSeconds: 10
          failureThreshold: 3
      volumes:
      - name: gangway
        configMap:
          name: gangway
---
kind: Service
apiVersion: v1
metadata:
  name: gangway
  namespace: gangway
  labels:
    app: gangway
spec:
  type: ClusterIP
  ports:
    - name: &quot;http&quot;
      protocol: TCP
      port: 80
      targetPort: &quot;http&quot;
  selector:
    app: gangway
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: gangway
  namespace: gangway
  annotations:
    ingress.kubernetes.io/rewrite-target: /
    cert-manager.io/cluster-issuer: ca-issuer
    ingress.kubernetes.io/force-ssl-redirect: &quot;true&quot;
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/proxy-buffer-size: &quot;64k&quot;
spec:
  rules:
  - host: gangway.devopstales.intra
    http:
      paths:
      - backend:
          serviceName: gangway
          servicePort: http
  tls:
  - secretName: gangway-tls
    hosts:
    - gangway.devopstales.intra
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Dashboard authentication with Keycloak]]></title>
            <link href="https://devopstales.github.io/home/k8s-dasboard-auth/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/k8s-dasboard-auth/</id>
            
            
            <published>2020-01-03T00:00:00+00:00</published>
            <updated>2020-01-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how to add a keycloak gatekeeper authentication proxy for Kubernetes Dashboard.</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../cloud/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../cloud/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../cloud/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../cloud/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part3: <a href="../../cloud/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-traefik-ingress --></li>
<li>Part4: <a href="../../cloud/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../cloud/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../cloud/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../cloud/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../cloud/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a>
<!-- CEPH in Kubernetes with rook --></li>
<li>Part5d: <a href="../../cloud/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes vMware --></li>
<li>Part6: <a href="../../cloud/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part7: <a href="../../cloud/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part8: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part9: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part10: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part11: <a href="../../cloud/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- prometheus operator + grafana operator -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<p>Kubernetes does not have its own user management and relies on external providers like Keycloak. First we need to integrate an OpeniD prodiver (for me keycloak) with the kubernetes api server.</p>

<pre><code>nano /etc/kubernetes/manifests/kube-apiserver.yaml
...
    command:
    - /hyperkube
    - apiserver
    - --advertise-address=10.10.40.30
...

    - --oidc-issuer-url=https://keycloak.devopstales.intra/auth/realms/mydomain
    - --oidc-client-id=k8s
    - --oidc-username-claim=email
    - --oidc-groups-claim=groups
    # for self sign cert or custom ca
    #- --oidc-ca-file=/etc/kubernetes/pki/rootca.pem
...

systemctl restart docker kubelet
</code></pre>

<p>We need an authentication proxy before the dasboard. I will use keycloak-gatekeeper for that purpose.</p>

<pre><code>nano proxy-deplayment.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dasboard-proxy
  labels:
    app.kubernetes.io/name: dasboard-proxy
  namespace: kubernetes-dashboard
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: dasboard-proxy
  template:
    metadata:
      labels:
        app.kubernetes.io/name: dasboard-proxy
    spec:
      containers:
        - name: dasboard-proxy
          image: &quot;keycloak/keycloak-gatekeeper:latest&quot;
          command:
            - /opt/keycloak-gatekeeper
            - --discovery-url=https://keycloak.devopstales.intra/auth/realms/mydomain/.well-known/openid-configuration
            - --client-id=k8s
            - --client-secret=43219919-0904-4338-bc0f-c986e1891a7a
            - --listen=0.0.0.0:3000
            - --encryption-key=AgXa7xRcoClDEU0ZDSH4X0XhL5Qy2Z2j
            - --redirection-url=https://dashboard.devopstales.intra
            - --enable-refresh-tokens=true
            - --upstream-url=https://kubernetes-dashboard
            # debug:
            #- --upstream-url=http://echo:8080
            # for self sign cert or custom ca
            #- --skip-upstream-tls-verify
            #- --skip-openid-provider-tls-verify
          ports:
            - name: http
              containerPort: 3000
              protocol: TCP
---
apiVersion: v1
kind: Service
metadata:
  name: dasboard-proxy
  labels:
    app.kubernetes.io/name: dasboard-proxy
  namespace: kubernetes-dashboard
spec:
  type: ClusterIP
  ports:
    - port: 3000
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: dasboard-proxy
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: dasboard-proxy
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/proxy-buffer-size: &quot;64k&quot;
    cert-manager.io/cluster-issuer: ca-issuer
  namespace: kubernetes-dashboard
spec:
  tls:
    - hosts:
        - dashboard.devopstales.intra
      secretName: dasboard-proxy-tls
  rules:
    - host: dashboard.devopstales.intra
      http:
        paths:
          - backend:
             serviceName: dasboard-proxy
             servicePort: 3000
</code></pre>

<p>Now you can login at dashboard.devopstales.intra but you haven&rsquo;t got any privileges so lets create. some.</p>

<pre><code>nano devops-group-rbac.yaml
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: devops-cluster-admin
  namespace: kubernetes-dashboard
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: devopstales
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
subjects:
- kind: User
  name: &quot;devopstales&quot;
  namespace: &quot;kube-system&quot;
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name:  cluster-admin
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Grafana Loki with Helm3]]></title>
            <link href="https://devopstales.github.io/home/helm3-loki/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/helm3-loki/?utm_source=atom_feed" rel="related" type="text/html" title="Install Grafana Loki with Helm3" />
                <link href="https://devopstales.github.io/cloud/openshift-gitlab-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Installing GitLab on OpenShift" />
                <link href="https://devopstales.github.io/home/openshift-gitlab-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Installing GitLab on OpenShift" />
                <link href="https://devopstales.github.io/home/k8s-tillerless-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Tillerless helm2 install" />
                <link href="https://devopstales.github.io/cloud/k8s-tillerless-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Tillerless helm2 install" />
            
                <id>https://devopstales.github.io/home/helm3-loki/</id>
            
            
            <published>2020-01-03T00:00:00+00:00</published>
            <updated>2020-01-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Helm is a template based package management system for kubernetes applications.</p>

<h3 id="wath-is-new-in-helm3">Wath is new in Helm3</h3>

<p>The most important change in Helm3, tiller was removed completely. Tiller was the server component (rinninf in a pod on the Kubernetes cluster) for helm&rsquo;s cli. When Helm 2 was developed, Kubernetes did not yet have role-based access control (RBAC) therefore to achieve mentioned goal, Helm had to take care of that itself. After Kubernetes 1.6 RBAC is enabled by default so you had to create a serviceaccount for tiller. With Tiller gone, Helm permissions are now simply evaluated using kubeconfig file.</p>

<p>Tiller was also used as a central hub for Helm release information and for maintaining the Helm state. In Helm 3 the same information are fetched directly from Kubernetes API Server and Charts are rendered client-side.</p>

<p>Helm 2 stored the informations of the releases in configmaps now in Helm 3 that is stored in secrets for better security.</p>

<h3 id="install-chart-with-helm3">Install Chart with Helm3</h3>

<p>The removal of Tiller means you didn&rsquo;t need a <code>helm init</code> for initializing the tiller.</p>

<pre><code>helm repo add loki https://grafana.github.io/loki/charts
helm repo add stable https://kubernetes-charts.storage.googleapis.com
helm repo update

kubectl create namespace loki-stackhtop

helm upgrade --install loki --namespace=loki-stack loki/loki-stack
elm3 upgrade --install grafana --namespace=loki-stack stable/grafana
</code></pre>

<p>Namespaces are important now. <code>helm ls</code> won’t show anything, we have to specify the namespace with it:</p>

<pre><code>helm -n loki-stack ls
</code></pre>

<h3 id="access-grafana-interface">Access Grafana Interface</h3>

<pre><code>kubectl get secret -n loki-stack grafana -o jsonpath=&quot;{.data.admin-password}&quot; | base64 --decode
kubectl port-forward -n loki-stack service/grafana 3000:80

# go to localhost:3000
# add loki datasource URL http://loki:3100 and press Save &amp; Test
</code></pre>

<p><img src="/img/include/loki_1.png" alt="Example image" /></p>

<p><img src="/img/include/loki_2.png" alt="Example image" /></p>

<p><img src="/img/include/loki_3.png" alt="Example image" /></p>

<p><img src="/img/include/loki_4.png" alt="Example image" /></p>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Backup your Kubernetes Cluster with Velero]]></title>
            <link href="https://devopstales.github.io/home/k8s-velero-backup/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/k8s-velero-backup/?utm_source=atom_feed" rel="related" type="text/html" title="Backup your Kubernetes Cluster with Velero" />
                <link href="https://devopstales.github.io/cloud/k8s-cert-manager/?utm_source=atom_feed" rel="related" type="text/html" title="Install cert-manager for Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-cert-manager/?utm_source=atom_feed" rel="related" type="text/html" title="Install cert-manager for Kubernetes" />
                <link href="https://devopstales.github.io/cloud/kind-install/?utm_source=atom_feed" rel="related" type="text/html" title="Starting local Kubernetes using kind" />
                <link href="https://devopstales.github.io/home/kind-install/?utm_source=atom_feed" rel="related" type="text/html" title="Starting local Kubernetes using kind" />
            
                <id>https://devopstales.github.io/home/k8s-velero-backup/</id>
            
            
            <published>2020-01-02T00:00:00+00:00</published>
            <updated>2020-01-02T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Velero (formerly Heptio Ark) gives you tools to back up and restore your Kubernetes cluster resources and persistent volumes. You can run Velero with a cloud provider or on-premises.</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../cloud/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../cloud/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../cloud/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../cloud/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part3: <a href="../../cloud/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-traefik-ingress --></li>
<li>Part4: <a href="../../cloud/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../cloud/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../cloud/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../cloud/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../cloud/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a>
<!-- CEPH in Kubernetes with rook --></li>
<li>Part5d: <a href="../../cloud/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes vMware --></li>
<li>Part6: <a href="../../cloud/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part7: <a href="../../cloud/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part8: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part9: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part10: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part11: <a href="../../cloud/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- prometheus operator + grafana operator -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<h3 id="how-it-s-work">How it&rsquo;s work</h3>

<p>Each Velero operation (on-demand backup, scheduled backup, restore) is a custom resource, stored in etcd. A backup opertaion is uploads a tarball of copied Kubernetes objects into cloud object storage. After that calls the cloud provider API to make disk snapshots of persistent volumes, if specified. Optionally you can specify hooks to be executed during the backup. When you create a backup, you can specify a TTL by adding the flag <code>--ttl &lt;DURATION&gt;</code>.</p>

<h3 id="velero-supported-providers">Velero supported providers</h3>

<table>
<thead>
<tr>
<th>Object Store</th>
<th>Volume Snapshotter</th>
</tr>
</thead>

<tbody>
<tr>
<td>AWS S3</td>
<td>AWS EBS</td>
</tr>

<tr>
<td>Google Cloud Storage</td>
<td>Google Compute Engine Disks</td>
</tr>

<tr>
<td>Azure Blob Storage</td>
<td>Azure Managed Disks</td>
</tr>

<tr>
<td>-</td>
<td>Portworx Volume</td>
</tr>

<tr>
<td>-</td>
<td>OpenEBS CStor Volume</td>
</tr>
</tbody>
</table>

<h3 id="install-cli">Install cli</h3>

<pre><code>wget https://github.com/vmware-tanzu/velero/releases/download/v1.2.0/velero-v1.2.0-linux-amd64.tar.gz
tar -xzf velero-v1.2.0-linux-amd64.tar.gz
sudo cp velero-v1.2.0-linux-amd64/velero /usr/local/sbin
</code></pre>

<h2 id="deploy-minio-and-deno-app">Deploy minio and deno app</h2>

<pre><code>kubctl apply -f velero-v1.2.0-linux-amd64/examples/minio/00-minio-deployment.yaml
kubctl apply -f velero-v1.2.0-linux-amd64/examples/nginx-app/base.yaml
</code></pre>

<h3 id="deploy-server-component">Deploy server component</h3>

<pre><code>nano velero.yaml
image:
  repository: velero/velero
  tag: v1.2.0
  pullPolicy: IfNotPresent

initContainers:
  - name: aws
    image: velero/velero-plugin-for-aws:v1.0.0
    imagePullPolicy: IfNotPresent
    volumeMounts:
      - mountPath: /target
        name: plugins

metrics:
  enabled: true
  scrapeInterval: 30s

  # Pod annotations for Prometheus
  podAnnotations:
    prometheus.io/scrape: &quot;true&quot;
    prometheus.io/port: &quot;8085&quot;
    prometheus.io/path: &quot;/metrics&quot;

  serviceMonitor:
    enabled: false
    additionalLabels: {}



configuration:
  provider: aws
  backupStorageLocation:
    name: aws
    bucket: velero
    config:
      region: minio
      s3ForcePathStyle: true
      publicUrl: https://minio.devopstales.intra
      s3Url: http://minio:9000
  volumeSnapshotLocation:
    name: aws
    bucket: kubernetes-pv
    config:
      region: minio
      s3ForcePathStyle: true
      publicUrl: https://minio.devopstales.intra
      s3Url: http://minio:9000

credentials:
  useSecret: true
  secretContents:
    cloud: |
      [default]
      aws_access_key_id = minio
      aws_secret_access_key = minio123

snapshotsEnabled: true
deployRestic: true
</code></pre>

<pre><code>helm repo add vmware-tanzu https://vmware-tanzu.github.io/helm-charts
helm repo update

helm install velero vmware-tanzu/velero --namespace velero -f velero.yaml
</code></pre>

<h3 id="create-backup">Create Backup</h3>

<pre><code>velero backup create nginx-backup --selector app=nginx
velero backup describe nginx-backup
velero backup logs nginx-backup
velero backup get

velero schedule create nginx-daily --schedule=&quot;0 1 * * *&quot; --selector app=nginx
velero schedule get
velero backup get
</code></pre>

<h3 id="restore-test">Restore test</h3>

<pre><code>kubectl delete ns nginx-example

velero restore create --from-backup nginx-backup
velero restore get

kubectl get po -n nginx-example
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install cert-manager for Kubernetes]]></title>
            <link href="https://devopstales.github.io/home/k8s-cert-manager/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/k8s-cert-manager/?utm_source=atom_feed" rel="related" type="text/html" title="Install cert-manager for Kubernetes" />
                <link href="https://devopstales.github.io/cloud/kind-install/?utm_source=atom_feed" rel="related" type="text/html" title="Starting local Kubernetes using kind" />
                <link href="https://devopstales.github.io/home/kind-install/?utm_source=atom_feed" rel="related" type="text/html" title="Starting local Kubernetes using kind" />
                <link href="https://devopstales.github.io/home/k8s-ceph-storage-with-csi-driver/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD volume with CSI driver" />
                <link href="https://devopstales.github.io/cloud/k8s-ceph-storage-with-csi-driver/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD volume with CSI driver" />
            
                <id>https://devopstales.github.io/home/k8s-cert-manager/</id>
            
            
            <published>2019-12-29T00:00:00+00:00</published>
            <updated>2019-12-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this tutorial I will show you how to install  cert-manager running on Kubernetes (k8s).</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../cloud/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../cloud/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../cloud/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../cloud/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part3: <a href="../../cloud/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-traefik-ingress --></li>
<li>Part4: <a href="../../cloud/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../cloud/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../cloud/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../cloud/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../cloud/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a>
<!-- CEPH in Kubernetes with rook --></li>
<li>Part5d: <a href="../../cloud/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes vMware --></li>
<li>Part6: <a href="../../cloud/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part7: <a href="../../cloud/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part8: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part9: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part10: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part11: <a href="../../cloud/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- prometheus operator + grafana operator -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<p>cert-manager is a native Kubernetes certificate management controller. It can help with issuing certificates from a variety of sources, such as Let’s Encrypt, HashiCorp Vault, Venafi, a simple signing key pair, or self signed.</p>

<h3 id="install-cert-managger">Install cert-managger</h3>

<p>In order to install cert-manager, we must first create a namespace to run it in.</p>

<pre><code>kubectl create namespace cert-manager
</code></pre>

<p>Install the <code>CustomResourceDefinitions</code> and cert-manager itself</p>

<pre><code>kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.12.0/cert-manager.yaml
</code></pre>

<p>Verifying the installation</p>

<pre><code>kubectl get pods --namespace cert-manager

NAME                                       READY   STATUS    RESTARTS   AGE
cert-manager-5c6866597-zw7kh               1/1     Running   0          2m
cert-manager-cainjector-577f6d9fd7-tr77l   1/1     Running   0          2m
cert-manager-webhook-787858fcdb-nlzsq      1/1     Running   0          2m
</code></pre>

<h3 id="create-a-clusterissuer">Create a ClusterIssuer</h3>

<p>Before you can begin issuing certificates, you must configure at least one <code>Issuer</code> or <code>ClusterIssuer</code> resource in your cluster. These resources represent a particular signing authority and detail how the certificate requests are going to be honored. For this Demo I will use my own CA as an Issuer.</p>

<pre><code>cat issuer.yaml
apiVersion: cert-manager.io/v1alpha2
kind: ClusterIssuer
metadata:
  name: ca-issuer
  namespace: cert-manager
spec:
  ca:
    secretName: ca-key-pair

kubectl apply -f issuer.yaml
</code></pre>

<p>In order to create my certs, I must submit my CA certificate and singing private key to the Kubernetes Cluster so that cert-manager is able to use them and sign certificates.</p>

<pre><code>cat  rootCA.key | base64
LS0tLS1CRUdJTiB...
cat rootCA.crt | base64
LS0tLSD5DUdJTiB...

cat ca-key-pair.yaml
apiVersion: v1
kind: Secret
metadata:
  name: ca-key-pair
  namespace: cert-manager
data:
  tls.key: LS0tLS1CRUdJTiB...
  tls.crt: LS0tLSD5DUdJTiB...

kubectl apply -f ca-key-pair.yaml
</code></pre>

<h3 id="demo">demo</h3>

<p>Create cert for test</p>

<pre><code>cat test-resources.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: cert-manager-test
---
apiVersion: cert-manager.io/v1alpha2
kind: Issuer
metadata:
  name: test-selfsigned
  namespace: cert-manager-test
spec:
  selfSigned: {}
---
apiVersion: cert-manager.io/v1alpha2
kind: Certificate
metadata:
  name: selfsigned-cert
  namespace: cert-manager-test
spec:
  commonName: example.com
  secretName: selfsigned-cert-tls
  issuerRef:
    name: test-selfsigned
---
apiVersion: cert-manager.io/v1alpha2
kind: Certificate
metadata:
  name: ca-cert
  namespace: cert-manager-test
spec:
  commonName: example.com
  secretName: ca-cert-tls
  issuerRef:
    name: ca-issuer
    kind: ClusterIssuer

kubectl apply -f test-resources.yaml
</code></pre>

<pre><code>kubectl describe certificate -n cert-manager-test

...
Spec:
  Common Name:  example.com
  Issuer Ref:
    Name:       test-selfsigned
  Secret Name:  selfsigned-cert-tls
Status:
  Conditions:
    Last Transition Time:  2019-12-29T17:34:30Z
    Message:               Certificate is up to date and has not expired
    Reason:                Ready
    Status:                True
    Type:                  Ready
  Not After:               2019-12-29T17:34:29Z
Events:
  Type    Reason      Age   From          Message
  ----    ------      ----  ----          -------
  Normal  CertIssued  4s    cert-manager  Certificate issued successfully
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift secondary route]]></title>
            <link href="https://devopstales.github.io/home/openshift-secondary-router/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/openshift-secondary-router/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift secondary route" />
                <link href="https://devopstales.github.io/home/openshift-gitlab-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Installing GitLab on OpenShift" />
                <link href="https://devopstales.github.io/cloud/openshift-gitlab-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Installing GitLab on OpenShift" />
                <link href="https://devopstales.github.io/home/openshift-kompose/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Run docker-compoe in Openshift" />
                <link href="https://devopstales.github.io/cloud/openshift-kompose/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Run docker-compoe in Openshift" />
            
                <id>https://devopstales.github.io/home/openshift-secondary-router/</id>
            
            
            <published>2019-12-20T00:00:00+00:00</published>
            <updated>2019-12-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I this tutorial I will show you how to create a secondari router for Openshift.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../cloud/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../cloud/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../cloud/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../cloud/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../cloud/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../cloud/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../cloud/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../cloud/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../cloud/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../cloud/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../cloud/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../cloud/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../cloud/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../cloud/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../cloud/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<h3 id="environment">Environment</h3>

<pre><code>192.168.1.40    deployer
192.168.1.41    openshift01 # master node
192.168.1.42    openshift02 # infra node
192.168.1.43    openshift03 # worker node with second router
192.168.1.44    openshift04 # worker node
</code></pre>

<h3 id="deploy-route">Deploy route</h3>

<pre><code>oc adm router router-public --replicas=2 --ports=&quot;8080:8080,8443:8443&quot; \
--stats-port=1937 --selector=&quot;router=public&quot; --labels=&quot;router=public&quot;

oc set env dc/router-public \
DEFAULT_CERTIFICATE_PATH=/etc/pki/tls/private/tls.crt \
NAMESPACE_LABELS=&quot;router=public&quot; \
ROUTER_ALLOW_WILDCARD_ROUTES=true \
ROUTER_ENABLE_HTTP2=true \
ROUTER_HAPROXY_CONFIG_MANAGER=true \
ROUTER_SERVICE_HTTP_PORT=8080 \
ROUTER_SERVICE_HTTPS_PORT=8443 \
ROUTER_TCP_BALANCE_SCHEME=roundrobin

oc label node openshift03 &quot;router=public&quot;
</code></pre>

<p>Configurate your firewall to create a NAT rule from publicIP:80 to openshift03:8080 and publicIP:443 to openshift03:8443</p>

<h3 id="demo">Demo</h3>

<pre><code>oc new-project test
oc label namespace test router=public
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Starting local Kubernetes using kind]]></title>
            <link href="https://devopstales.github.io/home/kind-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/kind-install/?utm_source=atom_feed" rel="related" type="text/html" title="Starting local Kubernetes using kind" />
                <link href="https://devopstales.github.io/cloud/k8s-ceph-storage-with-csi-driver/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD volume with CSI driver" />
                <link href="https://devopstales.github.io/home/k8s-ceph-storage-with-csi-driver/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD volume with CSI driver" />
                <link href="https://devopstales.github.io/home/k8s-metallb/?utm_source=atom_feed" rel="related" type="text/html" title="Install MetalLB load balancer for Kubernetes" />
                <link href="https://devopstales.github.io/cloud/k8s-metallb/?utm_source=atom_feed" rel="related" type="text/html" title="Install MetalLB load balancer for Kubernetes" />
            
                <id>https://devopstales.github.io/home/kind-install/</id>
            
            
            <published>2019-12-20T00:00:00+00:00</published>
            <updated>2019-12-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this article, I will show you how to run a cluster in single Docker container using kind.</p>

<h3 id="what-is-kind">What is kind?</h3>

<p>Kind (Kubernetes IN Docker) is a tool to start kubernetes nodes as a docker container. It is a cross-platform tool you can run with Docker for Windows too.</p>

<h3 id="install-kind-binary">Install kind binary</h3>

<pre><code>wget https://github.com/kubernetes-sigs/kind/releases/latest/download/kind-linux-amd64
chmod +x kind-linux-amd64
sudo mv kind-linux-amd64 /usr/local/sbin/kind
</code></pre>

<h3 id="start-a-cluster-for-ingress">Start a cluster for Ingress</h3>

<pre><code>cat &lt;&lt;EOF | kind create cluster --config=-
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  kubeadmConfigPatches:
  - |
    apiVersion: kubeadm.k8s.io/v1beta2
    kind: InitConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        node-labels: &quot;ingress-ready=true&quot;
        authorization-mode: &quot;AlwaysAllow&quot;
  extraPortMappings:
  - containerPort: 80
    hostPort: 80
  - containerPort: 443
    hostPort: 443
EOF
</code></pre>

<h3 id="install-ingress">Install ingress</h3>

<pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml

# patch ingress for kind
kubectl patch deployments -n ingress-nginx nginx-ingress-controller -p '{&quot;spec&quot;:{&quot;template&quot;:{&quot;spec&quot;:{&quot;containers&quot;:[{&quot;name&quot;:&quot;nginx-ingress-controller&quot;,&quot;ports&quot;:[{&quot;containerPort&quot;:80,&quot;hostPort&quot;:80},{&quot;containerPort&quot;:443,&quot;hostPort&quot;:443}]}],&quot;nodeSelector&quot;:{&quot;ingress-ready&quot;:&quot;true&quot;}}}}}'
</code></pre>

<h3 id="demo">Demo</h3>

<pre><code>kubectl apply -f https://kind.sigs.k8s.io/examples/ingress/usage.yaml

curl localhost/foo
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install icinga director modules to Icingaweb2]]></title>
            <link href="https://devopstales.github.io/home/icinga2_director/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/icinga2_director/</id>
            
            
            <published>2019-12-13T00:00:00+00:00</published>
            <updated>2019-12-13T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this tutorial I will show you how to install Icingaweb2 module director.</p>

<p>Icinga irector is designed for those who want to automate their configuration deployment and those who want to grant easy access for there users to the Icinga2 configuration.</p>

<h3 id="install-dependency">Install dependency</h3>

<pre><code>yum install git -y
yum install rh-php71-php-curl rh-php71-php-pcntl rh-php71-php-posix rh-php71-php-sockets rh-php71-php-xml rh-php71-php-zip -y
</code></pre>

<pre><code>nano
local   director      director                        md5
host    director      director      127.0.0.1/32      md5
host    director      director      ::1/128           md5

systemctl restart postgresql-10
</code></pre>

<h3 id="install-icingaweb2-modules">Install Icingaweb2 modules</h3>

<pre><code>MODULE_NAME=ipl
MODULE_VERSION=v0.4.0
REPO=&quot;https://github.com/Icinga/icingaweb2-module-${MODULE_NAME}&quot;
MODULES_PATH=&quot;/usr/share/icingaweb2/modules&quot;
git clone ${REPO} &quot;${MODULES_PATH}/${MODULE_NAME}&quot; --branch &quot;${MODULE_VERSION}&quot;
icingacli module enable &quot;${MODULE_NAME}&quot;

MODULE_NAME=incubator
MODULE_VERSION=v0.5.0
REPO=&quot;https://github.com/Icinga/icingaweb2-module-${MODULE_NAME}&quot;
MODULES_PATH=&quot;/usr/share/icingaweb2/modules&quot;
git clone ${REPO} &quot;${MODULES_PATH}/${MODULE_NAME}&quot; --branch &quot;${MODULE_VERSION}&quot;
icingacli module enable &quot;${MODULE_NAME}&quot;

MODULE_NAME=reactbundle
MODULE_VERSION=v0.7.0
REPO=&quot;https://github.com/Icinga/icingaweb2-module-${MODULE_NAME}&quot;
MODULES_PATH=&quot;/usr/share/icingaweb2/modules&quot;
git clone ${REPO} &quot;${MODULES_PATH}/${MODULE_NAME}&quot; --branch &quot;${MODULE_VERSION}&quot;
icingacli module enable &quot;${MODULE_NAME}&quot;

ICINGAWEB_MODULEPATH=&quot;/usr/share/icingaweb2/modules&quot;
REPO_URL=&quot;https://github.com/icinga/icingaweb2-module-director&quot;
TARGET_DIR=&quot;${ICINGAWEB_MODULEPATH}/director&quot;
MODULE_VERSION=&quot;1.7.2&quot;
git clone &quot;${REPO_URL}&quot; &quot;${TARGET_DIR}&quot;
cd &quot;${TARGET_DIR}&quot;
git fetch &amp;&amp; git fetch --tags
git checkout &quot;{MODULE_VERSION}&quot;
restorecon -R &quot;${TARGET_DIR}&quot;

MODULE_NAME=fileshipper
MODULE_VERSION=v1.1.0
REPO=&quot;https://github.com/Icinga/icingaweb2-module-${MODULE_NAME}&quot;
MODULES_PATH=&quot;/usr/share/icingaweb2/modules&quot;
git clone ${REPO} &quot;${MODULES_PATH}/${MODULE_NAME}&quot; --branch &quot;${MODULE_VERSION}&quot;
icingacli module enable &quot;${MODULE_NAME}&quot;
</code></pre>

<h3 id="create-db-for-director">Create db for director</h3>

<pre><code>sudo -u postgres psql -c &quot;CREATE DATABASE director WITH ENCODING 'utf8';&quot;
sudo -u postgres psql director -q -c &quot;CREATE USER director WITH PASSWORD 'director';
GRANT ALL PRIVILEGES ON DATABASE director TO director;
CREATE EXTENSION pgcrypto;&quot;
sudo -u postgres psql director &lt; /usr/share/icingaweb2/modules/director/schema/pgsql.sql
</code></pre>

<h3 id="edit-director-configuration">Edit director configuration</h3>

<pre><code>cat &lt;&lt;EOF &gt;&gt; /etc/icinga2/zones.conf

object Zone &quot;director-global&quot; {
  global = true
}
EOF

cat &lt;&lt;EOF &gt;&gt; /etc/icinga2/conf.d/api-users.conf

object ApiUser &quot;director&quot; {
        password = &quot;director&quot;
        permissions = [ &quot;*&quot; ]
}
EOF

cat &lt;&lt;EOF &gt;&gt; /etc/icingaweb2/resources.ini

[icinga_director]
type = &quot;db&quot;
db = &quot;pgsql&quot;
host = &quot;localhost&quot;
port = &quot;5432&quot;
dbname = &quot;director&quot;
username = &quot;director&quot;
password = &quot;director&quot;
charset = &quot;utf8&quot;
use_ssl = &quot;0&quot;
EOF

mkdir /etc/icingaweb2/modules/director/
cat &lt;&lt;EOF &gt;&gt; /etc/icingaweb2/modules/director/config.ini
[db]
resource = &quot;icinga_director&quot;
EOF

cat &lt;&lt;EOF &gt;&gt; /etc/icingaweb2/modules/director/kickstart.ini
[config]
endpoint = icinga.devopstales.intra
host = 127.0.0.1
port = 5665
username = director
password = director
EOF
</code></pre>

<pre><code>icingacli module enable director
icingacli director kickstart run
icingacli director migration run --verbose
icingacli director migration pending --verbose
</code></pre>

<pre><code>mkdir /etc/icingaweb2/modules/fileshipper
cat &lt;&lt;EOF &gt;&gt; /etc/icingaweb2/modules/fileshipper/imports.ini
[icinga2 - groups]
basedir = &quot;/etc/icinga2/conf.d/groups/

[icinga2 - hosts]
basedir = &quot;/etc/icinga2/conf.d/hosts/
EOF
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install nrpe tp Icinga2]]></title>
            <link href="https://devopstales.github.io/home/icinga2_nrpe/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/icinga2_nrpe/</id>
            
            
            <published>2019-12-12T00:00:00+00:00</published>
            <updated>2019-12-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this tutorial I will show you how to nrpe check in Icinga2.</p>

<h3 id="install-nrpe-on-the-clients">install nrpe on the clients</h3>

<pre><code>yum install nrpe nagios-plugins-all -y

firewall-cmd --permanent --add-port=5665/tcp
firewall-cmd --reload
</code></pre>

<h3 id="install-nrpe-on-the-server">install nrpe on the server</h3>

<pre><code>yum install nrpe nagios-plugins-all -y

firewall-cmd --permanent --add-port=5665/tcp
firewall-cmd --reload
</code></pre>

<h3 id="configurate-icinga-to-use-nrpe">configurate icinga to use nrpe</h3>

<pre><code>vi /etc/icinga2/conf.d/linux_services.conf
apply Service &quot;nrpe-disk-root&quot; {
  import &quot;generic-service&quot;
  check_command = &quot;nrpe&quot;
  vars.nrpe_command = &quot;check_disk&quot;
  vars.nrpe_arguments = [ &quot;20%&quot;, &quot;10%&quot;, &quot;/&quot; ]
  assign where &quot;linux-servers&quot; in host.groups
  ignore where match(&quot;*icinga*&quot;, host.name)
}

vi /etc/icinga2/conf.d/CLIENTS/server1.conf
object Host &quot;server1&quot; {
  address = &quot;192.168.10.60&quot;
  check_command = &quot;ping&quot;
  vars.os = &quot;Linux&quot;
}
</code></pre>

<p>Test the config and restart:</p>

<pre><code>icinga2 daemon -C
systemctl restart icinga2
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Add host to Icinga2]]></title>
            <link href="https://devopstales.github.io/home/icinga2_add_host/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/icinga2_add_host/</id>
            
            
            <published>2019-12-11T00:00:00+00:00</published>
            <updated>2019-12-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this tutorial I will show you how to add hosts to Icinga2.</p>

<h3 id="create-new-host">Create new host</h3>

<pre><code>nano /etc/icinga2/conf.d/my_router.conf
object Host &quot;Router&quot; {
  address = &quot;192.168.1.1&quot;
  check_command = &quot;hostalive&quot;
}

</code></pre>

<p>If we want to check the web admin interface of the router we can add a http check to see if the HTTP server is alive and responds with the proper HTTP codes</p>

<pre><code>nano /etc/icinga2/conf.d/my_router.conf
...
object Service &quot;http&quot; {
  host_name = &quot;Router&quot;
  check_command = &quot;http&quot;
}
</code></pre>

<h3 id="ad-custom-check">Ad custom check</h3>

<p>We will us the check_udpport script. This is the syntax.</p>

<pre><code>/usr/lib64/nagios/plugins/check_udpport –H 127.0.0.1 –p 69
</code></pre>

<p>Now we need to create a nwe command in icinga2:</p>

<pre><code>nano /etc/icinga2/conf.d/commands.conf
object CheckCommand &quot;myudp&quot; {
  command = [ PluginDir + &quot;/check_udpport&quot; ]

    arguments = {
    &quot;-H&quot; = &quot;$addr$&quot;
    &quot;-p&quot; = &quot;$port$&quot;
  }
  vars.addr = &quot;$address$&quot;
}
</code></pre>

<p>We have a nwe command so we can create a service to use rhis command:</p>

<pre><code>nano /etc/icinga2/conf.d/my_router.conf
...
object Service &quot;dhcp&quot; {
  host_name = Router&quot;
  check_command = &quot;myudp&quot;
vars.port = &quot;67&quot;
}
</code></pre>

<p>Test the config and restart:</p>

<pre><code>icinga2 daemon -C
systemctl restart icinga2
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configure Rundeck ACL]]></title>
            <link href="https://devopstales.github.io/home/rundeck-acl/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/rundeck-acl/</id>
            
            
            <published>2019-12-10T00:00:00+00:00</published>
            <updated>2019-12-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will configure access control in Rundeck.</p>

<h3 id="configurate-ad-groups-in-rundeck">Configurate AD groups in rundeck</h3>

<p>The modification of the <code>web.xml</code> no longer needed after 3.0.x.</p>

<pre><code>nano /var/lib/rundeck/exp/webapp/WEB-INF/web.xml
        &lt;security-role&gt;
               &lt;role-name&gt;rundeck-administrators&lt;/role-name&gt;
               &lt;role-name&gt;rundeck-project&lt;/role-name&gt;
        &lt;/security-role&gt;
</code></pre>

<h3 id="configure-the-privilege-for-ad-group">Configure the privilege for AD group</h3>

<pre><code>nano /etc/rundec/admin.aclpolicy

description: Admin, all access.
context:
  project: '.*' # all projects
for:
  resource:
    - allow: '*' # allow read/create all kinds
  adhoc:
    - allow: '*' # allow read/running/killing adhoc jobs
  job:
    - allow: '*' # allow read/write/delete/run/kill of all jobs
  node:
    - allow: '*' # allow read/run for all nodes
by:
  group: rundeck-administrators

---

description: Admin, all access.
context:
  application: 'rundeck'
for:
  resource:
    - allow: '*' # allow create of projects
  project:
    - allow: '*' # allow view/admin of all projects
  project_acl:
    - allow: '*' # allow admin of all project-level ACL policies
  storage:
    - allow: '*' # allow read/create/update/delete for all /keys/* storage content
by:
  group: rundeck-administrators
---

description: rundeck-project  PROJECT all access.
context:
  project: 'PROJECT'
for:
  resource:
    - allow: '*' # allow read/create all kinds
  adhoc:
    - allow: '*' # allow read/running/killing adhoc jobs
  job:
    - allow: '*' # allow read/write/delete/run/kill of all jobs
  node:
    - allow: '*' # allow read/run for all nodes
by:
  group: rundeck-project

---

description: rundeck-project, all access.
context:
  application: 'rundeck'
for:
  project:
    - match:
        name: 'PROJECT'
      allow: [read]
  system:
    - match:
        name: '.*'
      allow: [read]
  storage:
    - equals:
        path: 'keys'
      allow: [read]
    - match:
        path: 'keys/id_rsa*'
      allow: [read]
by:
  group: rundeck-project
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Icinga2]]></title>
            <link href="https://devopstales.github.io/home/icinga2_install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/icinga2_install/</id>
            
            
            <published>2019-12-10T00:00:00+00:00</published>
            <updated>2019-12-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this tutorial I will show you how to install Icinga2 and Icingaweb2 webinterface.</p>

<p>Icinga 2 is an open source, scalable and extensible monitoring tool which checks the availability of your network resources, notifies users of outages, and generates performance data for reporting.</p>

<h3 id="configure-syslinux-and-firewall-for-the-install">Configure syslinux and Firewall for the install</h3>

<pre><code>setsebool -P httpd_can_network_connect_db 1
setsebool -P httpd_can_network_connect 1

firewall-cmd --permanent --add-service=http
firewall-cmd --permanent --add-service=https
firewall-cmd --permanent --add-service=postgresql
firewall-cmd --permanent --add-port=5665/tcp
firewall-cmd --reload
</code></pre>

<h3 id="install-postgresql">Install postgresql</h3>

<pre><code>yum install postgresql-server postgresql
postgresql-setup initdb

nano /var/lib/pgsql/data/pg_hba.conf
# icinga
local   icinga      icinga                            md5
host    icinga      icinga      127.0.0.1/32          md5
host    icinga      icinga      ::1/128               md5
local   icinga_web  icinga_web                        md5
host    icinga_web  icinga_web  127.0.0.1/32          md5
host    icinga_web  icinga_web  ::1/128               md5

# &quot;local&quot; is for Unix domain socket connections only
local   all         all                               ident
# IPv4 local connections:
host    all         all         127.0.0.1/32          ident
# IPv6 local connections:
host    all         all         ::1/128               ident

systemctl enable  --now postgresql-10
</code></pre>

<h3 id="install-icinga2">Install Icinga2</h3>

<pre><code>yum install https://packages.icinga.com/epel/icinga-rpm-release-7-latest.noarch.rpm
yum install epel-release

yum install icinga2 icinga2-selinux nagios-plugins-all nano-icinga2

systemctl enable --now httpd

echo 'include &quot;/usr/share/nano/icinga2.nanorc&quot;' &gt;&gt; /etc/nanorc
cp /etc/nanorc ~/.nanorc

yum install icinga2-ido-pgsql

cd /tmp
sudo -u postgres psql -c &quot;CREATE ROLE icinga WITH LOGIN PASSWORD 'icinga'&quot;
sudo -u postgres createdb -O icinga -E UTF8 icinga

export PGPASSWORD=icinga
psql -U icinga -d icinga &lt; /usr/share/icinga2-ido-pgsql/schema/pgsql.sql

icinga2 feature enable ido-pgsql

cat &lt;&lt; EOF | sudo tee /etc/icinga2/features-enabled/ido-pgsql.conf
/**
 * The db_ido_pgsql library implements IDO functionality
 * for PostgreSQL.
 */

library &quot;db_ido_pgsql&quot;

object IdoPgsqlConnection &quot;ido-pgsql&quot; {
  user = &quot;icinga&quot;,
  password = &quot;icinga&quot;,
  host = &quot;localhost&quot;,
  database = &quot;icinga&quot;
}
EOF

icinga2 api setup


ecjo '
object ApiUser &quot;icingaweb2&quot; {
  password = &quot;Wijsn8Z9eRs5E25d&quot;
  permissions = [ &quot;status/query&quot;, &quot;actions/*&quot;, &quot;objects/modify/*&quot;, &quot;objects/query/*&quot; ]
}' &gt;&gt; /etc/icinga2/conf.d/api-users.conf

icinga2 feature enable command
</code></pre>

<pre><code>icinga2 node wizard
Welcome to the Icinga 2 Setup Wizard!

We'll guide you through all required configuration details.

Please specify if this is a satellite setup ('n' installs a master setup) [Y/n]: n
Starting the Master setup routine...
Please specifiy the common name (CN) [icinga.devopstales.intra]:
Checking for existing certificates for common name 'icinga.devopstales.intra'...
Certificates not yet generated. Running 'api setup' now.
information/cli: Generating new CA.
information/base: Writing private key to '/var/lib/icinga2/ca/ca.key'.
information/base: Writing X509 certificate to '/var/lib/icinga2/ca/ca.crt'.
information/cli: Generating new CSR in '/etc/icinga2/pki/icinga.devopstales.intra.csr'.
information/base: Writing private key to '/etc/icinga2/pki/icinga.devopstales.intra.key'.
information/base: Writing certificate signing request to '/etc/icinga2/pki/icinga.devopstales.intra.csr'.
information/cli: Signing CSR with CA and writing certificate to '/etc/icinga2/pki/icinga.devopstales.intra.crt'.
information/cli: Copying CA certificate to '/etc/icinga2/pki/ca.crt'.
Generating master configuration for Icinga 2.
information/cli: Adding new ApiUser 'root' in '/etc/icinga2/conf.d/api-users.conf'.
information/cli: Enabling the 'api' feature.
Enabling feature api. Make sure to restart Icinga 2 for these changes to take effect.
information/cli: Dumping config items to file '/etc/icinga2/zones.conf'.
information/cli: Created backup file '/etc/icinga2/zones.conf.orig'.
Please specify the API bind host/port (optional):
Bind Host []: Hit Enter
Bind Port []: Hit Enter
information/cli: Created backup file '/etc/icinga2/features-available/api.conf.orig'.
information/cli: Updating constants.conf.
information/cli: Created backup file '/etc/icinga2/constants.conf.orig'.
information/cli: Updating constants file '/etc/icinga2/constants.conf'.
information/cli: Updating constants file '/etc/icinga2/constants.conf'.
information/cli: Updating constants file '/etc/icinga2/constants.conf'.
Done.

Now restart your Icinga 2 daemon to finish the installation!

systemctl restart icinga2
</code></pre>

<h3 id="install-icingaweb2">Install IcingaWeb2</h3>

<pre><code>yum install centos-release-scl
yum install icingaweb2 icingacli icingaweb2-selinux

yum install httpd
systemctl start httpd.service
systemctl enable httpd.service

icingacli setup config webserver apache

## OR

yum install nginx rh-php71-php-fpm rh-php71-php-pgsql
systemctl enable --now rh-php71-php-fpm.service

## config
icingacli setup config webserver nginx &gt; /etc/nginx/default.d//icinga.conf
systemctl enable --now nginx

sudo -u postgres psql -c &quot;CREATE ROLE icinga_web WITH LOGIN PASSWORD 'icinga_web'&quot;
sudo -u postgres createdb -O icinga_web -E UTF8 icinga_web

icingacli setup token create

# go to
https://icinga.devopstales.intra/icingaweb2/
</code></pre>

<p><img src="/img/include/icingaweb1.png" alt="Example image" /></p>

<p><img src="/img/include/icingaweb2.png" alt="Example image" /></p>

<p><img src="/img/include/icingaweb3.png" alt="Example image" /></p>

<p><img src="/img/include/icingaweb4.png" alt="Example image" /></p>

<p><img src="/img/include/icingaweb5.png" alt="Example image" /></p>

<p><img src="/img/include/icingaweb6.png" alt="Example image" /></p>

<p><img src="/img/include/icingaweb7.png" alt="Example image" /></p>

<p><img src="/img/include/icingaweb8.png" alt="Example image" /></p>

<p><img src="/img/include/icingaweb9.png" alt="Example image" /></p>

<p><img src="/img/include/icingaweb10.png" alt="Example image" /></p>

<p><img src="/img/include/icingaweb11.png" alt="Example image" /></p>

<p><img src="/img/include/icingaweb12.png" alt="Example image" /></p>

<p><img src="/img/include/icingaweb13.png" alt="Example image" /></p>

<p><img src="/img/include/icingaweb14.png" alt="Example image" /></p>

<p><img src="/img/include/icingaweb15.png" alt="Example image" /></p>

<p><img src="/img/include/icingaweb16.png" alt="Example image" /></p>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configure Rundeck LADAP]]></title>
            <link href="https://devopstales.github.io/home/rundeck-ldap/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/rundeck-ldap/</id>
            
            
            <published>2019-12-09T00:00:00+00:00</published>
            <updated>2019-12-09T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will configure Rundeck to use LDAP as a User backend.</p>

<h3 id="rundeck-ldap-config-file">Rundeck LDAP config file</h3>

<pre><code>nano /etc/rundeck/jaas-ldap.conf

# openldap
ldap {
      com.dtolabs.rundeck.jetty.jaas.JettyCachingLdapLoginModule required
      contextFactory=&quot;com.sun.jndi.ldap.LdapCtxFactory&quot;
      debug=&quot;true&quot;
      providerUrl=&quot;ldap://openldap:389&quot;
      bindDn=&quot;cn=admin,dc=mydomain,dc=intra&quot;
      bindPassword=&quot;Password1&quot;
      authenticationMethod=&quot;simple&quot;
      forceBindingLogin=&quot;true&quot;
      userBaseDn=&quot;dc=mydomain,dc=intra&quot;
      userRdnAttribute=&quot;cn&quot;
      userIdAttribute=&quot;cn&quot;
      userPasswordAttribute=&quot;userPassword&quot;
      userObjectClass=&quot;inetOrgPerson&quot;
      roleBaseDn=&quot;dc=mydomain,dc=intra&quot;
      roleNameAttribute=&quot;cn&quot;
      roleMemberAttribute=&quot;uniqueMember&quot;
      roleObjectClass=&quot;groupOfUniqueNames&quot;
      supplementalRoles=&quot;admin, user&quot;;
      };

# windows AD
ldap {
      com.dtolabs.rundeck.jetty.jaas.JettyCachingLdapLoginModule required
      contextFactory=&quot;com.sun.jndi.ldap.LdapCtxFactory&quot;
      debug=&quot;true&quot;
      providerUrl=&quot;ldap://devopstales.intra:389&quot;
      bindDn=&quot;cn=admin,dc=mydomain,dc=intra&quot;
      bindPassword=&quot;Password1&quot;
      authenticationMethod=&quot;simple&quot;
      forceBindingLogin=&quot;true&quot;
      userBaseDn=&quot;dc=mydomain,dc=intra&quot;
      userRdnAttribute=&quot;sAMAccountName&quot;
      userIdAttribute=&quot;sAMAccountName&quot;
      userPasswordAttribute=&quot;unicodePwd&quot;
      userObjectClass=&quot;user&quot;
      roleBaseDn=&quot;dc=mydomain,dc=intra&quot;
      roleNameAttribute=&quot;cn&quot;
      roleMemberAttribute=&quot;member&quot;
      roleObjectClass=&quot;group&quot;
      supplementalRoles=&quot;admin, user&quot;;
      };
</code></pre>

<h3 id="rundeck-multibackend-config-file">Rundeck multibackend config file</h3>

<pre><code>nano /etc/rundeck/jaas-multiauth.conf

multiauth {
      com.dtolabs.rundeck.jetty.jaas.JettyCombinedLdapLoginModule required
      contextFactory=&quot;com.sun.jndi.ldap.LdapCtxFactory&quot;
      debug=&quot;true&quot;
      providerUrl=&quot;ldap://ad1:389&quot;
      bindDn=&quot;cn=admin,dc=mydomain,dc=intra&quot;
      bindPassword=&quot;Password1&quot;
      authenticationMethod=&quot;simple&quot;
      forceBindingLogin=&quot;true&quot;
      userBaseDn=&quot;ou=Users,dc=mydomain,dc=intra&quot;
      userRdnAttribute=&quot;cn&quot;
      userIdAttribute=&quot;cn&quot;
      userPasswordAttribute=&quot;userPassword&quot;
      userObjectClass=&quot;inetOrgPerson&quot;
      roleBaseDn=&quot;ou=Groups,dc=mydomain,dc=intra&quot;
      roleNameAttribute=&quot;cn&quot;
      roleMemberAttribute=&quot;uniqueMember&quot;
      roleObjectClass=&quot;groupOfUniqueNames&quot;

      ignoreRoles=&quot;true&quot;
      storePass=&quot;true&quot;
      clearPass=&quot;true&quot;
      useFirstPass=&quot;false&quot;
      tryFirstPass=&quot;false&quot;
      supplementalRoles=&quot;admin, user&quot;;

      org.rundeck.jaas.jetty.JettyRolePropertyFileLoginModule required
      debug=&quot;true&quot;
      useFirstPass=&quot;true&quot;
      file=&quot;/etc/rundeck/realm.properties&quot;;
      };
</code></pre>

<h3 id="configure-runceck-to-use-multibackend-config-file">Configure Runceck to use multibackend config file</h3>

<pre><code>nano /etc/rundeck/profile

JAAS_CONF=&quot;${JAAS_CONF:-$RDECK_CONFIG/jaas-ldap.conf}&quot;
LOGIN_MODULE=&quot;ldap&quot;

# OR based on your distro
nano /etc/default/rundeckd

export JAAS_CONF=&quot;/etc/rundeck/jaas-ldap.conf&quot;
export LOGIN_MODULE=&quot;ldap&quot;
</code></pre>

<h3 id="configure-runceck-to-use-ldap-config-file">Configure Runceck to use LDAP config file</h3>

<pre><code>nano /etc/rundeck/profile

JAAS_CONF=&quot;${JAAS_CONF:-$RDECK_CONFIG/jaas-multiauth.conf}&quot;
LOGIN_MODULE=&quot;multiauth&quot;

# OR based on your distro
nano /etc/default/rundeckd

export JAAS_CONF=&quot;/etc/rundeck/jaas-multiauth.conf&quot;
export LOGIN_MODULE=&quot;multiauth&quot;
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install docker on fedora 31]]></title>
            <link href="https://devopstales.github.io/home/docker-on-fedora31/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/docker-on-fedora31/?utm_source=atom_feed" rel="related" type="text/html" title="Install docker on fedora 31" />
            
                <id>https://devopstales.github.io/home/docker-on-fedora31/</id>
            
            
            <published>2019-12-04T00:00:00+00:00</published>
            <updated>2019-12-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>On Fedora 31 after starting docker container some error as follows because of cgroups v2.</p>

<pre><code>docker: Error response from daemon: OCI runtime create failed: container_linux.go:346: starting container process caused &quot;process_linux.go:297: applying cgroup configuration for process caused \&quot;open /sys/fs/cgroup/docker/cpuset.cpus.effective: no such file or directory\&quot;&quot;: unknown.
</code></pre>

<p>This is beacaue Fedora 31 uses cgroups v2 by default and docker doesn’t yet support cgroupsv2. In this tutorial I am going to show you how to change cgroups to v1 as a quick fix to run docker.</p>

<pre><code>sudo nano /etc/default/grub
GRUB_CMDLINE_LINUX=&quot;systemd.unified_cgroup_hierarchy=0&quot;

grub2-mkconfig -o /boot/efi/EFI/fedora/grub.cfg
reboot
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Analyzing PFsense squid logs in Graylog]]></title>
            <link href="https://devopstales.github.io/home/graylog-pfsense-squid/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/graylog-pfsense-squid/</id>
            
            
            <published>2019-11-24T00:00:00+00:00</published>
            <updated>2019-11-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>We will parse the access log records generated by the PfSense&rsquo;s squd plugin. We already have our graylog server running and we will start preparing the terrain to capture those logs records.</p>

<p>Many thanks to opc40772 developed the original contantpack for pfsense squid log agregation what I updated for the new Graylog3 and Elasticsearch 6.</p>

<h3 id="celebro-localinstall">Celebro localinstall</h3>

<pre><code># celebro van to use port 9000 so change graylog3 bindport
nano /etc/graylog/server/server.conf
http_bind_address = 127.0.0.1:9400
nano /etc/nginx/conf.d/graylog.conf

systemctl restart graylog-server.service
systemctl restart nginx

wget https://github.com/lmenezes/cerebro/releases/download/v0.8.3/cerebro-0.8.3-1.noarch.rpm
yum localinstall cerebro-0.8.3-1.noarch.rpm
</code></pre>

<h3 id="create-indices">Create indices</h3>

<p>We now create the Pfsense indice on Graylog at <code>System / Indexes</code> <br>
<img src="/img/include/squid_pfsense1.png" alt="image" /> <br></p>

<h3 id="import-index-template-for-elasticsearch-6-x">Import index template for elasticsearch 6.x</h3>

<pre><code>systemctl stop graylog-server.service
</code></pre>

<p>Go to <code>celebro &gt; more &gt; index templates</code>
Create new with name: <code>pfsense-custom</code> and copy the template from file <code>squid_custom_template_el6.json</code>
Edit other pfsense template to (sorrend 0)</p>

<p>In Cerebro we stand on top of the pfsense index and unfold the options and select delete index.</p>

<h3 id="geoip-database">Geoip database</h3>

<pre><code>wget -t0 -c http://geolite.maxmind.com/download/geoip/database/GeoLite2-City.tar.gz
tar -xvf GeoLite2-City.tar.gz
cp GeoLite2-City_*/GeoLite2-City.mmdb /etc/graylog/server

systemctl start graylog-server.service
</code></pre>

<p>Enable geoip database at <code>System \ Imput &gt; Configurations &gt; Plugins &gt; Geo-Location Processor &gt; update</code>
Chane the order of the <code>Message Processors Configuration</code></p>

<ul>
<li>AWS Instance Name Lookup</li>
<li>Message Filter Chain</li>
<li>Pipeline Processor</li>
<li>GeoIP Resolver</li>
</ul>

<p>Enable geoip database</p>

<h3 id="import-contantpack">Import contantpack</h3>

<pre><code>git clone https://github.com/devopstales/Squid-Graylog.git
</code></pre>

<p>import
chaneg date timezone in Pipeline rule
Go tu Stream menu and edit stream <br>
<img src="/img/include/squid_pfsense7.png" alt="image" /> <br></p>

<p><code>System &gt; Pipelines</code>
Manage rules and then Edit rule (Change the timezone)</p>

<pre><code>rule &quot;timestamp_pfsense_for_grafana&quot;
 when
 has_field(&quot;timestamp&quot;)
then
// the following date format assumes there's no time zone in the string
 let source_timestamp = parse_date(substring(to_string(now(&quot;Europe/Budapest&quot;)),0,23), &quot;yyyy-MM-dd'T'HH:mm:ss.SSS&quot;);
 let dest_timestamp = format_date(source_timestamp,&quot;yyyy-MM-dd HH:mm:ss&quot;);
 set_field(&quot;real_timestamp&quot;, dest_timestamp);
end
</code></pre>

<p>Add the existing pipeline to the squid stream by clicking the Edit connections at Pipeline connections.</p>

<p><img src="/img/include/squid_pfsense8.png" alt="image" /></p>

<h3 id="confifure-pfsense">Confifure pfsense</h3>

<pre><code># http://pkg.freebsd.org/FreeBSD:11:amd64/latest/All/
pkg add http://pkg.freebsd.org/FreeBSD:11:amd64/latest/All/beats-6.7.1.txz

nano /usr/local/etc/filebeat.yml
filebeat.prospectors:
- input_type: log
  document_type: squid3
  paths:
    - /var/squid/logs/access.log

output.logstash:
  # The Logstash hosts
  hosts: [&quot;192.168.0.112:5044&quot;]

  # Optional SSL. By default is off.
  # List of root certificates for HTTPS server verifications
  bulk_max_size: 2048
 #ssl.certificate_authorities: [&quot;/etc/filebeat/logstash.crt&quot;]
  template.name: &quot;filebeat&quot;
  template.path: &quot;filebeat.template.json&quot;
  template.overwrite: false
  # Certificate for SSL client authentication
  #ssl.certificate: &quot;/etc/pki/client/cert.pem&quot;

  # Client Certificate Key
  #ssl.key: &quot;/etc/pki/client/cert.key&quot;

/usr/local/sbin/filebeat -c /usr/local/etc/filebeat.yml test config
cp /usr/local/etc/rc.d/filebeat /usr/local/etc/rc.d/filebeat.sh
echo &quot;filebeat_enable=yes&quot; &gt;&gt; /etc/rc.conf.local
echo &quot;filebeat_conf=/usr/local/etc/filebeat.yml&quot; &gt;&gt; /etc/rc.conf.local

/usr/local/etc/rc.d/filebeat.sh start
ps aux | grep beat
</code></pre>

<h3 id="install-grafana-dashboard">Install grafana Dashboard</h3>

<pre><code># install nececery plugins
grafana-cli plugins install grafana-piechart-panel
grafana-cli plugins install grafana-worldmap-panel
grafana-cli plugins install savantly-heatmap-panel
grafana-cli plugins install briangann-datatable-panel
systemctl restart grafana-server
</code></pre>

<p>Create new datasource: <br>
<img src="/img/include/squid_pfsense9.jpg" alt="image" /> <br></p>

<p>Import dashboadr. <br></p>

<hr />

<h5 id="contantpack">Contantpack:</h5>

<p><a href="https://github.com/devopstales/Squid-Graylog.git">https://github.com/devopstales/Squid-Graylog.git</a></p>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Squid proxy]]></title>
            <link href="https://devopstales.github.io/home/install-squid/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/install-squid/</id>
            
            
            <published>2019-11-18T00:00:00+00:00</published>
            <updated>2019-11-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Squid is the most popular Proxy server for Linux systems. The squid proxy server is also useful for the web packet filtering.</p>

<h3 id="install-squid">Install Squid</h3>

<p>Squid packages are available in default yum repositories.</p>

<pre><code>yum install squid
</code></pre>

<h3 id="configuring-squid">Configuring Squid</h3>

<pre><code>nano /etc/squid/squid.conf
    #
    # Recommended minimum configuration:
    ## Example rule allowing access from your local networks.
    # Adapt to list your (internal) IP networks from where browsing
    # should be allowed
    acl localnet src 10.0.0.0/8     # RFC1918 possible internal network
    acl localnet src 172.16.0.0/12  # RFC1918 possible internal network
    acl localnet src 192.168.0.0/16 # RFC1918 possible internal network
    acl localnet src fc00::/7       # RFC 4193 local private network range
    acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged)

    machinesacl SSL_ports port 443
    acl Safe_ports port 80          # http
    acl Safe_ports port 21          # ftp
    acl Safe_ports port 443         # https
    acl Safe_ports port 70          # gopher
    acl Safe_ports port 210         # wais
    acl Safe_ports port 1025-65535  # unregistered ports
    acl Safe_ports port 280         # http-mgmt
    acl Safe_ports port 488         # gss-http
    acl Safe_ports port 591         # filemaker
    acl Safe_ports port 777         # multiling http
    acl CONNECT method CONNECT#
    # Recommended minimum Access Permission configuration:
    #
    # Deny requests to certain unsafe ports
    http_access deny !Safe_ports# Deny CONNECT to other than secure SSL ports
    http_access deny CONNECT !SSL_ports# Only allow cachemgr access from localhost
    http_access allow localhost manager
    http_access deny manager# We strongly recommend the following be uncommented to protect innocent
    # web applications running on the proxy server who think the only
    # one who can access services on &quot;localhost&quot; is a local user
    #http_access deny to_localhost#
    # INSERT YOUR OWN RULE(S) HERE TO ALLOW ACCESS FROM YOUR CLIENTS
    ## Example rule allowing access from your local networks.
    # Adapt localnet in the ACL section to list your (internal) IP networks
    # from where browsing should be allowed
    http_access allow localnet
    http_access allow localhost# And finally deny all other access to this proxy
    http_access deny all# Squid normally listens to port 3128
    http_port 3128# Uncomment and adjust the following to add a disk cache directory.
    #cache_dir ufs /var/spool/squid 100 16 256# Leave coredumps in the first cache dir
    coredump_dir /var/spool/squid

# # Add any of your own refresh_pattern entries above these.
refresh_pattern ^ftp:           1440    20%     10080
refresh_pattern ^gopher:        1440    0%	1440
refresh_pattern -i (/cgi-bin/|\?) 0     0%	0
refresh_pattern .               0       20%     4320
</code></pre>

<h3 id="allow-ip-address-to-use-the-internet-through-your-proxy-server">Allow IP Address to Use the Internet Through Your Proxy Server</h3>

<p>If your netwok is 110.220.330.0/24:</p>

<pre><code>acl localnet src 110.220.330.0/24
</code></pre>

<p>For changes to take effect you will need to restart your Squid server, use the following command for same.</p>

<pre><code>systemctl restart squid
</code></pre>

<h3 id="allow-a-specific-port-for-http-connections">Allow a Specific Port for HTTP Connections</h3>

<pre><code>acl Safe_ports port 8080
</code></pre>

<h3 id="using-basic-authentication-with-squid">Using Basic Authentication with Squid</h3>

<pre><code>yum -y install httpd-tools
touch /etc/squid/passwd &amp;&amp; chown squid /etc/squid/passwd

htpasswd /etc/squid/passwd proxyuser
    New password:
    Re-type new password:
    Adding password for user pxuser
</code></pre>

<pre><code>nano /etc/squid/squid.conf
...
auth_param basic program /usr/lib64/squid/basic_ncsa_auth /etc/squid/passwd
auth_param basic children 5
auth_param basic realm Squid Basic Authentication
auth_param basic credentialsttl 2 hours
acl auth_users proxy_auth REQUIRED
http_access allow auth_users
</code></pre>

<h3 id="blocking-websites">Blocking Websites</h3>

<pre><code>nano /etc/squid/blocked_sites
facebook.com
youtube.com
</code></pre>

<pre><code>nano /etc/squid/squid.conf
acl blocked_sites dstdomain &quot;/etc/squid/blocked_sites&quot;
http_access deny blocked_sites
</code></pre>

<h3 id="block-specific-keyword-with-squid">Block Specific Keyword with Squid</h3>

<pre><code>nano /etc/squid/blockkeywords.lst
yahoo
gmail
facebook
</code></pre>

<pre><code>acl blockkeywordlist url_regex &quot;/etc/squid/blockkeywords.lst&quot;
http_access deny blockkeywordlist
</code></pre>

<h3 id="disable-caching">Disable caching</h3>

<pre><code># Leave coredumps in the first cache dir
#coredump_dir /var/spool/squid
</code></pre>

<h3 id="changing-squid-port">Changing Squid Port</h3>

<pre><code>nano /etc/squid/squid.conf
http_port 3128
</code></pre>

<h3 id="export-proxy-server-settings">Export Proxy Server Settings</h3>

<pre><code>$ export http_proxy=&quot;http://PROXY_SERVER:PORT&quot;
$ export https_proxy=&quot;http://PROXY_SERVER:PORT&quot;
$ export ftp_proxy=&quot;http://PROXY_SERVER:PORT&quot;

$ export http_proxy=&quot;http://USER:PASSWORD@PROXY_SERVER:PORT&quot;
$ export https_proxy=&quot;http://USER:PASSWORD@PROXY_SERVER:PORT&quot;
$ export ftp_proxy=&quot;http://USER:PASSWORD@PROXY_SERVER:PORT&quot;
</code></pre>

<h3 id="test-caching">Test caching</h3>

<pre><code>env | grep proxy

tailf /var/log/squid/access.log
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Installing GitLab on OpenShift]]></title>
            <link href="https://devopstales.github.io/home/openshift-gitlab-helm/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/openshift-gitlab-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Installing GitLab on OpenShift" />
                <link href="https://devopstales.github.io/cloud/openshift-gitlabrunner/?utm_source=atom_feed" rel="related" type="text/html" title="Install Gitlab runner on Openshift" />
                <link href="https://devopstales.github.io/home/openshift-gitlabrunner/?utm_source=atom_feed" rel="related" type="text/html" title="Install Gitlab runner on Openshift" />
                <link href="https://devopstales.github.io/cloud/openshift-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Helm" />
                <link href="https://devopstales.github.io/home/openshift-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Helm" />
            
                <id>https://devopstales.github.io/home/openshift-gitlab-helm/</id>
            
            
            <published>2019-11-18T00:00:00+00:00</published>
            <updated>2019-11-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I  had to install Gitlab to Openshift recently. Turned out getting GitLab up and running on OpenShift is not so easy.</p>

<h3 id="create-new-project">Create new project</h3>

<pre><code>oc new-project gitlab-devopstales.intra
</code></pre>

<h3 id="deploy-helm">Deploy helm</h3>

<pre><code>nano helm-namespace-account.yaml
kind: ServiceAccount
apiVersion: v1
metadata:
  name: tiller-gitlab-devopstales.intra
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tiller-gitlab-devopstales.intra
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller-gitlab-devopstales.intra
    namespace: kube-system
</code></pre>

<p>Now set up Helm, install the Tiller plugin and add the GitLab repository.</p>

<pre><code>oc apply -f helm-namespace-account.yaml
oc get sa

helm init --service-account tiller-gitlab-devopstales.intra --tiller-namespace gitlab-mydomain-intra
oc get po -n kube-system

export TILLER_NAMESPACE=kube-system
echo $TILLER_NAMESPACE
helm version
</code></pre>

<h2 id="get-helmchart">Get helmchart</h2>

<pre><code>helm repo add gitlab https://charts.gitlab.io/
helm repo update

oc adm policy add-scc-to-user anyuid -z default -n gitlab-devopstales.intra
oc adm policy add-scc-to-user anyuid -z gitlab-runner -n gitlab-devopstales.intra

# gitlab-tst is the name of the helm deployment
oc adm policy add-scc-to-user anyuid -z gitlab-tst-shared-secrets
oc adm policy add-scc-to-user anyuid -z gitlab-tst-gitlab-runner
oc adm policy add-scc-to-user anyuid -z gitlab-tst-prometheus-server
oc adm policy add-scc-to-user anyuid -z default
</code></pre>

<h3 id="create-chart-values">Create chart values</h3>

<pre><code>nano gitlab-values.yml
certmanager:
  install: false
global:
  appConfig:
    enableUsagePing: true
    enableImpersonation: true
    defaultCanCreateGroup: true
    usernameChangingEnabled: true
    issueClosingPattern:
    defaultTheme:
    defaultProjectsFeatures:
      issues: true
      mergeRequests: true
      wiki: true
      snippets: true
      builds: true
      containerRegistry: true
    ldap:
      servers:
        main:
          base: dc=mydomain,dc=intra
          user_filter: (&amp;(objectClass=user)(memberof=cn=Users,dc=mydomain,dc=intra))
          bind_dn: Administrator@devopstales.intra
          host: 192.168.10.4
          label: devopstales.intra
          password:
            key: password
            secret: gitlab-ldap-secret
          port: 636
          encryption: simple_tls
          uid: sAMAccountName
          active_directory: true
          verify_certificates: false
          allow_username_or_email_login: true
    omniauth:
      enabled: true
      blockAutoCreatedUsers: false
      allowSingleSignOn: ['oauth2_generic']
      providers:
        - secret: gitlab-sso
          key: provider
    backups:
      bucket: gitlab-devopstales.intra
      tmpBucket: gitlab-devopstales.intra
      objectStorage:
        backend: s3
    lfs:
      bucket: gitlab-devopstales.intra
      connection:
        secret: ceph-storage
        key: gitlab
    artifacts:
      bucket: gitlab-devopstales.intra
      connection:
        secret: ceph-storage
        key: gitlab
    uploads:
      bucket: gitlab-devopstales.intra
      connection:
        secret: ceph-storage
        key: gitlab
    packages:
      bucket: gitlab-devopstales.intra
      connection:
        secret: ceph-storage
        key: gitlab
    externalDiffs:
      bucket: gitlab-devopstales.intra
      connection:
        secret: ceph-storage
        key: gitlab
    pseudonymizer:
      bucket: gitlab-devopstales.intra
      connection:
        secret: ceph-storage
        key: gitlab
  edition: ce
  email:
    from: gitlab@devopstales.intra
  hosts:
    domain: devopstales.intra
    externalIP: gitlab.devopstales.intra
    gitlab:
      name: gitlab.devopstales.intra
      https: false
    registry:
      name: gitlab-registry.devopstales.intra
      https: false
  ingress:
    enabled: false
    configureCertmanager: false
    tls:
      secretName: gitlab-certs
  smtp:
    address: mail.active.hu
    authentication: &quot;&quot;
    domain: devopstales.intra
    enabled: true
    port: 25
  gitlab-exporter:
    enabled: false
  registry:
    bucket: gitlab-registry
  minio:
    enabled: false
nginx-ingress:
  enabled: false
gitlab-runner:
  rbac:
    create: true
registry:
  enabled: true
  storage:
    secret: ceph-storage
    key: registry
  image:
    repository: docker.io/registry
    tag: 2.6.0
gitlab:
  task-runner:
    backups:
      objectStorage:
        config:
          secret: storage-config
          key: config
</code></pre>

<h3 id="create-secrets-for-deployment">Create secrets for deployment</h3>

<pre><code>nano ceph.gitlab-data.yaml
provider: AWS
region: default
aws_access_key_id: W3MNDO373H6LQUNCG4SG
aws_secret_access_key: vVFEWx3hqbcrGJyaZVie9YoFG6rPoRYmqnDzRwrn
endpoint: &quot;https://s3.devopstales.intra&quot;
enable_signature_v4_streaming: false

# admin jog kell a cephez
nano ceph.gitlab-registry.yaml
cache:
  blobdescriptor: inmemory
s3:
  region: default
  bucket: gitlab-registry
  accesskey: PZIOIH63CENHPG15XY42
  secretkey: K6K1lWO7Jtyp5rZiCwj77JC5BFMEAZ4a2PAkg9fB
  regionendpoint: https://s3.devopstales.intra
  rootdirectory: /
  secure: true
  v4auth: false
  encrypt: false
  chunksize: 5242880
redirect:
  disable: true
</code></pre>

<pre><code>nano ceph.backup.config
[default]
access_key = W3MNDO373H8LQUNCJ8QV
access_token = vVFEWx8hqbcrGJyaZVie8YoER8rPoRYmqnDzRwrn
host_base = s3.devopstales.intra
host_bucket = %(bucket)s.s3.devopstales.intra
bucket_location = US
use_https = True
check_ssl_certificate = False
</code></pre>

<pre><code>nano keycloak.sso.yaml
name: 'oauth2_generic'
label: 'mydomain'
app_id: 'gitlab'
app_secret: 'f2514bd4-92e4-40fa-bec4-382838db25f0'
args:
  client_options:
    site: 'https://sso.devopstales.intra'
    user_info_url: '/auth/realms/mydomain/protocol/openid-connect/userinfo'
    authorize_url: '/auth/realms/mydomain/protocol/openid-connect/auth'
    token_url: '/auth/realms/mydomain/protocol/openid-connect/token'
  user_response_structure:
    attributes:
      email: 'email'
      first_name: 'given_name'
      last_name: 'family_name'
      name: 'name'
      nickname: 'preferred_username'
    id_path: 'preferred_username'
</code></pre>

<h3 id="deploy-secrets">Deploy secrets</h3>

<pre><code># https ssl cert
oc create secret tls gitlab-certs --cert=tls.crt --key=tls.key

oc create secret generic storage-config --from-file=config=ceph.backup.config

oc create secret generic ceph-storage --from-file=registry=ceph.gitlab-registry.yaml --from-file=gitlab=ceph.gitlab-data.yaml

oc create secret generic gitlab-sso --from-file=provider=keycloak.sso.yaml
oc create secret generic gitlab-ldap-secret --from-literal=password=
</code></pre>

<h3 id="deploy-application-with-helm">Deploy application with helm</h3>

<pre><code>helm upgrade --install -f gitlab-values.yml gitlab-tst gitlab/gitlab --debug --dry-run
helm upgrade --install -f gitlab-values.yml gitlab-tst gitlab/gitlab --timeout 600
helm upgrade -f gitlab-values.yml gitlab-tst gitlab/gitlab --timeout 600

# https://docs.gitlab.com/charts/installation/version_mappings.html
helm upgrade -f gitlab-values.yml gitlab-tst gitlab/gitlab --version 2.3.5 --timeout 600

# gitlab-tst
oc get secret gitlab-tst-gitlab-initial-root-password -o jsonpath='{.data.password}' | base64 -d
</code></pre>

<p>###</p>

<pre><code>nano gitlab-ssh-nodeport-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: gitlab-shell-nodeport
  labels:
    app: gitlab-shell
    name: gitlab-shell-nodeport
spec:
  type: NodePort
  ports:
    - port: 2222
      nodePort: 32222
      name: ssh
  selector:
    app: gitlab-shell

</code></pre>

<pre><code>oc create -f gitlab-ssh-nodeport-svc.yaml
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Solution for: Proxmox backup error due to iothread]]></title>
            <link href="https://devopstales.github.io/home/proxmox-backup-error/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/proxmox-backup-error/?utm_source=atom_feed" rel="related" type="text/html" title="Solution for: Proxmox backup error due to iothread" />
                <link href="https://devopstales.github.io/home/proxmox-backup-pve-zsync/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with pve-zsync" />
                <link href="https://devopstales.github.io/cloud/proxmox-backup-pve-zsync/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with pve-zsync" />
                <link href="https://devopstales.github.io/cloud/proxmox-backup-sanoid/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with sanoid" />
                <link href="https://devopstales.github.io/home/proxmox-backup-sanoid/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with sanoid" />
            
                <id>https://devopstales.github.io/home/proxmox-backup-error/</id>
            
            
            <published>2019-11-18T00:00:00+00:00</published>
            <updated>2019-11-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>If you see the following error when trying to backup a KVM VM image on Proxmox:</p>

<pre><code>ERROR: Backup of VM 100 failed – disk ‘scsi0’ ‘zfsvols:vm-100-disk-1’ (iothread=on) can’t use backup feature currently. Please set backup=no for this drive at /usr/share/perl5/PVE/VZDump/QemuServer.pm line 77. INFO: Backup job finished with errors TASK ERROR: job errors
</code></pre>

<p>Posted on September 9, 2017 by Daniel Mettler
Solution for: Proxmox backup error due to iothread=1</p>

<p>If you see the following error when trying to backup a KVM VM image on Proxmox:</p>

<p>ERROR: Backup of VM 100 failed – disk ‘scsi0’ ‘zfsvols:vm-100-disk-1’ (iothread=on) can’t use backup feature currently. Please set backup=no for this drive at /usr/share/perl5/PVE/VZDump/QemuServer.pm line 77. INFO: Backup job finished with errors TASK ERROR: job errors</p>

<p>edit /etc/pve/qemu-server/100.conf, look for a line similar to</p>

<pre><code>scsi0: zfsvols:vm-100-disk-1,iothread=1,size=70G
</code></pre>

<p>and change it to</p>

<pre><code>scsi0: zfsvols:vm-100-disk-1,iothread=0,size=70G
# OR
scsi0: zfsvols:vm-100-disk-1,size=70G
</code></pre>

<p>After this you can backup the VM. This Problem was solvd in the proxmox 6 (pve-manager 6.0-11)</p>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Foreman PXE boot]]></title>
            <link href="https://devopstales.github.io/home/foreman-pxe/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/foreman-pxe/</id>
            
            
            <published>2019-11-07T00:00:00+00:00</published>
            <updated>2019-11-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Foreman is a complete lifecycle management tool for physical and virtual servers. We give system administrators the power to easily automate repetitive tasks, quickly deploy applications, and proactively manage servers, on-premise or in the cloud.</p>

<p>I hawe a VM with two virtual interface the enp0s3 for NAT and enp0s9 with an internal network.</p>

<h3 id="install-dhcp-server">Install DHCP server</h3>

<pre><code>yum install -y dhcp nano -y

echo &quot;DHCPDARGS=enp0s9&quot; &gt;&gt; /etc/sysconfig/dhcpd
</code></pre>

<pre><code>cat &gt; /etc/dhcp/dhcpd.conf &lt;&lt; EOF
#DHCP configuration for PXE boot server
ddns-update-style interim;
ignore client-updates;
authoritative;
allow booting;
allow bootp;
allow unknown-clients;

# A slightly different configuration for an internal subnet.
subnet 192.168.100.0
netmask 255.255.255.0
{
range 192.168.100.101 192.168.100.200;
option domain-name-servers 192.168.100.100;
option routers 192.168.100.100;
default-lease-time 600;
max-lease-time 7200;

# PXE SERVER IP
next-server 192.168.100.100; #  PXE server ip
filename &quot;pxelinux.0&quot;;
}
EOF
</code></pre>

<pre><code>systemctl start dhcpd.service
systemctl enable dhcpd.service
systemctl status dhcpd.service
</code></pre>

<h3 id="install-dns-server">Install DNS server</h3>

<pre><code>yum -y install bind bind-utils

nano /etc/named.conf
options {
        ...
        // listen-on port 53 { 127.0.0.1; };
        // listen-on-v6 port 53 { ::1; };
        ...
        allow-query     { localhost; 192.168.100.0/24; };
        ...
        forwarders {
                8.8.8.8;
                8.8.4.4;
        };
};
include &quot;/etc/named.my.zones&quot;;
</code></pre>

<pre><code>touch /etc/named.my.zones
chown root:named /etc/named.my.zones

nano /etc/named.my.zones
zone &quot;devopstales.intra&quot; IN {
         type master;
         file &quot;devopstales.intra.db&quot;;
         allow-update { none; };
};
</code></pre>

<pre><code>nano /var/named/devopstales.intra.db
@   IN  SOA     primary.devopstales.intra. root.mydomain.intra. (
                                                1001    ;Serial
                                                3H      ;Refresh
                                                15M     ;Retry
                                                1W      ;Expire
                                                1D      ;Minimum TTL
                                                )

;Name Server Information
@      IN  NS      primary.devopstales.intra.

;IP address of Name Server
primary IN  A       192.168.100.100

;Mail exchanger
devopstales.intra. IN  MX 10   mail.mydomain.intra.

;A - Record HostName To IP Address
foreman IN  A       192.168.100.100
mail    IN  A       192.168.100.50
</code></pre>

<pre><code>systemctl restart named
systemctl status named
systemctl enable named
</code></pre>

<h3 id="install-vtftp">Install vtftp</h3>

<pre><code>yum install vsftpd -y

nano /etc/vsftpd/vsftpd.conf
anonymous_enable=YES
write_enable=NO

systemctl enable vsftpd
systemctl restart vsftpd
systemctl status vsftpd

cd /opt
wget http://ftp.bme.hu/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1908.iso
wget http://ftp.bme.hu/centos/7/isos/x86_64/sha256sum.txt

sha256sum CentOS-7-x86_64-Minimal-1908.iso
cat sha256sum.txt

mount -o loop /opt/CentOS-7-x86_64-Minimal-1908.iso  /mnt

mkdir /var/ftp/pub/CentOS_7_x86_64
rsync -rv --progress /mnt/ /var/ftp/pub/CentOS_7_x86_64/
umount /mnt
restorecon -Rv /var/ftp/pub/
</code></pre>

<h3 id="install-foreman">Install Foreman</h3>

<pre><code>yum -y install https://yum.puppet.com/puppet6-release-el-7.noarch.rpm
yum -y install http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
yum -y install https://yum.theforeman.org/releases/1.23/el7/x86_64/foreman-release.rpm
yum -y install foreman-installer

foreman-installer \
--foreman-initial-organization &quot;mydomain&quot; \
--foreman-initial-location &quot;office&quot; \
--enable-foreman-plugin-ansible \
--enable-foreman-proxy-plugin-ansible \
--enable-foreman-plugin-remote-execution \
--enable-foreman-proxy-plugin-remote-execution-ssh \
--enable-foreman-plugin-cockpit \
--enable-foreman-plugin-openscap
</code></pre>

<h3 id="configure-hammer">Configure hammer</h3>

<pre><code>nano ~/.hammer/cli.modules.d/foreman.yml
:foreman:
 :host: 'https://foreman.devopstales.intra/'
 :username: 'admin'
 :password: '**********'

hammer defaults add --param-name organization --param-value &quot;mydomain&quot;
hammer defaults add --param-name location --param-value &quot;office&quot;
hammer defaults list
</code></pre>

<h3 id="configurate-pxeboot">Configurate PXEboot</h3>

<pre><code>sudo ss -lnup | grep 69
grep disa /etc/xinetd.d/tftp
ls -l /var/lib/tftpboot/

# create subnet
hammer subnet create \
--name PXEnet \
--network-type IPv4 \
--network 192.168.100.0 \
--mask 255.255.255.0 \
--dns-primary 192.168.100.100 \
--domains devopstales.intra \
--tftp-id 1 \
--httpboot-id 1 \
--ipam &quot;Internal DB&quot; \
--from 192.168.100.101 \
--to 192.168.100.200 \
--boot-mode Static

hammer medium create \
--name &quot;CentOS7_DVD_FTP&quot; \
--os-family &quot;Redhat&quot; \
--path &quot;ftp://foreman.devopstales.intra/pub/CentOS_7_x86_64/&quot;
</code></pre>

<p>Create a file hardened_ptable.txt with the content below.</p>

<pre><code>&lt;%#
kind: ptable
name: Kickstart hardened
oses:
- CentOS
- Fedora
- RedHat
%&gt;

# System bootloader configuration
bootloader --location=mbr --boot-drive=sda --timeout=3
# Partition clearing information
clearpart --all --drives=sda
zerombr

# Disk partitioning information
part /boot --fstype=&quot;xfs&quot; --ondisk=sda --size=1024 --label=boot --fsoptions=&quot;rw,nodev,noexec,nosuid&quot;

# 30GB physical volume
part pv.01  --fstype=&quot;lvmpv&quot; --ondisk=sda --size=30720
volgroup vg_os pv.01

logvol /        --fstype=&quot;xfs&quot;  --size=4096 --vgname=vg_os --name=lv_root
logvol /home    --fstype=&quot;xfs&quot;  --size=512  --vgname=vg_os --name=lv_home --fsoptions=&quot;rw,nodev,nosuid&quot;
logvol /tmp     --fstype=&quot;xfs&quot;  --size=1024 --vgname=vg_os --name=lv_tmp  --fsoptions=&quot;rw,nodev,noexec,nosuid&quot;
logvol /var     --fstype=&quot;xfs&quot;  --size=6144 --vgname=vg_os --name=lv_var  --fsoptions=&quot;rw,nosuid&quot;
logvol /var/log --fstype=&quot;xfs&quot;  --size=512  --vgname=vg_os --name=lv_log  --fsoptions=&quot;rw,nodev,noexec,nosuid&quot;
logvol swap     --fstype=&quot;swap&quot; --size=2048 --vgname=vg_os --name=lv_swap --fsoptions=&quot;swap&quot;
</code></pre>

<pre><code>hammer partition-table create \
  --name &quot;Kickstart hardened&quot; \
  --os-family &quot;Redhat&quot; \
  --operatingsystems &quot;CentOS 7.4.1708&quot; \
  --file &quot;hardened_ptable.txt&quot;

hammer os create \
  --name &quot;CentOS&quot; \
  --major &quot;7&quot; \
  --minor &quot;4.1708&quot; \
  --family &quot;Redhat&quot; \
  --password-hash &quot;SHA512&quot; \
  --architectures &quot;x86_64&quot; \
  --media &quot;CentOS7_DVD_FTP&quot; \
  --partition-tables &quot;Kickstart hardened&quot;

hammer hostgroup create \
  --name &quot;el7_group&quot; \
  --description &quot;Host group for CentOS 7 servers&quot; \
  --lifecycle-environment &quot;stable&quot; \
  --content-view &quot;el7_content&quot; \
  --content-source-id &quot;1&quot; \
  --environment &quot;homelab&quot; \
  --puppet-proxy &quot;foreman.devopstales.intra&quot; \
  --puppet-ca-proxy &quot;foreman.devopstales.intra&quot; \
  --domain &quot;devopstales.intra&quot; \
  --subnet &quot;PXEnet&quot; \
  --architecture &quot;x86_64&quot; \
  --operatingsystem &quot;CentOS 4.1708&quot; \
  --medium &quot;CentOS7_DVD_FTP&quot; \
  --partition-table &quot;Kickstart hardened&quot; \
  --pxe-loader &quot;PXELinux BIOS&quot; \
  --root-pass &quot;Password1&quot;

hammer hostgroup set-parameter  \
  --name &quot;selinux-mode&quot; \
  --value &quot;disabled&quot; \
  --hostgroup &quot;el7_group&quot;

hammer hostgroup set-parameter  \
  --name &quot;disable-firewall&quot; \
  --value &quot;true&quot; \
  --hostgroup &quot;el7_group&quot;

hammer hostgroup set-parameter  \
  --name &quot;bootloader-append&quot; \
  --value &quot;net.ifnames=0 biosdevname=0&quot; \
  --hostgroup &quot;el7_group&quot;

hammer host create \
  --name &quot;pxe-test&quot; \
  --hostgroup &quot;el7_group&quot; \
  --interface &quot;type=interface,mac=08:00:27:fb:ad:17,ip=192.168.100.110,managed=true,primary=true,provision=true&quot;
</code></pre>

<pre><code>ll /var/lib/tftpboot/pxelinux.cfg/
cat /var/lib/tftpboot/pxelinux.cfg/01-08-00-27-fb-ad-17
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes Ceph RBD volume with CSI driver]]></title>
            <link href="https://devopstales.github.io/home/k8s-ceph-storage-with-csi-driver/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/k8s-ceph-storage-with-csi-driver/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD volume with CSI driver" />
                <link href="https://devopstales.github.io/home/k8s-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/cloud/k8s-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/who-mapping-rbd-device/?utm_source=atom_feed" rel="related" type="text/html" title="Ceph: who&#39;s mapping a RBD device" />
                <link href="https://devopstales.github.io/cloud/who-mapping-rbd-device/?utm_source=atom_feed" rel="related" type="text/html" title="Ceph: who&#39;s mapping a RBD device" />
            
                <id>https://devopstales.github.io/home/k8s-ceph-storage-with-csi-driver/</id>
            
            
            <published>2019-10-08T00:00:00+00:00</published>
            <updated>2019-10-08T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how can you use CEPH RBD with CSI driver for persistent storagi on Kubernetes.</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../cloud/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../cloud/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../cloud/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../cloud/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part3: <a href="../../cloud/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-traefik-ingress --></li>
<li>Part4: <a href="../../cloud/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../cloud/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../cloud/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../cloud/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../cloud/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a>
<!-- CEPH in Kubernetes with rook --></li>
<li>Part5d: <a href="../../cloud/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes vMware --></li>
<li>Part6: <a href="../../cloud/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part7: <a href="../../cloud/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part8: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part9: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part10: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part11: <a href="../../cloud/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- prometheus operator + grafana operator -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<p>The Container Storage Interface (CSI) is a standard for exposing arbitrary block and file storage storage systems to Kubernetes. Using CSI third-party storage providers can write and deploy plugins exposing storage systems in Kubernetes. Bbefore we begin lets ensure that we have the following requirements:</p>

<ul>
<li>Kubernetes cluster v1.14+</li>
<li>allow-privileged flag enabled for both kubelet and API server</li>

<li><p>Running Ceph cluster</p>

<pre><code>git clone https://github.com/ceph/ceph-csi.git
cd ceph-csi/deploy/rbd/kubernetes/v1.14+/

kubectl create -f csi-nodeplugin-rbac.yaml
kubectl create -f csi-provisioner-rbac.yaml
</code></pre>

<pre><code>nano csi-config-map.yaml
---
apiVersion: v1
kind: ConfigMap
data:
config.json: |-
[
  {
    &quot;clusterID&quot;: &quot;k8s-ceph&quot;,
    &quot;monitors&quot;: [
      &quot;192.168.1.31:6790&quot;,
      &quot;192.168.1.32:6790&quot;,
      &quot;192.168.1.33:6790&quot;
    ]
  }
]
metadata:
name: ceph-csi-config


kubectl create -fcsi-config-map.yaml
</code></pre>

<pre><code>kubectl create -f csi-rbdplugin-provisioner.yaml
kubectl create -f csi-rbdplugin.yaml
</code></pre>

<pre><code>ceph auth get-key client.admin|base64
QVFDTDliVmNEb21I32SHoPxXNGhmRkczTFNtcXM0ZW5VaXlTZEE977==

nano csi-rbd-secret.yaml
---
apiVersion: v1
kind: Secret
metadata:
name: csi-rbd-secret
namespace: default
data:
userID: admin
userKey: QVFDTDliVmNEb21I32SHoPxXNGhmRkczTFNtcXM0ZW5VaXlTZEE977==

nano rbd-csi-sc.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
name: csi-rbd
provisioner: rbd.csi.ceph.com
parameters:
monitors: 192.168.1.31:6790,192.168.1.32:6790,192.168.1.33:6790
clusterID: k8s-ceph
pool: rbd
imageFeatures: layering
csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
csi.storage.k8s.io/provisioner-secret-namespace: default
csi.storage.k8s.io/node-publish-secret-name: csi-rbd-secret
csi.storage.k8s.io/node-publish-secret-namespace: default
adminid: admin
csi.storage.k8s.io/fstype: ext4
reclaimPolicy: Delete
mountOptions:
- discard

kubectl create -f csi-rbd-secret.yaml
kubectl create -f rbd-csi-sc.yaml

kubectl get storageclass
NAME      PROVISIONER        AGE
csi-rbd   rbd.csi.ceph.com   15s
</code></pre>

<pre><code>nano raw-block-pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
name: raw-block-pvc
spec:
accessModes:
- ReadWriteMany
volumeMode: Block
resources:
requests:
  storage: 1Gi
storageClassName: csi-rbd

kubectl create -f raw-block-pvc.yaml

kubectl get pvc
NAME            STATUS    VOLUME                                  
raw-block-pvc   Bound     pvc-fd66b4d6-757d-22e9-8f9e-4f86e2356a59
</code></pre></li>
</ul>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Ceph: who's mapping a RBD device]]></title>
            <link href="https://devopstales.github.io/home/who-mapping-rbd-device/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/who-mapping-rbd-device/?utm_source=atom_feed" rel="related" type="text/html" title="Ceph: who&#39;s mapping a RBD device" />
                <link href="https://devopstales.github.io/home/k8s-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/cloud/k8s-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/cloud/openshift-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/openshift-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Ceph RBD for dynamic provisioning" />
            
                <id>https://devopstales.github.io/home/who-mapping-rbd-device/</id>
            
            
            <published>2019-10-05T00:00:00+00:00</published>
            <updated>2019-10-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<h3 id="main-problem">Main problem</h3>

<p>I get this error in Openshift:</p>

<pre><code>  MountVolume.WaitForAttach failed for volume &quot;pvc-9fcd3d08-d14e-11e9-a958-66934f1af826&quot; :
  rbd image k8s-prod/kubernetes-dynamic-pvc-a0029613-d14e-11e9-aaf5-8611e6b1c395 is still being used
</code></pre>

<h3 id="solution">Solution</h3>

<p>So I Wanna know who is using this DBD device?</p>

<pre><code>rbd status k8s-prod/kubernetes-dynamic-pvc-a0029613-d14e-11e9-aaf5-8611e6b1c395
Watchers:
	watcher=192.168.1.43:0/3614154426 client.165467009 cookie=18446462598732840961
</code></pre>

<p>To solve this problem you need to restart Openshift&rsquo;s Kubernetes components.</p>

<pre><code>ssh 192.168.1.43
systemctl restart docker origin-node
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Export GCP VM to S3]]></title>
            <link href="https://devopstales.github.io/home/gcp-vm-export/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/gcp-vm-export/</id>
            
            
            <published>2019-10-04T00:00:00+00:00</published>
            <updated>2019-10-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Step by step guide to export virtual machine running in Google cloud computer engine to your S3 bucket.</p>

<pre><code>gcloud compute instances list
NAME        ZONE            MACHINE_TYPE               PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP  STATUS
demo1   europe-west1-b  g1-small                                10.132.0.3                TERMINATED
</code></pre>

<h3 id="create-snapshot">Create snapshot</h3>

<pre><code>gcloud compute disks snapshot europe-west1-b/disks/demo1 --storage-locatio europe-west1

gcloud compute snapshots list
NAME              DISK_SIZE_GB  SRC_DISK                        STATUS
demo1-backup  30            europe-west1-b/disks/demo1  READY
</code></pre>

<h3 id="create-custom-image">Create custom image</h3>

<pre><code>gcloud compute images create demo1-backup --source-snapshot demo1-backup
Created [https://www.googleapis.com/compute/v1/projects/demo-project-223110/global/images/demo1-backup].
NAME              PROJECT               FAMILY  DEPRECATED  STATUS
demo1-backup  demo-project-223110                      READY
</code></pre>

<h3 id="create-s3-storage">Create S3 storage</h3>

<pre><code>gsutil mb gs://backup-demo-project-223110/ -l europe-west1
</code></pre>

<h3 id="export-to-s3-storage">Export to S3 storage</h3>

<pre><code>gcloud compute images export --destination-uri gs://backup-demo-project-223110/demo1-beckup.tar.gz --image demo1-backup --export-format=vmdk
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Kubernetes]]></title>
            <link href="https://devopstales.github.io/home/ansible-k8s-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/ansible-k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Kubernetes" />
                <link href="https://devopstales.github.io/home/ansible-operator-overview/?utm_source=atom_feed" rel="related" type="text/html" title="Ansible Operator Overview" />
                <link href="https://devopstales.github.io/cloud/ansible-operator-overview/?utm_source=atom_feed" rel="related" type="text/html" title="Ansible Operator Overview" />
                <link href="https://devopstales.github.io/cloud/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
                <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
            
                <id>https://devopstales.github.io/home/ansible-k8s-install/</id>
            
            
            <published>2019-10-03T00:00:00+00:00</published>
            <updated>2019-10-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Kubespray is a pre made ansible playbook for Kubernetes installation. In this Post I will show you how to use to install a new Kubernetes cluster.</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../cloud/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../cloud/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../cloud/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../cloud/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part3: <a href="../../cloud/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-traefik-ingress --></li>
<li>Part4: <a href="../../cloud/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../cloud/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../cloud/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../cloud/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../cloud/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a>
<!-- CEPH in Kubernetes with rook --></li>
<li>Part5d: <a href="../../cloud/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes vMware --></li>
<li>Part6: <a href="../../cloud/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part7: <a href="../../cloud/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part8: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part9: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part10: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part11: <a href="../../cloud/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- prometheus operator + grafana operator -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<h3 id="environment">Environment</h3>

<pre><code>192.168.1.60    deployer.devopstales.intra (LB)
192.168.1.61    master0.devopstales.intra  (master)
192.168.1.62    master1.devopstales.intra  (master)
192.168.1.63    master2.devopstales.intra  (master)
192.168.1.64    worker0.devopstales.intra  (worker)
192.168.1.65    worker1.devopstales.intra  (worker)

# hardware requirement
4 CPU
16G RAM
</code></pre>

<h3 id="prerequirement">Prerequirement</h3>

<pre><code># deployer

nano ~/.ssh/config
Host master0
    Hostname master0.devopstales.intra
    User ansible

Host master1
    Hostname master1.devopstales.intra
    User ansible

Host master2
    Hostname master2.devopstales.intra
    User ansible

Host worker0.devopstales.intra
    Hostname worker0.devopstales.intra
    User ansible

Host worker1
    Hostname worker1.devopstales.intra
    User ansible
</code></pre>

<pre><code>yum install epel-release -y
yum update -y
yum install python-pip git tmux nano -y
git clone https://github.com/kubernetes-sigs/kubespray.git
cd kubespray
pip install --user -r requirements.txt

cp -rfp inventory/sample inventory/mycluster
</code></pre>

<h3 id="configurate-installer">Configurate Installer</h3>

<pre><code>nano inventory/mycluster/inventory.ini
master0   ansible_host=192.168.1.61 ip=192.168.1.61
master1   ansible_host=192.168.1.62 ip=192.168.1.62
master2   ansible_host=192.168.1.63 ip=192.168.1.63
worker0   ansible_host=192.168.1.64 ip=192.168.1.64
worker1   ansible_host=192.168.1.65 ip=192.168.1.65

# ## configure a bastion host if your nodes are not directly reachable
# bastion ansible_host=x.x.x.x ansible_user=some_user

[kube-master]
master0
master1
master2

[etcd]
master0
master1
master2

[kube-node]
worker0
worker1

[calico-rr]

[k8s-cluster:children]
kube-master
kube-node
calico-rr
</code></pre>

<h3 id="run-the-installer">Run the Installer</h3>

<pre><code>tmux new -s kubespray
ansible-playbook -i inventory/mycluster/inventory.ini --become \
--user=centos --become-user=root cluster.yml

test install on node:
sudo -i
kubectl get node
NAME      STATUS   ROLES    AGE   VERSION
master0   Ready    master   92m   v1.15.3
master1   Ready    master   91m   v1.15.3
master2   Ready    master   91m   v1.15.3
worker0   Ready    &lt;none&gt;   90m   v1.15.3
worker1   Ready    &lt;none&gt;   90m   v1.15.3

kubectl config get-clusters
</code></pre>

<h3 id="let-s-configure-an-external-loadbalancer">Let’s configure an external loadbalancer</h3>

<pre><code>sudo firewall-cmd --add-port=6443/tcp --permanent
sudo firewall-cmd --reload

yum -y install haproxy

nano ..
listen k8s-apiserver-https
  bind *:6443
  option ssl-hello-chk
  mode tcp
  balance roundrobin
  timeout client 3h
  timeout server 3h
  server master0 192.168.1.61:6443
  server master1 192.168.1.62:6443
  server master2 192.168.1.63:6443

systemctl enable --now haproxy
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install alerta on Centos8]]></title>
            <link href="https://devopstales.github.io/home/alerta-on-centos8/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/alerta-on-centos8/</id>
            
            
            <published>2019-09-28T00:00:00+00:00</published>
            <updated>2019-09-28T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>AIn this post I will show you how to install alerta monitoring dashboard on Centos 8.</p>

<pre><code>yum install epel-release -y
yum upgrade -y
</code></pre>

<h3 id="install-and-configure-postgresql">Install and configure postgresql</h3>

<p>In a previous post I wrote about how to <a href="/linux/install-postgresql/#install-postgresql-10-on-centos-8">Install PostgreSQL 10</a></p>

<pre><code>sudo su - postgres
createuser alerta
createdb -O alerta alerta
psql
ALTER USER &quot;alerta&quot; WITH PASSWORD 'alerta';
\q
exit
</code></pre>

<h3 id="install-python3-packages">install python3 packages</h3>

<pre><code>yum install python3 python3-pip python3-setuptools python3-devel python3-psycopg2 gcc git tmux nginx
</code></pre>

<p>Create user for alerta and install python3 moduls with it.</p>

<pre><code>useradd alerta
usermod -aG alerta nginx
su - alerta
pip3 install --user wheel alerta-server alerta uwsgi
</code></pre>

<h1 id="alerta-server">alerta server</h1>

<pre><code>nano /etc/alertad.conf
DATABASE_URL = 'postgres://alerta:alerta@localhost:5432/alerta'
PLUGINS=['reject']
ALLOWED_ENVIRONMENTS=['Production', 'Development', 'Code']
</code></pre>

<p>We use uwsgi to create a unix socket wgere the alerta webgui can connect.</p>

<pre><code>sudo mkdir -p /var/log/uwsgi
sudo chown -R alerta:alerta /var/log/uwsgi
mkdir /var/run/alerta
chown -R alerta.alerta /var/run/alerta/

echo &quot;from alerta import app&quot; &gt; /usr/share/nginx/wsgi.py
</code></pre>

<pre><code>nano /etc/uwsgi.ini
[uwsgi]
chdir = /usr/share/nginx/
mount = /api=wsgi.py
callable = app
manage-script-name = true
env = BASE_URL=/api

master = true
processes = 5
#logger = syslog:alertad
logto = /var/log/uwsgi/%n.log

socket = /var/run/alerta/uwsgi.sock
chmod-socket = 664
uid = alerta
gid = alerta
vacuum = true

die-on-term = true
</code></pre>

<pre><code>nano /etc/systemd/system/uwsgi.service
[Unit]
Description=uWSGI service
After=syslog.target

[Service]
ExecStart=/home/alerta/.local/bin/uwsgi --ini /etc/uwsgi.ini
RuntimeDirectory=uwsgi
Type=notify
StandardError=syslog
NotifyAccess=all

[Install]
WantedBy=multi-user.target
</code></pre>

<pre><code>systemctl enable uwsgi
systemctl start uwsgi
systemctl status uwsgi
</code></pre>

<h1 id="alerta-webgui">alerta webgui</h1>

<pre><code>wget https://github.com/alerta/alerta-webui/releases/latest/download/alerta-webui.tar.gz
tar zxvf alerta-webui.tar.gz
mv dist/ /usr/share/nginx/alerta
echo '{&quot;endpoint&quot;: &quot;/api&quot;}' &gt; /usr/share/nginx/dist/config.json
</code></pre>

<pre><code>nano /etc/nginx/nginx.conf
    #server {
    #    listen       80 default_server;
    #    listen       [::]:80 default_server;
    #    server_name  _;
    #    root         /usr/share/nginx/html;

    #    # Load configuration files for the default server block.
    #    include /etc/nginx/default.d/*.conf;

     #   location / {
     #   }

     #   error_page 404 /404.html;
     #       location = /40x.html {
     #   }

     #   error_page 500 502 503 504 /50x.html;
     #       location = /50x.html {
     #   }
    #}
</code></pre>

<pre><code>nano /etc/nginx/conf.d/alerta.conf
server {
        listen 80 default_server;

        location /api { try_files $uri @api; }
        location @api {
            include uwsgi_params;
            uwsgi_pass unix:/var/run/alerta/uwsgi.sock;
            proxy_set_header Host $host:$server_port;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }

        location / {
                root /usr/share/nginx/alerta;
        }
}
</code></pre>

<pre><code>nginx -t
systemctl restart nginx
systemctl enable nginx
</code></pre>

<h3 id="test">test</h3>

<p>Send testalert with alerta client.</p>

<pre><code>su - alerta

echo '[DEFAULT]
endpoint = http://localhost/api' &gt;  $HOME/.alerta.conf

echo 'export PATH=$PATH:/home/alerta/.local/bin' &gt;&gt; ~/.bashrc
PATH=$PATH:/home/alerta/.local/bin

alerta query
alerta send --resource net01 --event down --severity critical --environment Code --service Network --text 'net01 is down.'
alerta query
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install alerta on Centos7]]></title>
            <link href="https://devopstales.github.io/home/alerta-on-centos7/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/alerta-on-centos7/</id>
            
            
            <published>2019-09-27T00:00:00+00:00</published>
            <updated>2019-09-27T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>AIn this post I will show you how to install alerta monitoring dashboard on Centos 7.</p>

<pre><code>yum install epel-release nano -y
yum upgrade -y
</code></pre>

<h3 id="install-and-configure-postgresql">Install and configure postgresql</h3>

<p>In a previous post I wrote about how to <a href="/linux/install-postgresql/#install-postgresql-10-on-centos-7">Install PostgreSQL 10</a></p>

<pre><code>sudo su - postgres
createuser alerta
createdb -O alerta alerta
psql
ALTER USER &quot;alerta&quot; WITH PASSWORD 'alerta';
\q
</code></pre>

<h3 id="install-python3-packages">install python3 packages</h3>

<pre><code>yum install https://centos7.iuscommunity.org/ius-release.rpm -y
yum install python36u python36u-pip python36u-setuptools python36u-devel gcc git tmux
yum install uwsgi-plugin-psgi nginx -y
</code></pre>

<h1 id="alerta-server">alerta server</h1>

<pre><code>python3.6 -m venv alerta
alerta/bin/pip install --upgrade pip wheel alerta-server alerta uwsgi
</code></pre>

<pre><code>nano /etc/alertad.conf
DATABASE_URL = 'postgres://alerta:alerta@localhost:5432/alerta'
PLUGINS=['reject']
ALLOWED_ENVIRONMENTS=['Production', 'Development', 'Code']
</code></pre>

<p>We use uwsgi to create a unix socket wgere the alerta webgui can connect.</p>

<pre><code>sudo mkdir -p /var/log/uwsgi
sudo chown -R nginx:nginx /var/log/uwsgi
mkdir /var/run/alerta
chown -R nginx.nginx /var/run/alerta/

nano /var/www/wsgi.py
from alerta import app
</code></pre>

<pre><code>nano /etc/uwsgi.ini
[uwsgi]
chdir = /var/www
mount = /api=wsgi.py
callable = app
manage-script-name = true
env = BASE_URL=/api

master = true
processes = 5
#logger = syslog:alertad
logto = /var/log/uwsgi/%n.log

socket = /var/run/alerta/uwsgi.sock
chmod-socket = 664
uid = nginx
gid = nginx
vacuum = true

die-on-term = true
</code></pre>

<pre><code>nano /etc/systemd/system/alerta-app.service
[Unit]
Description=uWSGI service
After=syslog.target

[Service]
ExecStart=/opt/alerta/bin/uwsgi --ini /etc/uwsgi.ini
RuntimeDirectory=uwsgi
Type=notify
StandardError=syslog
NotifyAccess=all

[Install]
WantedBy=multi-user.target
</code></pre>

<pre><code>systemctl enable alerta-app
systemctl start alerta-app
systemctl status alerta-app
</code></pre>

<h1 id="alerta-webgui">alerta webgui</h1>

<pre><code>wget https://github.com/alerta/alerta-webui/releases/latest/download/alerta-webui.tar.gz
tar zxvf alerta-webui.tar.gz
mv dist/ /usr/share/nginx/alerta
echo '{&quot;endpoint&quot;: &quot;/api&quot;}' &gt; /usr/share/nginx/dist/config.json
</code></pre>

<pre><code>nano /etc/nginx/nginx.conf
    #server {
    #    listen       80 default_server;
    #    listen       [::]:80 default_server;
    #    server_name  _;
    #    root         /usr/share/nginx/html;

    #    # Load configuration files for the default server block.
    #    include /etc/nginx/default.d/*.conf;

     #   location / {
     #   }

     #   error_page 404 /404.html;
     #       location = /40x.html {
     #   }

     #   error_page 500 502 503 504 /50x.html;
     #       location = /50x.html {
     #   }
    #}
</code></pre>

<pre><code>nano /etc/nginx/conf.d/alerta.conf
server {
        listen 80 default_server;

        location /api { try_files $uri @api; }
        location @api {
            include uwsgi_params;
            uwsgi_pass unix:/var/run/alerta/uwsgi.sock;
            proxy_set_header Host $host:$server_port;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }

        location / {
                root /usr/share/nginx/alerta;
        }
}
</code></pre>

<pre><code>nginx -t
systemctl restart nginx
systemctl enable nginx
</code></pre>

<h3 id="test">test</h3>

<p>Send testalert with alerta client.</p>

<pre><code>nano $HOME/.alerta.conf
[DEFAULT]
endpoint = http://localhost/api

/opt/alerta/bin/alerta query
/opt/alerta/bin/alerta send --resource net01 --event down --severity critical --environment Code --service Network --text 'net01 is down.'
/opt/alerta/bin/alerta query
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Restrict access to OpenShift routes by IP address]]></title>
            <link href="https://devopstales.github.io/home/openshift-restrict-access/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/openshift-restrict-access/?utm_source=atom_feed" rel="related" type="text/html" title="Restrict access to OpenShift routes by IP address" />
                <link href="https://devopstales.github.io/home/openshift-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure Openshift vSphere Cloud Provider" />
                <link href="https://devopstales.github.io/cloud/openshift-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure Openshift vSphere Cloud Provider" />
                <link href="https://devopstales.github.io/home/openshift-kompose/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Run docker-compoe in Openshift" />
                <link href="https://devopstales.github.io/cloud/openshift-kompose/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Run docker-compoe in Openshift" />
            
                <id>https://devopstales.github.io/home/openshift-restrict-access/</id>
            
            
            <published>2019-09-20T00:00:00+00:00</published>
            <updated>2019-09-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how can restrict access to the routes by source IP address.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../cloud/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../cloud/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../cloud/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../cloud/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../cloud/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../cloud/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../cloud/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../cloud/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../cloud/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../cloud/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../cloud/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../cloud/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../cloud/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../cloud/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../cloud/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<h3 id="restricting-access-to-a-route">Restricting access to a route</h3>

<p>After creating and exposing a route, you can add an annotation to the route specifying the IP address(es) that you would like to whitelist. Whitelisting a IP address automatically blacklists everything else.</p>

<pre><code>oc annotate route test-route haproxy.router.openshift.io/ip_whitelist=192.168.0.0/24
</code></pre>

<p>To allow several IP addresses through to the route, separate each IP with a space:</p>

<pre><code>oc annotate route test-route haproxy.router.openshift.io/ip_whitelist=192.168.1.10 180.5.61.153 192.168.1.0/24 192.168.0.0/24
</code></pre>

<p>To delete the IPs from the annotation, you can run the command:</p>

<pre><code>oc annotate route test-route haproxy.router.openshift.io/ip_whitelist-
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift: Run docker-compoe in Openshift]]></title>
            <link href="https://devopstales.github.io/home/openshift-kompose/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/openshift-kompose/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Run docker-compoe in Openshift" />
                <link href="https://devopstales.github.io/cloud/openshift-add-node/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Add Nodes to a Cluster" />
                <link href="https://devopstales.github.io/home/openshift-add-node/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Add Nodes to a Cluster" />
                <link href="https://devopstales.github.io/cloud/openshift-letsencrypt/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Letsencrypt certificates" />
                <link href="https://devopstales.github.io/home/openshift-letsencrypt/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Letsencrypt certificates" />
            
                <id>https://devopstales.github.io/home/openshift-kompose/</id>
            
            
            <published>2019-09-08T00:00:00+00:00</published>
            <updated>2019-09-08T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Kompose is a open source tool that uses &ldquo;docker-compose&rdquo; file to deploy on kubernetes. Openshift is also Kubernetes based and Kompose is support Openshift too.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../cloud/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../cloud/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../cloud/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../cloud/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../cloud/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../cloud/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../cloud/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../cloud/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../cloud/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../cloud/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../cloud/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../cloud/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../cloud/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../cloud/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../cloud/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<p>Let’s say we have project with multiple microsevices that needs to deploy on Openshift and they have docker-compose.yml and Dockerfile. How do we deploy on Openshift?<br></p>

<p>Use Kompose to convert the docker-compose file to openshift compatible kubernetes config.</p>

<pre><code>kompose convert -f docker-compose.yaml --provider=openshift
</code></pre>

<p>This command creates a separet yaml file for all kubernetes building block like  &ldquo;-imagestream.yaml&rdquo;, &ldquo;-service.yaml&rdquo;, &ldquo;-deploymentconfig.yaml&rdquo; for each microservice. We can use these config files to deploy on Openshift easily.</p>

<pre><code>kompose up --provider=openshift -f docker-compose.yml --build build-config --namespace=devopstales
</code></pre>

<p>If you have &ldquo;build&rdquo; option in docker-compose file, kompose automatically detects remote git url to deploy automatically.</p>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How to backup Graylog's logs in elasticsearch]]></title>
            <link href="https://devopstales.github.io/home/elasticsearch-backup/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/elasticsearch-backup/?utm_source=atom_feed" rel="related" type="text/html" title="How to backup Graylog&#39;s logs in elasticsearch" />
            
                <id>https://devopstales.github.io/home/elasticsearch-backup/</id>
            
            
            <published>2019-09-07T00:00:00+00:00</published>
            <updated>2019-09-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Graylog store the log data in elasticsearch so I will show you how to create and restore snapshot with elasticsearch.</p>

<h3 id="requirement">Requirement</h3>

<p>First you will need to add the repo.path location to your elasticsearch.yml. This is the local path of the folder where the snapshot files will store.</p>

<pre><code>mkdir -p /mnt/elasticsearch-backup
chown -R elasticsearch. /mnt/elasticsearch-backup

cat &gt;&gt; /etc/elasticsearch/elasticsearch.yml &lt;&lt; EOF
path.repo: [&quot;/mnt/elasticsearch-backup&quot;]
EOF

systemctl restart elasticsearch
</code></pre>

<h3 id="elasticsearch">Elasticsearch</h3>

<p>Elasticsearch needs to know the backup path by registering a backup repository:</p>

<pre><code>curl -XPUT 'http://localhost:9200/_snapshot/my_backup' -d {
  &quot;type&quot;: &quot;fs&quot;,
  &quot;settings&quot;: {
     &quot;location&quot;: &quot;/mnt/elasticsearch-backup&quot;,
     &quot;compress&quot;: true
  }
}'
</code></pre>

<h3 id="create-backup">Create Backup</h3>

<pre><code>curl -XPUT &quot;localhost:9200/_snapshot/my_backup/snapshot_1?wait_for_completion=true&quot;

# list snapshots:
curl -XGET 'localhost:9200/_snapshot/my_backup/_all?pretty'
</code></pre>

<h3 id="restore-backup">Restore backup</h3>

<pre><code>curl -XPOST &quot;localhost:9200/_snapshot/my_backup/snapshot_1/_restore?wait_for_completion=true&quot;
</code></pre>

<h3 id="delete-snapshot">Delete snapshot</h3>

<pre><code>curl -XDELETE 'localhost:9200/_snapshot/my_backup/snapshot_1'
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install MetalLB load balancer for Kubernetes]]></title>
            <link href="https://devopstales.github.io/home/k8s-metallb/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/k8s-metallb/?utm_source=atom_feed" rel="related" type="text/html" title="Install MetalLB load balancer for Kubernetes" />
                <link href="https://devopstales.github.io/home/k8s-nginx-ingress/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
                <link href="https://devopstales.github.io/cloud/k8s-nginx-ingress/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
                <link href="https://devopstales.github.io/home/k8s-tillerless-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Tillerless helm2 install" />
                <link href="https://devopstales.github.io/cloud/k8s-tillerless-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Tillerless helm2 install" />
            
                <id>https://devopstales.github.io/home/k8s-metallb/</id>
            
            
            <published>2019-08-08T00:00:00+00:00</published>
            <updated>2019-08-08T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this tutorial I will show you how to install  Metal LB load balancer running on Kubernetes (k8s).</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../cloud/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../cloud/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../cloud/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../cloud/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part3: <a href="../../cloud/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-traefik-ingress --></li>
<li>Part4: <a href="../../cloud/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../cloud/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../cloud/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../cloud/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../cloud/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a>
<!-- CEPH in Kubernetes with rook --></li>
<li>Part5d: <a href="../../cloud/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes vMware --></li>
<li>Part6: <a href="../../cloud/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part7: <a href="../../cloud/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part8: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part9: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part10: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part11: <a href="../../cloud/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- prometheus operator + grafana operator -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<h3 id="enviroment">Enviroment</h3>

<pre><code>kubectl get node
NAME     STATUS   ROLES    EXTERNAL-IP
host-1   Ready    master   203.0.113.1
host-2   Ready    node     203.0.113.2
host-3   Ready    node     203.0.113.3
host-4   Ready    node     203.0.113.4
</code></pre>

<p>MetalLB provides a network load-balancer implementation for Kubernetes clusters that do not run on a supported cloud provider, effectively allowing the usage of LoadBalancer Services within ber-metal Installation. Kubernetes does not offer an implementation of network load-balancers (Services of type LoadBalancer) for bare metal clusters. The implementations of Network LB that Kubernetes does ship with are all glue code that calls out to various IaaS platforms (GCP, AWS, Azure…). If you’re not running on a supported IaaS platform (GCP, AWS, Azure…), LoadBalancers will remain in the “pending” state indefinitely when created.</p>

<p><img src="/img/include/metallb.jpg" alt="Example image" /></p>

<p>First we need to apply the MetalLB manifest.</p>

<pre><code>kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.8.1/manifests/metallb.yaml
</code></pre>

<p>Create a metallb-configmap.yaml file and modify your IP range accordingly.</p>

<pre><code>cat &lt; EOF &gt; metallb-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: my-ip-space
      protocol: layer2
      addresses:
      - 203.0.113.2-203.0.113.4
EOF

kubectl apply -f metallb-config.yaml
kubectl get pods -n metallb-system
</code></pre>

<p>Exposing a service through the load balancer</p>

<pre><code>cat &lt;EOF&gt;&gt; nginx.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  type: LoadBalancer
  selector:
    app: nginx
  ports:
  - port: 80
    name: http
EOF

kubectl apply -f nginx-deployment.yaml
kubectl get svc
NAME           TYPE           CLUSTER-IP       EXTERNAL-IP    PORT(S)        AGE
nginx          LoadBalancer   10.109.51.83     203.0.113.2    80:30452/TCP   5m
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install s3cmd with CEHP Radosgateway]]></title>
            <link href="https://devopstales.github.io/home/s3cmd-with-radosgw/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/s3cmd-with-radosgw/</id>
            
            
            <published>2019-08-04T00:00:00+00:00</published>
            <updated>2019-08-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>s3cmd is a cli utility for s3.</p>

<pre><code>root@pve1:~# apt-get install s3cmd
root@pve1:~# s3cmd --configure
Access Key: xxxxxxxxxxxxxxxxxxxxxx
Secret Key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

root@pve1:~#  s3cmd mb s3://devopstales
Bucket 's3://devopstales/' created
</code></pre>

<h3 id="create-user">Create User</h3>

<pre><code>adosgw-admin user create --uid={username} --display-name=&quot;{display-name}&quot; \[--email={email}\]
radosgw-admin user create --uid=devopstales --display-name=&quot;devopstales&quot; --email=devopstales@devopstales.intra
</code></pre>

<h3 id="user-info">User Info</h3>

<pre><code>radosgw-admin user info --uid=devopstales
</code></pre>

<h3 id="modify-user">Modify User</h3>

<pre><code>radosgw-admin user modify --uid=devopstales --display-name=&quot;John Doe&quot;
</code></pre>

<h3 id="disable-and-enable-user">Disable and Enable User</h3>

<pre><code>radosgw-admin user suspend --uid=devopstales
radosgw-admin user enable --uid=devopstales
</code></pre>

<h3 id="remove-user">Remove User</h3>

<pre><code>radosgw-admin user rm --uid=devopstales
</code></pre>

<h1 id="add-quota-for-user">Add quota for User</h1>

<pre><code>radosgw-admin quota set --uid=&quot;devopstales@devopstales.intra&quot; --quota-scope=bucket --max-size=30G
radosgw-admin quota enable --quota-scope=bucket --uid=&quot;devopstales@devopstales.intra&quot;
</code></pre>

<h3 id="set-bucket-policy">Set Bucket Policy</h3>

<p>Create a Bucket Policy fot devopstales@devopstales.intra to allow privileges for devopstales Bucket.</p>

<pre><code>cat &lt;EOF&gt; bucket-policy.json
{
  &quot;Version&quot;: &quot;2012-10-17&quot;,
  &quot;Statement&quot;: [{
    &quot;Effect&quot;: &quot;Allow&quot;,
    &quot;Principal&quot;: {&quot;AWS&quot;: [&quot;arn:aws:iam:::user/devopstales2@devopstales.intra&quot;]},
    &quot;Action&quot;: [&quot;s3:ListBucket&quot;, &quot;s3:GetObject&quot;, &quot;s3:PutObject&quot;, &quot;s3:DeleteObject&quot;],
    &quot;Resource&quot;: [
      &quot;arn:aws:s3:::devopstales&quot;,
      &quot;arn:aws:s3:::devopstales/*&quot;
    ]
  }]
}
EOF

s3cmd setpolicy ./bucket-policy.json s3://devopstales
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Tillerless helm2 install]]></title>
            <link href="https://devopstales.github.io/home/k8s-tillerless-helm/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/k8s-tillerless-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Tillerless helm2 install" />
                <link href="https://devopstales.github.io/home/k8s-copy-secret/?utm_source=atom_feed" rel="related" type="text/html" title="Copying Kubernetes Secrets Between Namespaces" />
                <link href="https://devopstales.github.io/cloud/k8s-copy-secret/?utm_source=atom_feed" rel="related" type="text/html" title="Copying Kubernetes Secrets Between Namespaces" />
                <link href="https://devopstales.github.io/cloud/k8s-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/k8s-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD for dynamic provisioning" />
            
                <id>https://devopstales.github.io/home/k8s-tillerless-helm/</id>
            
            
            <published>2019-07-23T00:00:00+00:00</published>
            <updated>2019-07-23T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>It looks like it is not so hard to have Tillerless Helm. So let me go to more details.</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../cloud/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../cloud/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../cloud/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../cloud/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part3: <a href="../../cloud/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-traefik-ingress --></li>
<li>Part4: <a href="../../cloud/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../cloud/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../cloud/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../cloud/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../cloud/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a>
<!-- CEPH in Kubernetes with rook --></li>
<li>Part5d: <a href="../../cloud/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes vMware --></li>
<li>Part6: <a href="../../cloud/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part7: <a href="../../cloud/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part8: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part9: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part10: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part11: <a href="../../cloud/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- prometheus operator + grafana operator -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<p>Since Helm v2, helm got a server part called The Tiller Server which is interacts with the helm client, and the Kubernetes API server. By default helm init installs a Tiller deployment to Kubernetes clusters and communicates via gRPC.</p>

<p><img src="/img/include/tiller1.png" alt="Example image" /></p>

<p>The community voted that Helm v3 should be Tillerless. If we can run tiller localli we can achieve the same goal.</p>

<p><img src="/img/include/tiller2.png" alt="Example image" /></p>

<p>There is a helm plugin for this same purpose.</p>

<pre><code>$ helm plugin install https://github.com/rimusz/helm-tiller
Installed plugin: tiller
</code></pre>

<h3 id="use-this-plugin-locally">Use this plugin locally</h3>

<pre><code>helm tiller start
</code></pre>

<p>It will start the tiller locally and kube-system namespace will be used to store helm releases but you can change the name of the namespace if you want:</p>

<pre><code>helm tiller start my-team-namespace

# stop tiller
helm tiller stop
</code></pre>

<h3 id="how-to-use-this-plugin-in-ci-cd-pipelines">How to use this plugin in CI/CD pipelines</h3>

<pre><code>helm tiller start-ci
export HELM_HOST=localhost:44134
</code></pre>

<p>Then your helm will know where to connect to Tiller and you do not need to make any changes in your CI pipelines.</p>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes Ceph RBD for dynamic provisioning]]></title>
            <link href="https://devopstales.github.io/home/k8s-ceph/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/k8s-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/k8s-nginx-ingress/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
                <link href="https://devopstales.github.io/cloud/k8s-nginx-ingress/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
                <link href="https://devopstales.github.io/cloud/k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install kubernetes with kubeadm" />
                <link href="https://devopstales.github.io/home/k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install kubernetes with kubeadm" />
            
                <id>https://devopstales.github.io/home/k8s-ceph/</id>
            
            
            <published>2019-07-18T00:00:00+00:00</published>
            <updated>2019-07-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how can you use CEPH RBD for persistent storagi on Kubernetes.</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../cloud/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../cloud/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../cloud/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../cloud/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part3: <a href="../../cloud/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-traefik-ingress --></li>
<li>Part4: <a href="../../cloud/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../cloud/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../cloud/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../cloud/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../cloud/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a>
<!-- CEPH in Kubernetes with rook --></li>
<li>Part5d: <a href="../../cloud/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes vMware --></li>
<li>Part6: <a href="../../cloud/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part7: <a href="../../cloud/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part8: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part9: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part10: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part11: <a href="../../cloud/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- prometheus operator + grafana operator -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<h3 id="environment">Environment</h3>

<pre><code># openshift cluster
192.168.1.41  kubernetes01 # master node
192.168.1.42  kubernetes02 # frontend node
192.168.1.43  kubernetes03 # worker node
192.168.1.44  kubernetes04 # worker node
192.168.1.45  kubernetes05 # worker node

# ceph cluster
192.168.1.31    ceph01
192.168.1.32    ceph02
192.168.1.33    ceph03
</code></pre>

<h3 id="prerequirement">Prerequirement</h3>

<p>RBD volume provisioner needs admin key from Ceph to provision storage. To get the admin key from Ceph cluster use this command:</p>

<pre><code>sudo ceph --cluster ceph auth get-key client.admin

nano ceph-admin-secret.yaml
apiVersion: v1
data:
  key: QVFBOFF2SlZheUJQRVJBQWgvS2cwT1laQUhPQno3akZwekxxdGc9PQ==
kind: Secret
metadata:
  name: ceph-admin-secret
  namespace: kube-system
type: kubernetes.io/rbd

</code></pre>

<p>I will also create a separate Ceph pool for</p>

<pre><code>sudo ceph --cluster ceph osd pool create k8s 1024 1024
sudo ceph --cluster ceph auth get-or-create client.k8s mon 'allow r' osd 'allow rwx pool=k8s'
sudo ceph --cluster ceph auth get-key client.k8s

nano ceph-secret-k8s.yaml
apiVersion: v1
data:
  key: QVFBOFF2SlZheUJQRVJBQWgvS2ctS2htOFNSZnRvclJPRk1jdXc9PQ==
kind: Secret
metadata:
  name: ceph-secret-k8s
  namespace: kube-system
type: kubernetes.io/rbd

</code></pre>

<pre><code># on all openshift node
wget http://download.proxmox.com/debian/proxmox-ve-release-5.x.gpg \
-O /etc/apt/trusted.gpg.d/proxmox-ve-release-5.x.gpg
chmod +r /etc/apt/trusted.gpg.d/proxmox-ve-release-5.x.gpg

echo deb http://download.proxmox.com/debian/ceph-luminous $(lsb_release -sc) main \
&gt; /etc/apt/sources.list.d/ceph.list

apt-get update
apt-get install ceph-common -y
</code></pre>

<pre><code>cat &lt;&lt;EOF &gt; rbd-provisioner.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: rbd-provisioner
rules:
  - apiGroups: [&quot;&quot;]
    resources: [&quot;persistentvolumes&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;persistentvolumeclaims&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]
  - apiGroups: [&quot;storage.k8s.io&quot;]
    resources: [&quot;storageclasses&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;events&quot;]
    verbs: [&quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;services&quot;]
#    resourceNames: [&quot;kube-dns&quot;]
    verbs: [&quot;list&quot;, &quot;get&quot;]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: rbd-provisioner
subjects:
  - kind: ServiceAccount
    name: rbd-provisioner
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: rbd-provisioner
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: rbd-provisioner
rules:
- apiGroups: [&quot;&quot;]
  resources: [&quot;secrets&quot;]
  verbs: [&quot;get&quot;]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: rbd-provisioner
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: rbd-provisioner
subjects:
- kind: ServiceAccount
  name: rbd-provisioner
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: rbd-provisioner
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: rbd-provisioner
spec:
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: rbd-provisioner
    spec:
      containers:
      - name: rbd-provisioner
        image: &quot;quay.io/external_storage/rbd-provisioner:v1.0.0-k8s1.10&quot;
        env:
        - name: PROVISIONER_NAME
          value: ceph.com/rbd
      serviceAccount: rbd-provisioner
EOF

kubectl create -n kube-system -f rbd-provisioner.yaml
kubectl get pods -l app=rbd-provisioner -n kube-system
</code></pre>

<p>Please check that <code>quay.io/external_storage/rbd-provisioner:latest</code> image has the same Ceph version as your Ceph cluster.</p>

<pre><code>docker pull quay.io/external_storage/rbd-provisioner:latest
docker history quay.io/external_storage/rbd-provisioner:latest | grep CEPH_VERSION

# pfroxmox ceph use luminous so I'will use v1.0.0-k8s1.10
docker history quay.io/external_storage/rbd-provisioner:v1.0.0-k8s1.10 | grep CEPH_VERSION
&lt;missing&gt;           13 months ago       /bin/sh -c #(nop)  ENV CEPH_VERSION=luminous    0B
</code></pre>

<pre><code># on one openshift master node
nano  k8s-storage.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: &quot;true&quot;
  name: k8s
parameters:
  adminId: admin
  adminSecretName: ceph-admin-secret
  adminSecretNamespace: kube-system
  imageFeatures: layering
  imageFormat: &quot;2&quot;
  monitors: 192.168.1.31.xip.io:6789, 192.168.1.32.xip.io:6789, 192.168.1.33.xip.io:6789
  pool: k8s
  userId: k8s
  userSecretName: ceph-secret-k8s
provisioner: ceph.com/rbd
reclaimPolicy: Delete
volumeBindingMode: Immediate
allowVolumeExpansion: true


kubectl apply -f ceph-admin-secret.yaml
kubectl apply -f ceph-secret-k8s.yaml
kubectl apply -f k8s-storage.yaml
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install clonedeploy pxeboot server]]></title>
            <link href="https://devopstales.github.io/home/clonedeploy/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/clonedeploy/</id>
            
            
            <published>2019-07-17T00:00:00+00:00</published>
            <updated>2019-07-17T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I’ll show you how to create network booting (PXE) with clusterdeploy.</p>

<h3 id="install-web-application">Install Web Application</h3>

<pre><code>yum -y install yum-utils
yum -y install epel-release
rpm --import &quot;http://keyserver.ubuntu.com/pks/lookup?op=get&amp;search=0x3FA7E0328081BFF6A14DA29AA6A19B38D3D831EF&quot;
yum-config-manager --add-repo http://download.mono-project.com/repo/centos7/

yum -y install mono-devel apache2-mod_mono httpd udpcast lz4 mkisofs wget
</code></pre>

<pre><code>wget &quot;https://sourceforge.net/projects/clonedeploy/files/CloneDeploy 1.4.0/clonedeploy-1.4.0.tar.gz&quot;
tar xvzf clonedeploy-1.4.0.tar.gz
cd clonedeploy
cp clonedeploy.conf /etc/httpd/conf.d/
mkdir /var/www/html/clonedeploy
cp -r frontend /var/www/html/clonedeploy
cp -r api /var/www/html/clonedeploy
cp -r tftpboot /

ln -s ../../images /tftpboot/proxy/bios/images
ln -s ../../images /tftpboot/proxy/efi32/images
ln -s ../../images /tftpboot/proxy/efi64/images
ln -s ../../kernels /tftpboot/proxy/bios/kernels
ln -s ../../kernels /tftpboot/proxy/efi32/kernels
ln -s ../../kernels /tftpboot/proxy/efi64/kernels

mkdir -p /cd_dp/images
mkdir /cd_dp/resources
mkdir /var/www/.mono
mkdir /usr/share/httpd/.mono
mkdir /etc/mono/registry
chown -R apache:apache /tftpboot /cd_dp /var/www/html/clonedeploy /var/www/.mono /usr/share/httpd/.mono /etc/mono/registry
chmod 1777 /tmp

sysctl fs.inotify.max_user_instances=1024
echo fs.inotify.max_user_instances=1024 &gt;&gt; /etc/sysctl.conf
chkconfig httpd on
</code></pre>

<h3 id="install-database">Install Database</h3>

<pre><code>echo &quot;[mariadb]
name = MariaDB
baseurl = http://yum.mariadb.org/10.3/centos7-amd64
gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDB
gpgcheck=1&quot; &gt;&gt; /etc/yum.repos.d/MariaDB.repo

yum -y install mariadb-server
service mariadb start
chkconfig mariadb on

mysql

create database clonedeploy;
CREATE USER 'cduser'@'localhost' IDENTIFIED BY 'Password1';
GRANT ALL PRIVILEGES ON clonedeploy.* TO 'cduser'@'localhost';
quit

mysql clonedeploy &lt; cd.sql -v
</code></pre>

<p>Open <code>/var/www/html/clonedeploy/api/Web.config</code> with a text editor and change the following values:<br>
<code>xx_marker1_xx</code> to your cduser database password you created earlier<br>
On that same line change <code>Uid=root to Uid=cduser</code><br>
<code>xx_marker2_xx</code> to some random characters(alphanumeric only), probably should be a minimum of 8<br></p>

<pre><code>service httpd restart
</code></pre>

<h3 id="install-samba-server">Install Samba Server</h3>

<pre><code>yum -y install samba
groupadd cdsharewriters
useradd cd_share_ro
useradd cd_share_rw -G cdsharewriters
usermod -a -G cdsharewriters apache

smbpasswd -a cd_share_ro
smbpasswd -a cd_share_rw

echo &quot;[cd_share]
path = /cd_dp
valid users = @cdsharewriters, cd_share_ro
create mask = 02775
directory mask = 02775
guest ok = no
writable = yes
browsable = yes
read list = @cdsharewriters, cd_share_ro
write list = @cdsharewriters
force create mode = 02775
force directory mode = 02775
force group = +cdsharewriters&quot; &gt;&gt; /etc/samba/smb.conf

chown -R apache:cdsharewriters /cd_dp
chmod -R 2775 /cd_dp
service smb restart
chkconfig smb on
</code></pre>

<h3 id="install-tftp-server">Install TFTP Server</h3>

<pre><code>yum -y install tftp-server
sed -i 's/\/var\/lib\/tftpboot/\/tftpboot -m \/tftpboot\/remap/g' /usr/lib/systemd/system/tftp.service
systemctl daemon-reload
service tftp restart
chkconfig tftp on
</code></pre>

<h3 id="create-firewall-exceptions">Create Firewall Exceptions</h3>

<pre><code>firewall-cmd --permanent --add-service=http
firewall-cmd --permanent --add-service=https
firewall-cmd --permanent --add-service=samba
firewall-cmd --permanent --add-service=tftp
firewall-cmd --permanent --add-service=dhcp
firewall-cmd --permanent --add-service=proxy-dhcp
firewall-cmd --permanent --add-port=9000-10002/udp
service firewalld reload
</code></pre>

<h3 id="post-install-setup">Post Install Setup</h3>

<pre><code># Open the CloneDeploy Web Interface
http://server-ip/clonedeploy

# éogin with
clonedeploy / password

# Upon login you will be greeted with the Initial Setup Page
# Fill out the fields and click Finalize Setup
</code></pre>

<pre><code>yum -y install syslinux vsftpd
cp -v /usr/share/syslinux/pxelinux.0 /tftpboot/
cp -v /usr/share/syslinux/vesamenu.c32 /tftpboot/

mkdir /var/ftp/pub/rhel7
mkdir -p /tftpboot/networkboot/rhel7/

cd /opt
wget  http://files.clonedeploy.org/CloneDeploy-Services.dll
cp CloneDeploy-Services.dll /var/www/html/clonedeploy/api/bin/

wget http://ftp.bme.hu/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1810.iso
wget http://ftp.bme.hu/centos/7/isos/x86_64/sha256sum.txt

sha256sum CentOS-7-x86_64-Minimal-1810.iso
cat sha256sum.txt

mount -o loop /opt/CentOS-7-x86_64-Minimal-1810.iso  /mnt
cp -raf /mnt/* /var/ftp/pub/rhel7
cp /var/ftp/pub/rhel7/images/pxeboot/{initrd.img,vmlinuz} /tftpboot/networkboot/rhel7/

systemctl enable vsftpd.service &amp;&amp; systemctl start vsftpd.service

LABEL CentOS 7 X64
MENU LABEL CentOS 7 X64
kernel /networkboot/rhel7/vmlinuz
append initrd=/networkboot/rhel7/initrd.img inst.repo=ftp://192.168.100.100/pub/rhel7 devfs=nomount
</code></pre>

<hr />

<p><a href="http://clonedeploy.org/docs/create-and-deploy-your-first-image/0">http://clonedeploy.org/docs/create-and-deploy-your-first-image/0</a></p>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install pxeboot server]]></title>
            <link href="https://devopstales.github.io/home/pxe1/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/pxe1/</id>
            
            
            <published>2019-07-16T00:00:00+00:00</published>
            <updated>2019-07-16T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Have you ever needed to troubleshoot or diagnose a problematic computer and you forgot where the utility CD is? We’ll show you how to utilize network booting (PXE) to make that problem a thing of the past.</p>

<h3 id="dhcp">dhcp</h3>

<pre><code>yum install -y dhcp nano

# ha több interfész van
echo &quot;DHCPDARGS=enp0s8&quot; &gt;&gt; /etc/sysconfig/dhcpd

cat &gt; /etc/dhcp/dhcpd.conf &lt;&lt; EOF
#DHCP configuration for PXE boot server
ddns-update-style interim;
ignore client-updates;
authoritative;
allow booting;
allow bootp;
allow unknown-clients;

# A slightly different configuration for an internal subnet.
subnet 192.168.100.0
netmask 255.255.255.0
{
range 192.168.100.101 192.168.100.200;
option domain-name-servers 192.168.100.100;
option routers 192.168.100.100;
default-lease-time 600;
max-lease-time 7200;

# PXE SERVER IP
next-server 192.168.100.100; #  DHCP server ip
filename &quot;pxelinux.0&quot;;
}
EOF

systemctl start dhcpd.service
systemctl enable dhcpd.service
firewall-cmd --permanent --add-service={dhcp,proxy-dhcp}
firewall-cmd --reload
</code></pre>

<pre><code>cd /opt
wget http://ftp.bme.hu/centos/7/isos/x86_64/CentOS-7-x86_64-Minimal-1810.iso
wget http://ftp.bme.hu/centos/7/isos/x86_64/sha256sum.txt

sha256sum CentOS-7-x86_64-Minimal-1810.iso
cat sha256sum.txt

mount -o loop /opt/CentOS-7-x86_64-Minimal-1810.iso  /mnt
</code></pre>

<h3 id="ftp">ftp</h3>

<pre><code>yum install -y tftp-server vsftpd syslinux syslinux-tftpboot

cp -v /usr/share/syslinux/pxelinux.0 /var/lib/tftpboot/
cp -v /usr/share/syslinux/menu.c32 /var/lib/tftpboot/
cp -v /usr/share/syslinux/mboot.c32 /var/lib/tftpboot/
cp -v /usr/share/syslinux/chain.c32 /var/lib/tftpboot/

mkdir /var/lib/tftpboot/pxelinux.cfg
mkdir -p /var/lib/tftpboot/networkboot/rhel7
mkdir /var/ftp/pub/rhel7

cp -raf /mnt/* /var/ftp/pub/rhel7
cp /var/ftp/pub/rhel7/images/pxeboot/{initrd.img,vmlinuz} /var/lib/tftpboot/networkboot/rhel7/
umount /mnt
</code></pre>

<pre><code>cat &gt; /var/lib/tftpboot/pxelinux.cfg/default &lt;&lt; EOF
default menu.c32
prompt 0
timeout 30
menu title mydomail.local PXE Menu

label 1
menu label ^1) Boot from local drive
localboot 0x00

label 2
menu label ^2) CentOS 7 X64
kernel /networkboot/rhel7/vmlinuz
append initrd=/networkboot/rhel7/initrd.img ks=ftp://192.168.100.100/pub/rhel7/ks.cfg inst.repo=ftp://192.168.100.100/pub/rhel7 devfs=nomount

label 3
menu label ^3) CentOS 7 X64
kernel /networkboot/rhel7/vmlinuz
append initrd=/networkboot/rhel7/initrd.img inst.repo=ftp://192.168.100.100/pub/rhel7 devfs=nomount

#label 4
#menu label ^4) Install CentOS 7 x64 with Local Repo using VNC
#kernel /networkboot/rhel7/vmlinuz
#append  initrd=/networkboot/rhel7/initrd.img inst.repo=ftp://192.168.100.100/pub/rhel7 devfs=nomount inst.vnc inst.vncpassword=Aa123456
EOF
</code></pre>

<pre><code>echo '# Use network installation
url --url=&quot;ftp://192.168.100.100/pub/rhel7/&quot;
install

firstboot --disable
selinux --disabled
firewall --disabled
services --enabled=NetworkManager,sshd,chronyd
eula --agreed
ignoredisk --only-use=sda
# Keyboard layouts
keyboard --vckeymap=hu --xlayouts='hu'
# System language
lang en_US.UTF-8

#Text mode installation
text

# Network information
network  --bootproto=dhcp --device=eth0 --hostname=test.mydomain --noipv6 --activate
# Root password
authconfig --enableshadow --enablemd5
# Gen hash: openssl passwd -1 &quot;Password1&quot;
rootpw --iscrypted $1$Skulk114$/QcbiPYHtUnB/rJaAehAH0
# System timezone
timezone Europe/Budapest --ntpservers=0.hu.pool.ntp.org,.hu.pool.ntp.org,2.hu.pool.ntp.org

# System bootloader configuration
bootloader --append=&quot;net.ifnames=0 biosdevname=0&quot; --location=mbr
zerombr

# Disk partitioning information
clearpart --all --drives=sda
part /boot --fstype=ext4 --size=512
part pv.01 --size=1 --grow

volgroup vg00 pv.01
logvol swap --name=lv_swap --vgname=vg00 --size=1024
logvol / --name=lv_01 --vgname=vg00 --size=1 --grow

reboot --eject

%packages
@core
chrony
openssh-clients
net-tools
%end' &gt; /var/ftp/pub/rhel7/ks.cfg
</code></pre>

<pre><code>systemctl start tftp.service
systemctl enable tftp.service
systemctl enable vsftpd.service
systemctl start vsftpd.service

firewall-cmd --permanent --add-service=tftp
firewall-cmd --permanent --add-service=ftp
firewall-cmd --reload
</code></pre>

<p>CLIENT NEEDS AT LEASET 2 GB RAM</p>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Kubernetes nginx ingress with helm]]></title>
            <link href="https://devopstales.github.io/home/k8s-nginx-ingress/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/k8s-nginx-ingress/?utm_source=atom_feed" rel="related" type="text/html" title="Kubernetes nginx ingress with helm" />
                <link href="https://devopstales.github.io/cloud/k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install kubernetes with kubeadm" />
                <link href="https://devopstales.github.io/home/k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install kubernetes with kubeadm" />
                <link href="https://devopstales.github.io/home/k8s-copy-secret/?utm_source=atom_feed" rel="related" type="text/html" title="Copying Kubernetes Secrets Between Namespaces" />
                <link href="https://devopstales.github.io/cloud/k8s-copy-secret/?utm_source=atom_feed" rel="related" type="text/html" title="Copying Kubernetes Secrets Between Namespaces" />
            
                <id>https://devopstales.github.io/home/k8s-nginx-ingress/</id>
            
            
            <published>2019-07-14T00:00:00+00:00</published>
            <updated>2019-07-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how can you use install IngressControllert on Kubernetes with helm.</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../cloud/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../cloud/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../cloud/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../cloud/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part3: <a href="../../cloud/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-traefik-ingress --></li>
<li>Part4: <a href="../../cloud/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../cloud/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../cloud/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../cloud/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../cloud/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a>
<!-- CEPH in Kubernetes with rook --></li>
<li>Part5d: <a href="../../cloud/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes vMware --></li>
<li>Part6: <a href="../../cloud/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part7: <a href="../../cloud/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part8: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part9: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part10: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part11: <a href="../../cloud/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- prometheus operator + grafana operator -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<h3 id="environment">Environment</h3>

<pre><code># openshift cluster
192.168.1.41  kubernetes01 # master node
192.168.1.42  kubernetes02 # frontend node
192.168.1.43  kubernetes03 # worker node
192.168.1.44  kubernetes04 # worker node
192.168.1.45  kubernetes05 # worker node
</code></pre>

<h3 id="helm-with-cluster-admin-permissions">Helm with cluster-admin permissions</h3>

<pre><code>at &lt;&lt;EOF&gt; helm-cluster-admin.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller-admin
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tiller-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: tiller-admin
  namespace: kube-system
EOF
</code></pre>

<h3 id="init-helm">Init Helm</h3>

<pre><code>kubectl create -f helm-cluster-admin.yaml

helm init --service-account helm
kubectl get po --all-namespaces | grep tiller
</code></pre>

<h3 id="tag-node-for-ingress">Tag node for ingress</h3>

<pre><code>kubectl get nodes --show-labels
kubectl label nodes kubernetes02 node-role.kubernetes.io/frontend= --overwrite=true

helm install stable/nginx-ingress \
    --name nginx-ingress \
    --namespace=nginx-ingress \
    --set rbac.create=true \
    --set controller.kind=DaemonSet \
    --set controller.hostNetwork=true \
    --set controller.daemonset.useHostPort=true \
    --set controller.nodeSelector.&quot;node-role\.kubernetes\.io/frontend&quot;= \
    --set controller.stats.enabled=true \
    --set controller.metrics.enabled=true

kubectl --namespace nginx-ingress get services -o wide -w nginx-ingress-controller
kubectl create secret tls default-ingress-tls --key /path/to/private.pem --cert /path/to/cert.pem --namespace nginx-ingress
</code></pre>

<pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta1/aio/deploy/recommended.yaml

kubectl create secret tls default-ingress-tls --key /path/to/private.pem --cert /path/to/cert.pem --namespace kubernetes-dashboard

cat &lt;&lt;EOF&gt; dashboard_ingress.yml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kubernetes-dashboard-ingress
  namespace: kubernetes-dashboard
  annotations:
    kubernetes.io/tls-acme: 'true'
    ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-passthrough: &quot;true&quot;
    nginx.ingress.kubernetes.io/secure-backends: &quot;true&quot;
    ingress.kubernetes.io/ssl-redirect: &quot;true&quot;
    ingress.kubernetes.io/force-ssl-redirect: &quot;true&quot;
spec:
  tls:
  - hosts:
    - dashboard.devopstales.intra
    secretName: default-ingress-tls
  rules:
  - host: dashboard.devopstales.intra
    http:
     paths:
     - backend:
         serviceName: kubernetes-dashboard
         servicePort: 443
EOF

kubectl apply -f dashboard_ingress.yml
</code></pre>

<pre><code>kubectl create serviceaccount dashboard-admin-sa
kubectl create clusterrolebinding dashboard-admin-sa --clusterrole=cluster-admin --serviceaccount=default:dashboard-admin-sa

kubectl get secrets
NAME                  TYPE                                  DATA   AGE
dashboard-admin-sa-token-XXXXX   kubernetes.io/service-account-token   3      22h

kubectl describe secret dashboard-admin-sa-token-XXXXX
Name:         dashboard-admin-sa-token-bq9cr
...
token:      XXXXXXXXXXXXXXXXXXXXXXXXXX

# use this token to login
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install kubernetes with kubeadm]]></title>
            <link href="https://devopstales.github.io/home/k8s-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/k8s-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install kubernetes with kubeadm" />
                <link href="https://devopstales.github.io/cloud/k8s-copy-secret/?utm_source=atom_feed" rel="related" type="text/html" title="Copying Kubernetes Secrets Between Namespaces" />
                <link href="https://devopstales.github.io/home/k8s-copy-secret/?utm_source=atom_feed" rel="related" type="text/html" title="Copying Kubernetes Secrets Between Namespaces" />
                <link href="https://devopstales.github.io/home/k8s-helm-rbac/?utm_source=atom_feed" rel="related" type="text/html" title="RBAC permissions for Helm" />
                <link href="https://devopstales.github.io/cloud/k8s-helm-rbac/?utm_source=atom_feed" rel="related" type="text/html" title="RBAC permissions for Helm" />
            
                <id>https://devopstales.github.io/home/k8s-install/</id>
            
            
            <published>2019-07-12T00:00:00+00:00</published>
            <updated>2019-07-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Kubeadm is a tool that helps you bootstrap a simple Kubernetes cluster and simplifies the deployment process.</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../cloud/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../cloud/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../cloud/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../cloud/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part3: <a href="../../cloud/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-traefik-ingress --></li>
<li>Part4: <a href="../../cloud/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../cloud/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../cloud/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../cloud/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../cloud/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a>
<!-- CEPH in Kubernetes with rook --></li>
<li>Part5d: <a href="../../cloud/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes vMware --></li>
<li>Part6: <a href="../../cloud/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part7: <a href="../../cloud/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part8: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part9: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part10: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part11: <a href="../../cloud/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- prometheus operator + grafana operator -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<pre><code>192.168.1.41  kubernetes01 # master node
192.168.1.42  kubernetes02 # frontend node
192.168.1.43  kubernetes03 # worker node
192.168.1.44  kubernetes04 # worker node
192.168.1.45  kubernetes05 # worker node

# hardware requirement
4 CPU
16G RAM
</code></pre>

<h3 id="install-docker">Install Docker</h3>

<pre><code>apt-get update
apt-get install apt-transport-https ca-certificates curl gnupg2 software-properties-common
curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add -
add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/debian $(lsb_release -cs) stable&quot;
apt-get update
apt-get install docker-ce docker-ce-cli containerd.io
systemct start docker
systemct enable docker
</code></pre>

<h3 id="configuuration">Configuuration</h3>

<pre><code>cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
EOF

echo '1' &gt; /proc/sys/net/ipv4/ip_forward
echo '1' &gt; /proc/sys/net/bridge/bridge-nf-call-iptables
</code></pre>

<h3 id="disable-swap">Disable swap</h3>

<pre><code>free -h
swapoff -a
swapoff -a
sed -i.bak -r 's/(.+ swap .+)/#\1/' /etc/fstab
free -h
</code></pre>

<h3 id="install-kubeadm">Install kubeadm</h3>

<pre><code>apt-get install ebtables ethtool apt-transport-https

curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF

apt-get update &amp;&amp; apt-get install -y kubelet kubeadm kubectl
apt-mark hold kubelet kubeadm kubect
systemctl enable kubelet &amp;&amp; systemctl start kubelet
kubeadm config images pul
</code></pre>

<h3 id="init-master">Init master</h3>

<pre><code>kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=192.168.1.41


mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config


kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/62e44c867a2846fefb68bd5f178daf4da3095ccb/Documentation/kube-flannel.yml
</code></pre>

<h3 id="join-workers-to-cluster">Join workers to cluster</h3>

<pre><code>kubeadm join 192.168.1.41:6443 --token XXXXXXXX \
    --discovery-token-ca-cert-hash sha256:XXXXXXXX
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Ansible Operator Overview]]></title>
            <link href="https://devopstales.github.io/home/ansible-operator-overview/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/ansible-operator-overview/?utm_source=atom_feed" rel="related" type="text/html" title="Ansible Operator Overview" />
                <link href="https://devopstales.github.io/cloud/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
                <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
                <link href="https://devopstales.github.io/home/openshift-add-node/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Add Nodes to a Cluster" />
                <link href="https://devopstales.github.io/cloud/openshift-add-node/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Add Nodes to a Cluster" />
            
                <id>https://devopstales.github.io/home/ansible-operator-overview/</id>
            
            
            <published>2019-07-10T00:00:00+00:00</published>
            <updated>2019-07-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Operators make it easy to manage complex stateful applications on top of Kubernetes. In this pos I will show you how to read an ansible based Openshift operator.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../cloud/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../cloud/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../cloud/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../cloud/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../cloud/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../cloud/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../cloud/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../cloud/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../cloud/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../cloud/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../cloud/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../cloud/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../cloud/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../cloud/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../cloud/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<h3 id="why-an-operator">Why an Operator?</h3>

<p>Operators make it easy to manage complex stateful applications on top of Kubernetes. However writing an operator today can be difficult because of challenges such as using low level APIs, writing boilerplate, and a lack of modularity which leads to duplication.</p>

<h3 id="what-is-an-ansible-operator">What is an Ansible Operator?</h3>

<p>Ansible Operator is one of the available types of Operators that Operator SDK is able to generate. Operator SDK can create an operator using with Golang, Helm, or Ansible.
It is a collection of building blocks from Operator SDK that enables Ansible to handle the reconciliation logic for an Operator.</p>

<h3 id="how-ansible-operator-works">How Ansible Operator works?</h3>

<p>We want to trigger this Ansible logic when a Custom Resource changes. The Ansible Operator uses a Watches file, written in YAML, which holds the mapping between Custom Resources and Ansible Roles/Playbooks. The Operator expects this mapping file in a predefined location: <code>/opt/ansible/watches.yaml</code></p>

<p>Each mapping within the Watches file has mandatory fields:</p>

<ul>
<li>group: Group of the Custom Resource that you will be watching.</li>
<li>version: Version of the Custom Resource that you will be watching.</li>
<li>kind: Kind of the Custom Resource that you will be watching.</li>
<li>role (default): Path to the Role that should be run by the Operator for a particular Group-Version-Kind (GVK). This field is mutually exclusive with the &ldquo;playbook&rdquo; field.</li>
<li>playbook (optional): Path to the Playbook that should be run by the Operator.</li>
</ul>

<h3 id="initialize-new-operator-template">Initialize new operator template</h3>

<pre><code># istall sdk client
brew install operator-sdk

# generate base temlate
operator-sdk new memcached-operator --type=ansible --api-version=cache.example.com/v1alpha1 --kind=Memcached --skip-git-init
cd memcached-operator
</code></pre>

<p>The sdk cli generated the base structure for all the componets. The main parts are the <code>watches.yaml</code> the Dockerfile in <code>build/Dockerfile</code> and the ansible role in <code>roles/</code><br> folder. Fore this tutorial I will use this role <a href="https://github.com/dymurray/memcached-operator-role">https://github.com/dymurray/memcached-operator-role</a>.<br>
In the role we must use k8s ansible module to deploy kubernetes compnets to the cluster.<br></p>

<pre><code>nano roles/memcached/tasks/main.yml
---
- name: start memcached
  k8s:
    definition:
      kind: Deployment
      apiVersion: apps/v1
      metadata:
        name: '{{ meta.name }}-memcached'
        namespace: '{{ meta.namespace }}'
      spec:
        replicas: &quot;{{size}}&quot;
        selector:
          matchLabels:
            app: memcached
        template:
          metadata:
            labels:
              app: memcached
          spec:
            containers:
            - name: memcached
              command:
              - memcached
              - -m=64
              - -o
              - modern
              - -v
              image: &quot;memcached:1.4.36-alpine&quot;
              ports:
                - containerPort: 11211
</code></pre>

<p>You can use variables in ansible Jinja temlate from the CR spec or from kubernetes enviroment like namespace: <code>{{ meta.namespace }}</code>.<br>
Add default value to the <code>{{size}}</code> variable:</p>

<pre><code>nano roles/memcached/defaults/main.yml
---
size: 1
</code></pre>

<h4 id="variable-sharing-example">Variable sharing Example</h4>

<pre><code>apiVersion: &quot;foo.example.com/v1alpha1&quot;
kind: &quot;Foo&quot;
metadata:
  name: &quot;example&quot;
annotations:
  ansible.operator-sdk/reconcile-period: &quot;30s&quot;
  name: &quot;example&quot;
spec:
  message: &quot;Hello world 2&quot;
  newParameter: &quot;newParam&quot;
# associates GVK with Role
role: /opt/ansible/roles/Foo
</code></pre>

<pre><code>- debug:
    msg: &quot;message value from CR spec: {{ message }}&quot;

- debug:
    msg: &quot;newParameter value from CR spec: {{ new_parameter }}&quot;

- debug:
    msg: &quot;name: {{ meta.name }}, namespace: {{ meta.namespace }}&quot;
</code></pre>

<p>The Openshidt SDK created a simple Dockerfile in <code>build/Dockerfile</code> to run the newly created ansible role. We nead to build a docker image from this Dockerfile and use this image in our memcached-operator deployment.</p>

<pre><code># buid image
operator-sdk build memcached-operator:v0.0.1

# Edit deploy/operator.yaml to use the newly created memcached-operator:v0.0.1 docker image
sed -i 's|{{ REPLACE_IMAGE }}|memcached-operator:v0.0.1|g' deploy/operator.yaml

# If we did not want to download the image (besause we build it on the worker or it is representid on all of my workes) we can disable image pulling.
sed -i &quot;s|{{ pull_policy\|default('Always') }}|Never|g&quot; deploy/operator.yaml
</code></pre>

<h3 id="creating-the-operator-from-deploy-manifests">Creating the Operator from deploy manifests</h3>

<pre><code>oc create -f deploy/crds/cache_v1alpha1_memcached_crd.yaml

oc new-project tutorial
oc create -f deploy/service_account.yaml
oc create -f deploy/role.yaml
oc create -f deploy/role_binding.yaml
oc create -f deploy/operator.yaml

oc get deployment
</code></pre>

<p>Now that we have deployed our Operator, let&rsquo;s create a CR and deploy an instance of memcached.
There is a sample CR in the scaffolding created as part of the Operator SDK. Inspect <code>deploy/crds/cache_v1alpha1_memcached_cr.yaml</code>, and then use it to create a Memcached custom resource.</p>

<pre><code>oc create -f deploy/crds/cache_v1alpha1_memcached_cr.yaml



$ oc get deployment
NAME                 DESIRED CURRENT UP-TO-DATE AVAILABLE AGE
memcached-operator   1       1       1          1         2m
example-memcached    3       3       3          3         1m

#Check the pods to confirm 3 replicas were created:

$ oc get pods
NAME                                READY STATUS   RESTARTS AGE
example-memcached-6cc844747c-2hbln  1/1   Running  0        1m
example-memcached-6cc844747c-54q26  1/1   Running  0        1m
example-memcached-6cc844747c-7jfhc  1/1   Running  0        1m
memcached-operator-68b5b558c5-dxjwh 1/1   Running  0        2m
</code></pre>

<h3 id="removing-memcached-from-the-cluster">Removing Memcached from the cluster</h3>

<pre><code># First, delete the 'memcached' CR, which will remove the 4 Memcached pods and the associated deployment.
oc delete -f deploy/crds/cache_v1alpha1_memcached_cr.yaml

# Then, delete the memcached-operator deployment.
oc delete -f deploy/operator.yaml
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Analyzing PFsense logs in Graylog]]></title>
            <link href="https://devopstales.github.io/home/graylog-pfsense/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/graylog-pfsense/</id>
            
            
            <published>2019-07-04T00:00:00+00:00</published>
            <updated>2019-07-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>We will parse the log records generated by the PfSense Firewall. We already have our graylog server running and we will start preparing the terrain to capture those logs records.</p>

<p>Many thanks to opc40772 developed the original contantpack for pfsense log agregation what I updated for the new Graylog3 and Elasticsearch 6.</p>

<h3 id="celebro-localinstall">Celebro localinstall</h3>

<pre><code># celebro van to use port 9000 so change graylog3 bindport
nano /etc/graylog/server/server.conf
http_bind_address = 127.0.0.1:9400
nano /etc/nginx/conf.d/graylog.conf

systemctl restart graylog-server.service
systemctl restart nginx

wget https://github.com/lmenezes/cerebro/releases/download/v0.8.3/cerebro-0.8.3-1.noarch.rpm
yum localinstall cerebro-0.8.3-1.noarch.rpm
</code></pre>

<h3 id="create-indices">Create indices</h3>

<p>We now create the Pfsense indice on Graylog at <code>System / Indexes</code> <br>
<img src="/img/include/graylog_pfsense1.png" alt="image" /> <br></p>

<h3 id="import-index-template-for-elasticsearch-6-x">Import index template for elasticsearch 6.x</h3>

<pre><code>systemctl stop graylog-server.service
</code></pre>

<p>Go to <code>celebro &gt; more &gt; index templates</code>
Create new with name: <code>pfsense-custom</code> and copy the template from file <code>pfsense_custom_template_es6.json</code>
Edit other pfsense template to (sorrend 0)</p>

<p>In Cerebro we stand on top of the pfsense index and unfold the options and select delete index.</p>

<h3 id="geoip-database">Geoip database</h3>

<pre><code>wget -t0 -c http://geolite.maxmind.com/download/geoip/database/GeoLite2-City.tar.gz
tar -xvf GeoLite2-City.tar.gz
cp GeoLite2-City_*/GeoLite2-City.mmdb /etc/graylog/server

systemctl start graylog-server.service
</code></pre>

<p>Enable geoip database at <code>System \ Imput &gt; Configurations &gt; Plugins &gt; Geo-Location Processor &gt; update</code>
Chane the order of the <code>Message Processors Configuration</code></p>

<ul>
<li>AWS Instance Name Lookup</li>
<li>Message Filter Chain</li>
<li>Pipeline Processor</li>
<li>GeoIP Resolver</li>
</ul>

<p>Enable geoip database</p>

<h3 id="import-contantpack">Import contantpack</h3>

<pre><code>git clone https://github.com/devopstales/pfsense-graylog.git
cp service-names-port-numbers.csv /etc/graylog/server/
</code></pre>

<p>import
chaneg date timezone in Pipeline rule
Go tu Stream menu and edit stream <br>
<img src="/img/include/graylog_pfsense2.png" alt="image" /> <br></p>

<p><code>System &gt; Pipelines</code>
Manage rules and then Edit rule (Change the timezone)</p>

<pre><code>rule &quot;timestamp_pfsense_for_grafana&quot;
 when
 has_field(&quot;timestamp&quot;)
then
// the following date format assumes there's no time zone in the string
 let source_timestamp = parse_date(substring(to_string(now(&quot;Europe/Budapest&quot;)),0,23), &quot;yyyy-MM-dd'T'HH:mm:ss.SSS&quot;);
 let dest_timestamp = format_date(source_timestamp,&quot;yyyy-MM-dd HH:mm:ss&quot;);
 set_field(&quot;real_timestamp&quot;, dest_timestamp);
end
</code></pre>

<h3 id="confifure-pfsense">Confifure pfsense</h3>

<p><code>Status &gt; System Logs &gt; Settings</code> <br>
<img src="/img/include/graylog_pfsense3.png" alt="image" /> <br></p>

<h3 id="install-grafana-dashboard">Install grafana Dashboard</h3>

<pre><code># install nececery plugins
grafana-cli plugins install grafana-piechart-panel
grafana-cli plugins install grafana-worldmap-panel
grafana-cli plugins install savantly-heatmap-panel
systemctl restart grafana-server
</code></pre>

<p>Create new datasource: <br>
<img src="/img/include/graylog_pfsense4.png" alt="image" /> <br></p>

<p>Import dashboadr from store: <br>
id: 5420</p>

<hr />

<h5 id="contantpack">Contantpack:</h5>

<p><a href="https://github.com/devopstales/pfsense-graylog">https://github.com/devopstales/pfsense-graylog</a></p>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[SSO login to Grafana]]></title>
            <link href="https://devopstales.github.io/home/grafana-sso/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/grafana-sso/</id>
            
            
            <published>2019-06-28T00:00:00+00:00</published>
            <updated>2019-06-28T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Configurate Gitab to use Keycloak as SSO Identity Proider.</p>

<h3 id="configurate-keycloak">Configurate Keycloak</h3>

<p>Login to Keycloak and create client for Grafana:
<img src="/img/include/gitlab-keycloak1.png" alt="Example image" /></p>

<h3 id="configurate-gitlab">Configurate Gitlab</h3>

<pre><code>nano /etc/grafana/grafana.ini
#################################### Generic OAuth ##########################
[auth.generic_oauth]
enabled = true
name = SSO
allow_sign_up = true
client_id = gitlab
client_secret = 47fd3013-4333-4825-bbfa-b7688548d9cf
scopes = user:email,read:org
auth_url = https://sso.devopstales.intra/auth/realms/gitlab/protocol/openid-connect/auth
token_url = https://sso.devopstales.intra/auth/realms/gitlab/protocol/openid-connect/token
api_url = https://sso.devopstales.intra/auth/realms/gitlab/protocol/openid-connect/userinfo
;team_ids =
;allowed_organizations =

</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift: Add Nodes to a Cluster]]></title>
            <link href="https://devopstales.github.io/home/openshift-add-node/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/openshift-add-node/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: Add Nodes to a Cluster" />
                <link href="https://devopstales.github.io/home/openshift-letsencrypt/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Letsencrypt certificates" />
                <link href="https://devopstales.github.io/cloud/openshift-letsencrypt/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Letsencrypt certificates" />
                <link href="https://devopstales.github.io/home/openshift-hostalreadyclaimed/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Error: HostAlreadyClaimed" />
                <link href="https://devopstales.github.io/cloud/openshift-hostalreadyclaimed/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Error: HostAlreadyClaimed" />
            
                <id>https://devopstales.github.io/home/openshift-add-node/</id>
            
            
            <published>2019-06-27T00:00:00+00:00</published>
            <updated>2019-06-27T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Add Nodes to an existing Cluster.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../cloud/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../cloud/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../cloud/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../cloud/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../cloud/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../cloud/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../cloud/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../cloud/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../cloud/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../cloud/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../cloud/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../cloud/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../cloud/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../cloud/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../cloud/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<p>In the last post I used the basic htpasswd authentication method for the installatipn.<br>
But I can use Ansible-openshift to configure an LDAP backed at the install for the authentication.</p>

<h3 id="environment">Environment</h3>

<pre><code>192.168.1.40    deployer
192.168.1.41    openshift01 # master node
192.168.1.42    openshift02 # infra node
192.168.1.43    openshift03 # worker node
192.168.1.44    openshift04 # new-worker node
192.168.1.45    openshift00 # new-master node
</code></pre>

<p>###</p>

<pre><code>useradd origin
passwd origin
echo -e 'Defaults:origin !requiretty\norigin ALL = (root) NOPASSWD:ALL' | tee /etc/sudoers.d/openshift
chmod 440 /etc/sudoers.d/openshift

# if Firewalld is running, allow SSH

firewall-cmd --add-service=ssh --permanent
firewall-cmd --reload

yum -y install centos-release-openshift-origin36 docker
vgcreate vg_origin01 /dev/sdb1

Volume group &quot;vg_origin01&quot; successfully created
echo VG=vg_origin01 &gt;&gt; /etc/sysconfig/docker-storage-setup
systemctl start docker
systemctl enable docker
</code></pre>

<h3 id="configurate-installer">Configurate Installer</h3>

<pre><code>nano /etc/ansible/hosts
# add into OSEv3 section

[OSEv3:children]
masters
nodes
new_nodes
new_masters
new_etcd

[new_nodes]
openshift04.devopstales.intra openshift_node_group_name='node-config-compute'
openshift00.devopstales.intra openshift_node_group_name='node-config-master'

[new_masters]
openshift00.devopstales.intra openshift_node_group_name='node-config-master'

[new_etcd]
openshift00.devopstales.intra containerized=true
</code></pre>

<h3 id="run-the-installer">Run the Installer</h3>

<p>Run Ansible Playbook for scaleout the Cluster.</p>

<pre><code># deployer
cd /usr/share/ansible/openshift-ansible/

ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-node/scaleup.yml
ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-master/scaleup.yml
ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/openshift-etcd/scaleup.yml
</code></pre>

<h3 id="configurate-installer-1">Configurate Installer</h3>

<p>After finishing to add new Nodes, Open [/etc/ansible/hosts] again and move new definitions to existing [nodes] section like follows.</p>

<pre><code>nano /etc/ansible/hosts
# add into OSEv3 section

[OSEv3:children]
masters
nodes
new_nodes
new_masters

[nodes]
openshift00.devopstales.intra openshift_node_group_name='node-config-master'
openshift01.devopstales.intra openshift_node_group_name='node-config-master'
openshift02.devopstales.intra openshift_node_group_name='node-config-infra'
openshift03.devopstales.intra openshift_node_group_name='node-config-compute'
openshift04.devopstales.intra openshift_node_group_name='node-config-compute

[new_nodes]

[masters]
openshift00.devopstales.intra openshift_node_group_name='node-config-master'
openshift01.devopstales.intra openshift_node_group_name='node-config-master'

[new_masters]

[etcd]
openshift01.devopstales.intra containerized=true
openshift00.devopstales.intra containerized=true

[new_etcd]
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Graylog]]></title>
            <link href="https://devopstales.github.io/home/graylog-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/graylog-install/</id>
            
            
            <published>2019-06-24T00:00:00+00:00</published>
            <updated>2019-06-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Graylog is defined in terms of log management platform for collecting, indexing, and analyzing both structured and unstructured data from almost any source.</p>

<h3 id="install-requirement">Install requirement</h3>

<pre><code>yum install epel-release -y
yum install java-1.8.0-openjdk-headless.x86_64 pwgen -y
java -version
</code></pre>

<h3 id="elasticsearch">Elasticsearch</h3>

<pre><code>rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch

echo '[elasticsearch-6.x]
name=Elasticsearch repository for 6.x packages
baseurl=https://artifacts.elastic.co/packages/oss-6.x/yum
gpgcheck=1
gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
enabled=1
autorefresh=1
type=rpm-md
' | tee /etc/yum.repos.d/elasticsearch.repo

sudo yum -y install elasticsearch-oss

nano /etc/elasticsearch/elasticsearch.yml
cluster.name: graylog

systemctl restart elasticsearch
systemctl enable elasticsearch

curl -XGET 'http://localhost:9200/_cluster/health?pretty=true'
</code></pre>

<h3 id="mongodb">Mongodb</h3>

<pre><code>echo '[mongodb-org-4.0]
name=MongoDB Repository
baseurl=https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/4.0/x86_64/
gpgcheck=1
enabled=1
gpgkey=https://www.mongodb.org/static/pgp/server-4.0.asc' | tee /etc/yum.repos.d/mongodb-org.repo

yum -y install mongodb-org
systemctl restart mongod
systemctl enable  mongod
</code></pre>

<h3 id="graylogv3">Graylogv3</h3>

<pre><code>rpm -Uvh https://packages.graylog2.org/repo/packages/graylog-3.0-repository_latest.rpm
yum -y install graylog-server

SECRET=$(pwgen -s 96 1)
sudo -E sed -i -e 's/password_secret =.*/password_secret = '$SECRET'/' /etc/graylog/server/server.conf
PASSWORD=$(echo -n Password1 | sha256sum | awk '{print $1}')
sudo -E sed -i -e 's/root_password_sha2 =.*/root_password_sha2 = '$PASSWORD'/' /etc/graylog/server/server.conf

nano /etc/graylog/server/server.conf
root_email = &quot;admin@devopstales.intra&quot;
root_timezone = Europe/Budapest
is_master = true
elasticsearch_max_docs_per_index = 20000000
elasticsearch_max_number_of_indices = 20
elasticsearch_shards = 1
elasticsearch_replicas = 0
http_bind_address = 127.0.0.1:9400

systemctl daemon-reload
systemctl restart graylog-server
systemctl enable graylog-server

tailf /var/log/graylog-server/server.log

If everything goes well, you should see below message in the logfile:
2019-06-20T13:37:04.059Z INFO  [ServerBootstrap] Graylog server up and running.
</code></pre>

<h3 id="nginx-proxy">Nginx Proxy</h3>

<pre><code>yum install nginx -y

echo 'server {
    listen 80;
    listen [::]:80 ipv6only=on;
    server_name graylog.devopstales.intra;

    location / {
      proxy_set_header Host $http_host;
      proxy_set_header X-Forwarded-Host $host;
      proxy_set_header X-Forwarded-Server $host;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      proxy_set_header X-Graylog-Server-URL http://$server_name/;
      proxy_pass       http://127.0.0.1:9400;
    }
}' &gt; /etc/nginx/conf.d/graylog.conf

nginx -t
systemctl restart nginx
systemctl enable nginx
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Grafana Loki]]></title>
            <link href="https://devopstales.github.io/home/grafana-loki/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/grafana-loki/?utm_source=atom_feed" rel="related" type="text/html" title="Grafana Loki" />
            
                <id>https://devopstales.github.io/home/grafana-loki/</id>
            
            
            <published>2019-06-22T00:00:00+00:00</published>
            <updated>2019-06-22T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Loki is a Prometheus-inspired logging service for cloud native infrastructure.
It’s similar to well-known ELK stack but more simple use and is intended to be used mostly Kubernetes.</p>

<h3 id="loki-components">Loki components</h3>

<p>Loki-stack consists of three main components:
- promtail – agent to collect logs on a host and push them to a Loki instance
- loki – TSDB (Time-series database) logs aggregation and processing server
- Grafana – for querying and displaying logs</p>

<h3 id="deployment">Deployment</h3>

<pre><code>yum install nginx -y
systemct start nginx

mkdir /opt/loki
cd /opt/loki/
</code></pre>

<pre><code>nano loki-promtail-conf.yml
server:

  http_listen_port: 9080
  grpc_listen_port: 0

positions:

  filename: /tmp/positions.yaml

client:

  url: http://loki:3100/api/prom/push

scrape_configs:

  - job_name: system
    entry_parser: raw
    static_configs:
    - targets:
        - localhost
      labels:
        job: varlogs
        __path__: /var/log/*log

  - job_name: nginx
    entry_parser: raw
    static_configs:
    - targets:
        - localhost
      labels:
        job: nginx
        __path__: /var/log/nginx/*log
</code></pre>

<pre><code>nano docker-compose.yml
version: &quot;3&quot;

networks:
  loki:

services:
  loki:
    image: grafana/loki:master
    ports:
      - &quot;3100:3100&quot;
    command: -config.file=/etc/loki/local-config.yaml
    networks:
      - loki

  promtail:
    image: grafana/promtail:master
    volumes:
       - /opt/loki/loki-promtail-conf.yml:/etc/promtail/docker-config.yaml
      - /var/log:/var/log
    command: -config.file=/etc/promtail/docker-config.yaml
    networks:
      - loki

  grafana:
    image: grafana/grafana:master
    ports:
      - &quot;3000:3000&quot;
    networks:
      - loki
</code></pre>

<pre><code>docker-compose up -d

# add datasource
key=”{job=\”nginx\”}” appeared – all good.
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift SSO with Gitlab]]></title>
            <link href="https://devopstales.github.io/home/openshift-sso-2/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/openshift-sso-2/</id>
            
            
            <published>2019-06-17T00:00:00+00:00</published>
            <updated>2019-06-17T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Configure Openshift Cluster to use Gitlab as a user backend for login with oauth2 and SSO.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../cloud/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../cloud/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../cloud/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../cloud/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../cloud/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../cloud/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../cloud/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../cloud/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../cloud/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../cloud/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../cloud/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../cloud/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../cloud/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../cloud/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../cloud/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<p>With Ansible-openshift you can not change the authetication method after Install !! If you installed the cluster with htpasswd, then change to LDAP the playbook trys to add a second authentication methot for the config. It is forbidden to add a second type of identity provider in the version 3.11 of Ansible-openshift. To solve this problem we must change the configuration manually.</p>

<h3 id="environment">Environment</h3>

<pre><code>192.168.1.40    deployer
192.168.1.41    openshift01 # master node
192.168.1.42    openshift02 # infra node
192.168.1.43    openshift03 # worker node
</code></pre>

<h3 id="configuration-gitlab">Configuration Gitlab</h3>

<p>Login to Gitlab and create client for the app:
<img src="/img/include/openshift_gitlab_sso.png" alt="Example image" /></p>

<h3 id="configurate-the-cluster">Configurate The cluster</h3>

<pre><code># on all openshift hosts
nano /etc/origin/master/master-config.yaml
...
  identityProviders:
  - name: gitlabsso
    challenge: true
    login: true
    mappingMethod: claim
    provider:
      apiVersion: v1
      kind: GitLabIdentityProvider
      legacy: true
      clientID: 7305abce637a123654a2c9dd4f8caec1156a1bc41cd80be4db0f14253fe24e58
      clientSecret: 2d5aebe7831c99383d876cc235febb401906263de748a29b03b058f62f15c2f7
      url: https://gitlab.devopstales.intra/
  - challenge: true
</code></pre>

<h3 id="reconfigurate-the-cluster">Reconfigurate the cluster</h3>

<pre><code># on all openshift hosts
master-restart api
master-restart controllers
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install CEHP Radosgateway on Proxmox]]></title>
            <link href="https://devopstales.github.io/home/proxmox-ceph-radosgw/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/proxmox-ceph-radosgw/</id>
            
            
            <published>2019-06-14T00:00:00+00:00</published>
            <updated>2019-06-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>RADOS Gateway  is an object storage interface in Ceph. It provides interfaces compatible with OpenStack Swift and Amazon S3.</p>

<p>First create a keyring than generated the keys and added them to the keyring:</p>

<pre><code>root@pve1:~# ceph-authtool --create-keyring /etc/ceph/ceph.client.radosgw.keyring

root@pve1:~# ceph-authtool /etc/ceph/ceph.client.radosgw.keyring -n client.radosgw.pve1 --gen-key
root@pve1:~# ceph-authtool /etc/ceph/ceph.client.radosgw.keyring -n client.radosgw.pve2 --gen-key
root@pve1:~# ceph-authtool /etc/ceph/ceph.client.radosgw.keyring -n client.radosgw.pve3 --gen-key
</code></pre>

<p>And then I added the proper capabilities and add the keys to the cluster:</p>

<pre><code>root@pve1:~# ceph-authtool -n client.radosgw.pve1 --cap osd 'allow rwx' --cap mon 'allow rwx' /etc/ceph/ceph.client.radosgw.keyring
root@pve1:~# ceph-authtool -n client.radosgw.pve2 --cap osd 'allow rwx' --cap mon 'allow rwx' /etc/ceph/ceph.client.radosgw.keyring
root@pve1:~# ceph-authtool -n client.radosgw.pve3 --cap osd 'allow rwx' --cap mon 'allow rwx' /etc/ceph/ceph.client.radosgw.keyring

root@pve1:~# ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.radosgw.pve1 -i /etc/ceph/ceph.client.radosgw.keyring
root@pve1:~# ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.radosgw.pve2 -i /etc/ceph/ceph.client.radosgw.keyring
root@pve1:~# ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.radosgw.pve3 -i /etc/ceph/ceph.client.radosgw.keyring
</code></pre>

<p>Copy the rings to the proxmox ClusterFS</p>

<pre><code>root@pve1:~# cp /etc/ceph/ceph.client.radosgw.keyring /etc/pve/priv
</code></pre>

<p>Add the following lines to <code>/etc/ceph/ceph.conf</code>:</p>

<pre><code>[client.radosgw.pve1]
        host = pve1
        keyring = /etc/pve/priv/ceph.client.radosgw.keyring
        log file = /var/log/ceph/client.radosgw.$host.log
        rgw_dns_name = s3.devopstales.intra

[client.radosgw.pve2]
        host = pve2
        keyring = /etc/pve/priv/ceph.client.radosgw.keyring
        log file = /var/log/ceph/client.radosgw.$host.log
        rgw_dns_name = s3.devopstales.intra

[client.radosgw.pve3]
        host = pve3
        keyring = /etc/pve/priv/ceph.client.radosgw.keyring
        log file = /var/log/ceph/client.rados.$host.log
        rgw_dns_name = s3.devopstales.intra
</code></pre>

<p>Install the pcakages and start the service. If all goes well, RADOSGW will create some default pools for you.</p>

<pre><code>root@pve1:~# apt install radosgw
root@pve1:~# service radosgw start

root@pve1:~# tail -f /var/log/ceph/client.rados.pve1.log
</code></pre>

<pre><code>root@pve1:~# ceph osd pool application enable .rgw.root rgw
root@pve1:~# ceph osd pool application enable default.rgw.control rgw
root@pve1:~# ceph osd pool application enable default.rgw.data.root rgw
root@pve1:~# ceph osd pool application enable default.rgw.gc rgw
root@pve1:~# ceph osd pool application enable default.rgw.log rgw
root@pve1:~# ceph osd pool application enable default.rgw.users.uid rgw
root@pve1:~# ceph osd pool application enable default.rgw.users.email rgw
root@pve1:~# ceph osd pool application enable default.rgw.users.keys rgw
root@pve1:~# ceph osd pool application enable default.rgw.buckets.index rgw
root@pve1:~# ceph osd pool application enable default.rgw.buckets.data rgw
root@pve1:~# ceph osd pool application enable default.rgw.lc rgw
</code></pre>

<pre><code>root@pve1:~#  ssh pve2 'apt install radosgw &amp;&amp; service radosgw start'
root@pve1:~#  ssh pve3 'apt install radosgw &amp;&amp; service radosgw start'

root@pve1:~#  ceph osd pool ls
</code></pre>

<pre><code>root@pve1:~# radosgw-admin user create --uid=devopstales --display-name=&quot;devopstales&quot; --email=devopstales@devopstales.intra
root@pve1:~# radosgw-admin user info devopstales

root@pve1:~# ceph osd pool application enable default.rgw.buckets.index rgw
root@pve1:~# ceph osd pool application enable default.rgw.buckets.data rgw
</code></pre>

<pre><code>root@pve1:~# apt-get install s3cmd
root@pve1:~# s3cmd --configure
Access Key: xxxxxxxxxxxxxxxxxxxxxx
Secret Key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

root@pve1:~#  s3cmd mb s3://devopstales
Bucket 's3://devopstales/' created
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Prometheus for Gitlab]]></title>
            <link href="https://devopstales.github.io/home/install-prometheus-for-gitlab/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/install-prometheus-for-gitlab/</id>
            
            
            <published>2019-06-10T00:00:00+00:00</published>
            <updated>2019-06-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Prometheus is an open-source monitoring system with a built-in noSQL time-series database. It offers a multi-dimensional data model, a flexible query language, and diverse visualization possibilities. Prometheus collects metrics from http nedpoint. Most service dind’t have this endpoint so you need optional programs that generate additional metrics cald exporters.</p>

<h3 id="install-prometheus-from-package">Install prometheus from package</h3>

<pre><code>curl -s https://packagecloud.io/install/repositories/prometheus-rpm/release/script.rpm.sh | sudo bash

yum install prometheus2 alertmanager -y
</code></pre>

<h3 id="configurate-prometheus">Configurate Prometheus</h3>

<pre><code>nano /etc/prometheus/promethsu.yml
---
global:
  scrape_interval: 15s
  scrape_timeout: 15s
  evaluation_interval: 15s

alerting:
  alertmanagers:
  - static_configs:
    - targets:
       - localhost:9095

rule_files:
  - &quot;/etc/prometheus/alert.rules&quot;

scrape_configs:
- job_name: prometheus
  static_configs:
  - targets:
    - localhost:9090
- job_name: redis
  static_configs:
  - targets:
    - localhost:9121
- job_name: postgres
  static_configs:
  - targets:
    - localhost:9187
- job_name: node
  static_configs:
  - targets:
    - localhost:9100
- job_name: gitlab-workhorse
  static_configs:
  - targets:
    - localhost:9229
- job_name: gitlab-unicorn
  metrics_path: &quot;/-/metrics&quot;
  static_configs:
  - targets:
    - 127.0.0.1:8080
- job_name: gitlab-sidekiq
  static_configs:
  - targets:
    - 127.0.0.1:8082
- job_name: gitlab_monitor_database
  metrics_path: &quot;/database&quot;
  static_configs:
  - targets:
    - localhost:9168
- job_name: gitlab_monitor_sidekiq
  metrics_path: &quot;/sidekiq&quot;
  static_configs:
  - targets:
    - localhost:9168
- job_name: gitlab_monitor_process
  metrics_path: &quot;/process&quot;
  static_configs:
  - targets:
    - localhost:9168
- job_name: gitaly
  static_configs:
  - targets:
    - localhost:9236
- job_name: nginx
  static_configs:
  - targets:
    - localhost:9913
- job_name: 'playframework-app'
  scrape_interval: 5s
  metrics_path: '/metrics'
  static_configs:
  - targets: ['localhost:9000']
</code></pre>

<pre><code>nano /etc/prometheus/alert.rules
groups:
- name: host
  rules:
  - alert: low_connected_users
    expr: play_current_users &lt; 2
    for: 30s
    labels:
      severity: slack
    annotations:
      summary: &quot;Instance {{ $labels.instance }} under lower load&quot;
      description: &quot;{{ $labels.instance }} of job {{ $labels.job }} is under lower load.&quot;
</code></pre>

<p><img src="/img/include/mattermost_gitlab4.png" alt="Example image" /><br><br>
<img src="/img/include/mattermost_gitlab5.png" alt="Example image" /><br><br>
<img src="/img/include/mattermost_gitlab6.png" alt="Example image" /><br><br>
<img src="/img/include/mattermost_gitlab7.png" alt="Example image" /><br><br>
<img src="/img/include/mattermost_gitlab8.png" alt="Example image" /><br><br></p>

<pre><code>nano /etc/prometheus/alertmanager.yml
global:

templates:
- '/etc/prometheus/template/*.tmpl'

route:
 group_by: [alertname, job]
 # If an alert isn't caught by a route, send it to slack.
 receiver: slack_general
 routes:
  - match:
      severity: slack
    receiver: slack_general

receivers:
- name: slack_general
  slack_configs:
  - api_url: http://mattermost.devopstales.intra/hooks/9g4qwgpkzi898jzzeszzzzutmc
    channel: 'monitoring'
    username: &quot;prometheus&quot; #name ins mattermost
    text: &quot;&quot;
    send_resolved: true
</code></pre>

<pre><code>mkdir /etc/prometheus/template/
nano /etc/prometheus/template/alertmessage.tmpl
{{ define &quot;__slack_text&quot; }}
{{ range .Alerts }}{{ .Annotations.description}}{{ end }}
{{ end }}

{{ define &quot;__slack_title&quot; }}
{{ range .Alerts }} :scream: {{ .Annotations.summary}} :scream: {{ end }}
{{ end }}

{{ define &quot;slack.default.text&quot; }}{{ template &quot;__slack_text&quot; . }}{{ end }}
{{ define &quot;slack.default.title&quot; }}{{ template &quot;__slack_title&quot; . }}{{ end }}
</code></pre>

<h3 id="configurate-gitlab">Configurate Gitlab</h3>

<p><img src="/img/include/mattermost_gitlab1.png" alt="Example image" /><br><br>
<img src="/img/include/mattermost_gitlab2.png" alt="Example image" /><br><br>
<img src="/img/include/mattermost_gitlab3.png" alt="Example image" /><br><br></p>

<pre><code>nano /etc/gitlab/gitlab.rb
alertmanager['enable'] = false
prometheus['enable'] = false
node_exporter['enable'] = true
redis_exporter['enable'] = true
postgres_exporter['enable'] = true
gitlab_monitor['enable'] = true

gitlab-ctl reconfigure

systemctl start alertmanager.service
systemctl status alertmanager.service
systemctl start prometheus.service
systemctl status prometheus.service
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install mattermost for Gitlab]]></title>
            <link href="https://devopstales.github.io/home/install-mattermost-for-gitlab/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/install-mattermost-for-gitlab/?utm_source=atom_feed" rel="related" type="text/html" title="Install mattermost for Gitlab" />
            
                <id>https://devopstales.github.io/home/install-mattermost-for-gitlab/</id>
            
            
            <published>2019-06-09T00:00:00+00:00</published>
            <updated>2019-06-09T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Mattermost is an open source on premise alternative of Slack.</p>

<h3 id="install-mattermost-from-package">Install mattermost from package</h3>

<pre><code>sudo yum -y install https://harbottle.gitlab.io/harbottle-main/7/x86_64/harbottle-main-release.rpm

yum install mattermost-server -y
</code></pre>

<h3 id="configurate-mattermost">Configurate mattermost</h3>

<pre><code>nano /etc/mattermost/config.json
# postgresql
&quot;SiteURL&quot;: &quot;http://mattermost.devopstales.intra&quot;
&quot;DriverName&quot;: &quot;postgres&quot;,
&quot;DataSource&quot;: &quot;postgres://mmuser:Password1@localhost:5432/mattermost?sslmode=disable&amp;connect_timeout=10&quot;

# gitlab austh
&quot;AllowedUntrustedInternalConnections&quot;: &quot;gitlab.devopstales.intra&quot;
...
    &quot;GitLabSettings&quot;: {
        &quot;Enable&quot;: true,
        &quot;Secret&quot;: &quot;&lt;secret&gt;&quot;,
        &quot;Id&quot;: &quot;&lt;id&gt;&quot;,
        &quot;Scope&quot;: &quot;&quot;,
        &quot;AuthEndpoint&quot;: &quot;http://gitlab.devopstales.intra/oauth/authorize&quot;,
        &quot;TokenEndpoint&quot;: &quot;http://gitlab.devopstales.intra/oauth/token&quot;,
        &quot;UserApiEndpoint&quot;: &quot;http://gitlab.devopstales.intra/api/v4/user&quot;
    },
</code></pre>

<h3 id="edit-systemd-serice">Edit systemd serice</h3>

<pre><code>nano /usr/lib/systemd/system/mattermost.service
[Unit]
Description=Mattermost
After=syslog.target network.target
After=postgresql.service
Requires=postgresql-9.6.service

[Service]
Type=notify
NotifyAccess=main
WorkingDirectory=/usr/share/mattermost
User=mattermost
Group=mattermost
ExecStart=/usr/share/mattermost/bin/mattermost
TimeoutStartSec=3600
LimitNOFILE=49152

[Install]
WantedBy=multi-user.target

systemctl daemon-reload
systemctl start mattermost.service
</code></pre>

<h3 id="configurate-nginx-proxy">Configurate nginx proxy</h3>

<pre><code>nano /etc/nginx/conf.d/mattermost.conf
upstream backend {
   server localhost:8065;
   keepalive 32;
}

proxy_cache_path /var/cache/nginx levels=1:2 keys_zone=mattermost_cache:10m max_size=3g inactive=120m use_temp_path=off;

server {
   listen 80;
   server_name    mattermost.devopstales.intra;

   location ~ /api/v[0-9]+/(users/)?websocket$ {
       proxy_set_header Upgrade $http_upgrade;
       proxy_set_header Connection &quot;upgrade&quot;;
       client_max_body_size 50M;
       proxy_set_header Host $http_host;
       proxy_set_header X-Real-IP $remote_addr;
       proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
       proxy_set_header X-Forwarded-Proto $scheme;
       proxy_set_header X-Frame-Options SAMEORIGIN;
       proxy_buffers 256 16k;
       proxy_buffer_size 16k;
       client_body_timeout 60;
       send_timeout 300;
       lingering_timeout 5;
       proxy_connect_timeout 90;
       proxy_send_timeout 300;
       proxy_read_timeout 90s;
       proxy_pass http://backend;
   }

   location / {
       client_max_body_size 50M;
       proxy_set_header Connection &quot;&quot;;
       proxy_set_header Host $http_host;
       proxy_set_header X-Real-IP $remote_addr;
       proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
       proxy_set_header X-Forwarded-Proto $scheme;
       proxy_set_header X-Frame-Options SAMEORIGIN;
       proxy_buffers 256 16k;
       proxy_buffer_size 16k;
       proxy_read_timeout 600s;
       proxy_cache mattermost_cache;
       proxy_cache_revalidate on;
       proxy_cache_min_uses 2;
       proxy_cache_use_stale timeout;
       proxy_cache_lock on;
       proxy_http_version 1.1;
       proxy_pass http://backend;
   }
}
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Gitlab Install]]></title>
            <link href="https://devopstales.github.io/home/gitlab-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/gitlab-install/</id>
            
            
            <published>2019-06-05T00:00:00+00:00</published>
            <updated>2019-06-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Install Gitab with custom postgresql and nginx proxy.</p>

<h3 id="install-ntpd">Install NTPD</h3>

<pre><code>yum install -y epel-release yum-utils
yum-config-manager --enable epel

sudo chkconfig ntpd on
sudo ntpdate 0.hu.pool.ntp.org
sudo service ntpd start
</code></pre>

<h3 id="install-postgresql">Install postgresql</h3>

<p>In a previous post I wrote about how to <a href="/linux/install-postgresql/#install-postgresql-10-on-centos-7">Install PostgreSQL 10</a></p>

<pre><code>su - postgres
createdb -U postgres gitlab
createdb -U postgres gitlab_ci
createdb -U postgres mattermost

createuser gituser
createuser ciuser
createuser mmuser

psql -U postgres gitlab
ALTER USER &quot;gituser&quot; WITH PASSWORD 'Password1';
GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO gituser;
ALTER ROLE gituser CREATEROLE SUPERUSER;
\q

psql -U postgres gitlab_ci
ALTER USER &quot;ciuser&quot; WITH PASSWORD 'Password1';
GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO ciuser;
\q

psql -U postgres mattermost
ALTER USER &quot;mmuser&quot; WITH PASSWORD 'Password1';
GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO mmuser;
\q
</code></pre>

<h3 id="install-nginx">install nginx</h3>

<pre><code>yum install -y nginx
mkdir /var/log/gitlab/nginx/

echo 'upstream gitlab-workhorse {
        server unix:/var/opt/gitlab/gitlab-workhorse/socket;
}

server {
        listen *:80;
#  listen *:443 ssl;
        server_name gitlab.devopstales.intra;
        server_tokens off;
        root /opt/gitlab/embedded/service/gitlab-rails/public;

        client_max_body_size 256m;

        real_ip_header X-Forwarded-For;

        access_log /var/log/gitlab/nginx/gitlab_access.log;
        error_log  /var/log/gitlab/nginx/gitlab_error.log;

 # ssl_certificate   /etc/nginx/ssl.d/gitlab.pem;
 # ssl_certificate_key   /etc/nginx/ssl.d/gitlab.key;


        location / {
                proxy_read_timeout	300;
                proxy_connect_timeout   300;
                proxy_redirect          off;

                proxy_buffering off;

                proxy_set_header    Host                $http_host;
                proxy_set_header    X-Real-IP           $remote_addr;
                proxy_set_header    X-Forwarded-For     $proxy_add_x_forwarded_for;
                proxy_set_header    X-Forwarded-Proto   $scheme;

                proxy_pass http://gitlab-workhorse;

    proxy_request_buffering off;
    proxy_http_version 1.1;
        }

        location ~ ^/(assets)/ {
                root /opt/gitlab/embedded/service/gitlab-rails/public;
                gzip_static on; # to serve pre-gzipped version
                        expires max;
               	add_header Cache-Control public;
        }

        error_page 502 /502.html;
}' &gt; /etc/nginx/conf.d/gitlab.conf

nano /etc/nginx/nginx.conf
log_format gitlab_access '$remote_addr - $remote_user [$time_local] &quot;$request&quot; $status $body_bytes_sent &quot;$http_referer&quot; &quot;$http_user_agent&quot;';
log_format gitlab_ci_access '$remote_addr - $remote_user [$time_local] &quot;$request&quot; $status $body_bytes_sent &quot;$http_referer&quot; &quot;$http_user_agent&quot;';
log_format gitlab_mattermost_access '$remote_addr - $remote_user [$time_local] &quot;$request&quot; $status $body_bytes_sent &quot;$http_referer&quot; &quot;$http_user_agent&quot;';

nginx -t
nginx -s reload

sudo usermod -aG gitlab-www nginx
</code></pre>

<h3 id="install-gitlab">install gitlab</h3>

<pre><code>curl -sS https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh | sudo bash
sudo yum install gitlab-ce -y

cd /opt/gitlab/embedded/bin
mv psql psql_moved
mv pg_dump pg_dump_moved

which pg_dump psql

ln -s /usr/bin/pg_dump /usr/bin/psql /opt/gitlab/embedded/bin/
</code></pre>

<pre><code>nano /etc/gitlab/gitlab.rb
external_url 'http://gitlab.devopstales.intra'
gitlab_rails['time_zone'] = 'Europe/Budapest'

gitlab_rails['db_adapter'] = &quot;postgresql&quot;
gitlab_rails['db_encoding'] = &quot;unicode&quot;
gitlab_rails['db_database'] = &quot;gitlab&quot;
gitlab_rails['db_pool'] = 10
gitlab_rails['db_username'] = &quot;gituser&quot;
gitlab_rails['db_password'] = &quot;Password1&quot;
gitlab_rails['db_host'] = '127.0.0.1'
gitlab_rails['db_port'] = 5432

gitlab_rails['redis_socket'] = &quot;/var/opt/gitlab/redis/redis.socket&quot;

unicorn['socket'] = '/var/opt/gitlab/gitlab-rails/sockets/gitlab.socket'
unicorn['log_directory'] = &quot;/var/log/gitlab/unicorn&quot;
unicorn['port'] = 8081

user['username'] = &quot;git&quot;
user['group'] = &quot;git&quot;

postgresql['enable'] = false

nginx['enable'] = false
web_server['external_users'] = ['nginx']
</code></pre>

<pre><code>gitlab-ctl reconfigure
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Override a single external hostname with internal DNS-entry]]></title>
            <link href="https://devopstales.github.io/home/migrate-bind-to-windows-dns/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/migrate-bind-to-windows-dns/</id>
            
            
            <published>2019-06-04T00:00:00+00:00</published>
            <updated>2019-06-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Migrate dns Zones from bind to Windows DNS Server</p>

<h3 id="linux">Linux</h3>

<pre><code>nano /etc/bind/zones.active
allow-transfer { 192.168.0.8; };
</code></pre>

<h3 id="windows">Windows</h3>

<pre><code>Add-DnsServerSecondaryZone -Name &quot;devopstales.intra&quot; -ZoneFile &quot;devopstales.intra.dns&quot; -MasterServers 192.168.0.60
ConvertTo-DnsServerPrimaryZone -Name &quot;devopstales.intra&quot; -PassThru -Verbose -ZoneFile &quot;devopstales.intra.dns&quot; -Force
Set-DnsServerPrimaryZone -Name &quot;devopstales.intra&quot; –Notify Notifyservers –notifyservers &quot;192.168.0.5&quot; -SecondaryServers &quot;192.168.0.5&quot; –SecureSecondaries TransferToSecureServers
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Override a single external hostname with internal DNS-entry]]></title>
            <link href="https://devopstales.github.io/home/override-a-single-external-hostname-with-internal-dns-entry/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/override-a-single-external-hostname-with-internal-dns-entry/</id>
            
            
            <published>2019-06-04T00:00:00+00:00</published>
            <updated>2019-06-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Override a single external hostname with internal DNS-entry</p>

<h3 id="problem">Problem:</h3>

<p>Company.com has an exernal dns-record for service.company.com which should be resolved to an inernal IP by internal clients.</p>

<p>Let’s say that service.company.com resolves to 1.1.1.1 by the external DNS but when computers are connecting to this URL from inside the company network the internal DNS servers at ad.company.com needs to resolve service.company.com to 172.16.51.25.</p>

<p>Adding an entry to the hosts-file on each client computer to override service.company.com will not work when clients connect on exteral networks like from home or a coffeeshop.</p>

<h3 id="solution">Solution:</h3>

<p>The solution is to add a new Forward Lookup Zone named service.company.com and add a new Host-record, enter the internal IP-address but leave the Name blank.</p>

<p>On a DNS server running Windows Server 2012 this is of course achieved by using PowerShell!</p>

<p>First off, create a new DNS Forward Lookup Zone using PowerShell:</p>

<pre><code>Add-DnsServerPrimaryZone -Name service.company.com -ReplicationScope Forest

#Then add a host record to the zone:

Add-DnsServerResourceRecordA -IPv4Address 172.16.51.25 -ZoneName service.company.com -Name service.company.com
</code></pre>

<p>By specifying service.company.com as both ZoneName and Name a record with the name “(same as parent folder)” will be created.</p>

<p>This will only override DNS queries for the FQDN service.company.com and will not affect other records in company.com</p>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Sonarqube Install]]></title>
            <link href="https://devopstales.github.io/home/sonarkube-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/sonarkube-install/</id>
            
            
            <published>2019-05-30T00:00:00+00:00</published>
            <updated>2019-05-30T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Sonarqube Repository OSS is an artifact repository with universal support for popular formats.</p>

<pre><code>yum install epel-release -y
yum update -y
</code></pre>

<h3 id="install-postgresql">Install Postgresql</h3>

<p>In a previous post I wrote about how to <a href="/linux/install-postgresql/#install-postgresql-9-6-on-centos-7">Install PostgreSQL 10</a></p>

<pre><code>su - postgres
createuser -S sonar
createdb -O sonar sonar
psql
ALTER USER &quot;sonar&quot; WITH PASSWORD 'Password1';
\q
exit
</code></pre>

<h3 id="install-sonarqube">Install Sonarqube</h3>

<pre><code># https://binaries.sonarsource.com/Distribution/sonarqube/
cd /opt
wget https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-7.6.zip
unzip sonarqube-7.6.zip
ln -s /opt/sonarqube-7.6 /opt/sonarqube

nano /opt/sonarqube/conf/sonar.properties
sonar.jdbc.username=sonar
sonar.jdbc.password=Password1
sonar.jdbc.url=jdbc:postgresql://localhost/sonar

/opt/sonarqube/bin/linux-x86-64/sonar.sh
RUN_AS_USER=sonar

adduser -s /bin/false sonar
chown -R sonar:sonar /opt/sonarqube/

sysctl -w vm.max_map_count=262144
sysctl -w fs.file-max=65536
ulimit -n 65536
ulimit -u 4096
</code></pre>

<h3 id="create-sistemd-serice-for-sonarqube">Create sistemd serice for Sonarqube</h3>

<pre><code>echo '[Unit]
Description=Sonar
After=network.target network-online.target
Wants=network-online.target

[Service]
ExecStart=/opt/sonarqube/bin/linux-x86-64/sonar.sh start
ExecStop=/opt/sonarqube/bin/linux-x86-64/sonar.sh stop
ExecReload=/opt/sonarqube/bin/linux-x86-64/sonar.sh restart
PIDFile=/opt/sonarqube/bin/linux-x86-64/./SonarQube.pid
Type=forking
User=sonar
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target' &gt; /etc/systemd/system/sonar.service
</code></pre>

<h3 id="start-sonarqube">Start Sonarqube</h3>

<pre><code>sudo systemctl daemon-reload
sudo systemctl enable sonar.service
sudo systemctl start sonar.service

tailf /opt/sonarqube/logs/es.log
### To check, point your browser to http://localhost:9000. Default username is admin with password admin.
</code></pre>

<h3 id="apache-proxy">Apache proxy</h3>

<pre><code>echo 'ProxyRequests Off
ProxyPreserveHost On
&lt;VirtualHost *:80&gt;
  ServerName sonar.devopstales.intra
  ServerAdmin admin@somecompany.com
  ProxyPass / http://localhost:9000/
  ProxyPassReverse / http://localhost:9000/
  ErrorLog /var/log/sonar-error.log
  CustomLog /var/log/sonar-access.log common
&lt;/VirtualHost&gt;' &gt; /etc/httpd/conf.d/sonar.conf

systemctl start httpd
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift Letsencrypt certificates]]></title>
            <link href="https://devopstales.github.io/home/openshift-letsencrypt/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/openshift-letsencrypt/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Letsencrypt certificates" />
                <link href="https://devopstales.github.io/cloud/openshift-hostalreadyclaimed/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Error: HostAlreadyClaimed" />
                <link href="https://devopstales.github.io/home/openshift-hostalreadyclaimed/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Error: HostAlreadyClaimed" />
                <link href="https://devopstales.github.io/cloud/openshift-gitlabrunner/?utm_source=atom_feed" rel="related" type="text/html" title="Install Gitlab runner on Openshift" />
                <link href="https://devopstales.github.io/home/openshift-gitlabrunner/?utm_source=atom_feed" rel="related" type="text/html" title="Install Gitlab runner on Openshift" />
            
                <id>https://devopstales.github.io/home/openshift-letsencrypt/</id>
            
            
            <published>2019-05-28T00:00:00+00:00</published>
            <updated>2019-05-28T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Thanks to Tomáš Nožička developed openshift-acme as an ACME Controller for OpenShift and Kubernetes clusters. <br>
It automatically provision certficates</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../cloud/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../cloud/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../cloud/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../cloud/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../cloud/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../cloud/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../cloud/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../cloud/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../cloud/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../cloud/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../cloud/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../cloud/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../cloud/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../cloud/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../cloud/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<h3 id="environment">Environment</h3>

<pre><code>192.168.1.40    deployer
192.168.1.41    openshift01 # master node
192.168.1.42    openshift02 # infra node
192.168.1.43    openshift03 # worker node
</code></pre>

<h3 id="deploy-route">Deploy route</h3>

<pre><code>oc project default

oc label node openshift02.devopstales.intra &quot;router=letsencrypt&quot;
oc get node --show-labels

oc adm policy add-scc-to-user hostnetwork -z router
oc adm router router-letsencrypt --replicas=0 --ports=&quot;8080:8080,8443:8443&quot; --stats-port=1937 --selector=&quot;router=letsencrypt&quot; --labels=&quot;router=letsencrypt&quot;

oc set env dc/router-letsencrypt \
NAMESPACE_LABELS=&quot;router=letsencrypt&quot; \
ROUTER_ALLOW_WILDCARD_ROUTES=true \
ROUTER_SERVICE_HTTP_PORT=8080 \
ROUTER_SERVICE_HTTPS_PORT=8443 \
ROUTER_TCP_BALANCE_SCHEME=roundrobin

oc set env dc/router NAMESPACE_LABELS=&quot;router != letsencrypt&quot;

oc scale dc/router-letsencrypt --replicas=3
</code></pre>

<h3 id="deploy-letsencrypt">Deploy letsencrypt</h3>

<pre><code>GIT_REPO=https://raw.githubusercontent.com/devopstales/openshift-examples
GIT_PATH=/master/letsencrypt
oc new-project letsencrypt
oc create -f$GIT_REPO/$GIT_PATH/{clusterrole,serviceaccount,imagestream,deployment}.yaml
oc adm policy add-cluster-role-to-user openshift-acme -z openshift-acme
</code></pre>

<h1 id="demo">Demo</h1>

<pre><code>oc new-project test
oc label namespace test router=letsencrypt
oc new-app centos/ruby-25-centos7~https://github.com/sclorg/ruby-ex.git
oc expose svc/ruby-ex

oc patch route ruby-ex \
    -p '{&quot;metadata&quot;:{&quot;annotations&quot;:{  &quot;kubernetes.io/tls-acme&quot; : &quot;true&quot;   }}}'
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configure OpenVPN HA opnsense cluster]]></title>
            <link href="https://devopstales.github.io/home/opnsense-openvpn/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/opnsense-openvpn/</id>
            
            
            <published>2019-05-25T00:00:00+00:00</published>
            <updated>2019-05-25T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this LAB I&rsquo;ll be creating OpenVPN SSL Peer to Peer connection.</p>

<h3 id="the-architecture">The Architecture</h3>

<pre><code> ------ WAN ------
 |               |
PF1 -- sync -- PF2
 |               |
 ----- LAN -------  

WAN: 192.168.0.0/24 (Bridgelt)
LAN: 192.168.20.0/24
SYNC: 192.168.30.0/24
</code></pre>

<pre><code>opn01:
WAN 192.168.0.28
LAN: 192.168.20.28
SYNC:192.168.30.28

opn02:
WAN 192.168.0.29
LAN: 192.168.20.29
SYNC:192.168.30.29
</code></pre>

<h3 id="configurate-the-opevpn-service">Configurate the OpeVPN service</h3>

<p>Got to <code>VPN &gt; OpenVPN &gt; Wizards</code>
<img src="/img/include/opnsense_ovpn1.png" alt="Example image" /></p>

<p>If you ulodad your certificate seledt that in the drop doew menu or select Add new Certificate to generate a new one.
<img src="/img/include/opnsense_ovpn2.png" alt="Example image" /></p>

<p><img src="/img/include/opnsense_ovpn3.png" alt="Example image" /></p>

<p>Edit the Adwanced Configuration: <br>
<img src="/img/include/opnsense_ovpn4.png" alt="Example image" /></p>

<p><img src="/img/include/opnsense_ovpn5.png" alt="Example image" /></p>

<p><img src="/img/include/opnsense_ovpn6.png" alt="Example image" /></p>

<h3 id="configurate-nat-rules-to-ha">Configurate NAT Rules to HA</h3>

<p>Go to <code>Firewall &gt; NAT &gt; Outbound</code> and clone the manul LAN Rule
<img src="/img/include/opnsense_ovpn8.png" alt="Example image" /></p>

<h3 id="enable-connection-from-openvpn-to-master-and-slave">Enable Connection from OpenVPN to master and slave</h3>

<p>In default there in no rout to the salve nod. <br>
 Go to <code>Firewll &gt; Aliases &gt; Add</code> and create alias for CARP members: <br>
<img src="/img/include/opnsense_ovpn7.png" alt="Example image" /></p>

<p>Then go back to <code>Firewall &gt; NAT &gt; Outbound &gt; Settings</code> and create a new rule:
<img src="/img/include/opnsense_ovpn9.png" alt="Example image" /></p>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configure opnsense nextcloud backup]]></title>
            <link href="https://devopstales.github.io/home/opnsense-nextcloud/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/opnsense-nextcloud/</id>
            
            
            <published>2019-05-25T00:00:00+00:00</published>
            <updated>2019-05-25T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this LAB I&rsquo;ll be configurate the opnsense cloud backup solutuon for nextcloud.</p>

<h3 id="configurate-the-nextcloud">Configurate the nextcloud</h3>

<ul>
<li>login to your Nextcloud instance with the admin account</li>

<li><p>go to users
<img src="/img/include/nextcloud_sso1.png" alt="Example image" /></p></li>

<li><p>create a new user for opnsense
<img src="/img/include/opnsense_nextcloud2.png" alt="Example image" /></p></li>

<li><p>login with tehe new user and go to <code>profiele &gt; settings &gt; seurity</code></p></li>

<li><p>create token for user
<img src="/img/include/opnsense_nextcloud3.png" alt="Example image" /></p></li>
</ul>

<h3 id="configurate-opnsense-backup">Configurate opnsense backup</h3>

<ul>
<li>login to opnsense</li>
<li>go to <code>system &gt; config &gt; bckups</code></li>
<li>enable the nextclod config
<img src="/img/include/opnsense_nextcloud4.png" alt="Example image" /></li>
<li>click the <code>setup/test nextcloud</code> button</li>
</ul>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Nextcloud SSO]]></title>
            <link href="https://devopstales.github.io/home/nextcloud-sso/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/nextcloud-sso/</id>
            
            
            <published>2019-05-25T00:00:00+00:00</published>
            <updated>2019-05-25T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Nextcloud is a suite of client-server software for creating and using file hosting services. Nextcloud application functionally is similar to Dropbox.</p>

<h2 id="configuring-keycloak-and-nextcloud">Configuring Keycloak and Nextcloud</h2>

<h3 id="keycloak-side">Keycloak side</h3>

<ul>
<li>login to keycloak using the admin account</li>
<li>Under <code>Clients</code>, create a new client with <code>Client ID</code> &ldquo;nextcloud&rdquo; and <code>Root URL</code> &ldquo;cloud.devopstales.intra&rdquo;</li>
<li>On next screen, under the <code>Settings</code> tab, change <code>Access Type</code> from <code>public</code> to <code>confidential</code>, then Save</li>
<li>Go the the <code>Credentials</code> tab, note the <code>Secret</code></li>
<li>OPTIONAL: If there is no registered user yet you can create a test user: go to <code>Users</code>, click the <code>Add User</code> button, fill the <code>Username</code> with &ldquo;test&rdquo; and save. Then go to the <code>Credentials</code> tab, put the new password, toggle the <code>Temporary</code> option to <code>OFF</code>, press <code>Reset Password</code> and confirm</li>
</ul>

<p>Keycloak is now ready to be used for Nextcloud.</p>

<h3 id="nextcloud-side">NextCloud side</h3>

<ul>
<li>login to your Nextcloud instance with the admin account</li>
<li>Click on the user profile, then <code>Apps</code>
<img src="/img/include/nextcloud_sso1.png" alt="Example image" /><br><br></li>
<li>Go to <code>Social &amp; communication</code> and install the <code>Social Login</code> app</li>
<li>Go to <code>Settings</code> (in your user profile) the <code>Social Login</code><br><br>
<img src="/img/include/nextcloud_sso2.png" alt="Example image" /><br><br></li>
<li>Add a new <code>Custom OpenID Connect</code> by clicking on the <code>+</code> to its side</li>
<li>Fill the following:

<ul>
<li><code>Title</code> -&gt; &ldquo;keycloak&rdquo;</li>
<li><code>Authorize url</code> -&gt; <code>https://keycloak.devopstales.intra:8443/auth/realms/mydomain/protocol/openid-connect/auth</code></li>
<li><code>Token url</code> -&gt; <code>https:/keycloak.devopstales.intra:8443/auth/realms/mydomain/protocol/openid-connect/token</code></li>
<li><code>Client id</code> -&gt; &ldquo;nextcloud&rdquo;</li>
<li><code>Client Secret</code> -&gt; put the secret you noted down during the Keycloak configuration</li>
<li><code>Scope</code> -&gt; &ldquo;openid&rdquo;</li>
</ul></li>
<li>Press <code>Save</code></li>
</ul>

<p>Your Nextcloud instance is now configured. Log out and log back in using the <code>Alternative Logins -&gt; keycloak</code> method on the login page. It should redirect you to a keycloak auth form where you can log in with a registered keycloak user, then back to Nextcloud where you are now logged.
<img src="/img/include/nextcloud_sso3.png" alt="Example image" /><br><br></p>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configurate HA opnsense cluster]]></title>
            <link href="https://devopstales.github.io/home/opnsense-ha/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/opnsense-ha/</id>
            
            
            <published>2019-05-24T00:00:00+00:00</published>
            <updated>2019-05-24T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will configure 2 opnsense server to a HA cluster.</p>

<h3 id="the-architecture">The Architecture</h3>

<pre><code> ------ WAN ------
 |               |
PF1 -- sync -- PF2
 |               |
 ----- LAN -------  

WAN: 192.168.0.0/24 (Bridgelt)
LAN: 192.168.20.0/24
SYNC: 192.168.30.0/24
</code></pre>

<pre><code>opn01:
WAN 192.168.0.28
LAN: 192.168.20.28
SYNC:192.168.30.28

opn02:
WAN 192.168.0.29
LAN: 192.168.20.29
SYNC:192.168.30.29
</code></pre>

<h3 id="firewall-rules-for-sync">Firewall rules For sync</h3>

<p>On both firewalls add two rules to allow traffic on the SYNC interface: <br>
go to <code>Firewall &gt; Rules &gt; Sync</code> and click <code>Add</code>.</p>

<p>Rule 1:
<img src="/img/include/opnsense_carp1.png" alt="Example image" /></p>

<p>Rule 2:
<img src="/img/include/opnsense_carp2.png" alt="Example image" /></p>

<p>Rule 3:
<img src="/img/include/opnsense_carp3.png" alt="Example image" /></p>

<h3 id="synchronization-settings">Synchronization Settings</h3>

<p>Go to <code>System &gt; High Availalility &gt; Settings</code>. Configure the sections like on the pictures.</p>

<p>Master:
<img src="/img/include/opnsense_carp4.png" alt="Example image" /></p>

<p>Slave:
<img src="/img/include/opnsense_carp5.png" alt="Example image" /></p>

<p>Test the synchronisation. Go to <code>System &gt; User management</code> and createa new user on the master node. <br>
Then check on the slave node.</p>

<p>If it doesn&rsquo;t work, check:</p>

<ul>
<li>Are the firewall web interfaces running on the same protocols and ports?</li>
<li>Is the admin password set correctly? (User Manager &gt; Users &gt; admin.)</li>
<li>Are the firewall rules to allow synch set to use the correct interface (SYNC)?</li>
<li>If you&rsquo;re using VMs, are the firewalls on the same internal network?</li>
</ul>

<h3 id="create-virtual-ips">create virtual IPs</h3>

<p>On the master node go to<code>Firewall &gt; Virtual IPs &gt; Settings</code> and click Add. Create a new VIP adres for LAN and WAN interfaces.</p>

<p>WAN VIP on master:
<img src="/img/include/opnsense_carp6.png" alt="Example image" /></p>

<p>LAN VIP on master:
<img src="/img/include/opnsense_carp7.png" alt="Example image" /></p>

<h3 id="change-outbound-nat">Change outbound NAT</h3>

<p>Change the configuration of the outbound NAT to use the shared public IP (the WAN VIP) <br>
Go to <code>Firewall &gt; NAT &gt; Outbound</code> and set the mode to Hybrid Outbound NAT rule generation. <br> <br>
Create a new Outbound rule like this: <br>
<img src="/img/include/opnsense_carp8.png" alt="Example image" /></p>

<p>The translatino / target must be the WANIP IP. <br>
It should end up looking like this: <br></p>

<p><img src="/img/include/opnsense_carp8.png" alt="Example image" /></p>

<p>If you’ll be using your opnsense firewall as a DNS resolver you must change the settings of the DNS service (<code>Services &gt; DNS Resolver &gt; General Settings</code>) to lissen on the LAN VIP address. Then chnage the address of the DNS server in the DHCP configuration to us the LAN VIP adress.</p>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Sonatype Nexus SSO]]></title>
            <link href="https://devopstales.github.io/home/nexus-sso/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/nexus-sso/</id>
            
            
            <published>2019-05-23T00:00:00+00:00</published>
            <updated>2019-05-23T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Nexus Repository OSS is an artifact repository with universal support for popular formats.</p>

<h3 id="install-nexus">Install Nexus</h3>

<pre><code>cd /opt
wget https://download.sonatype.com/nexus/3/latest-unix.tar.gz
tar xvf latest-unix.tar.gz -C /opt
ln -s /opt/nexus-3.16.1-02/ /opt/nexus

adduser -s /bin/false nexus
chown -R nexus:nexus /opt/nexus
chown -R nexus:nexus /opt/sonatype-work/

echo 'run_as_user=&quot;nexus&quot;' &gt; /opt/nexus/bin/nexus.rc

nano /opt/nexus/bin/nexus
INSTALL4J_JAVA_HOME_OVERRIDE=/usr/lib/jvm/jre-1.8.0
</code></pre>

<h3 id="create-sistemd-serice-for-nexus">Create sistemd serice for Nexus</h3>

<pre><code>echo '[Unit]
Description=nexus service
After=network.target

[Service]
Type=forking
LimitNOFILE=65536
ExecStart=/opt/nexus/bin/nexus start
ExecStop=/opt/nexus/bin/nexus stop
User=nexus
Restart=on-abort

[Install]
WantedBy=multi-user.target' &gt; /etc/systemd/system/nexus.service
</code></pre>

<h3 id="start-nexus">Start Nexus</h3>

<pre><code>sudo systemctl daemon-reload
sudo systemctl enable nexus.service
sudo systemctl start nexus.service

tailf /opt/sonatype-work/nexus3/log/nexus.log
### To check, point your browser to http://localhost:8081. Default username is admin with password admin123.
</code></pre>

<h3 id="install-keycloak-authentication-plugin">Install Keycloak authentication plugin</h3>

<pre><code>NEXUS_PLUGINS=/opt/nexus/system
KEYCLOAK_PLUGIN_VERSION=0.3.3-SNAPSHOT
cd /opt
mkdir -p ${NEXUS_PLUGINS}/org/github/flytreeleft/nexus3-keycloak-plugin/${KEYCLOAK_PLUGIN_VERSION}/
cd ${NEXUS_PLUGINS}/org/github/flytreeleft/nexus3-keycloak-plugin/${KEYCLOAK_PLUGIN_VERSION}/
wget https://github.com/flytreeleft/nexus3-keycloak-plugin/releases/download/${KEYCLOAK_PLUGIN_VERSION}/nexus3-keycloak-plugin-${KEYCLOAK_PLUGIN_VERSION}.jar
chmod 644 ${NEXUS_PLUGINS}/org/github/flytreeleft/nexus3-keycloak-plugin/${KEYCLOAK_PLUGIN_VERSION}/nexus3-keycloak-plugin-${KEYCLOAK_PLUGIN_VERSION}.jar
echo &quot;mvn\\:org.github.flytreeleft/nexus3-keycloak-plugin/${KEYCLOAK_PLUGIN_VERSION} = 200&quot; &gt;&gt; /opt/nexus/etc/karaf/startup.properties
</code></pre>

<p>Login to your Keycloak, and navigate relm &gt; client
<img src="/img/include/nexus_sso1.png" alt="Example image" /><br><br>
Configurate Service Account Roles
<img src="/img/include/nexus_sso2.png" alt="Example image" /><br><br>
Configurate User Roles
<img src="/img/include/nexus_sso4.png" alt="Example image" /><br><br></p>

<pre><code>nano /opt/nexus/etc/keycloak.json
{
  &quot;realm&quot;: &quot;mydomain&quot;,
  &quot;auth-server-url&quot;: &quot;http://nexus.devopstales.intra:8080/auth&quot;,
  &quot;ssl-required&quot;: &quot;external&quot;,
  &quot;resource&quot;: &quot;web&quot;,
  &quot;credentials&quot;: {
    &quot;secret&quot;: &quot;41e39b6b-e23a-4fb1-be21-d30c02941ffc&quot;
  },
  &quot;confidential-port&quot;: 0
}

systemct restart nexus
</code></pre>

<p>After login to nexus you can navigate to the realm administration. Activate the Keycloak Authentication Realm plugin by dragging it to the right hand side.
<img src="/img/include/nexus_sso3.png" alt="Example image" /><br><br>
Mapp the Keycloak roles to nexus
<img src="/img/include/nexus_sso5.png" alt="Example image" /><br><br>
Go to server administration &gt; system &gt; capabilities &gt; add <br>
type: Ruth auth <br>
HTTP Header: X-Proxy-REMOTE-USER <br>
<img src="/img/include/nexus_sso6.png" alt="Example image" /><br><br></p>

<pre><code>yum install mod_auth_openidc httpd mod_ssl -y
nano /etc/httpd/conf.d/nexus-site.conf
ProxyRequests Off
ProxyPreserveHost On

&lt;VirtualHost *:80&gt;
    ServerName nexus.devopstales.intra
     Redirect permanent / https://nexus.devopstales.intra
    ErrorLog /var/log/httpd/error.log
    CustomLog /var/log/httpd/access.log common
&lt;/VirtualHost&gt;

&lt;VirtualHost *:443&gt;
    ServerAdmin webmaster@example.com
    ServerName nexus.devopstales.intra
    ServerAlias www.nexus.devopstales.intra
    DirectoryIndex index.html index.php

    SSLEngine on
    SSLCertificateFile /etc/httpd/ssl/domain.pem
    SSLCertificateKeyFile /etc/httpd/ssl/domain.pem
    SSLCertificateChainFile /etc/httpd/ssl/domain.pem

    AllowEncodedSlashes NoDecode
    AllowEncodedSlashes On
    RequestHeader set X-Forwarded-Proto &quot;https&quot;

    # keycloak
    OIDCProviderMetadataURL https://nexus.devopstales.intra:8443/auth/realms/mydomain/.well-known/openid-configuration
    OIDCSSLValidateServer Off
    OIDCClientID web
    OIDCClientSecret 41e39b6b-e23a-4fb1-be21-d30c02941ffc
    OIDCRedirectURI https://nexus.devopstales.intra/redirect_uri
    OIDCCryptoPassphrase passphrase
    OIDCJWKSRefreshInterval 3600
    OIDCScope &quot;openid email profile&quot;
    # maps the prefered_username claim to the REMOTE_USER environment variable
    OIDCRemoteUserClaim preferred_username

    &lt;Location /&gt;
      AuthType openid-connect
      Require valid-user
      RequestHeader set &quot;X-Proxy-REMOTE-USER&quot; %{REMOTE_USER}s
      ProxyPass http://localhost:8081/ nocanon
      ProxyPassReverse http://localhost:8081/
    &lt;/Location&gt;

    ErrorLog /var/log/httpd/error.log
    CustomLog /var/log/httpd/access.log common
&lt;/VirtualHost&gt;

# secure neus server
nano /opt/nexus/etc/nexus-default.properties
application-host=127.0.0.1
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[SSO for hashicorp vault]]></title>
            <link href="https://devopstales.github.io/home/hashicorp-sso/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/hashicorp-sso/</id>
            
            
            <published>2019-05-20T00:00:00+00:00</published>
            <updated>2019-05-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I wil shiw you hiw to configure Hashicorp vault with Keycloak for SSO.</p>

<pre><code>vault auth enable oidc

vault write auth/oidc/config \
    oidc_discovery_url=&quot;https://sso.devopstales.intra/auth/realms/mydomain&quot; \
    oidc_client_id=&quot;web&quot; \
    oidc_client_secret=&quot;07d66ebd-1018-46c6-9c88-80aa3d4c2f68&quot; \
    default_role=&quot;reader&quot;
</code></pre>

<pre><code>vault write auth/oidc/role/reader \
        bound_audiences=&quot;web&quot; \
        allowed_redirect_uris=&quot;http://192.168.0.112:8200/ui/vault/auth/oidc/oidc/callback&quot; \
        allowed_redirect_uris=&quot;http://192.168.0.112:8250/oidc/callback&quot; \
        user_claim=&quot;sub&quot; \
        policies=&quot;reader&quot;
</code></pre>

<pre><code>nano reader.hcl
# Read permission on the k/v secrets
path &quot;/secret/*&quot; {
    capabilities = [&quot;read&quot;, &quot;list&quot;]
}

nano manager.hcl
# Manage k/v secrets
path &quot;/secret/*&quot; {
    capabilities = [&quot;create&quot;, &quot;read&quot;, &quot;update&quot;, &quot;delete&quot;, &quot;list&quot;]
}
</code></pre>

<pre><code>vault policy write reader reader.hcl
vault policy write manager manager.hcl

vault policy list
</code></pre>

<p><img src="/img/include/vault-oidc.png" alt="Example image" /></p>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install hashicorp vault]]></title>
            <link href="https://devopstales.github.io/home/hashicorp-vault/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/hashicorp-vault/</id>
            
            
            <published>2019-05-17T00:00:00+00:00</published>
            <updated>2019-05-17T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Hashicorp vault is a highly scalable, highly available, environment agnostic way to generate, manage, and store secrets.</p>

<h3 id="dowload-vault">Dowload  Vault</h3>

<pre><code># https://releases.hashicorp.com/vault/
cd /opt
VAULT_VERSION=&quot;1.1.2&quot;
curl -sO https://releases.hashicorp.com/vault/${VAULT_VERSION}/vault_${VAULT_VERSION}_linux_amd64.zip

unzip vault_${VAULT_VERSION}_linux_amd64.zip
mv vault /usr/bin/
vault --version
</code></pre>

<pre><code>vault -autocomplete-install
complete -C /usr/bin/vault vault
mkdir /etc/vault
mkdir -p /var/lib/vault/data

sudo useradd --system --home /etc/vault --shell /bin/false vault
sudo chown -R vault:vault /etc/vault /var/lib/vault/
</code></pre>

<h3 id="configure-vault-systemd-service">Configure Vault systemd service</h3>

<pre><code>nano /etc/systemd/system/vault.service
[Unit]
Description=vault server
Requires=network-online.target
After=network-online.target
ConditionFileNotEmpty=/etc/vault/config.hcl

[Service]
User=vault
Group=vault
Restart=on-failure
ExecStart=/usr/bin/vault server -config=/etc/vault
ExecStop=/usr/bin/vault step-down
#ExecReload=/bin/kill --signal HUP $MAINPID

[Install]
WantedBy=multi-user.target
</code></pre>

<h3 id="create-vault-config">Create vault config</h3>

<pre><code>nano /etc/vault/config.hcl
disable_cache = true
disable_mlock = true
ui = true
listener &quot;tcp&quot; {
   address          = &quot;0.0.0.0:8200&quot;
   tls_disable      = 1
}
storage &quot;file&quot; {
   path  = &quot;/var/lib/vault/data&quot;
 }
api_addr         = &quot;http://0.0.0.0:8200&quot;
max_lease_ttl         = &quot;10h&quot;
default_lease_ttl    = &quot;10h&quot;
cluster_name         = &quot;vault&quot;
raw_storage_endpoint     = true
disable_sealwrap     = true
disable_printable_check = true
</code></pre>

<pre><code>systemctl daemon-reload
systemctl enable --now vault
systemctl status vault
</code></pre>

<h3 id="configurate-client">Configurate Client</h3>

<pre><code>export VAULT_ADDR=http://127.0.0.1:8200
echo &quot;export VAULT_ADDR=http://127.0.0.1:8200&quot; &gt;&gt; ~/.bashrc

sudo rm -rf  /var/lib/vault/data/*
vault operator init &gt; /etc/vault/init.file

cat /etc/vault/init.file | grep &quot;Initial Root Token:&quot;
export VAULT_TOKEN=&quot;s.RcW0LuNIyCoTLWxrDPtUDkCw&quot;

### go to gou and un seal with 3 keys

vault status
</code></pre>

<h3 id="configurate-user-base-authentication">Configurate user-base authentication</h3>

<pre><code>vault auth enable userpass
vault write auth/userpass/users/devopstales \
    password=Password1 \
    policies=admins
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Rundeck SSO]]></title>
            <link href="https://devopstales.github.io/home/rundeck-sso/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/rundeck-sso/</id>
            
            
            <published>2019-05-14T00:00:00+00:00</published>
            <updated>2019-05-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will use Preauthenticated Mode for Rundeck with mod_auth_openidc and Keycloak</p>

<p>“Preauthenticated” means that the user name and role list are provided to Rundeck from another system, usually a reverse proxy set up “in front” of the Rundeck web application, such as Apache HTTPD.</p>

<h3 id="configurate-mapping-in-keycloak">Configurate mapping in Keycloak</h3>

<p>Login to Keycloak and create client for the app:
<img src="/img/include/gitlab-keycloak1.png" alt="Example image" /></p>

<p>At Mappers create mappers for user information:</p>

<ul>
<li>name: Audience

<ul>
<li>mapper type: Audience</li>
<li>Included Client Audience: web</li>
<li>Add to ID token: on</li>
</ul></li>
<li>name: groups

<ul>
<li>Mapper Type: Group Membership</li>
<li>Token Claim Name: groups</li>
<li>Full group path: off</li>
</ul></li>
</ul>

<h3 id="install-httpd">Install httpd</h3>

<pre><code>yum install mod_auth_openidc httpd php mod_ssl -y
</code></pre>

<h3 id="configurate-rundeck">Configurate Rundeck</h3>

<pre><code>nano /etc/rundeck/rundeck-config.properties
grails.serverURL=https://oauth.devopstales.intra
# Pre Auth mode settings
rundeck.security.authorization.preauthenticated.enabled=true
rundeck.security.authorization.preauthenticated.attributeName=REMOTE_USER_GROUPS
rundeck.security.authorization.preauthenticated.delimiter=;
# Header from which to obtain user name
rundeck.security.authorization.preauthenticated.userNameHeader=X-Forwarded-User
# Header from which to obtain list of roles
rundeck.security.authorization.preauthenticated.userRolesHeader=X-Forwarded-Roles
</code></pre>

<h3 id="create-vhost">Create vhost</h3>

<pre><code>nano /etc/httpd/conf.d/outh-site.conf
# NameVirtualHost *:80
&lt;VirtualHost *:80&gt;
   ServerName oauth.devopstales.intra
   DocumentRoot /var/www/oauth/
   Redirect permanent / https://oauth.devopstales.intra
&lt;/VirtualHost&gt;

&lt;VirtualHost *:443&gt;
    ServerAdmin webmaster@example.com
    ServerName oauth.devopstales.intra
    ServerAlias www.oauth.devopstales.intra
    DocumentRoot /var/www/oauth/
    DirectoryIndex index.html index.php
    ErrorLog /var/log/httpd/oauth-error.log
    CustomLog /var/log/httpd/oauth-access.log combined

    SSLEngine on
    SSLCertificateFile /etc/httpd/ssl/domain.pem
    SSLCertificateKeyFile /etc/httpd/ssl/domain.pem
    SSLCertificateChainFile /etc/httpd/ssl/domain.pem

    # keycloak
    OIDCProviderMetadataURL https://sso.devopstales.intra/auth/realms/mydomain/.well-known/openid-configuration
    OIDCSSLValidateServer Off
    OIDCClientID web
    OIDCClientSecret 6b6c68c3-ad51-4124-ac37-4784ed58797e
    OIDCRedirectURI https://oauth.devopstales.intra/redirect_uri
    OIDCCryptoPassphrase passphrase
    OIDCJWKSRefreshInterval 3600
    OIDCScope &quot;openid email profile&quot;
    # maps the prefered_username claim to the REMOTE_USER environment variable
    OIDCRemoteUserClaim preferred_username

    ProxyPreserveHost On

    &lt;Location /&gt;
        AuthType openid-connect
            Require valid-user
        RequestHeader set &quot;X-Forwarded-User&quot; %{REMOTE_USER}s
        RequestHeader set &quot;X-Forwarded-Roles&quot; %{OIDC_CLAIM_groups}e
        ProxyPass  http://localhost:4440/
        ProxyPassReverse http://localhost:4440/
    &lt;/Location&gt;

&lt;/VirtualHost&gt;
</code></pre>

<h3 id="start-apache">Start apache</h3>

<pre><code>systemct start httpd
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Apaceh2 oauth plugin]]></title>
            <link href="https://devopstales.github.io/home/mod-auth-openidc/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/mod-auth-openidc/</id>
            
            
            <published>2019-05-13T00:00:00+00:00</published>
            <updated>2019-05-13T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Configure Apache plugin to use Keycloak as a user backend for login with OpenID and SSO.</p>

<p>mod_auth_openidc is an OpenID Connect Relying Party implementation for Apache HTTP Server 2.x</p>

<h3 id="install-the-plugin">Install the plugin</h3>

<pre><code>yum install mod_auth_openidc httpd php mod_ssl -y

mkdir -p /var/www/html/oauth/protected
echo &quot;index&quot; &gt; /var/www/html/oauth/index.htm
</code></pre>

<pre><code>nano /var/www/html/oauth/protected/index.php
&lt;!DOCTYPE html&gt;
&lt;html lang=&quot;en&quot;&gt;

&lt;head&gt;

   &lt;meta charset=&quot;utf-8&quot;&gt;
   &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot;&gt;
   &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot;&gt;
   &lt;meta name=&quot;description&quot; content=&quot;&quot;&gt;
   &lt;meta name=&quot;author&quot; content=&quot;&quot;&gt;

   &lt;title&gt;OpenID Connect: Received Claims&lt;/title&gt;

&lt;/head&gt;

&lt;body&gt;
         &lt;h3&gt;
            Claims sent back from OpenID Connect via the Apache module
         &lt;/h3&gt;
         &lt;br/&gt;


   &lt;!-- OpenAthens attribtues --&gt;
      &lt;?php session_start(); ?&gt;

         &lt;h2&gt;Claims&lt;/h2&gt;
         &lt;br/&gt;
         &lt;div class=&quot;row&quot;&gt;

               &lt;table class=&quot;table&quot; style=&quot;width:80%;&quot; border=&quot;1&quot;&gt;
                 &lt;?php foreach ($_SERVER as $key=&gt;$value): ?&gt;
                    &lt;?php if ( preg_match(&quot;/OIDC_/i&quot;, $key) ): ?&gt;
                       &lt;tr&gt;
                          &lt;td data-toggle=&quot;tooltip&quot; title=&lt;?php echo $key; ?&gt;&gt;&lt;?php echo $key; ?&gt;&lt;/td&gt;
                          &lt;td data-toggle=&quot;tooltip&quot; title=&lt;?php echo $value; ?&gt;&gt;&lt;?php echo $value; ?&gt;&lt;/td&gt;
                       &lt;/tr&gt;
                    &lt;?php endif; ?&gt;
                 &lt;?php endforeach; ?&gt;
               &lt;/table&gt;

&lt;/body&gt;&lt;/html&gt;
</code></pre>

<h3 id="create-vhost">Create vhost</h3>

<pre><code>nano /etc/httpd/conf.d/aouth-site.conf
# NameVirtualHost *:80
&lt;VirtualHost *:80&gt;
   ServerName oauth.devopstales.intra
   DocumentRoot /var/www/oauth/
   Redirect permanent / https://oauth.devopstales.intra
&lt;/VirtualHost&gt;

&lt;VirtualHost *:443&gt;
    ServerAdmin webmaster@example.com
    ServerName oauth.devopstales.intra
    ServerAlias www.oauth.devopstales.intra
    DocumentRoot /var/www/html/oauth/
    DirectoryIndex index.html index.php
    ErrorLog /var/log/httpd/oauth-error.log
    CustomLog /var/log/httpd/oauth-access.log combined

    SSLEngine on
    SSLCertificateFile /etc/httpd/ssl/domain.pem
    SSLCertificateKeyFile /etc/httpd/ssl/domain.pem
    SSLCertificateChainFile /etc/httpd/ssl/domain.pem

    # keycloak server
    OIDCProviderMetadataURL http://sso.devopstales.intra/auth/realms/mydomain/.well-known/openid-configuration
    # for self signed certificate
    OIDCSSLValidateServer Off
    OIDCClientID web
    OIDCClientSecret 5b721a2b-681f-402d-807c-b98c80672c16
    OIDCRedirectURI http://oauth.devopstales.intra/protected/redirect_uri
    OIDCCryptoPassphrase passphrase
    OIDCJWKSRefreshInterval 3600

    &lt;Location /protected/&gt;
       AuthType openid-connect
       Require valid-user
    &lt;/Location&gt;

&lt;/VirtualHost&gt;
</code></pre>

<h3 id="start-apache">Start apache</h3>

<pre><code>systemct start httpd
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Nagios Cross Platform Agent]]></title>
            <link href="https://devopstales.github.io/home/nagios-ncpa/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/nagios-ncpa/</id>
            
            
            <published>2019-05-07T00:00:00+00:00</published>
            <updated>2019-05-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this article I will show you how to add Remote Linux machine and it’s services to Nagios Monitoring host using NCPA agent.</p>

<h3 id="what-is-ncpa">What is NCPA</h3>

<p>NCPA is written in Python and is able to run on almost any Operating System. IT build official binaries for Windows, Mac OS X, and various Linux flavors.</p>

<h3 id="ncpa-client">NCPA Client</h3>

<pre><code>rpm -Uvh https://repo.nagios.com/nagios/7/nagios-repo-7-3.el7.noarch.rpm
yum install ncpa -y

nano /usr/local/ncpa/etc/ncpa.cfg
# [listener]
# allowed_hosts = &lt;nagios host&gt;
[api]
community_string = Password1
[nrdp]
# hostname =
# [nrdp]
# parent =
# token =
[plugin directives]
plugin_path = /usr/lib64/nagios/plugins/

systemctl enable ncpa_listener
systemctl start ncpa_listener
systemctl status ncpa_listener

# https://192.168.0.100:5693/
</code></pre>

<h3 id="ncpa-server">NCPA Server</h3>

<pre><code>cd /tmp
wget https://assets.nagios.com/downloads/ncpa/check_ncpa.tar.gz
tar xvf check_ncpa.tar.gz
chown nagios:nagios check_ncpa.py
chmod 775 check_ncpa.py
mv check_ncpa.py /usr/lib64/nagios/plugins
</code></pre>

<p>Test the commands</p>

<pre><code>/usr/lib64/naemon/plugins/check_ncpa.py -H localhost -t 'Password1' -P 5693 -M system/agent_version
/usr/lib64/naemon/plugins/check_ncpa.py -H localhost -t 'Password1' -P 5693 -M cpu/percent -w 20 -c 40 -q 'aggregate=avg'
/usr/lib64/naemon/plugins/check_ncpa.py -H localhost -t 'Password1' -P 5693 -M memory/virtual -w 50 -c 80 -u G
/usr/lib64/naemon/plugins/check_ncpa.py -H localhost -t 'Password1' -P 5693 -M processes -w 150 -c 200

# Run custom plugin trouth NCPA
/usr/lib64/naemon/plugins/check_ncpa.py -H localhost -t 'Password1' -P 5693 -M plugins/check_users -q args=&quot;-w 5 -c 10&quot;
</code></pre>

<pre><code># store password in macro
nano /etc/nagios/resource.cfg
USER10 = Password1

# createcustom commands for ncpa
nano /etc/nagios/commands.cfg
define command {
    command_name    check_ncpa
    command_line    $USER1$/check_ncpa.py -H $HOSTADDRESS$ -t $USER10$ -P 5693 $ARG1$
}

define command {
    command_name    check_ncpa_cpu
    command_line    $USER1$/check_ncpa.py -H $HOSTADDRESS$ -t $USER10$ -P 5693 cpu/percent -w 20 -c 40 -q 'aggregate=avg'
}
</code></pre>

<pre><code>nano /etc/nagios/conf.d/ncpa-test.cfg
        define host{
        use                    generic-host
        host_name              devopstales
        address                192.168.0.20
}

define service{
        use                     generic-service
        host_name               devopstales
        service_description     NCPA Version
        check_command           check_ncpa|system/agent_version
        }

define service{
        use                     generic-service
        host_name               devopstales
        service_description     CPU Load
        check_command           check_ncpa_cpu
        }
</code></pre>

<h3 id="restart-nagios">Restart nagios</h3>

<pre><code>nagios -v /etc/nagios/nagios.cfg

Total Warnings: 0
Total Errors:   0

service nagios restart
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Nagios Remote Plugin Executor]]></title>
            <link href="https://devopstales.github.io/home/nagios-nrpe/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/nagios-nrpe/</id>
            
            
            <published>2019-05-06T00:00:00+00:00</published>
            <updated>2019-05-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this article I will show you how to add Remote Linux machine and it’s services to Nagios Monitoring host using NRPE agent.</p>

<h3 id="what-is-nrpe">What is NRPE</h3>

<p>NRPE allows you to remotely execute Nagios plugins on other Linux/Unix machines.</p>

<p><img src="/img/include/nrpe.png" alt="Example image" /></p>

<h3 id="nrpe-client">NRPE Client</h3>

<pre><code>yum install nrpe nagios-plugins-all
# OR
apt-get install nagios-nrpe-server nagios-plugins
</code></pre>

<h3 id="nrpe-client-config">NRPE Client Config</h3>

<pre><code>nano /etc/nagios/nrpe.cfg
...
only_from = 127.0.0.1 localhost &lt;nagios_ip_address&gt;
...
command[check_users]=/usr/lib64/nagios/plugins/check_users -w 5 -c 10
command[check_load]=/usr/lib64/nagios/plugins/check_load -r -w 8.0,7.5,7.0 -c 11.0,10.0,9.0
command[check_disk]=/usr/lib64/nagios/plugins/check_disk -w 15% -c 10% /
command[check_mem]=/usr/lib64/nagios/plugins/check_mem -w 75% -c 90%
command[check_total_procs]=/usr/lib64/nagios/plugins/check_procs -w 300 -c 400
command[check_swap]=/usr/lib64/nagios/plugins/check_swap -w 10 -c 5
</code></pre>

<h3 id="nrpe-client-logging">NRPE Client Logging</h3>

<pre><code>nano /etc/nagios/nrpe.cfg
log_facility=local1
debug=1

nano  /etc/rsyslog.conf
local1.*                                                /var/log/nrpe.log
</code></pre>

<h3 id="start-nrpe-client">Start NRPE Client</h3>

<pre><code>systemctl start nrpe
systemctl enable nrp

ss -altn | grep 5666
LISTEN   0         5                   0.0.0.0:5666             0.0.0.0:*       
LISTEN   0         5                      [::]:5666                [::]:*

/usr/lib64/nagios/plugins/check_nrpe -H 127.0.0.1 -c check_total_procs
PROCS OK: 105 processes | procs=105;300;400;0;
</code></pre>

<h3 id="nrpe-server">NRPE Server</h3>

<pre><code>yum install nagios-plugins-nrpe
</code></pre>

<pre><code># createcustom commands for nrpe
nano /etc/nagios/commands.cfg
define command {
    command_name check_nrpe
    command_line $USER1$/check_nrpe -H $HOSTADDRESS$ -c $ARG1$
}
</code></pre>

<pre><code>nano /etc/nagios/conf.d/nrpe-test.cfg
        define host{
        use                    generic-host
        host_name              devopstales
        address                192.168.0.20
}


define service{
        use                     generic-service
        host_name               tecmint
        service_description     CPU Load
        check_command           check_nrpe!check_load
        }
</code></pre>

<h3 id="restart-nagios">Restart nagios</h3>

<pre><code>nagios -v /etc/nagios/nagios.cfg

Total Warnings: 0
Total Errors:   0

service nagios restart
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift Error: HostAlreadyClaimed]]></title>
            <link href="https://devopstales.github.io/home/openshift-hostalreadyclaimed/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/openshift-hostalreadyclaimed/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Error: HostAlreadyClaimed" />
                <link href="https://devopstales.github.io/cloud/openshift-gitlabrunner/?utm_source=atom_feed" rel="related" type="text/html" title="Install Gitlab runner on Openshift" />
                <link href="https://devopstales.github.io/home/openshift-gitlabrunner/?utm_source=atom_feed" rel="related" type="text/html" title="Install Gitlab runner on Openshift" />
                <link href="https://devopstales.github.io/cloud/openshift-extregistry/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: External registry" />
                <link href="https://devopstales.github.io/home/openshift-extregistry/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: External registry" />
            
                <id>https://devopstales.github.io/home/openshift-hostalreadyclaimed/</id>
            
            
            <published>2019-05-05T00:00:00+00:00</published>
            <updated>2019-05-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>How to solvee Openshift Error: HostAlreadyClaimed</p>

<p>I created a new route for a service and the rout not created. Get this error:</p>

<pre><code>Name:			keycloak-gatekeeper
Namespace:		phpmyadmin
Created:		17 minutes ago
Labels:			app=keycloak-gatekeeper
Annotations:		&lt;none&gt;
Requested Host:		phpmyadmin.devopstales.intra
			  rejected by router router: HostAlreadyClaimed (36 seconds ago)
			    route phpmyadmin/phpmyadmin-phpmyadmin has host phpmyadmin.devopstales.intra
Path:			&lt;none&gt;
TLS Termination:	edge
Insecure Policy:	Redirect
Endpoint Port:		http

Service:	keycloak-gatekeeper
Weight:		100 (100%)
Endpoints:	10.130.2.149:3000
</code></pre>

<p>So I listed all the routes but I dod not found this route. In the end the force delete solved my problem.</p>

<pre><code>oc delete route --grace-period=0 --force=true --ignore-not-found=true -n phpmyadmin phpmyadmin-phpmyadmin
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Katello client]]></title>
            <link href="https://devopstales.github.io/home/katello-client/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/katello-client/</id>
            
            
            <published>2019-05-04T00:00:00+00:00</published>
            <updated>2019-05-04T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Katello brings the full power of content management alongside the provisioning and configuration capabilities of Foreman. Katello is the upstream community project from which the Red Hat Satellite product is derived after Red Hat Satellite Server 6.</p>

<h3 id="install-subscription-manager">Install subscription manager</h3>

<pre><code>yum install subscription-manager
rpm -ivh http://192.168.0.109/pub/katello-ca-consumer-latest.noarch.rpm
subscription-manager register --org=&quot;mydomain&quot; --activationkey=&quot;el7-key&quot;

subscription-manager repos --list

cd /etc/yum.repos.d/
mv CentOS-* epel* katello-client.repo /tmp/
yum clean all
yum repolist
</code></pre>

<h3 id="install-katello-client">Install Katello client</h3>

<pre><code>yum install katello-agent -y
systemctl start goferd
systemctl enable goferd
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Copying Kubernetes Secrets Between Namespaces]]></title>
            <link href="https://devopstales.github.io/home/k8s-copy-secret/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/k8s-copy-secret/?utm_source=atom_feed" rel="related" type="text/html" title="Copying Kubernetes Secrets Between Namespaces" />
                <link href="https://devopstales.github.io/cloud/openshift-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Helm" />
                <link href="https://devopstales.github.io/home/openshift-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Helm" />
                <link href="https://devopstales.github.io/cloud/k8s-helm-rbac/?utm_source=atom_feed" rel="related" type="text/html" title="RBAC permissions for Helm" />
                <link href="https://devopstales.github.io/home/k8s-helm-rbac/?utm_source=atom_feed" rel="related" type="text/html" title="RBAC permissions for Helm" />
            
                <id>https://devopstales.github.io/home/k8s-copy-secret/</id>
            
            
            <published>2019-05-03T00:00:00+00:00</published>
            <updated>2019-05-03T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>A simple way of copying common secret data between namespaces</p>

<pre><code>kubectl get secret private-registry --namespace=dev1 --export -o yaml |\
   kubectl apply --namespace=dev2 -f -
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Errata]]></title>
            <link href="https://devopstales.github.io/home/katello-errata/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/katello-errata/</id>
            
            
            <published>2019-05-02T00:00:00+00:00</published>
            <updated>2019-05-02T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Katello brings the full power of content management alongside the provisioning and configuration capabilities of Foreman. Katello is the upstream community project from which the Red Hat Satellite product is derived after Red Hat Satellite Server 6.</p>

<h3 id="errata">Errata</h3>

<pre><code>yum install -y git \
pulp-admin-client \
pulp-rpm-admin-extensions \
pulp-rpm-consumer-extensions \
pulp-rpm-handlers \
pulp-rpm-yumplugins \
pulp-rpm-admin-extensions \
pulp-consumer-client \
python-pulp-agent-lib \
perl-Text-Unidecode \
perl-XML-Simple \
perl-XML-Parser

cd /opt
git clone https://github.com/rdrgmnzs/pulp_centos_errata_import.git
cd ./pulp_centos_errata_import
wget -N https://cefs.steve-meier.de/errata.latest.xml.bz2
bunzip2 ./errata.latest.xml.bz2
mkdir -m0700 ~/.pulp
cat /etc/pki/katello/certs/pulp-client.crt /etc/pki/katello/private/pulp-client.key &gt; ~/.pulp/user-cert.pem
chmod 0400 ~/.pulp/user-cert.pem

# import Errata
perl ./errata_import.pl --errata=errata.latest.xml

pulp-admin repo list | less

add rarrata to rebo by repoid

perl ./errata_import.pl \
--errata=errata.latest.xml \
--include-repo=6356cfe9-766e-4f07-a85e-f287b25395e3 \
--include-repo=2-el7_content-v1_0-6356cfe9-766e-4f07-a85e-f287b25395e3 \
--include-repo=2-el7_content-Library-6356cfe9-766e-4f07-a85e-f287b25395e3 \
--include-repo=2-el7_content-stable-6356cfe9-766e-4f07-a85e-f287b25395e3 \
--include-repo=cb299646-b1a3-4225-9c78-986851a1725f \
--include-repo=2-el7_content-v1_0-cb299646-b1a3-4225-9c78-986851a1725f \
--include-repo=2-el7_content-Library-cb299646-b1a3-4225-9c78-986851a1725f \
--include-repo=2-el7_content-stable-cb299646-b1a3-4225-9c78-986851a1725f
</code></pre>

<h3 id="update-repo">Update repo</h3>

<pre><code>hammer repository synchronize \
--skip-metadata-check true \
--name &quot;base_x86_64&quot; \
--product &quot;el7_repos&quot;

hammer repository synchronize \
--skip-metadata-check true \
--name &quot;updates_x86_64&quot; \
--product &quot;el7_repos&quot;

hammer content-view publish \
--name &quot;el7_content&quot; \
--description &quot;Publishing repositories&quot;

hammer content-view version promote \
--content-view &quot;el7_content&quot; \
--version &quot;4.0&quot; \
--to-lifecycle-environment &quot;stable&quot;
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install AWX]]></title>
            <link href="https://devopstales.github.io/home/awx-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/awx-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install AWX" />
                <link href="https://devopstales.github.io/linux/awx-docker/?utm_source=atom_feed" rel="related" type="text/html" title="Install AWX in docker" />
                <link href="https://devopstales.github.io/home/awx-docker/?utm_source=atom_feed" rel="related" type="text/html" title="Install AWX in docker" />
                <link href="https://devopstales.github.io/cloud/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
                <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
            
                <id>https://devopstales.github.io/home/awx-install/</id>
            
            
            <published>2019-04-30T00:00:00+00:00</published>
            <updated>2019-04-30T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>AWX is an open source web application that provides a user interface, REST API, and task engine for Ansible.</p>

<h3 id="install-postgresql">Install Postgresql</h3>

<pre><code>yum install epel-release -y
yum update -y
rpm -Uvh https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-centos10-10-2.noarch.rpm

yum -y install postgresql10-server postgresql10-contrib postgresql10
/usr/pgsql-10/bin/postgresql-10-setup initdb

sudo systemctl start postgresql-10
sudo systemctl enable postgresql-10

sudo -u postgres createuser -S awx
sudo -u postgres createdb -O awx awx
</code></pre>

<h3 id="install-requirements-and-awx">Install requirements and AWX</h3>

<pre><code># if selinux enabled
yum -y install policycoreutils-python
setsebool -P httpd_can_network_connect 1

# if firewall enabled
firewall-cmd --permanent --add-service=http
firewall-cmd --reload


yum -y install memcached ansible epel-release nginx
systemctl enable memcached
systemctl start memcached

yum install https://github.com/rabbitmq/erlang-rpm/releases/download/v20.1.7.1/erlang-20.1.7.1-1.el7.centos.x86_64.rpm
yum -y install https://dl.bintray.com/rabbitmq/all/rabbitmq-server/3.7.5/rabbitmq-server-3.7.5-1.el7.noarch.rpm
systemctl enable rabbitmq-server
systemctl start rabbitmq-server

cp -p /etc/nginx/nginx.conf{,.org}
wget -O /etc/nginx/nginx.conf https://raw.githubusercontent.com/sunilsankar/awx-build/master/nginx.conf
systemctl start nginx
systemctl enable nginx

yum -y install centos-release-scl centos-release-scl-rh
yum -y install rh-python36 rh-python36-Django rh-python36-django-split-settings \
rh-python36-django-qsstats-magic rh-python36-ansiconv rh-python36-prometheus_client \
rh-python36-python-memcached rh-python36-asn1crypto rh-python36-asgiref \
rh-python36-hyperlink rh-python36-Automat rh-python36-asgi_amqp rh-python36-uwsgi \
rh-python36-msgpack-python rh-python36-msgpack-python-debuginfo rh-python36-jsonpickle \
rh-python36-django-radius rh-python36-python-django-radius rh-python36-python-radius \
rh-python36-future rh-python36-pyrad rh-python36-netaddr rh-python36-tacacs_plus \
rh-python36-python3-saml rh-python36-xmlsec rh-python36-lxml rh-python36-defusedxml \
rh-python36-boto

wget -O /etc/yum.repos.d/ansible-awx.repo https://copr.fedorainfracloud.org/coprs/mrmeee/ansible-awx/repo/epel-7/mrmeee-ansible-awx-epel-7.repo
yum install -y ansible-awx


sudo -u awx scl enable rh-python36 rh-postgresql10 &quot;awx-manage migrate&quot;
echo &quot;from django.contrib.auth.models import User; User.objects.create_superuser('admin', 'root@localhost', 'Password1')&quot; \
| sudo -u awx scl enable rh-python36 rh-postgresql10 &quot;awx-manage shell&quot;

sudo -u awx scl enable rh-python36 rh-postgresql10 &quot;awx-manage create_preload_data&quot;
sudo -u awx scl enable rh-python36 rh-postgresql10 &quot;awx-manage provision_instance --hostname=$(hostname)&quot;
sudo -u awx scl enable rh-python36 rh-postgresql10 &quot;awx-manage register_queue --queuename=tower --hostnames=$(hostname)&quot;
</code></pre>

<h3 id="configure-database">Configure database</h3>

<pre><code>systemctl start awx-cbreceiver
systemctl start awx-dispatcher
systemctl start awx-channels-worker
systemctl start awx-daphne
systemctl start awx-web

systemctl status awx-cbreceiver
systemctl status awx-dispatcher
systemctl status awx-channels-worker
systemctl status awx-daphne
systemctl status awx-web

systemctl enable awx-cbreceiver
systemctl enable awx-dispatcher
systemctl enable awx-channels-worker
systemctl enable awx-daphne
systemctl enable awx-web
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install AWX in docker]]></title>
            <link href="https://devopstales.github.io/home/awx-docker/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/awx-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install AWX" />
                <link href="https://devopstales.github.io/linux/awx-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install AWX" />
                <link href="https://devopstales.github.io/linux/awx-docker/?utm_source=atom_feed" rel="related" type="text/html" title="Install AWX in docker" />
                <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
                <link href="https://devopstales.github.io/cloud/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
            
                <id>https://devopstales.github.io/home/awx-docker/</id>
            
            
            <published>2019-04-30T00:00:00+00:00</published>
            <updated>2019-04-30T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>AWX is an open source web application that provides a user interface, REST API, and task engine for Ansible.</p>

<h3 id="install-docker-and-docker-compose">Install Docker and docker-compose</h3>

<pre><code>sudo yum install -y yum-utils \
device-mapper-persistent-data \
lvm2

sudo yum-config-manager \
--add-repo \
https://download.docker.com/linux/centos/docker-ce.repo

sudo yum -y install docker-ce docker-ce-cli containerd.io

service docker start
systemctl enable docker

yum install epel-release
yum install python-pip -y
pip install docker-compose
</code></pre>

<pre><code>yum install git ansible -y

cd /opt
git clone https://github.com/ansible/awx.git
cd awx/installer/

nano inventory
postgres_data_dir=/opt/pgdocker
docker_compose_dir=/opt/awxcompose
pg_username=awx
pg_password=Password1
rabbitmq_password=Password1
admin_user=admin
admin_password=Password1
project_data_dir=/var/lib/awx/projects

ansible-playbook -i inventory install.yml

docker logs awx_task -f
</code></pre>

<p>dockerhub_base=ansible
dockerhub_version=latest</p>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configurate ipmitool]]></title>
            <link href="https://devopstales.github.io/home/ipmitool-config/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/ipmitool-config/</id>
            
            
            <published>2019-04-29T00:00:00+00:00</published>
            <updated>2019-04-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>The Intelligent Platform Management Interface (IPMI) is a set of computer interface specifications for an autonomous computer subsystem that provides management and monitoring capabilities independently of the host system&rsquo;s CPU, firmware (BIOS or UEFI) and operating system.</p>

<h3 id="ipmitool-on-pfsense">Ipmitool on pfsense</h3>

<pre><code>[2.3.3-RELEASE][root@fw.makz.me]/root: ipmitool
Could not open device at /dev/ipmi0 or /dev/ipmi/0 or /dev/ipmidev/0: No such file or directory

# solution
kldload ipmi
nano /boot/loader.conf
#Load ipmi.ko into the kernel
ipmi_load=&quot;YES&quot;
</code></pre>

<h3 id="ipmitool-on-linux">Ipmitool on Linux</h3>

<pre><code>modprobe ipmi_msghandler
modprobe ipmi_devintf
modprobe ipmi_si

nano /etc/modules
# OR
nano /etc/modprobe.d/*.conf
ipmi_msghandler
ipmi_devintf
ipmi_si
</code></pre>

<h3 id="ipmitool-configuration">Ipmitool Configuration</h3>

<pre><code>ipmitool lan set 1 ipsrc static
ipmitool lan set 1 ipaddr 192.168.0.211
ipmitool lan set 1 netmask 255.255.255.0
ipmitool lan set 1 defgw ipaddr 192.168.0.1
ipmitool lan set 1 arp respond on
ipmitool lan set 1 access on

ipmitool lan print 1
</code></pre>

<h3 id="monitor-ipmi-with-telegraf">Monitor ipmi with telegraf</h3>

<pre><code>nano /etc/telegraf/telegraf.conf
[[inputs.ipmi_sensor]]
        path = &quot;/usr/bin/ipmitool&quot;
        servers = [&quot;username:password@lan(192.168.0.211)&quot;]
        interval = &quot;30s&quot;
        timeout = &quot;20s&quot;
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configure spacewalk 2.9]]></title>
            <link href="https://devopstales.github.io/home/spacewalk-software-channels/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/spacewalk-software-channels/</id>
            
            
            <published>2019-04-29T00:00:00+00:00</published>
            <updated>2019-04-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how to create software channels on spacewalk server.</p>

<p>Login to the spacewalk web interface</p>

<h3 id="get-gpg-key">Get GPG key</h3>

<p>First, we need have an extracted GPG key information, To get the key information download it and extract it using GPG command.</p>

<pre><code>cd /etc/pki/rpm-gpg
wget http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-7
gpg --with-fingerprint RPM-GPG-KEY-CentOS-7
wget https://copr-be.cloud.fedoraproject.org/results/@spacewalkproject/spacewalk-2.9/pubkey.gpg -o RPM-GPG-KEY-spacewalk-2.9
gpg --with-fingerprint RPM-GPG-KEY-spacewalk-2.9

cp /etc/pki/rpm-gpg/RPM-GPG-KEY-* /var/www/html/pub/
</code></pre>

<h3 id="create-software-chanel">Create software chanel</h3>

<p>Channels (top) &gt; Manage Software Channels(Left side pane) &gt; Create Channel(Right side top corner).<br></p>

<p><img src="/img/include/spacewalk_1.png" alt="Example image" /></p>

<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>

<tbody>
<tr>
<td>Channel Name</td>
<td>centos 7 base - x86_64</td>
</tr>

<tr>
<td>Channel Label</td>
<td>centos7-base-x86_64</td>
</tr>

<tr>
<td>Parent Channel</td>
<td>none</td>
</tr>

<tr>
<td>Architecture</td>
<td>x86_64</td>
</tr>

<tr>
<td>Channel Summary</td>
<td>centos7-base-x86_64</td>
</tr>

<tr>
<td>GPG Key URL</td>
<td>file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7</td>
</tr>

<tr>
<td>GPG Key ID</td>
<td>F4A80EB5</td>
</tr>

<tr>
<td>GPG Fingerprint</td>
<td>6341 AB27 53D7 8A78 A7C2 7BB1 24C6 A8A7 F4A8 0EB5</td>
</tr>
</tbody>
</table>

<p><img src="/img/include/spacewalk_2.png" alt="Example image" /></p>

<h3 id="create-repositories">Create repositories</h3>

<p>Channels (Top) &gt; Manage Software Channels (Left side pane) &gt; Manage Repositories (Left side pane) &gt; Create Repository(Right side top corner).</p>

<table>
<thead>
<tr>
<th>Repository Label:</th>
<th>centos7_base_x86_64_repo</th>
</tr>
</thead>

<tbody>
<tr>
<td>Repository URL:</td>
<td><a href="http://mirror.centos.org/centos/7/os/x86_64/">http://mirror.centos.org/centos/7/os/x86_64/</a></td>
</tr>

<tr>
<td>Repository Type:</td>
<td>yum</td>
</tr>
</tbody>
</table>

<table>
<thead>
<tr>
<th>Repository Label:</th>
<th>centos7_update_x86_64_repo</th>
</tr>
</thead>

<tbody>
<tr>
<td>Repository URL:</td>
<td><a href="http://mirror.centos.org/centos/7/updates/x86_64/">http://mirror.centos.org/centos/7/updates/x86_64/</a></td>
</tr>

<tr>
<td>Repository Type:</td>
<td>yum</td>
</tr>
</tbody>
</table>

<table>
<thead>
<tr>
<th>Repository Label:</th>
<th>centos7_extra_x86_64_repo</th>
</tr>
</thead>

<tbody>
<tr>
<td>Repository URL:</td>
<td><a href="http://mirror.centos.org/centos/7/extras/x86_64/">http://mirror.centos.org/centos/7/extras/x86_64/</a></td>
</tr>

<tr>
<td>Repository Type:</td>
<td>yum</td>
</tr>
</tbody>
</table>

<table>
<thead>
<tr>
<th>Repository Label:</th>
<th>centos7_opnstk_x86_64_repo</th>
</tr>
</thead>

<tbody>
<tr>
<td>Repository URL:</td>
<td><a href="http://mirror.centos.org/centos/7/cloud/x86_64/">http://mirror.centos.org/centos/7/cloud/x86_64/</a></td>
</tr>

<tr>
<td>Repository Type:</td>
<td>yum</td>
</tr>
</tbody>
</table>

<h3 id="adding-repository-to-channel">Adding Repository to Channel</h3>

<p>Channels (Top) &gt; Manage Software Channels (Left side pane) &gt; Centos 7 Base x86_64  &gt; Repositories (Tab)  &gt; centos7_base_x86_64_repo (Check box)  &gt; Update Repositories (Bottom right corner). <br></p>

<p><img src="/img/include/spacewalk_3.png" alt="Example image" /></p>

<h3 id="creating-activation-key">Creating activation Key</h3>

<p>System  &gt; ( Top menu) Activation Keys (Left Side pane)  &gt; Create Key (Right side top corner) &gt; Fill description <br></p>

<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>

<tbody>
<tr>
<td>Description:</td>
<td>CentOS Linux 7 x86_64</td>
</tr>

<tr>
<td>Key:</td>
<td>centoslinux7-x86_64</td>
</tr>

<tr>
<td>Usage:</td>
<td></td>
</tr>

<tr>
<td>Base channels:</td>
<td>Centos 7 Base - x86_64</td>
</tr>

<tr>
<td>Add-On Entitlements:</td>
<td>Choose all available feature you about to use.</td>
</tr>
</tbody>
</table>

<p><img src="/img/include/spacewalk_4.png" alt="Example image" />
<img src="/img/include/spacewalk_5.png" alt="Example image" /></p>

<h3 id="start-syncing-repositories">Start Syncing repositories</h3>

<pre><code>spacewalk-repo-sync --channel centos-7-base-x86_64 --type yum
tail -f /var/log/rhn/reposync/centos-7-base-x86_64.log

df -hP /var/satellite
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Google Authenticator on pfSense]]></title>
            <link href="https://devopstales.github.io/home/pfsense-2fa/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/pfsense-2fa/</id>
            
            
            <published>2019-04-29T00:00:00+00:00</published>
            <updated>2019-04-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>This article explains how to set up OpenVPN with Google Authenticator on pfSense.</p>

<h3 id="set-up-the-freeradius">Set up the FreeRADIUS</h3>

<ul>
<li>Go to  <code>System &gt; Package Manager &gt; Available Packages</code> and install <code>FreeRADIUS</code> package.</li>
<li><code>Services &gt; FreeRADIUS &gt; Interfaces &gt; Add</code></li>
</ul>

<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>

<tbody>
<tr>
<td>Interface IP Address</td>
<td>127.0.0.1</td>
</tr>

<tr>
<td>Port</td>
<td>1812</td>
</tr>

<tr>
<td>Interface Type</td>
<td>Authentication</td>
</tr>

<tr>
<td>IP Version</td>
<td>IPv4</td>
</tr>

<tr>
<td>Description</td>
<td>Authentication</td>
</tr>
</tbody>
</table>

<p><img src="/img/include/pfsense_2fa_1.png" alt="Example image" /></p>

<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>

<tbody>
<tr>
<td>Interface IP Address</td>
<td>127.0.0.1</td>
</tr>

<tr>
<td>Port</td>
<td>1813</td>
</tr>

<tr>
<td>Interface Type</td>
<td>Authentication</td>
</tr>

<tr>
<td>IP Version</td>
<td>IPv4</td>
</tr>

<tr>
<td>Description</td>
<td>Accounting</td>
</tr>
</tbody>
</table>

<p><img src="/img/include/pfsense_2fa_2.png" alt="Example image" /></p>

<h3 id="add-a-nas-client">Add a NAS client</h3>

<ul>
<li><code>Services &gt; FreeRADIUS &gt; NAS/Clients &gt; Add</code></li>
</ul>

<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>

<tbody>
<tr>
<td>Client IP Address</td>
<td>127.0.0.1</td>
</tr>

<tr>
<td>Client IP Version</td>
<td>IPv4</td>
</tr>

<tr>
<td>Client Shortname</td>
<td>pfsenselocal</td>
</tr>

<tr>
<td>Client Shared Secret</td>
<td>Password1</td>
</tr>

<tr>
<td>Client Protocol</td>
<td>UDP</td>
</tr>

<tr>
<td>Client Type</td>
<td>other</td>
</tr>

<tr>
<td>Require Message Authenticator</td>
<td>No</td>
</tr>

<tr>
<td>Max Connections</td>
<td>16</td>
</tr>

<tr>
<td>Description</td>
<td>pfsenselocal</td>
</tr>
</tbody>
</table>

<p><img src="/img/include/pfsense_2fa_3.png" alt="Example image" /></p>

<h3 id="add-an-authentication-server-ro-pfsense">Add an authentication server ro pfSense</h3>

<ul>
<li><code>System &gt; User Manager &gt; Authentication Servers &gt; Add</code></li>
</ul>

<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>

<tbody>
<tr>
<td>Descriptive Name</td>
<td>localfreeradius</td>
</tr>

<tr>
<td>Type</td>
<td>RADIUS</td>
</tr>

<tr>
<td>Protocol</td>
<td>PAP</td>
</tr>

<tr>
<td>Hostname or IP address</td>
<td>127.0.0.1</td>
</tr>

<tr>
<td>Shared Secret</td>
<td>Password1</td>
</tr>

<tr>
<td>Services offered</td>
<td>Authentication and Accounting</td>
</tr>

<tr>
<td>Authentiocation port</td>
<td>1812</td>
</tr>

<tr>
<td>Accounting port</td>
<td>1813</td>
</tr>

<tr>
<td>Authentication Timeout</td>
<td>5</td>
</tr>

<tr>
<td>RADIUS NAS IP Attribute</td>
<td>LAN</td>
</tr>
</tbody>
</table>

<p><img src="/img/include/pfsense_2fa_4.png" alt="Example image" /></p>

<h3 id="configurate-otp-for-users">Configurate OTP for Users</h3>

<ul>
<li><code>Services &gt; FreeRADIUS &gt; Users &gt; Add</code></li>
</ul>

<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>

<tbody>
<tr>
<td>Username</td>
<td>tester</td>
</tr>

<tr>
<td>Password</td>
<td></td>
</tr>

<tr>
<td>Password Encryption</td>
<td>Cleartext-Password</td>
</tr>

<tr>
<td>One-Time Password</td>
<td>Enable One-Time Password (OTP) for this user</td>
</tr>

<tr>
<td>OTP Auth Method</td>
<td>Google-Authenticator</td>
</tr>

<tr>
<td>Init-Secret</td>
<td>click Generator OTP Secret</td>
</tr>

<tr>
<td>PIN</td>
<td>enter 4-8 numbers and remember them.</td>
</tr>

<tr>
<td>QR Code</td>
<td>click Generate QR Code.</td>
</tr>
</tbody>
</table>

<p>At this point open Google Authenticator on your phone and scan the QRCODE.</p>

<p><img src="/img/include/pfsense_2fa_5.png" alt="Example image" /></p>

<p>You can use One-Time Password (OTP) only for local FreeRadius users. FreeRadius users from diferent backenl like mysql or ldap did not work.</p>

<h3 id="configurate-openvpn">Configurate openvpn</h3>

<ul>
<li>Go to <code>VPN &gt; OpenVPN &gt; Servers &gt; Edit</code></li>
<li>Select localfreeradius for Backend for authentication</li>
</ul>

<p><img src="/img/include/pfsense_2fa_6.png" alt="Example image" /></p>

<ul>
<li>In the OpenVPN Server configuration, under <code>Advanced Configuration &gt; Custom options</code></li>
<li>add: <code>reneg-sec 0</code></li>
</ul>

<p>If you connect your OpenVPN client you must enter your username and the PIN + the Google Authenticator one-time code as your password. <br> If PIN is 1234 and the Google Authenticator code is 445 745 then the password is: 1234445745</p>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Katello]]></title>
            <link href="https://devopstales.github.io/home/katello-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/katello-install/</id>
            
            
            <published>2019-04-29T00:00:00+00:00</published>
            <updated>2019-04-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Katello brings the full power of content management alongside the provisioning and configuration capabilities of Foreman. Katello is the upstream community project from which the Red Hat Satellite product is derived after Red Hat Satellite Server 6.</p>

<h3 id="base-komponents">Base komponents</h3>

<ul>
<li>Foreman: provisioning on new clients.</li>
<li>Pulp: patch and content (package repository) management.</li>
<li>Candlepin: subscription and entitlement management.</li>
<li>Puppet: configuration management (actual running of modules assigned in Foreman).</li>
<li>Katello: unified workflow and WebUI for content (Pulp) and subscriptions (Candlepin).</li>
</ul>

<h3 id="hardware-requirements">Hardware Requirements</h3>

<ul>
<li>Two Logical CPUs</li>
<li>8 GB of memory (12 GB highly recommended)</li>
<li>The filesystem holding /var/lib/pulp needs to be large</li>
</ul>

<h3 id="required-repositories">Required Repositories</h3>

<pre><code># hostnevet beállítani !!!

yum -y localinstall https://fedorapeople.org/groups/katello/releases/yum/3.11/katello/el7/x86_64/katello-repos-latest.rpm
yum -y localinstall https://yum.theforeman.org/releases/1.21/el7/x86_64/foreman-release.rpm
yum -y localinstall https://yum.puppetlabs.com/puppetlabs-release-pc1-el-7.noarch.rpm
yum -y localinstall https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
</code></pre>

<h3 id="installation">Installation</h3>

<pre><code class="language-bash">yum -y install foreman-release-scl python-django
yum -y update
yum -y install katello


foreman-installer \
--scenario &quot;katello&quot; \
--foreman-initial-organization &quot;mydomain&quot; \
--foreman-initial-location &quot;office&quot; \
--enable-foreman-plugin-ansible \
--enable-foreman-proxy-plugin-ansible \
--enable-foreman-plugin-remote-execution \
--enable-foreman-proxy-plugin-remote-execution-ssh

# reset/gen Password
foreman-rake permissions:reset
</code></pre>

<h3 id="configure-hammer-cli">Configure hammer-cli</h3>

<pre><code>nano ~/.hammer/cli.modules.d/foreman.yml
:foreman:
 :host: 'https://katello.devopstales.intra/'
 :username: 'admin'
 :password: '**********'

hammer defaults add --param-name organization --param-value &quot;mydomain&quot;
hammer defaults add --param-name location --param-value &quot;office&quot;
hammer defaults list
</code></pre>

<h3 id="configure-gpg-keys">Configure gpg keys</h3>

<pre><code>hammer product create \
--name &quot;el7_repos&quot; \
--description &quot;Various repositories to use with CentOS 7&quot;

mkdir /etc/pki/rpm-gpg/import/
cd /etc/pki/rpm-gpg/import/
wget https://repo.mysql.com/RPM-GPG-KEY-mysql
wget http://mirror.centos.org/centos/7/os/x86_64/RPM-GPG-KEY-CentOS-7
wget https://archive.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-7Server
wget https://rpms.remirepo.net/RPM-GPG-KEY-remi
wget https://packages.cisofy.com/keys/cisofy-software-rpms-public.key

hammer gpg create \
--key &quot;RPM-GPG-KEY-CentOS-7&quot; \
--name &quot;RPM-GPG-KEY-CentOS-7&quot;

hammer gpg create \
--key &quot;RPM-GPG-KEY-mysql&quot; \
--name &quot;RPM-GPG-KEY-mysql&quot;

hammer gpg create \
--key &quot;RPM-GPG-KEY-EPEL-7Server&quot; \
--name &quot;RPM-GPG-KEY-EPEL-7Server&quot;

hammer gpg create \
--key &quot;RPM-GPG-KEY-remi&quot; \
--name &quot;RPM-GPG-KEY-remi&quot;

hammer gpg create \
--key &quot;cisofy-software-rpms-public.key&quot; \
--name &quot;RPM-GPG-KEY-cisofy&quot;
</code></pre>

<h3 id="create-yum-repositories">Create yum repositories</h3>

<pre><code>hammer gpg list

hammer repository create \
--product &quot;el7_repos&quot; \
--name &quot;base_x86_64&quot; \
--label &quot;base_x86_64&quot; \
--content-type &quot;yum&quot; \
--download-policy &quot;on_demand&quot; \
--gpg-key &quot;RPM-GPG-KEY-CentOS-7&quot; \
--url &quot;http://mirror.centos.org/centos/7/os/x86_64/&quot; \
--mirror-on-sync &quot;no&quot;

hammer repository create \
--product &quot;el7_repos&quot; \
--name &quot;extras_x86_64&quot; \
--label &quot;extras_x86_64&quot; \
--content-type &quot;yum&quot; \
--download-policy &quot;on_demand&quot; \
--gpg-key &quot;RPM-GPG-KEY-CentOS-7&quot; \
--url &quot;http://mirror.centos.org/centos/7/extras/x86_64/&quot; \
--mirror-on-sync &quot;no&quot;

hammer repository create \
--product &quot;el7_repos&quot; \
--name &quot;updates_x86_64&quot; \
--label &quot;updates_x86_64&quot; \
--content-type &quot;yum&quot; \
--download-policy &quot;on_demand&quot; \
--gpg-key &quot;RPM-GPG-KEY-CentOS-7&quot; \
--url &quot;http://mirror.centos.org/centos/7/updates/x86_64/&quot; \
--mirror-on-sync &quot;no&quot;

hammer repository create \
--product &quot;el7_repos&quot; \
--name &quot;epel_x86_64&quot; \
--label &quot;epel_x86_64&quot; \
--content-type &quot;yum&quot; \
--download-policy &quot;on_demand&quot; \
--gpg-key &quot;RPM-GPG-KEY-EPEL-7Server&quot; \
--url &quot;https://dl.fedoraproject.org/pub/epel/7Server/x86_64/&quot;

hammer repository create \
--product &quot;el7_repos&quot; \
--name &quot;lynis&quot; \
--label &quot;lynis&quot; \
--content-type &quot;yum&quot; \
--download-policy &quot;on_demand&quot; \
--gpg-key &quot;RPM-GPG-KEY-cisofy&quot; \
--url &quot;https://packages.cisofy.com/community/lynis/rpm/&quot;

hammer repository create \
--product &quot;el7_repos&quot; \
--name &quot;mysql_57_x86_64&quot; \
--label &quot;mysql_57_x86_64&quot; \
--content-type &quot;yum&quot; \
--download-policy &quot;on_demand&quot; \
--gpg-key &quot;RPM-GPG-KEY-mysql&quot; \
--url &quot;https://repo.mysql.com/yum/mysql-5.7-community/el/7/x86_64/&quot;

hammer repository create \
--product &quot;el7_repos&quot; \
--name &quot;katello_agent_x86_64&quot; \
--label &quot;katello_agent_x86_64&quot; \
--content-type &quot;yum&quot; \
--download-policy &quot;on_demand&quot; \
--url &quot;https://fedorapeople.org/groups/katello/releases/yum/latest/client/el7/x86_64/&quot;

hammer repository create \
--product &quot;el7_repos&quot; \
--name &quot;remi_php_56_x86_64&quot; \
--label &quot;remi_php_56_x86_64&quot; \
--content-type &quot;yum&quot; \
--download-policy &quot;on_demand&quot; \
--gpg-key &quot;RPM-GPG-KEY-remi&quot; \
--url &quot;https://mirrors.ukfast.co.uk/sites/remi/enterprise/7/php56/x86_64/&quot;

hammer repository create \
--product &quot;el7_repos&quot; \
--name &quot;remi_php_72_x86_64&quot; \
--label &quot;remi_php_72_x86_64&quot; \
--content-type &quot;yum&quot; \
--download-policy &quot;on_demand&quot; \
--gpg-key &quot;RPM-GPG-KEY-remi&quot; \
--url &quot;https://mirrors.ukfast.co.uk/sites/remi/enterprise/7/php72/x86_64/&quot;

hammer repository create \
--product &quot;el7_repos&quot; \
--name &quot;remi_safe_x86_64&quot; \
--label &quot;remi_safe_x86_64&quot; \
--content-type &quot;yum&quot; \
--download-policy &quot;on_demand&quot; \
--gpg-key &quot;RPM-GPG-KEY-remi&quot; \
--url &quot;https://mirrors.ukfast.co.uk/sites/remi/enterprise/7/safe/x86_64/&quot;
</code></pre>

<h3 id="sync-repos">Sync repos</h3>

<pre><code>hammer repository list

for i in $(seq 1 12); do \
hammer repository synchronize \
--product &quot;el7_repos&quot; \
--id &quot;$i&quot;; \
done

# Create a Content View
hammer content-view create \
--name &quot;el7_content&quot; \
--description &quot;Content view for CentOS 7&quot;

hammer product list

# Add Repositories to Content View
for i in $(seq 1 12); do \
hammer content-view add-repository \
--name &quot;el7_content&quot; \
--product &quot;el7_repos&quot; \
--repository-id &quot;$i&quot;; \
done

# Create a Lifecycle Environment
hammer lifecycle-environment create \
--name &quot;stable&quot; \
--label &quot;stable&quot; \
--prior &quot;Library&quot;

hammer lifecycle-environment list

# Publish a Content View
hammer content-view publish \
--name &quot;el7_content&quot; \
--description &quot;Publishing repositories&quot;

hammer content-view version list

# Promote Version to Lifecycle Environment
hammer content-view version promote \
--content-view &quot;el7_content&quot; \
--version &quot;1.0&quot; \
--to-lifecycle-environment &quot;stable&quot;

hammer content-view version list

# Create an Activation Key
hammer activation-key create \
--name &quot;el7-key&quot; \
--description &quot;Key to use with CentOS7&quot; \
--lifecycle-environment &quot;stable&quot; \
--content-view &quot;el7_content&quot; \
--unlimited-hosts

hammer activation-key list

# Add Subscription to Activation Key
hammer subscription list

hammer activation-key add-subscription \
--name &quot;el7-key&quot; \
--quantity &quot;1&quot; \
--subscription-id &quot;1&quot;

# Backup Katello Configuration
foreman-maintain backup snapshot -y /mnt/backup/
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install privacyIDEA]]></title>
            <link href="https://devopstales.github.io/home/privacyidea-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/privacyidea-install/</id>
            
            
            <published>2019-04-29T00:00:00+00:00</published>
            <updated>2019-04-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>privacyIDEA is a Two Factor Authentication System which is multi-tenency- and multi-instance-capable. It is opensource, written in Python and hosted at GitHub.</p>

<h3 id="configure-privacyidea-repo">Configure privacyidea repo</h3>

<pre><code># base os: Debian 9

apt update
apt install dirmngr -y

nano /etc/apt/sources.list.d/privacyidea.list
deb http://lancelot.netknights.it/community/xenial/stable xenial main

wget https://lancelot.netknights.it/NetKnights-Release.asc
apt-key add NetKnights-Release.asc
</code></pre>

<h3 id="install-privacyidea">Install privacyidea</h3>

<pre><code>apt update
apt install privacyidea-apache2

ln -s /etc/apache2/mods-available/rewrite.load /etc/apache2/mods-enabled/rewrite.load

nano nano sites-enabled/privacyidea.conf
# enable 80 to 443 redirection
systemctl restart apache2
</code></pre>

<h3 id="create-admin-user">Create admin user</h3>

<pre><code>pi-manage admin add admin -e admin@localhost
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install spacewalk 2.9]]></title>
            <link href="https://devopstales.github.io/home/spacewalk-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/spacewalk-install/</id>
            
            
            <published>2019-04-29T00:00:00+00:00</published>
            <updated>2019-04-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Spacewalk is an open source Linux systems management solution. Spacewalk is the upstream community project from which the Red Hat Satellite product is derived before Red Hat Satellite Server 6.</p>

<h3 id="create-answerfile">Create answerfile</h3>

<pre><code>cat &gt; /root/spacewalk_answers.txt &lt;&lt; EOF
admin-email = operation@devopstales.intra
ssl-set-cnames = spacewalk
ssl-set-org = Spacewalk Org
ssl-set-org-unit = spacewalk
ssl-set-city = Budapest
ssl-set-state = non
ssl-set-country = HU
ssl-password = Password1
ssl-set-email = operation@devopstales.intra
ssl-config-sslvhost = Y
enable-tftp=Y
EOF
</code></pre>

<h3 id="install-requirements">Install requirements</h3>

<pre><code>yum install ntp -y
service ntpd restart


rpm -Uvh https://copr-be.cloud.fedoraproject.org/results/@spacewalkproject/spacewalk-2.9/epel-7-x86_64/00830557-spacewalk-repo/spacewalk-repo-2.9-4.el7.noarch.rpm

rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm

yum clean all &amp;&amp; yum repolist
</code></pre>

<h3 id="install-spacewalk">Install Spacewalk</h3>

<pre><code>yum install -y spacewalk-setup-postgresql
yum install -y spacewalk-postgresql
spacewalk-setup --answer-file=/root/spacewalk_answers.txt
yum install spacecmd -y
</code></pre>

<p>Go to the WebUI and configure the admin user.</p>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Update ILO firmware]]></title>
            <link href="https://devopstales.github.io/home/update-ilo/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/update-ilo/</id>
            
            
            <published>2019-04-29T00:00:00+00:00</published>
            <updated>2019-04-29T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Integrated Lights-Out, or iLO, is a proprietary embedded server management technology by Hewlett-Packard which provides out-of-band management facilities.</p>

<h3 id="update-from-linux-cli">Update from Linux CLI</h3>

<p>Download iLO firmware say CP032487.scexe</p>

<pre><code>cd /tmp
chnmod 755 CP032487.scexe
./CP032487.scexe
curl http:///xmldata?item=All
</code></pre>

<h3 id="update-from-ilo-cli">Update from ILO CLI</h3>

<pre><code>./CP032487.scexe --unpack=firmware
cd firmware
cp firmware/ilo2_NNN.bin $DocumentRoot (usually /var/www/html/)

# Login on iLO over ssh and upload firmware
ssh -l Administrator

iLO&gt; show
iLO&gt; cd /map1
iLO&gt; oemhp_ping
iLO&gt; load -source http:///firmware/ilo2_NNN.bin

curl http:///xmldata?item=All
</code></pre>

<h3 id="update-from-ilo-webui">Update from iLO WebUI</h3>

<ul>
<li>access iLO Web UI, go to Administration tab and upload bin file to upgrade iLO firmware</li>
</ul>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Proxmox Mail Gateway]]></title>
            <link href="https://devopstales.github.io/home/proxmox-mail-gateway/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/proxmox-mail-gateway/</id>
            
            
            <published>2019-04-28T00:00:00+00:00</published>
            <updated>2019-04-28T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Proxmox Mail Gateway is a full featured, open-source mail proxy and protects your mail server from spam, viruses, trojans and phishing emails.</p>

<h3 id="configurate-cluster">Configurate Cluster</h3>

<pre><code>pmg1.devopstales.intra 192.168.0.27
pmg2.devopstales.intra 192.168.0.28
</code></pre>

<p>After the base installation login the web interfate:</p>

<ul>
<li>192.168.0.27:8006</li>
<li>192.168.0.28:8006</li>
</ul>

<p>At Configuration &gt; Cluster create a new cluster
<img src="/img/include/pmg_1.png" alt="Example image" />
<img src="/img/include/pmg_2.png" alt="Example image" /></p>

<p>Copy the cluster info:
<img src="/img/include/pmg_3.png" alt="Example image" /></p>

<p>On the other host (pmg2) go to the same menu and click Join
<img src="/img/include/pmg_4.png" alt="Example image" />
Add the datat copyd from the master node (pmg1)
<img src="/img/include/pmg_5.png" alt="Example image" /></p>

<h3 id="basic-configuration">Basic Configuration</h3>

<p>On the master node&rsquo;s (pmg1) weg interface go to Configuration &gt; Mail Proxy
<img src="/img/include/pmg_6.png" alt="Example image" />
Edit the Default Relay and add your interbal mailservers ip:
<img src="/img/include/pmg_7.png" alt="Example image" />
On the pmg2 check the config is replicated</p>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Centreon on Centos 7]]></title>
            <link href="https://devopstales.github.io/home/centreon-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/centreon-install/</id>
            
            
            <published>2019-04-27T00:00:00+00:00</published>
            <updated>2019-04-27T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Centreon is a free Open Source monitoring software which allows an administrator to easily configure alerts based on thresholds, generate email alerts, add systems to be monitored quickly without the need of configuring complicated configuration files.</p>

<h3 id="install-requisites">Install requisites</h3>

<pre><code>selinuxenabled &amp;&amp; echo enabled || echo disabled

nano /etc/selinux/config
SELINUX=disabled

yum install -y epel-release mariadb-server
mkdir /etc/systemd/system/mariadb.service.d/
echo -ne &quot;[Service]\nLimitNOFILE=32000\n&quot; | tee /etc/systemd/system/mariadb.service.d/limits.conf

systemctl daemon-reload
systemctl enable mariadb
systemctl start mariadb
systemctl status mariadb

yum install centos-release-scl
</code></pre>

<h3 id="install-centreon">Install Centreon</h3>

<pre><code>cd /opt
wget http://yum.centreon.com/standard/19.04/el7/stable/noarch/RPMS/centreon-release-19.04-1.el7.centos.noarch.rpm
yum install --nogpgcheck centreon-release-19.04-1.el7.centos.noarch.rpm
yum install -y centreon-base-config-centreon-engine centreon

echo &quot;date.timezone = Europe/Budapest&quot; &gt; /etc/opt/rh/rh-php71/php.d/php-timezone.ini
systemctl restart rh-php71-php-fpm

systemctl enable httpd24-httpd
systemctl enable snmpd
systemctl enable snmptrapd
systemctl enable rh-php71-php-fpm
systemctl enable centcore
systemctl enable centreontrapd
systemctl enable cbd
systemctl enable centengine
systemctl enable centreon

systemctl start rh-php71-php-fpm
systemctl start httpd24-httpd
systemctl start mysqld
systemctl start cbd
systemctl start snmpd
systemctl start snmptrapd
</code></pre>

<h3 id="configurate-centreon">Configurate centreon</h3>

<p><img src="/img/include/centreon_1.png" alt="Example image" />
<img src="/img/include/centreon_2.png" alt="Example image" />
<img src="/img/include/centreon_3.png" alt="Example image" />
<img src="/img/include/centreon_4.png" alt="Example image" />
<img src="/img/include/centreon_5.png" alt="Example image" />
<img src="/img/include/centreon_6.png" alt="Example image" />
<img src="/img/include/centreon_7.png" alt="Example image" />
<img src="/img/include/centreon_8.png" alt="Example image" />
<img src="/img/include/centreon_9.png" alt="Example image" /></p>

<pre><code>systemctl start cbd
systemctl start centcore
systemctl start centreontrapd
yum install centreon-widget* -y
</code></pre>

<p><img src="/img/include/centreon_10.png" alt="Example image" />
Select Central and Click Export Configuration. <br>
Then the poller will ativated.
<img src="/img/include/centreon_11.png" alt="Example image" /></p>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Pfsese USG S2S VPN]]></title>
            <link href="https://devopstales.github.io/home/pfsense-usg/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/pfsense-usg/</id>
            
            
            <published>2019-04-27T00:00:00+00:00</published>
            <updated>2019-04-27T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I will show you how to create a site-to-site VPN for pfSense ang unifi usg.</p>

<h3 id="creating-a-new-ipsec-vpn-on-pfsense">Creating a new IPsec VPN on pfsense</h3>

<p>At <code>VPN &gt; IPsec &gt; Add</code><br></p>

<p><img src="/img/include/usg-pfsense-1.png" alt="Example image" />
<img src="/img/include/usg-pfsense-2.png" alt="Example image" />
<img src="/img/include/usg-pfsense-3.png" alt="Example image" /></p>

<p>At <code>Firewall &gt; Roles &gt; IPsec &gt; Add</code><br>
<img src="/img/include/usg-pfsense-4.png" alt="Example image" /></p>

<h3 id="configure-usg">Configure USG</h3>

<p><img src="/img/include/usg-pfsense-5.png" alt="Example image" />
<img src="/img/include/usg-pfsense-6.png" alt="Example image" /></p>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Gitlab runner on Openshift]]></title>
            <link href="https://devopstales.github.io/home/openshift-gitlabrunner/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/openshift-gitlabrunner/?utm_source=atom_feed" rel="related" type="text/html" title="Install Gitlab runner on Openshift" />
                <link href="https://devopstales.github.io/cloud/openshift-extregistry/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: External registry" />
                <link href="https://devopstales.github.io/home/openshift-extregistry/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: External registry" />
                <link href="https://devopstales.github.io/home/openshift-cert/?utm_source=atom_feed" rel="related" type="text/html" title="Change Certificates in Openshift" />
                <link href="https://devopstales.github.io/cloud/openshift-cert/?utm_source=atom_feed" rel="related" type="text/html" title="Change Certificates in Openshift" />
            
                <id>https://devopstales.github.io/home/openshift-gitlabrunner/</id>
            
            
            <published>2019-04-20T00:00:00+00:00</published>
            <updated>2019-04-20T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will configure a gtlab rubber for Openshift.</p>

<h3 id="creating-a-service-account">Creating a Service Account</h3>

<pre><code>oc new-project gitlab-rubber
oc create sa gitlab-ci
oc policy add-role-to-user edit system:serviceaccount:gitlab-rubber:gitlab-ci

oc get sa
NAME         SECRETS   AGE
builder      2         2d
default      2         2d
deployer     2         2d
gitlab-ci    2         2d

oc describe sa gitlab-ci
Name:           gitlab-ci
Namespace:      constellation
Labels:         &lt;none&gt;
Annotations:    &lt;none&gt;

Image pull secrets:     gitlab-ci-dockercfg-q5mj9

Mountable secrets:      gitlab-ci-token-gvvkv
                        gitlab-ci-dockercfg-q5mj9

Tokens:                 gitlab-ci-token-gvvkv
                        gitlab-ci-token-tfsf7

oc describe secret gitlab-ci-token-gvvkv
...
token:          eyJ...&lt;very-long-token&gt;...-cw

oc login --token=eyJ...&lt;very-long-token&gt;...-cw
</code></pre>

<h3 id="edit-gitlab-ci-config">Edit Gitlab-ci config</h3>

<pre><code>nano  .gitlab-ci.yml
image: ebits/openshift-client

stages:
  - deployToOpenShift

variables:
  OPENSHIFT_SERVER: https://master.openshift.devopstales.intra:443
  OPENSHIFT_DOMAIN: openshift.devopstales.intra
  # Configure this variable in Secure Variables:
  OPENSHIFT_TOKEN: eyJ...&lt;very-long-token&gt;...-cw

.deploy: &amp;deploy
  before_script:
    - oc login &quot;$OPENSHIFT_SERVER&quot; --token=&quot;$OPENSHIFT_TOKEN&quot; --insecure-skip-tls-verify
  # login with the service account
    - oc project &quot;slides-openshift&quot;
  # enter into our slides project on OpenShift
  script:
    - &quot;oc get services $APP 2&gt; /dev/null || oc new-app . --name=$APP&quot;
  # create a new application from the image in the OpenShift registry
    - &quot;oc start-build $APP --from-dir=. --follow || sleep 3s&quot;
  # start a new build
    - &quot;oc get routes $APP 2&gt; /dev/null || oc expose service $APP --hostname=$APP_HOST&quot;
  # expose our application

develop:
  &lt;&lt;: *deploy
  stage: deployToOpenShift
  tags:
    - docker
  variables:
    APP: slides-openshift
    APP_HOST: demo-slides.$OPENSHIFT_DOMAIN
  environment:
    name: develop
    url: http://demo-slides.$OPENSHIFT_DOMAIN
  except:
    - master
</code></pre>

<h3 id="create-a-kubernetes-runner-in-openshift-from-template">Create a kubernetes runner in Openshift from template:</h3>

<pre><code>wget https://raw.githubusercontent.com/devopstales/openshift-examples/master/template/gitlab-runner-template.yml
oc deploy gitlab-runner-template.yml
</code></pre>

<p>Deploy the template from the gui:</p>

<pre><code>oc adm policy add-scc-to-user privileged system:serviceaccount:gitlab-rubber:&lt;application-name&gt;-user
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift: External registry]]></title>
            <link href="https://devopstales.github.io/home/openshift-extregistry/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/openshift-extregistry/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift: External registry" />
                <link href="https://devopstales.github.io/home/openshift-cert/?utm_source=atom_feed" rel="related" type="text/html" title="Change Certificates in Openshift" />
                <link href="https://devopstales.github.io/cloud/openshift-cert/?utm_source=atom_feed" rel="related" type="text/html" title="Change Certificates in Openshift" />
                <link href="https://devopstales.github.io/home/openshift-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure Openshift vSphere Cloud Provider" />
                <link href="https://devopstales.github.io/cloud/openshift-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure Openshift vSphere Cloud Provider" />
            
                <id>https://devopstales.github.io/home/openshift-extregistry/</id>
            
            
            <published>2019-04-19T00:00:00+00:00</published>
            <updated>2019-04-19T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I this post I will demonstrate howyou can use an external registry in Openshift.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../cloud/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../cloud/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../cloud/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../cloud/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../cloud/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../cloud/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../cloud/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../cloud/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../cloud/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../cloud/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../cloud/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../cloud/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../cloud/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../cloud/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../cloud/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<h3 id="configuring-openshift">Configuring Openshift</h3>

<p>From your client machine, create a Kubernetes secret object for Harbor.:</p>

<pre><code>oc new-proyect registrytest

oc create secret docker-registry harbor \
--docker-server=https://harbor.devopstales.intra \
--docker-username=admin \
--docker-email=admin@devopstales.intra \
--docker-password='[your_admin_harbor_password]'
</code></pre>

<p>If you want you can add this secret to the deafult template of the project creation.</p>

<h3 id="deploy-the-private-image-on-the-openshift-cluster">Deploy the private image on the Openshift cluster</h3>

<pre><code>nano registrytest-deployment.yaml
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  labels:
    run: nginx
  name: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      run: nginx
  template:
    metadata:
      labels:
        run: nginx
    spec:
      containers:
      - image: harbor.devopstales.intra/test/nginx:V2
        name: nginx
      imagePullSecrets:
      - name: harbor

oc apply -f kuard-deployment.yaml
oc get pods
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install vMWare Harbor]]></title>
            <link href="https://devopstales.github.io/home/harbor-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/harbor-install/</id>
            
            
            <published>2019-04-18T00:00:00+00:00</published>
            <updated>2019-04-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Vmware harbor ia an open source trusted cloud native registry project that stores, signs, and scans content.</p>

<p>Why harbor? Opeshift and Gitlab has its own docker regytry but nether can intgrate with clair Vulnerability scanner.</p>

<h3 id="install-docker-and-docker-compose">Install Docker and Docker-Compose</h3>

<pre><code>yum install epel-release wget -y
yum install -y yum-utils
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum install -y docker-ce

sudo yum install -y python-pip
pip install docker-compose

sudo systemctl start docker
sudo systemctl enable docker
</code></pre>

<h3 id="generate-your-own-ssl-certificate">Generate your own SSL certificate</h3>

<pre><code>nano certgen.sh
#!/bin/sh

export PASSPHRASE=$(head -c 500 /dev/urandom | tr -dc a-z0-9A-Z | head -c 128; echo)
DOMAIN=devopstales.intra

subj=&quot;
C=HU
ST=Pest
O=My Company
localityName=Budapest
commonName=*.$DOMAIN
organizationalUnitName=OU
emailAddress=root@$DOMAIN
&quot;

openssl genrsa -des3 -out domain.key -passout env:PASSPHRASE 2048

openssl req \
    -new \
    -batch \
    -subj &quot;$(echo -n &quot;$subj&quot; | tr &quot;\n&quot; &quot;/&quot;)&quot; \
    -key domain.key \
    -out domain.csr \
    -passin env:PASSPHRASE

cp domain.key domain.key.org

openssl rsa -in domain.key.org -out domain.key -passin env:PASSPHRASE

openssl x509 -req -days 3650 -in domain.csr -signkey domain.key -out domain.crt
cat domain.crt domain.key &gt; domain.pem
</code></pre>

<pre><code>chmod +x certgen.sh
./certgen.sh

mkdir -p /etc/docker/certs.d/harbor.devopstales.intra
cp domain.crt domain.key /etc/docker/certs.d/harbor.devopstales.intra/
cp domain.crt /etc/docker/certs.d/harbor.devopstales.intra/domain.cert
sudo systemctl restart docker
</code></pre>

<h3 id="install-notary">Install notary</h3>

<pre><code>curl -L https://github.com/theupdateframework/notary/releases/download/v0.6.1/notary-$(uname -s)-amd64 -o /usr/local/bin/notary
chmod +x /usr/local/bin/notary

mkdir -p ~/.docker/tls/harbor.devopstales.intra:4443/
cp ~/domain.crt ~/.docker/tls/harbor.devopstales.intra:4443/
cp ~/domain.key ~/.docker/tls/harbor.devopstales.intra:4443/
cp ~/domain.crt ~/.docker/tls/harbor.devopstales.intra:4443/domain.cert
</code></pre>

<h3 id="install-harbor">Install Harbor</h3>

<pre><code># https://github.com/vmware/harbor/releases/
wget https://storage.googleapis.com/harbor-releases/release-1.7.0/harbor-online-installer-v1.7.5.tgz
tar -xzf harbor-online-installer-v1.7.5.tgz

cd harbor
nano harbor.cfg
hostname = harbor.devopstales.intra
ui_url_protocol = https
ssl_cert = /root/domain.crt
ssl_cert_key = /root/domain.key

./prepare
./install.sh --with-notary --with-clair

docker login harbor.devopstales.intra
</code></pre>

<p>Access the Harbor UI with the username &ldquo;admin&rdquo; and password &ldquo;Harbor12345&rdquo;
<img src="/img/include/harbor_1.png" alt="Example image" /></p>

<p>Create a nwe project.
<img src="/img/include/harbor_2.png" alt="Example image" /></p>

<p>Configure automatic Vulnerability scan for project.
<img src="/img/include/harbor_3.png" alt="Example image" /></p>

<pre><code>docker pull nginx
docker tag nginx:latest harbor.devopstales.intra/test/nginx:V1
docker push harbor.devopstales.intra/test/nginx:V1
</code></pre>

<p><img src="/img/include/harbor_4.png" alt="Example image" /></p>

<pre><code>docker tag nginx:latest harbor.devopstales.intra/test/nginx:V2
export DOCKER_CONTENT_TRUST_SERVER=https://harbor.devopstales.intra:4443
export DOCKER_CONTENT_TRUST=1
docker push harbor.devopstales.intra/test/nginx:V2
</code></pre>

<p><img src="/img/include/harbor_5.png" alt="Example image" /></p>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Change Certificates in Openshift]]></title>
            <link href="https://devopstales.github.io/home/openshift-cert/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/openshift-cert/?utm_source=atom_feed" rel="related" type="text/html" title="Change Certificates in Openshift" />
                <link href="https://devopstales.github.io/cloud/openshift-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure Openshift vSphere Cloud Provider" />
                <link href="https://devopstales.github.io/home/openshift-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure Openshift vSphere Cloud Provider" />
                <link href="https://devopstales.github.io/cloud/openshift-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Helm" />
                <link href="https://devopstales.github.io/home/openshift-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Helm" />
            
                <id>https://devopstales.github.io/home/openshift-cert/</id>
            
            
            <published>2019-04-17T00:00:00+00:00</published>
            <updated>2019-04-17T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how can you chnage certificate in Openshift.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../cloud/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../cloud/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../cloud/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../cloud/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../cloud/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../cloud/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../cloud/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../cloud/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../cloud/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../cloud/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../cloud/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../cloud/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../cloud/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../cloud/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../cloud/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<h3 id="configure-certs">Configure certs:</h3>

<p>If you want to configure your Openshift cluster to use your own certificate you can do that wit this configuration.<br>
In my case the certificate files is MyCert.crt MyCert.key and the root CA is ccca.pem.</p>

<pre><code>nano /ec/ansible/hosts
openshift_master_overwrite_named_certificates=true
openshift_hosted_router_certificate={&quot;certfile&quot;: &quot;/root/cert/MyCert.crt&quot;, &quot;keyfile&quot;: &quot;/root/cert/MyCert.key&quot;, &quot;cafile&quot;: &quot;/root/cert/ccca.pem&quot;}
openshift_master_named_certificates=[{&quot;names&quot;: [&quot;master.openshit.devopstales.intra&quot;],&quot;certfile&quot;: &quot;/root/cert/MyCert.crt&quot;, &quot;keyfile&quot;: &quot;/root/cert/MyCert.key&quot;, &quot;cafile&quot;: &quot;/root/cert/ccca.pem&quot;}]

# registry
openshift_hosted_registry_routecertificates={&quot;certfile&quot;: &quot;/root/cert/MyCert.crt&quot;, &quot;keyfile&quot;: &quot;/root/cert/MyCert.key&quot;, &quot;cafile&quot;: &quot;/root/cert/ccca.pem&quot;}
openshift_hosted_registry_routetermination=reencrypt
</code></pre>

<h3 id="run-the-installer">Run the Installer</h3>

<p>If your certificate is renewd you can cahge the certificate in the cluster with this playbooks.</p>

<pre><code>ansible-playbook -i hosts /usr/share/ansible/openshift-ansible/playbooks/openshift-master/redeploy-openshift-ca.yml
ansible-playbook -i hosts /usr/share/ansible/openshift-ansible/playbooks/redeploy-certificates.yml
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configure Openshift vSphere Cloud Provider]]></title>
            <link href="https://devopstales.github.io/home/openshift-vmware/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/openshift-vmware/?utm_source=atom_feed" rel="related" type="text/html" title="Configure Openshift vSphere Cloud Provider" />
                <link href="https://devopstales.github.io/home/openshift-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Helm" />
                <link href="https://devopstales.github.io/cloud/openshift-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Helm" />
                <link href="https://devopstales.github.io/cloud/openshift-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/openshift-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Ceph RBD for dynamic provisioning" />
            
                <id>https://devopstales.github.io/home/openshift-vmware/</id>
            
            
            <published>2019-04-16T00:00:00+00:00</published>
            <updated>2019-04-16T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how can you use vmware for persistent storagi on Openshift.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../cloud/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../cloud/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../cloud/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../cloud/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../cloud/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../cloud/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../cloud/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../cloud/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../cloud/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../cloud/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../cloud/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../cloud/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../cloud/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../cloud/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../cloud/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<h3 id="vsphere-configuration">vSphere Configuration</h3>

<ul>
<li>Create a folder for all the VMs in vCenter

<ul>
<li>In the navigator, select the data center</li>
<li>Right-click and select the menu option to create the folder.</li>
<li>Select All vCenter Actions &gt; New VM and Template Folder.</li>
<li>Move Openshift vms to this folder</li>
</ul></li>
<li>The name of the virtual machine must match the name of the nodes for the OpenShift cluster.</li>
</ul>

<h3 id="set-up-the-govc-environment">Set up the GOVC environment:</h3>

<pre><code># on deployer
curl -LO https://github.com/vmware/govmomi/releases/download/v0.20.0/govc_linux_amd64.gz
gunzip govc_linux_amd64.gz
chmod +x govc_linux_amd64
cp govc_linux_amd64 /usr/bin/govc
echo &quot;export GOVC_URL='vCenter IP OR FQDN'&quot; &gt;&gt; /etc/profile
echo &quot;export GOVC_USERNAME='vCenter User'&quot; &gt;&gt; /etc/profile
echo &quot;export GOVC_PASSWORD='vCenter Password'&quot; &gt;&gt; /etc/profile
echo &quot;export GOVC_INSECURE=1&quot; &gt;&gt; /etc/profile
source /etc/profile

govc vm.info &lt;vm&gt;
govc ls /Datacenter/vm/&lt;vm-folder-name&gt;
govc vm.change -e=&quot;disk.enableUUID=1&quot; -vm='VM Path'
</code></pre>

<h3 id="configure-ansible-installer">Configure ansible installer</h3>

<pre><code>nano /etc/hosts
openshift_master_dynamic_provisioning_enabled=true
openshift_cloudprovider_kind=vsphere
openshift_cloudprovider_vsphere_username=&lt;vCenter User&gt;
openshift_cloudprovider_vsphere_password=&lt;vCenter Password&gt;
openshift_cloudprovider_vsphere_host=&lt;vCenter IP OR FQDN&gt;
openshift_cloudprovider_vsphere_datacenter=&lt;Datacenter&gt;
openshift_cloudprovider_vsphere_datastore=&lt;Datastore&gt;
openshift_cloudprovider_vsphere_folder=&lt;vm-folder-name&gt;
</code></pre>

<h3 id="add-providerid">Add providerID</h3>

<pre><code>nano openshift-vmware-pacher.sh
DATACENTER='&lt;Datacenter&gt;'
FOLDER='&lt;vm-folder-name&gt;'
for vm in $(govc ls /$DATACENTER/vm/$FOLDER ); do
  MACHINE_INFO=$(govc vm.info -json -dc=$DATACENTER -vm.ipath=&quot;$vm&quot; -e=true)
  # My VMs are created on vmware with upper case names, so I need to edit the names with awk
  VM_NAME=$(jq -r ' .VirtualMachines[] | .Name' &lt;&lt;&lt; $MACHINE_INFO | awk '{print tolower($0)}')
  # UUIDs come in lowercase, upper case then
  VM_UUID=$( jq -r ' .VirtualMachines[] | .Config.Uuid' &lt;&lt;&lt; $MACHINE_INFO | awk '{print toupper($0)}')
  echo &quot;Patching $VM_NAME with UUID:$VM_UUID&quot;
  # This is done using dry-run to avoid possible mistakes, remove when you are confident you got everything right.
  kubectl patch node $VM_NAME -p &quot;{\&quot;spec\&quot;:{\&quot;providerID\&quot;:\&quot;vsphere://$VM_UUID\&quot;}}&quot;
done

chmod +x openshift-vmware-pacher.sh
./openshift-vmware-pacher.sh
</code></pre>

<h3 id="run-the-installer">Run the Installer</h3>

<pre><code># deployer
cd /usr/share/ansible/openshift-ansible/
sudo ansible-playbook -i inventory/hosts.localhost playbooks/prerequisites.yml
sudo ansible-playbook -i inventory/hosts.localhost playbooks/deploy_cluster.yml

# If installastion failed or went wrong, the following uninstallation script can be run, and running installation can be tried again:
sudo ansible-playbook -i inventory/hosts.localhost playbooks/adhoc/uninstall.yml
</code></pre>

<h3 id="create-vsphere-storage-class">Create vSphere storage-class</h3>

<pre><code>nano vmware-sc.yml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: &quot;true&quot;
  name: &quot;vsphere-standard&quot;
provisioner: kubernetes.io/vsphere-volume
parameters:
    diskformat: zeroedthick
    datastore: &quot;NFS&quot;
reclaimPolicy: Delete

oc aplay -f vmware-sc.yml
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift Helm]]></title>
            <link href="https://devopstales.github.io/home/openshift-helm/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/openshift-helm/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Helm" />
                <link href="https://devopstales.github.io/home/openshift-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/cloud/openshift-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/cloud/openshift-ldap/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift LDAP authentication" />
                <link href="https://devopstales.github.io/home/openshift-ldap/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift LDAP authentication" />
            
                <id>https://devopstales.github.io/home/openshift-helm/</id>
            
            
            <published>2019-04-15T00:00:00+00:00</published>
            <updated>2019-04-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I this post I will demonstrate the basic configuration of Helm on Openshift.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../cloud/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../cloud/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../cloud/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../cloud/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../cloud/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../cloud/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../cloud/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../cloud/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../cloud/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../cloud/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../cloud/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../cloud/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../cloud/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../cloud/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../cloud/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<h3 id="helm">Helm</h3>

<p>Helm is a package manager and teplating engine for Kubernetes. It based on tree main components:</p>

<ul>
<li>the helm cli client</li>
<li>the helm server called tiller</li>
<li>the template pcakage called halm chart</li>
</ul>

<h3 id="install-helm-cli">Install helm cli</h3>

<pre><code># https://github.com/helm/helm/releases
curl -s https://storage.googleapis.com/kubernetes-helm/helm-v2.9.1-linux-amd64.tar.gz | tar xz
cd linux-amd64
cp helm /usr/bin
</code></pre>

<h3 id="helm-with-cluster-admin-permissions">Helm with cluster-admin permissions</h3>

<pre><code>nano helm-cluster-admin.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller-admin
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tiller-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: tiller-admin
  namespace: kube-system
</code></pre>

<h3 id="init-helm">Init helm</h3>

<pre><code>oc login master.openshift.devopstales.intra:443
kubectl apply -f helm-cluster-admin.yaml
helm init --service-account tiller-admin
</code></pre>

<h3 id="test-hem">Test hem</h3>

<pre><code>oc new-project myapp
helm install stable/ghost -n blog

oc get pods -n myapp
export APP_HOST=$(kubectl get svc --namespace myapp blog-ghost --template &quot;{{ range (index .status.loadBalancer.ingress 0) }}{{ . }}{{ end }}&quot;)
export APP_PASSWORD=$(kubectl get secret --namespace myapp blog-ghost -o jsonpath=&quot;{.data.ghost-password}&quot; | base64 --decode)
export APP_DATABASE_PASSWORD=$(kubectl get secret --namespace myapp blog-mariadb -o jsonpath=&quot;{.data.mariadb-password}&quot; | base64 --decode)
helm upgrade blog stable/ghost --set service.type=LoadBalancer,ghostHost=$APP_HOST,ghostPassword=$APP_PASSWORD,mariadb.db.password=$APP_DATABASE_PASSWORD

oc get pods -n myapp
echo Password: $(kubectl get secret --namespace myapp blog-ghost -o jsonpath=&quot;{.data.ghost-password}&quot; | base64 --decode)
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Pfsese https]]></title>
            <link href="https://devopstales.github.io/home/pfsense-cert/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/pfsense-cert/</id>
            
            
            <published>2019-04-15T00:00:00+00:00</published>
            <updated>2019-04-15T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I will show you how to Enable SSL for pfSense.</p>

<h3 id="creating-a-new-certificate">Creating a new Certificate</h3>

<p>At <code>System &gt; Certificate Manager &gt; Certificates &gt; Add</code><br>
Make sure you choose &ldquo;Import an existing Certificate&rdquo; under Method and enter Descriptive name so you know what the certificate is.
<img src="/img/include/pfsense_cert_1.png" alt="Example image" /></p>

<p>At System &gt; Advanced &gt; Admin Access<br>
Make sure HTTPS is selected as Protocol and now change the SSL Certificate to the one you have created. Scroll down and click on Save. Now, when you restart your Web Browser, you should see a Secure Connection to pfSense when accessing it.
<img src="/img/include/pfsense_cert_2.png" alt="Example image" /></p>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[RBAC permissions for Helm]]></title>
            <link href="https://devopstales.github.io/home/k8s-helm-rbac/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/k8s-helm-rbac/?utm_source=atom_feed" rel="related" type="text/html" title="RBAC permissions for Helm" />
                <link href="https://devopstales.github.io/home/kubectl-multi-cluster-config/?utm_source=atom_feed" rel="related" type="text/html" title="Configure kubectl for multiple clusters" />
                <link href="https://devopstales.github.io/cloud/kubectl-multi-cluster-config/?utm_source=atom_feed" rel="related" type="text/html" title="Configure kubectl for multiple clusters" />
            
                <id>https://devopstales.github.io/home/k8s-helm-rbac/</id>
            
            
            <published>2019-04-14T00:00:00+00:00</published>
            <updated>2019-04-14T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I this post I will demonstrate the basic mechanism of helm and Role-based access control (RBAC).</p>

<h3 id="parst-of-the-kubernetes-series">Parst of the Kubernetes series</h3>

<ul>
<li>Part1a: <a href="../../cloud/ansible-k8s-install/">Install K8S with ansible</a></li>
<li>Part1b: <a href="../../cloud/k8s-install/">Install K8S with kubeadm</a></li>
<li>Part1c: <a href="../../cloud/k8s-kubeadm-ha/">Install K8S with kubeadm in HA mode</a></li>
<li>Part2: <a href="../../cloud/k8s-metallb/">Intall metal-lb with K8S</a></li>
<li>Part3: <a href="../../cloud/k8s-nginx-ingress/">Install Nginx ingress to K8S</a>
<!-- * Part3b: k8s-multiple-nginx-ingress -->
<!-- * Part3c: k8s-traefik-ingress --></li>
<li>Part4: <a href="../../cloud/k8s-cert-manager/">Install cert-manager to K8S</a>
<!-- local folder (+ autoprovisioning ) --></li>
<li>Part5a: <a href="../../cloud/k8s-local-pv/">Use local persisten volume with K8S</a></li>
<li>Part5b: <a href="../../cloud/k8s-ceph/">Use ceph persisten volume with K8S</a></li>
<li>Part5c: <a href="../../cloud/k8s-ceph-storage-with-csi-driver/">Use ceph CSI persisten volume with K8S</a></li>
<li>Part5d: <a href="../../cloud/k8s-longhorn/">Use Project Longhorn as persisten volume with K8S</a>
<!-- CEPH in Kubernetes with rook --></li>
<li>Part5d: <a href="../../cloud/k8s-install-openebs/">Use OpenEBS as persisten volume with K8S</a>
<!-- Kadalu (based on glustergs) -->
<!-- linstore (based on drbd) -->
<!-- kubernetes vMware --></li>
<li>Part6: <a href="../../cloud/k8s-helm-rbac/">Use Helm with K8S</a></li>
<li>Part7: <a href="../../cloud/k8s-tillerless-helm/">Tillerless helm2 install</a></li>
<li>Part8: <a href="../../sso/k8s-dasboard-auth/">Kubernetes Dashboard SSO</a></li>
<li>Part9: <a href="../../sso/k8s-kuberos/">Kuberos for K8S</a></li>
<li>Part10: <a href="../../sso/k8s-gangway/">Gangway for K8S</a>
<!-- istio-install.md --></li>
<li>Part11: <a href="../../cloud/k8s-velero-backup/">Velero Backup for K8S</a>
<!-- prometheus operator + grafana operator -->
<!-- grafana loki -->
<!-- logs to graylog --></li>
</ul>

<p>I whant to use helm on Openshift but firt I startid with the basics of helm and Role-based access control (RBAC) on a simple Kubernestes cluster. Most people seem to be running Helm with their own credentials or a dedicated service account with cluster-admin permissions. This isn’t very good from a security perspective, especially so if it’s being run within CI/CD.</p>

<h3 id="helm">Helm</h3>

<p>Helm is a package manager and teplating engine for Kubernetes. It based on tree main components:</p>

<ul>
<li>the helm cli client</li>
<li>the helm server called tiller</li>
<li>the template pcakage called halm chart</li>
</ul>

<h3 id="helm-with-cluster-admin-permissions">Helm with cluster-admin permissions</h3>

<pre><code>nano helm-cluster-admin.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller-admin
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tiller-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: tiller-admin
  namespace: kube-system
</code></pre>

<h3 id="helm-with-namespace-permissions">Helm with namespace permissions</h3>

<p>We are granting permissions on only the API groups and resources that Tiller needs to deploy and manage releases in its namespace.</p>

<pre><code>nano helm-dev-namespace.yaml
kind: Namespace
apiVersion: v1
metadata:
  name: dev
---
kind: ServiceAccount
apiVersion: v1
metadata:
  name: tiller
  namespace: dev
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: tiller-manager
  namespace: dev
rules:
- apiGroups: [&quot;&quot;, &quot;batch&quot;, &quot;extensions&quot;, &quot;apps&quot;]
  resources: [&quot;*&quot;]
  verbs: [&quot;*&quot;]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: tiller-binding
  namespace: dev
subjects:
- kind: ServiceAccount
  name: tiller
  namespace: dev
roleRef:
  kind: Role
  name: tiller-manager
  apiGroup: rbac.authorization.k8s.io
</code></pre>

<pre><code>nano helm-prod-namespace.yaml
kind: Namespace
apiVersion: v1
metadata:
  name: prod
---
kind: ServiceAccount
apiVersion: v1
metadata:
  name: tiller
  namespace: prod
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: tiller-manager
  namespace: prod
rules:
- apiGroups: [&quot;&quot;, &quot;batch&quot;, &quot;extensions&quot;, &quot;apps&quot;]
  resources: [&quot;*&quot;]
  verbs: [&quot;*&quot;]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: tiller-binding
  namespace: prod
subjects:
- kind: ServiceAccount
  name: tiller
  namespace: prod
roleRef:
  kind: Role
  name: tiller-manager
  apiGroup: rbac.authorization.k8s.io
</code></pre>

<pre><code>kubectl create -f helm-dev-namespace.yaml
kubectl create -f helm-prod-namespace.yaml

kubectl -n dev get sa
kubectl -n prod get sa

helm init --service-account tiller --tiller-namespace dev
helm init --service-account tiller --tiller-namespace prod
</code></pre>

<h3 id="helm-with-minimal-cluster-permissions">Helm with minimal cluster permissions</h3>

<pre><code>nano helm-cluster-role.yml
kind: Namespace
apiVersion: v1
metadata:
  name: helm
---
kind: ServiceAccount
apiVersion: v1
metadata:
  name: helm
  namespace: helm
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: helm-clusterrole
rules:
  - apiGroups: [&quot;&quot;]
    resources: [&quot;pods/portforward&quot;]
    verbs: [&quot;create&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;pods&quot;]
    verbs: [&quot;list&quot;, &quot;get&quot;]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: helm-clusterrolebinding
roleRef:
  kind: ClusterRole
  apiGroup: rbac.authorization.k8s.io
  name: helm-clusterrole
subjects:
  - kind: ServiceAccount
    name: helm
    namespace: helm
</code></pre>

<h3 id="generate-a-kubeconfig-file-from-the-helm-service-account">Generate a Kubeconfig file from the Helm Service Account</h3>

<p>Credit to Ami Mahloof for this script.</p>

<pre><code>NAMESPACE=helm
# Find the secret associated with the Service Account
SECRET=$(kubectl -n $NAMESPACE get sa helm -o jsonpath='{.secrets[].name}')
# Get the token from the secret
TOKEN=$(kubectl get secrets -n $NAMESPACE $SECRET -o jsonpath='{.data.token}' | base64 -D)
# Get the CA from the secret
kubectl get secrets -n $NAMESPACE $SECRET -o jsonpath='{.data.ca\.crt}' | base64 -D &gt; ca.crt

CONTEXT=$(kubectl config current-context)
CLUSTER_NAME=$(kubectl config get-contexts $CONTEXT --no-headers=true | awk '{print $3}')
SERVER=$(kubectl config view -o jsonpath=&quot;{.clusters[?(@.name == \&quot;${CLUSTER_NAME}\&quot;)].cluster.server}&quot;)
KUBECONFIG_FILE=config
USER=helm
CA=ca.crt

# Set up config
kubectl config set-cluster $CLUSTER_NAME \
--kubeconfig=$KUBECONFIG_FILE \
--server=$SERVER \
--certificate-authority=$CA \
--embed-certs=true

kubectl config set-credentials $USER \
--kubeconfig=$KUBECONFIG_FILE \
--token=$TOKEN

kubectl config set-context $USER \
--kubeconfig=$KUBECONFIG_FILE \
--cluster=$CLUSTER_NAME \
--user=$USER

kubectl config use-context $USER \
--kubeconfig=$KUBECONFIG_FILE
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift SSO authentication]]></title>
            <link href="https://devopstales.github.io/home/openshift-sso/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/openshift-sso/</id>
            
            
            <published>2019-04-13T00:00:00+00:00</published>
            <updated>2019-04-13T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Configure Openshift Cluster to use Keycloak as a user backend for login with oauth2 and SSO.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../cloud/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../cloud/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../cloud/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../cloud/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../cloud/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../cloud/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../cloud/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../cloud/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../cloud/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../cloud/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../cloud/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../cloud/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../cloud/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../cloud/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../cloud/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<p>With Ansible-openshift you can not change the authetication method after Install !! If you installed the cluster with htpasswd, then change to LDAP the playbook trys to add a second authentication methot for the config. It is forbidden to add a second type of identity provider in the version 3.11 of Ansible-openshift. To solv this problem we must change the configuration manually.</p>

<h3 id="environment">Environment</h3>

<pre><code>192.168.1.40    deployer
192.168.1.41    openshift01 # master node
192.168.1.42    openshift02 # infra node
192.168.1.43    openshift03 # worker node
</code></pre>

<h3 id="configuration-on-keycloak">Configuration on Keycloak</h3>

<pre><code>create new client on keycloak in relm mydomain
Client ID: openshift
Clyent Protocol: openid-connect
Access type: confidential
Valid Redirect URIs: https://master.openshift.mydomain.itra/*
delete other urls

# On Credentials tap copy the secret to clientSecrethez in config.
</code></pre>

<h3 id="configurate-the-cluster">Configurate The cluster</h3>

<pre><code># on all openshift hosts
nano /etc/origin/master/master-config.yaml
...
  identityProviders:
  - name: keycloak
    challenge: false
    login: true
    provider:
      apiVersion: v1
      kind: OpenIDIdentityProvider
      clientID: openshift
      clientSecret: ef03ffe6-854a-48b4-a26d-190c2861e3c8
      claims:
        id:
        - sub
        preferredUsername:
        - preferred_username
        name:
        - name
        email:
        - email
      urls:
        authorize: https://sso.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/auth
        token: https://sso.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/token
        logoutURL: &quot;https://sso.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/logout?redirect_uri=https://master.openshift.mydomain.itra/console&quot;
  - challenge: true
</code></pre>

<h3 id="reconfigurate-the-cluster">Reconfigurate the cluster</h3>

<pre><code># on all openshift hosts
master-restart api
master-restart controllers
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift Ceph RBD for dynamic provisioning]]></title>
            <link href="https://devopstales.github.io/home/openshift-ceph/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/openshift-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/cloud/openshift-ldap/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift LDAP authentication" />
                <link href="https://devopstales.github.io/home/openshift-ldap/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift LDAP authentication" />
                <link href="https://devopstales.github.io/cloud/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
                <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
            
                <id>https://devopstales.github.io/home/openshift-ceph/</id>
            
            
            <published>2019-04-12T00:00:00+00:00</published>
            <updated>2019-04-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will show you how can you use CEPH RBD for persistent storagi on Openshift.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../cloud/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../cloud/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../cloud/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../cloud/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../cloud/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../cloud/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../cloud/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../cloud/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../cloud/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../cloud/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../cloud/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../cloud/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../cloud/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../cloud/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../cloud/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<h3 id="environment">Environment</h3>

<pre><code># openshift cluster
192.168.1.41    openshift01 # master node
192.168.1.42    openshift02 # infra node
192.168.1.43    openshift03 # worker node

# ceph cluster
192.168.1.31    ceph01
192.168.1.32    ceph02
192.168.1.33    ceph03
</code></pre>

<h3 id="prerequirement">Prerequirement</h3>

<p>RBD volume provisioner needs admin key from Ceph to provision storage. To get the admin key from Ceph cluster use this command:</p>

<pre><code>sudo ceph --cluster ceph auth get-key client.admin | base64
QVFBOFF2SlZheUJQRVJBQWgvS2cwT1laQUhPQno3akZwekxxdGc9PQ==

nano ceph-admin-secret.yaml
apiVersion: v1
data:
  key: QVFBOFF2SlZheUJQRVJBQWgvS2cwT1laQUhPQno3akZwekxxdGc9PQ==
kind: Secret
metadata:
  name: ceph-admin-secret
  namespace: kube-system
type: kubernetes.io/rbd

</code></pre>

<p>I will also create a separate Ceph pool for</p>

<pre><code>sudo ceph --cluster ceph osd pool create k8s 1024 1024
sudo ceph --cluster ceph auth get-or-create client.k8s mon 'allow r' osd 'allow rwx pool=k8s'
sudo ceph --cluster ceph auth get-key client.k8s | base64
QVFBOFF2SlZheUJQRVJBQWgvS2ctS2htOFNSZnRvclJPRk1jdXc9PQ==

nano ceph-secret-k8s.yaml
apiVersion: v1
data:
  key: QVFBOFF2SlZheUJQRVJBQWgvS2ctS2htOFNSZnRvclJPRk1jdXc9PQ==
kind: Secret
metadata:
  name: ceph-secret-k8s
  namespace: kube-system
type: kubernetes.io/rbd

</code></pre>

<pre><code># on all openshift node
yum install -y ceph-common

# on one openshift master node
nano  k8s-storage.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: &quot;true&quot;
  name: k8s
parameters:
  adminId: admin
  adminSecretName: ceph-admin-secret
  adminSecretNamespace: kube-system
  imageFeatures: layering
  imageFormat: &quot;2&quot;
  monitors: 192.168.1.31:6789, 192.168.1.32:6789, 192.168.1.33:6789
  pool: k8s
  userId: k8s
  userSecretName: ceph-secret-k8s
provisioner: kubernetes.io/rbd
reclaimPolicy: Delete
volumeBindingMode: Immediate


oc create -f ceph-admin-secret.yaml
oc create -f ceph-secret-k8s.yaml
oc create -f k8s-storage.yaml
</code></pre>

<h3 id="add-secrets-to-existng-namespaces">Add secrets to existng namespaces</h3>

<pre><code># on one openshift master node
oc project default
oc apply -f ceph-secret-k8s.yaml

oc project management-infra
oc apply -f ceph-secret-k8s.yaml

oc project openshift-infra
oc apply -f ceph-secret-k8s.yaml

oc project openshift-logging
oc apply -f ceph-secret-k8s.yaml

oc project openshift-metrics-server
oc apply -f ceph-secret-k8s.yaml

oc project openshift-monitoring
oc apply -f ceph-secret-k8s.yaml
</code></pre>

<h3 id="add-secret-to-template">Add secret to template</h3>

<p>If we add the secret to the template iw will be present in all of the newly created namespaces.</p>

<pre><code># on one openshift master node
su - origin
oc adm create-bootstrap-project-template -o yaml &gt; template.yaml
# add secrets to  the yml without namespace
nano template.yaml
...
- apiVersion: v1
  data:
    key: QVFBOFF2SlZheUJQRVJBQWgvS2ctS2htOFNSZnRvclJPRk1jdXc9PQ==
  kind: Secret
  metadata:
    name: ceph-secret-k8s
  type: kubernetes.io/rbd
...
oc create -f template.yaml -n default

# on all the openshift master nodes
nano /etc/origin/master/master-config.yaml
...
projectConfig:
  projectRequestTemplate: &quot;default/project-request&quot;
...

master-restart api
master-restart controllers
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Openshift LDAP authentication]]></title>
            <link href="https://devopstales.github.io/home/openshift-ldap/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/openshift-ldap/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift LDAP authentication" />
                <link href="https://devopstales.github.io/home/openshift-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/cloud/openshift-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Openshift Ceph RBD for dynamic provisioning" />
                <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
                <link href="https://devopstales.github.io/cloud/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
            
                <id>https://devopstales.github.io/home/openshift-ldap/</id>
            
            
            <published>2019-04-12T00:00:00+00:00</published>
            <updated>2019-04-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Configure Openshift Cluster to use LDAP as a user backend for login with Ansible-openshift</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../cloud/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../cloud/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../cloud/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../cloud/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../cloud/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../cloud/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../cloud/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../cloud/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../cloud/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../cloud/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../cloud/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../cloud/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../cloud/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../cloud/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../cloud/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<p>In the last post I used the basic htpasswd authentication method for the installatipn.<br>
But I can use Ansible-openshift to configure an LDAP backed at the install for the authentication.</p>

<h3 id="environment">Environment</h3>

<pre><code>192.168.1.40    deployer
192.168.1.41    openshift01 # master node
192.168.1.42    openshift02 # infra node
192.168.1.43    openshift03 # worker node
</code></pre>

<p>With Ansible-openshift you can not change the authetication method after Install !! If you installed the cluster with htpasswd, then change to LDAP the playbook trys to add a second authentication methot for the config. It is forbidden to add a second type of identity provider in the version 3.11 of Ansible-openshift so choose wisely.</p>

<h3 id="configurate-installer">Configurate Installer</h3>

<pre><code># deployer
nano /etc/ansible/ansible.cfg
# use HTPasswd for authentication
#openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'}]

# LDAP
openshift_master_identity_providers=[{'name': 'email_jira_ldap', 'challenge': 'true', 'login': 'true', 'kind': 'LDAPPasswordIdentityProvider', 'attributes': {'id': ['mail'], 'email': ['mail'], 'name': ['displayName'], 'preferredUsername': ['mail']}, 'bindDN': 'CN=ldapbrowser,DC=mydomain,DC=myintra', 'bindPassword': '*******', 'insecure': 'true', 'url': 'ldap://ldap01.mydomain.myintra/dc=mydomain,dc=myintra?mail?sub?(objectClass=*)'}]
</code></pre>

<h3 id="run-the-installer">Run the Installer</h3>

<pre><code># deployer
cd /usr/share/ansible/openshift-ansible/
sudo ansible-playbook -i inventory/hosts.localhost playbooks/prerequisites.yml
sudo ansible-playbook -i inventory/hosts.localhost playbooks/deploy_cluster.yml
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[WAN failower on pfsense]]></title>
            <link href="https://devopstales.github.io/home/pfsense-wlan/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/pfsense-wlan/</id>
            
            
            <published>2019-04-12T00:00:00+00:00</published>
            <updated>2019-04-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this pos I&rsquo;ll create a WAN failower configuration.</p>

<h3 id="the-architecture">The Architecture</h3>

<pre><code>
------- WAN1 ------
| ----- WAN2 ---- |
| |             | |
PF1 -- sync -- PF2
 |               |
 ----- LAN -------  


WAN1: 192.168.0.0/24 (Bridgelt)
LAN: 10.0.1.0/24
SYNC: 10.0.2.0/24
WAN2: 10.0.4.0/24
</code></pre>

<h3 id="configurate-wip-for-wan2">Configurate WIP for WAN2</h3>

<p>At <code>Firewall &gt; Virtual IPs &gt; Add</code>
<img src="/img/include/pfsenseWAN_1.jpg" alt="Example image" /></p>

<h3 id="add-gateway-for-wan-interfaces">Add Gateway for WAN interfaces</h3>

<p>At <code>System &gt; Routing &gt; Add</code>
<img src="/img/include/pfsenseWAN_2.jpg" alt="Example image" /></p>

<h3 id="configuring-monitor-ip">Configuring Monitor IP</h3>

<p>At<code>System &gt; Routing &gt; Edit gateways</code> and add google dns ad monitoring ip
<img src="/img/include/pfsenseWAN_3.jpg" alt="Example image" /></p>

<p><img src="/img/include/pfsenseWAN_4.jpg" alt="Example image" /></p>

<h3 id="configuring-gateway-group">Configuring Gateway Group</h3>

<p>At<code>System &gt; Routing &gt; Gateway Groups</code> Create 3 Groups
<img src="/img/include/pfsenseWAN_5.jpg" alt="Example image" /></p>

<p><img src="/img/include/pfsenseWAN_6.jpg" alt="Example image" /></p>

<p><img src="/img/include/pfsenseWAN_7.jpg" alt="Example image" /></p>

<h3 id="configuring-firewall-rules">Configuring Firewall Rules</h3>

<p>Got to <code>Firewall &gt; Rules &gt; LAN</code> and edit the IPv4 rule. Chane the Gateway</p>

<p><img src="/img/include/pfsenseWAN_8.jpg" alt="Example image" /></p>

<p><img src="/img/include/pfsenseWAN_9.jpg" alt="Example image" /></p>

<p>Clone the changed roles to two other rules and change the Gateway to the other Gateway Groups.
<img src="/img/include/pfsenseWAN_10.jpg" alt="Example image" /></p>

<h3 id="configurate-nat">Configurate NAT</h3>

<p>Go to<code>Firewall &gt; NAT &gt; Outbound</code> <br>
Clone WAN1 rules and edit them to WLAN2
<img src="/img/include/pfsenseWAN_11.jpg" alt="Example image" /></p>

<p><img src="/img/include/pfsenseWAN_12.jpg" alt="Example image" /></p>

<h2 id="pfsense-email-notification-when-wan-connection-goes-down">pfSense email notification when WAN connection goes down</h2>

<p>Go to <code>System &gt; Advanced &gt; Notifications</code></p>

<h3 id="example-with-google-gmail-smtp">Example with Google Gmail SMTP</h3>

<p><img src="/img/include/pfsenseWAN_13.jpg" alt="Example image" /></p>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configure OpenVPN HA pfsense cluster]]></title>
            <link href="https://devopstales.github.io/home/pfsense-openvpn/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/pfsense-openvpn/</id>
            
            
            <published>2019-04-11T00:00:00+00:00</published>
            <updated>2019-04-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this LAB I&rsquo;ll be creating OpenVPN SSL Peer to Peer connection.</p>

<h3 id="generating-ca-certificate">Generating CA Certificate</h3>

<p>At <code>System &gt; Cert.Manager &gt; CAs &gt; Add</code>
<img src="/img/include/OpenVPN_1.jpg" alt="Example image" /></p>

<p><img src="/img/include/OpenVPN_2.jpg" alt="Example image" /></p>

<h3 id="generate-server-certificate">Generate Server Certificate</h3>

<p>At <code>System &gt; Cert.Manager &gt; Certificates &gt; Add</code>
<img src="/img/include/OpenVPN_3.jpg" alt="Example image" /></p>

<h3 id="generate-user-certificate">Generate User Certificate</h3>

<p>For this demo I will&rsquo;create one certificate for all users, but in live you should create a separate certificate for all users.</p>

<p>At <code>System &gt; Cert.Manager &gt; Certificates &gt; Add</code>
<img src="/img/include/OpenVPN_4.jpg" alt="Example image" /></p>

<p>At <code>SystemUser &gt; ManagerUsers</code> add the User certificate for the users.
<img src="/img/include/OpenVPN_5.jpg" alt="Example image" /></p>

<h3 id="intall-openvpn-package-exporter">Intall Openvpn package exporter</h3>

<p>Got to<code>System &gt; Package Manager &gt; Available Packages</code> and install <code>openvpn-client-export</code> plugin.</p>

<h3 id="configurate-the-opevpn-service">Configurate the OpeVPN service</h3>

<p>Got to <code>VPN &gt; OpenVPN &gt; Wizards</code>
<img src="/img/include/OpenVPN_6.jpg" alt="Example image" /></p>

<p><img src="/img/include/OpenVPN_7.jpg" alt="Example image" /></p>

<p><img src="/img/include/OpenVPN_8.jpg" alt="Example image" /></p>

<p><img src="/img/include/OpenVPN_9.jpg" alt="Example image" /></p>

<p>Edit the Adwanced Configuration:
<img src="/img/include/OpenVPN_18.jpg" alt="Example image" /></p>

<p><img src="/img/include/OpenVPN_10.jpg" alt="Example image" /></p>

<p><img src="/img/include/OpenVPN_11.jpg" alt="Example image" /></p>

<h3 id="configurate-nat-rules-to-ha">Configurate NAT Rules to HA</h3>

<p>Go to <code>Firewall &gt; NAT &gt; Outbound</code> and clone the LAN Rules?
<img src="/img/include/OpenVPN_12.jpg" alt="Example image" /></p>

<p><img src="/img/include/OpenVPN_13.jpg" alt="Example image" /></p>

<p><img src="/img/include/OpenVPN_14.jpg" alt="Example image" /></p>

<p><img src="/img/include/OpenVPN_15.jpg" alt="Example image" /></p>

<h3 id="enable-connection-from-openvpn-to-master-and-slave">Enable Connection from OpenVPN to master and slave</h3>

<p>In default there in no rout to the salve nod. Go to <code>Firewll &gt; Aliases &gt; Add</code> and create alias for CARP members:
<img src="/img/include/OpenVPN_16.png" alt="Example image" /></p>

<p>Then go back to <code>Firewall &gt; NAT &gt; Outbound</code> and create a new rule:
<img src="/img/include/OpenVPN_17.png" alt="Example image" /></p>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configurate HA pfsense cluster]]></title>
            <link href="https://devopstales.github.io/home/pfsense-ha/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/pfsense-ha/</id>
            
            
            <published>2019-04-10T00:00:00+00:00</published>
            <updated>2019-04-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this post I will configure 2 pfsense server to a HA cluster.</p>

<h3 id="the-architecture">The Architecture</h3>

<pre><code> ------ WAN ------
 |               |
PF1 -- sync -- PF2
 |               |
 ----- LAN -------  

WAN: 192.168.0.0/24 (Bridgelt)
LAN: 10.0.1.0/24
SYNC: 10.0.2.0/24
</code></pre>

<pre><code>pf1:
WAN 192.168.0.21
LAN: 10.0.1.21
SYNC:10.0.2.21

pf2:
WAN 192.168.0.22
LAN: 10.0.1.22
SYNC:10.0.2.22
</code></pre>

<p><img src="/img/include/carp_1.png" alt="Example image" /></p>

<p><img src="/img/include/carp_2.png" alt="Example image" /></p>

<p><img src="/img/include/carp_3.png" alt="Example image" /></p>

<h3 id="firewall-rules-for-sync">Firewall rules For sync</h3>

<p>On both firewalls add two rules to allow traffic on the SYNC interface: <br>
go to <code>Firewall &gt; Rules &gt; Sync</code> and click <code>Add</code>.</p>

<p>Rule 1:
<img src="/img/include/carp_4.png" alt="Example image" /></p>

<p>Rule 2:
<img src="/img/include/carp_5.png" alt="Example image" /></p>

<p>Rule 3:
<img src="/img/include/carp_6.png" alt="Example image" /></p>

<h3 id="synchronization-settings">Synchronization Settings</h3>

<p>Go to <code>System &gt; High Availability Sync</code> and configure the sections like on the pictures.</p>

<p>Master:
<img src="/img/include/carp_7.png" alt="Example image" /></p>

<p>Slave:
<img src="/img/include/carp_8.png" alt="Example image" /></p>

<p>Test the synchronisation. Go to <code>System &gt; User management</code> and createa new user on the master node. <br>
Then check on the slave node.</p>

<p>If it doesn&rsquo;t work, check:</p>

<ul>
<li>Are the firewall web interfaces running on the same protocols and ports?</li>
<li>Is the admin password set correctly? (<code>User Manager &gt; Users &gt; admin</code>.)</li>
<li>Are the firewall rules to allow synch set to use the correct interface (SYNC)?</li>
<li>If you&rsquo;re using VMs, are the firewalls on the same internal network?</li>
</ul>

<h3 id="create-virtual-ips">create virtual IPs</h3>

<p>On the master node go to <code>Firewall &gt; Virtual IPs</code> and click <code>Add</code>. Create a new VIP adres for LAN and WAN interfaces.</p>

<p>WAN VIP on master:
<img src="/img/include/carp_9.png" alt="Example image" /></p>

<p>WAN VIP on salave:
<img src="/img/include/carp_10.png" alt="Example image" /></p>

<p>LAN VIP on master:
<img src="/img/include/carp_11.png" alt="Example image" /></p>

<p>LAN VIP on slave:
<img src="/img/include/carp_12.png" alt="Example image" /></p>

<h3 id="change-outbound-nat">Change outbound NAT</h3>

<p>Change the configuration of the outbound NAT to use the shared public IP (the WAN VIP) <br>
Go to <code>Firewall &gt; NAT &gt; Outbound</code> and set the mode to <code>Hybrid Outbound NAT</code> rule generation.</p>

<p><img src="/img/include/carp_13.png" alt="Example image" /></p>

<p><img src="/img/include/carp_14.png" alt="Example image" /></p>

<p>Find your LAN IP ranges (there should be two) and click the edit icon and change the Translation Address to the WAN VIP address.</p>

<p><img src="/img/include/carp_15.png" alt="Example image" /></p>

<p>Do the same for the other LAN network mapping. It should end up looking like this:</p>

<p><img src="/img/include/carp_16.png" alt="Example image" /></p>

<p>If you’ll be using your pfSense firewall as a DNS resolver you must change the settings of the DNS service (<code>Services &gt; DNS Resolver &gt; General Settings</code>) to lissen on the LAN VIP address. Then chnage the address of the DNS server in the DHCP configuration to us the LAN VIP adress.</p>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Rundeck]]></title>
            <link href="https://devopstales.github.io/home/rundeck/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/rundeck/</id>
            
            
            <published>2019-04-09T00:00:00+00:00</published>
            <updated>2019-04-09T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Rundeck is open source software that powers self-service operations.</p>

<h3 id="install-mysql">Install MySQL</h3>

<pre><code>echo &quot;[mariadb]
name = MariaDB
baseurl = http://yum.mariadb.org/10.0/centos7-amd64
gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDB
gpgcheck=1&quot; &gt; /etc/yum.repos.d/MariaDB.repo

yum -y install httpd MariaDB-Galera-server.x86_64

systemctl enable httpd
systemctl enable mysql
systemctl start httpd
systemctl start mysql

mysql_secure_installation
mysql -u root -p
create database rundeck;
grant all on rundeck.* to 'rundeck'@'localhost' identified by 'Password1';
quit
</code></pre>

<h3 id="install-rundeck">Install rundeck</h3>

<pre><code>yum install java-1.8.0 httpd -y
rpm -Uvh http://repo.rundeck.org/latest.rpm
yum install rundeck
</code></pre>

<h3 id="configure-rundeck">Configure rundeck</h3>

<pre><code>nano /etc/rundeck/rundeck-config.properties
# change hostname here
# grails.serverURL=http://localhost:4440
grails.serverURL=http://rundeck.devopstales.intra
#dataSource.url = jdbc:h2:file:/var/lib/rundeck/data/rundeckdb
dataSource.url = jdbc:mysql://localhost/rundeck?autoReconnect=true&amp;useSSL=false
dataSource.username=rundeck
dataSource.password=Password1
dataSource.driverClassName=com.mysql.jdbc.Driver
# mail server
grails.mail.host=localhost
grails.mail.port=25

service rundeckd start
</code></pre>

<h3 id="configure-httpd">Configure httpd</h3>

<pre><code>nano /etc/httpd/conf.d/rundeck_proxy.conf
&lt;virtualhost *:80&gt;
        ServerName rundeck.devopstales.intra
        ServerAlias www.rundeck.devopstales.intra
        ServerAdmin admin@rundeck.devopstales.intra

        ProxyRequests Off
        ProxyPass / http://localhost:4440/
        ProxyPassReverse / http://localhost:4440/
&lt;/virtualhost&gt;

service httpd restart
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Nextcloud]]></title>
            <link href="https://devopstales.github.io/home/nextcloud/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/nextcloud/</id>
            
            
            <published>2019-04-08T00:00:00+00:00</published>
            <updated>2019-04-08T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Nextcloud is a suite of client-server software for creating and using file hosting services. Nextcloud application functionally is similar to Dropbox.</p>

<h3 id="install-postgresql">Install Postgresql</h3>

<pre><code>wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
sudo sh -c 'echo &quot;deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -sc)-pgdg main&quot; &gt; /etc/apt/sources.list.d/PostgreSQL.list'

apt update
apt-get install postgresql-10
apt-get install pgadmin4

systemctl enable postgresql.service
</code></pre>

<h3 id="configure-database">Configure database</h3>

<pre><code>createuser cloud
psql
ALTER USER cloud WITH ENCRYPTED password 'Password1';
CREATE DATABASE cloud WITH ENCODING='UTF8' OWNER=cloud;
\q
</code></pre>

<h3 id="install-requirements">Install requirements</h3>

<pre><code>apt-get install -y libapache2-mod-php php7.0 php7.0-xml php7.0-curl php7.0-gd php7.0 php7.0-cgi php7.0-cli php7.0-zip php7.0-mbstring wget unzip php7.0-pgsql
</code></pre>

<h3 id="configurate-php">Configurate php</h3>

<pre><code>nano /etc/php/7.0/apache2/php.ini
file_uploads = On
allow_url_fopen = On
short_open_tag = On
memory_limit = 256M
upload_max_filesize = 100M
max_execution_time = 360
date.timezone = Europe/Budapest
</code></pre>

<h3 id="install-them">Install Them</h3>

<pre><code>cd /usr/share/redmine/public/themes
# https://github.com/akabekobeko/redmine-theme-minimalflat2/releases
wget https://github.com/akabekobeko/redmine-theme-minimalflat2/releases/download/v1.5.0/minimalflat2-1.5.0.zip
unzip minimalflat2-1.5.0.zip
</code></pre>

<h3 id="configurate-apache">Configurate Apache</h3>

<pre><code>mkdir /var/www/nextcloud
chown www-data:www-data /var/www/nextcloud
chmod 750 /var/www/nextcloud

mkdir -p /var/nextcloud/data
chown www-data:www-data /var/nextcloud/data
chmod 750 /var/nextcloud/data

cd  /var/www/nextcloud
wget https://download.nextcloud.com/server/installer/setup-nextcloud.php
chown www-data:www-data setup-nextcloud.php
</code></pre>

<h3 id="create-vhostfile">Create vhostfile</h3>

<pre><code>echo '&lt;VirtualHost *:80&gt;
ServerAdmin admin@example.com
DocumentRoot &quot;/var/www/nextcloud&quot;
ServerName cloud.devopstales.intra
&lt;Directory &quot;/var/www/nextcloud/&quot;&gt;
Options MultiViews FollowSymlinks

AllowOverride All
Order allow,deny
Allow from all
&lt;/Directory&gt;
TransferLog /var/log/apache2/nextcloud_access.log
ErrorLog /var/log/apache2/nextcloud_error.log
&lt;/VirtualHost&gt;' &gt; /etc/apache2/sites-available/nextcloud.conf

a2dissite 000-default
a2ensite nextcloud
a2enmod rewrite
a2enmod headers
a2enmod env
a2enmod dir
a2enmod mime
service apache2 reload
</code></pre>

<h3 id="install-nextcloud">Install nextcloud</h3>

<p>Go to <a href="http://cloud.devopstales.intra/setup-nextcloud.php">http://cloud.devopstales.intra/setup-nextcloud.php</a> and add the db configuration to install the aplication.</p>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Redmine]]></title>
            <link href="https://devopstales.github.io/home/redmine/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/redmine/</id>
            
            
            <published>2019-04-06T00:00:00+00:00</published>
            <updated>2019-04-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Redmine is a free and open source, web-based project management and issue tracking tool. I will install it on Ubuntu becous on CetOS there in no pre build package for redmine.</p>

<h3 id="install-and-configure-postgresql">Install and configure Postgresql</h3>

<p>In a previous post I wrote about how to <a href="/linux/install-postgresql/#install-postgresql-10-on-debian">Install PostgreSQL 10</a></p>

<pre><code>su - postgres
createuser redmine
psql
ALTER USER redmine WITH ENCRYPTED password 'Password1';
CREATE DATABASE redmine WITH ENCODING='UTF8' OWNER=redmine;
\q
</code></pre>

<h3 id="install-redmine">Install Redmine</h3>

<pre><code>apt install git redmine redmine-pgsql
chmod 777 /usr/share/redmine/instances/default/tmp/cache/
cd /usr/share/redmine
ruby bin/rails server webrick –e production
</code></pre>

<pre><code>echo '[Unit]
Description=Redmine server
After=syslog.target
After=network.target

[Service]
Type=simple
User=redmine
Group=redmine
WorkingDirectory=/usr/share/redmine
ExecStart=/usr/bin/ruby /usr/share/redmine/bin/rails server webrick –e production

# Give a reasonable amount of time for the server to start up/shut down
TimeoutSec=300

[Install]
WantedBy=multi-user.target' &gt; /etc/systemd/system/redmine.service
</code></pre>

<pre><code>apt install apache2 libapache2-mod-passenger
cp /usr/share/doc/redmine/examples/apache2-passenger-host.conf /etc/apache2/sites-available/redmine.conf
nano /etc/apache2/sites-available/redmine.conf

a2enmod passenger
a2enmod proxy
a2enmod rewrite
a2ensite redmine.conf
a2dissite 000-default
service apache2 reload
</code></pre>

<h3 id="install-them">Install Them</h3>

<pre><code>cd /usr/share/redmine/public/themes
# https://github.com/akabekobeko/redmine-theme-minimalflat2/releases
wget https://github.com/akabekobeko/redmine-theme-minimalflat2/releases/download/v1.5.0/minimalflat2-1.5.0.zip
unzip minimalflat2-1.5.0.zip
</code></pre>

<h3 id="install-plugin">Install plugin</h3>

<pre><code>ln -s /usr/share/redmine/bin /usr/share/rubygems-integration/all/specifications/

mkdir /usr/share/redmine/plugins
cd /usr/share/redmine/plugins

git clone https://github.com/applewu/redmine_omniauth_gitlab
cd /usr/share/redmine
bundle install --without development test
bundle exec rake redmine:plugins NAME=redmine_omniauth_gitlab RAILS_ENV=production
ll /usr/share/redmine/public/plugin_assets/
service apache2 restart
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[OpenProject SSO]]></title>
            <link href="https://devopstales.github.io/home/openproject-sso/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/openproject-sso/</id>
            
            
            <published>2019-04-06T00:00:00+00:00</published>
            <updated>2019-04-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Configurate openproject to use Keycloak as sso Identity Provider.</p>

<h3 id="configure-openproject">Configure OpenProject</h3>

<p>Login to openproject with admin and change the config of Self-registrtion to automatic account activation:
Administraion &gt; System Settings &gt; Authentication &gt; Self-registration</p>

<pre><code>nano /opt/openproject/config/configuration.yml
default:
  omniauth_direct_login_provider: openid
  openid_connect:
    openid:
      host: &quot;sso.devopstales.intra&quot;
      identifier: &quot;project&quot;
      secret: &quot;57583084-b54b-4b32-935b-73776f27b89f&quot;
      icon: &quot;openid_connect/auth_provider-google.png&quot;
      display_name: &quot;SSO&quot;
      authorization_endpoint: &quot;http://sso.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/auth&quot;
      token_endpoint: 'http://sso.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/token'
      userinfo_endpoint: 'http://sso.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/userinfo'
      end_session_endpoint: 'http://sso.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/logout'
      check_session_iframe: 'http://sso.devopstales.intra/auth/realms/mydomain/protocol/openid-connect/login-status-iframe.html'
      sso: true
      issuer: 'http://project.devopstales.intra/login'
      discovery: false

service openproject restart
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[SSO login to Gitlab]]></title>
            <link href="https://devopstales.github.io/home/gitlab-keycloak/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/gitlab-keycloak/</id>
            
            
            <published>2019-04-06T00:00:00+00:00</published>
            <updated>2019-04-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Configurate Gitab to use Keycloak as SSO Identity Proider.</p>

<h3 id="configurate-keycloak">Configurate Keycloak</h3>

<p>Login to Keycloak and create client for Gitlab:
<img src="/img/include/gitlab-keycloak1.png" alt="Example image" /></p>

<p>At Mappers create mappers for all user information to GitLab:</p>

<ul>
<li>Name: name

<ul>
<li>Mapper Type: User Property</li>
<li>Property: Username</li>
</ul></li>
<li>Name: email

<ul>
<li>Mapper Type: User Property</li>
<li>Property: Email</li>
</ul></li>
<li>Name: first_name

<ul>
<li>Mapper Type: User Property</li>
<li>Property: FirstName</li>
</ul></li>
<li>Name: last_name

<ul>
<li>Mapper Type: User Property</li>
<li>Property: LastName</li>
</ul></li>
</ul>

<h3 id="configurate-gitlab">Configurate Gitlab</h3>

<pre><code>nano /etc/gitlab/gitlab.rb
gitlab_rails['omniauth_enabled'] = true
gitlab_rails['omniauth_block_auto_created_users'] = false
gitlab_rails['omniauth_allow_single_sign_on'] = ['oauth2_generic']
# gitlab_rails['omniauth_auto_sign_in_with_provider'] = 'oauth2_generic'

gitlab_rails['omniauth_providers'] = [
{
        'name' =&gt; 'oauth2_generic',
        'app_id' =&gt; 'gitlab',
        'app_secret' =&gt; 'KEYCLOAK SECRET GOES HERE',
        'args' =&gt; {
        client_options: {
                'site' =&gt; 'http://sso.devopstales.intra', # including port if necessary
                'user_info_url' =&gt; '/auth/realms/devopstales/protocol/openid-connect/userinfo',
                'authorize_url' =&gt; '/auth/realms/devopstales/protocol/openid-connect/auth',
                'token_url' =&gt; '/auth/realms/devopstales/protocol/openid-connect/token',
        },
        user_response_structure: {
        #root_path: ['user'], # i.e. if attributes are returned in JsonAPI format (in a 'user' node nested under a 'data' node)
        attributes: { email:'email', first_name:'given_name', last_name:'family_name', name:'name', nickname:'preferred_username' }, # if the nickname attribute of a user is called 'username'
        id_path: 'preferred_username'
        },
        }
}
]

gitlab-ctl reconfigure
</code></pre>

<h3 id="gitlab-mattermost-config">Gitlab Mattermost config</h3>

<pre><code># on gitlab gui:
login: admin area / Applications / new
Redirect URI use:
http://mattermost.devopstales.intra/login/gitlab/complete
http://mattermost.devopstales.intra/signup/gitlab/complete

# configfile
nano /etc/gitlab/gitlab.rb

mattermost_external_url 'http://mattermost.devopstales.intra'
mattermost['enable'] = true
mattermost['service_address'] = &quot;127.0.0.1&quot;
mattermost['service_port'] = &quot;8065&quot;
mattermost['sql_driver_name'] = 'postgres'
mattermost['sql_data_source'] = &quot;postgres://mmuser:Password1@127.0.0.1:5432/mattermost?sslmode=disable&amp;connect_timeout=10&quot;
mattermost['log_file_directory'] = '/var/log/gitlab/mattermost/'
mattermost_nginx['enable'] = false

mattermost['gitlab_enable'] = true
mattermost['gitlab_id'] = &quot;&lt;ID&gt;&quot; # oauth id drom gitlab gui
mattermost['gitlab_secret'] = &quot;&lt;token&gt;&quot; # oauth token drom gitlab gui
mattermost['gitlab_scope'] = &quot;&quot;
mattermost['gitlab_auth_endpoint'] = &quot;http://gitlab.devopstales.intra/oauth/authorize&quot;
mattermost['gitlab_token_endpoint'] = &quot;http://gitlab.devopstales.intra/oauth/token&quot;
mattermost['gitlab_user_api_endpoint'] = &quot;http://gitlab.devopstales.intra/api/v4/user&quot;

gitlab-ctl reconfigure
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install keycloak with mysql]]></title>
            <link href="https://devopstales.github.io/home/keycloak1/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/keycloak1/</id>
            
            
            <published>2019-04-05T00:00:00+00:00</published>
            <updated>2019-04-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Keycloak is an open source identity and access management solution.</p>

<h3 id="install-dependencies">Install dependencies</h3>

<pre><code>yum install -y epel-release
yum install -y java-1.8.0-openjdk-headless tmux nano mariadb-server unzip nginx

cd /opt/
wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.47.zip
unzip mysql-connector-java-5.1.47.zip
</code></pre>

<h3 id="configure-database">Configure database</h3>

<pre><code>service mariadb start

mysql -uroot
CREATE DATABASE keycloak CHARACTER SET utf8 COLLATE utf8_unicode_ci;
GRANT ALL PRIVILEGES ON keycloak.* TO 'keycloak'@'%' identified by 'Password1';
GRANT ALL PRIVILEGES ON keycloak.* TO 'keycloak'@'localhost' identified by 'Password1';
FLUSH privileges;
exit;
</code></pre>

<h3 id="install-keycloak">Install keycloak</h3>

<pre><code>groupadd -r keycloak
useradd -m -d /var/lib/keycloak -s /sbin/nologin -r -g keycloak keycloak

mkdir -p /opt/keycloak/
cd /opt/keycloak/

# https://www.keycloak.org/downloads.html
wget https://downloads.jboss.org/keycloak/4.8.2.Final/keycloak-4.8.2.Final.tar.gz

tar -xzf keycloak-4.8.2.Final.tar.gz
ln -s /opt/keycloak/keycloak-4.8.2.Final /opt/keycloak/current
chown keycloak: -R /opt/keycloak
sudo -u keycloak chmod 700 /opt/keycloak/current/standalone

mkdir /var/log/keycloak
chown keycloak: -R /var/log/keycloak

chown keycloak: -R /opt/keycloak
sudo -u keycloak chmod 700 /opt/keycloak/current/standalone
</code></pre>

<pre><code>echo '[Unit]
Description=Keycloak
After=network.target syslog.target

[Service]
Type=idle
User=keycloak
Group=keycloak
ExecStart=/opt/keycloak/current/bin/standalone.sh -b 0.0.0.0
TimeoutStartSec=600
TimeoutStopSec=600

StandardOutput=syslog
StandardError=syslog
SyslogIdentifier=keycloak

[Install]
WantedBy=multi-user.target
' &gt; /etc/systemd/system/keycloak.service
</code></pre>

<pre><code>echo 'if $programname == &quot;keycloak&quot; then /var/log/keycloak/jboss.log
&amp; stop
'&gt;/etc/rsyslog.d/keycloak.conf

systemctl daemon-reload
service rsyslog restart
systemctl start keycloak.service
</code></pre>

<h3 id="configure-wildfly">Configure wildfly</h3>

<pre><code>cd /opt/keycloak/current/

./bin/jboss-cli.sh -c 'module add --name=org.mysql  --dependencies=javax.api,javax.transaction.api --resources=/opt/mysql-connector-java-5.1.47/mysql-connector-java-5.1.47.jar'

./bin/jboss-cli.sh 'embed-server,/subsystem=datasources/jdbc-driver=org.mysql:add(driver-name=org.mysql,driver-module-name=org.mysql,driver-class-name=com.mysql.jdbc.Driver)'

./bin/jboss-cli.sh 'embed-server,/subsystem=datasources/data-source=KeycloakDS:remove'

./bin/jboss-cli.sh 'embed-server,/subsystem=datasources/data-source=KeycloakDS:add(driver-name=org.mysql,enabled=true,use-java-context=true,connection-url=&quot;jdbc:mysql://localhost:3306/keycloak?useSSL=false&amp;amp;useLegacyDatetimeCode=false&amp;amp;serverTimezone=Europe/Budapest&amp;amp;characterEncoding=UTF-8&quot;,jndi-name=&quot;java:/jboss/datasources/KeycloakDS&quot;,user-name=keycloak,password=&quot;Password1&quot;,valid-connection-checker-class-name=org.jboss.jca.adapters.jdbc.extensions.mysql.MySQLValidConnectionChecker,validate-on-match=true,exception-sorter-class-name=org.jboss.jca.adapters.jdbc.extensions.mysql.MySQLValidConnectionChecker)'

./bin/add-user-keycloak.sh -u admin -p Password1 -r master

# for nginx proxy
./bin/jboss-cli.sh 'embed-server,/subsystem=undertow/server=default-server/http-listener=default:write-attribute(name=proxy-address-forwarding,value=true)'

./bin/jboss-cli.sh 'embed-server,/socket-binding-group=standard-sockets/socket-binding=proxy-https:add(port=443)'

./bin/jboss-cli.sh 'embed-server,/subsystem=undertow/server=default-server/http-listener=default:write-attribute(name=redirect-socket,value=proxy-https)'

# disabla color in log
./bin/jboss-cli.sh -c '/subsystem=logging/console-handler=CONSOLE:write-attribute(name=named-formatter, value=PATTERN)'
</code></pre>

<h3 id="configurate-proxy">Configurate proxy</h3>

<pre><code>systemctl restart keycloak.service

echo 'upstream keycloak {
    # Use IP Hash for session persistence
    ip_hash;

    # List of Keycloak servers
    server 127.0.0.1:8080;
}


server {
    listen 80;
    server_name sso.devopstales.intra;

    # Redirect all HTTP to HTTPS
    location / {
      return 301 https://$server_name$request_uri;
    }
}

server {
    listen 443 ssl http2;
    server_name sso.devopstales.intra;

    ssl_certificate /etc/nginx/ssl/domain.pem;
    ssl_certificate_key /etc/nginx/ssl/domain.pem;
    ssl_session_cache shared:SSL:1m;
    ssl_prefer_server_ciphers on;

    location / {
      proxy_set_header Host $host;
      proxy_set_header X-Real-IP $remote_addr;
      proxy_set_header X-Forwarded-For  $proxy_add_x_forwarded_for;
      proxy_set_header X-Forwarded-Proto  $scheme;
      proxy_pass http://keycloak;
    }
}
' &gt; /etc/nginx/conf.d/keycloak.conf

mkdir /etc/nginx/ssl

systemctl restart nginx

# go to sso.devopstales.intra
# login admin / Password1
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install keycloak with postgresql]]></title>
            <link href="https://devopstales.github.io/home/keycloak2/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/keycloak2/</id>
            
            
            <published>2019-04-05T00:00:00+00:00</published>
            <updated>2019-04-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Keycloak is an open source identity and access management solution.</p>

<h3 id="install-dependencies">Install dependencies</h3>

<pre><code>yum install -y epel-release
yum install -y java-1.8.0-openjdk-headless tmux nano mariadb-server unzip httpd

cd /opt
# https://jdbc.postgresql.org/download.html
wget https://jdbc.postgresql.org/download/postgresql-42.2.5.jar
</code></pre>

<h3 id="install-and-configure-database">Install and configure database</h3>

<p>In a previous post I wrote about how to <a href="/linux/install-postgresql/#install-postgresql-10-on-centos-7">Install PostgreSQL 10</a></p>

<pre><code>nano /var/lib/pgsql/10/data/pg_hba.conf
# TYPE  DATABASE        USER            ADDRESS                 METHOD
local   all             keycloak                                md5

su - postgres
createuser keycloak
psql
ALTER USER keycloak WITH ENCRYPTED password 'Password1';
CREATE DATABASE keycloak WITH ENCODING='UTF8' OWNER=keycloak;
\q
</code></pre>

<h3 id="install-keycloak">Install keycloak</h3>

<pre><code>groupadd -r keycloak
useradd -m -d /var/lib/keycloak -s /sbin/nologin -r -g keycloak keycloak

mkdir -p /opt/keycloak/
cd /opt/keycloak/

# https://www.keycloak.org/downloads.html
wget https://downloads.jboss.org/keycloak/4.8.2.Final/keycloak-4.8.2.Final.tar.gz

tar -xzf keycloak-4.8.2.Final.tar.gz
ln -s /opt/keycloak/keycloak-4.8.2.Final /opt/keycloak/current
chown keycloak: -R /opt/keycloak
sudo -u keycloak chmod 700 /opt/keycloak/current/standalone

mkdir /var/log/keycloak
chown keycloak: -R /var/log/keycloak

chown keycloak: -R /opt/keycloak
sudo -u keycloak chmod 700 /opt/keycloak/current/standalone
</code></pre>

<pre><code>echo '[Unit]
Description=Keycloak
After=network.target syslog.target

[Service]
Type=idle
User=keycloak
Group=keycloak
ExecStart=/opt/keycloak/current/bin/standalone.sh -b 0.0.0.0
TimeoutStartSec=600
TimeoutStopSec=600

StandardOutput=syslog
StandardError=syslog
SyslogIdentifier=keycloak

[Install]
WantedBy=multi-user.target
' &gt; /etc/systemd/system/keycloak.service
</code></pre>

<pre><code>echo 'if $programname == &quot;keycloak&quot; then /var/log/keycloak/jboss.log
&amp; stop
'&gt;/etc/rsyslog.d/keycloak.conf

systemctl daemon-reload
service rsyslog restart
systemctl start keycloak.service
</code></pre>

<h3 id="configure-wildfly">Configure wildfly</h3>

<pre><code>cd /opt/keycloak/current/modules
mkdir -p org/postgresql/main
cp /opt/postgresql-42.2.5.jar .

echo '&lt;?xml version=&quot;1.0&quot; ?&gt;
&lt;module xmlns=&quot;urn:jboss:module:1.3&quot; name=&quot;org.postgresql&quot;&gt;

    &lt;resources&gt;
        &lt;resource-root path=&quot;postgresql-42.2.5.jar&quot;/&gt;
	&lt;/resources&gt;

	&lt;dependencies&gt;
		&lt;module name=&quot;javax.api&quot;/&gt;
		&lt;module name=&quot;javax.transaction.api&quot;/&gt;
	&lt;/dependencies&gt;
&lt;/module&gt;' &gt; org/postgresql/main/module.xml
</code></pre>

<pre><code>cd /opt/keycloak/current/standalone/configuration/
nano standalone.xml
...
        &lt;datasources&gt;
				&lt;datasource jndi-name=&quot;java:jboss/datasources/KeycloakDS&quot; pool-name=&quot;KeycloakDS&quot; enabled=&quot;true&quot; use-java-context=&quot;true&quot;&gt;
					&lt;connection-url&gt;jdbc:postgresql://localhost:5432/keycloak&lt;/connection-url&gt;
					&lt;driver&gt;postgresql&lt;/driver&gt;
					&lt;pool&gt;
						&lt;max-pool-size&gt;20&lt;/max-pool-size&gt;
					&lt;/pool&gt;
					&lt;security&gt;
						&lt;user-name&gt;keycloak&lt;/user-name&gt;
						&lt;password&gt;Password1&lt;/password&gt;
					&lt;/security&gt;
				&lt;/datasource&gt;
...
        &lt;drivers&gt;
					&lt;driver name=&quot;postgresql&quot; module=&quot;org.postgresql&quot;&gt;
						&lt;xa-datasource-class&gt;org.postgresql.xa.PGXADataSource&lt;/xa-datasource-class&gt;
				&lt;/driver&gt;
...
&lt;default-bindings context-service=&quot;java:jboss/ee/concurrency/context/default&quot; datasource=&quot;java:jboss/datasources/KeycloakDS&quot;
</code></pre>

<pre><code>cd /opt/keycloak/current
./bin/add-user-keycloak.sh -u admin -p Password1 -r master
systemctl restart keycloak.service
</code></pre>

<h3 id="configurate-proxy">Configurate proxy</h3>

<pre><code>echo '&lt;VirtualHost *:80&gt;
    ServerName sso.devopstales.intra

    ProxyPreserveHost On
#    SSLProxyEngine On
#    SSLProxyCheckPeerCN on
#    SSLProxyCheckPeerExpire on
    RequestHeader set X-Forwarded-Proto &quot;https&quot;
    RequestHeader set X-Forwarded-Port &quot;80&quot; #443
    ProxyPass / http://127.0.0.1:8080/
    ProxyPassReverse / http://127.0.0.1:8080/
&lt;/VirtualHost&gt;' &gt; /etc/apache2/sites-available/keycloak.conf

sudo a2enmod headers
a2enmod proxy
a2enmod rewrite
a2ensite keycloak.confcd
service httpd restart

# go to sso.devopstales.intra
# login admin / Password1
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Openproject]]></title>
            <link href="https://devopstales.github.io/home/openproject/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/openproject/</id>
            
            
            <published>2019-04-05T00:00:00+00:00</published>
            <updated>2019-04-05T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Openproject is a free and open source, web-based project management and issue tracking tool.</p>

<h3 id="install-openproject">Install OpenProject</h3>

<pre><code>sudo wget -O /etc/yum.repos.d/openproject-ce.repo https://dl.packager.io/srv/opf/openproject-ce/stable/8/installer/el/7.repo

yum install openproject memcached -y
service memcached start
</code></pre>

<h3 id="install-postgresql">Install Postgresql</h3>

<p>In a previous post I wrote about how to <a href="/linux/install-postgresql/#install-postgresql-9-6-on-centos-7">Install PostgreSQL 10</a></p>

<pre><code>su - postgres
createuser project
psql
ALTER USER project WITH ENCRYPTED password 'Password1';
CREATE DATABASE project WITH ENCODING='UTF8' OWNER=project;
\q
</code></pre>

<h3 id="configure-openproject">Configure OpenProject</h3>

<pre><code>openproject configure
</code></pre>

<h3 id="reset-password">Reset Password</h3>

<pre><code>openproject run console

admin = User.find_by(login: 'admin')
admin.password = 'Password11' # minimum 10 characters
admin.password_confirmation = 'Password11'

admin.save! # Watch the output for errors
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Ceph cluster]]></title>
            <link href="https://devopstales.github.io/home/install-ceph/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/install-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ceph cluster" />
                <link href="https://devopstales.github.io/home/ceph-block-device/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph Block Device" />
                <link href="https://devopstales.github.io/cloud/ceph-block-device/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph Block Device" />
                <link href="https://devopstales.github.io/cloud/ceph-cephfs/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph CephFS" />
                <link href="https://devopstales.github.io/home/ceph-cephfs/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph CephFS" />
            
                <id>https://devopstales.github.io/home/install-ceph/</id>
            
            
            <published>2019-03-12T00:00:00+00:00</published>
            <updated>2019-03-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Ceph is free and open source distributed objectstorage solution. With Ceph we can easily provide and manage block storage, object storage and file storage.</p>

<h3 id="base-components">Base Components</h3>

<ul>
<li>Monitors (ceph-mon) : As the name suggests a ceph monitor nodes keep an eye on cluster state, OSD Map and Crush map</li>
<li>OSD ( Ceph-osd): These are the nodes which are part of cluster and provides data store, data replication and recovery functionalities. OSD also provides information to monitor nodes.</li>
<li>MDS (Ceph-mds) : It is a ceph meta-data server and stores the meta data of ceph file systems like block storage.</li>
<li>Ceph Deployment Node : It is used to deploy the Ceph cluster, it is also called as Ceph-admin or Ceph-utility node.</li>
</ul>

<h3 id="environment">Environment</h3>

<pre><code>192.168.1.31    ceph01
192.168.1.32    ceph02
192.168.1.33    ceph03
</code></pre>

<h3 id="install-requirements">Install Requirements</h3>

<pre><code># all hosts
yum install ntp ntpdate ntp-doc epel-release -y
ntpdate europe.pool.ntp.org
systemctl start ntpd
systemctl enable ntpd

useradd cephuser &amp;&amp; echo &quot;Password1&quot; | passwd --stdin cephuser
echo &quot;cephuser ALL = (root) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/cephuser
chmod 0440 /etc/sudoers.d/cephuser

sed -i --follow-symlinks 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux
systemctl mask firewalld

reboot
</code></pre>

<pre><code># cep01
ssh-keygen
ssh-copy-it ceph02
ssh-copy-it ceph03

nano ~/.ssh/config
Host ceph01
   Hostname ceph01
   User cephuser
Host ceph02
   Hostname ceph02
   User cephuser
Host ceph03
   Hostname ceph03
   User cephuser

chmod 644 ~/.ssh/config
</code></pre>

<h3 id="install-ceph-deployer">Install ceph-deployer</h3>

<pre><code># cep01
sudo rpm -Uvh https://download.ceph.com/rpm-mimic/el7/noarch/ceph-release-1-1.el7.noarch.rpm

yum update
yum install -y ceph-deploy

mkdir /home/ceph/cluster1
cd ~/cluster1
</code></pre>

<h3 id="delete-ceph-config-if-exists">Delete ceph config if exists</h3>

<pre><code># cep01
ceph-deploy purge ceph01 ceph02 ceph03
# unmount if not working
ceph-deploy purgedata ceph01 ceph02 ceph03
ceph-deploy forgetkeys
</code></pre>

<h3 id="install-ceph">Install ceph</h3>

<pre><code>ceph-deploy install ceph01 ceph02 ceph03
ceph-deploy --cluster &lt;cluster-name&gt; new ceph01 ceph02 ceph03

# edit before pupulate config
nano &lt;cluster-name&gt;.conf
osd_max_object_name_len = 256
osd_max_object_namespace_len = 64

ceph-deploy --cluster &lt;cluster-name&gt; mon create ceph01 ceph02 ceph03
ceph-deploy --cluster &lt;cluster-name&gt; gatherkeys ceph01 ceph02 ceph03
</code></pre>

<pre><code>ceph-deploy disk list ceph01
ceph-deploy disk list ceph02
ceph-deploy disk list ceph03


ceph-deploy disk zap ceph01:sdb
ceph-deploy disk zap ceph02:sdb
ceph-deploy disk zap ceph03:sdb

ceph-deploy osd create ceph01:sdb
ceph-deploy osd create ceph02:sdb
ceph-deploy osd create ceph03:sdb

ceph-deploy osd create ceph01:sdc
ceph-deploy osd create ceph02:sdc
ceph-deploy osd create ceph03:sdc
</code></pre>

<h3 id="test-cluster">Test cluster</h3>

<pre><code>sudo ceph health
sudo ceph -s
sudo ceph osd tree

# install adminkey-ring
ceph-deploy admin ceph01 ceph02 ceph03
ssh ceph node01 sudo ceph osd lspools
ssh ceph node01 sudo ceph osd create mycorp 128
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Openshift]]></title>
            <link href="https://devopstales.github.io/home/ansible-openshift-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/ansible-openshift-install/?utm_source=atom_feed" rel="related" type="text/html" title="Install Openshift" />
            
                <id>https://devopstales.github.io/home/ansible-openshift-install/</id>
            
            
            <published>2019-03-12T00:00:00+00:00</published>
            <updated>2019-03-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Ansible-openshift is a pre made ansible playbook for Openshift installation. In this Post I will show you how to use to install a new Openshift cluster.</p>

<h3 id="parst-of-the-openshift-series">Parst of the Openshift series</h3>

<ul>
<li>Part1: <a href="../../cloud/ansible-openshift-install/">Install Opeshift</a></li>
<li>Part2: <a href="../../cloud/openshift-auto-approval-csr/">How to Enable Auto Approval of CSR in Openshift v3.11</a></li>
<li>Part3: <a href="../../cloud/openshift-add-node/">Add new workers to Openshift cluster</a></li>
<li>Part4: <a href="../../cloud/openshift-cert/">Chane the certificates of the Openshift cluster</a></li>
<li>Part5: <a href="../../cloud/openshift-ldap/">LDAP authentication for Openshift</a></li>
<li>Part6: <a href="../../sso/openshift-sso/">Keycloak SSO authentication for Openshift</a></li>
<li>Part7: <a href="../../sso/openshift-sso2/">Gitlab SSO authentication for Openshift</a></li>
<li>Part8a: <a href="../../cloud/openshift-ceph/">Ceph persistent storage for Openshift</a></li>
<li>Part8b: <a href="../../cloud/openshift-vmware/">vSphere persistent storage for Openshift</a></li>
<li>Part9: <a href="../../cloud/openshift-helm/">Helm on Openshift</a></li>
<li>Part10: <a href="../../cloud/k8s-tillerless-helm/">Tillerless Helm on Openshift</a></li>
<li>Part11: <a href="../../cloud/openshift-extregistry/">Use external docker registry on Openshift</a></li>
<li>Part12: <a href="../../cloud/openshift-secondary-router/">Secondary router on Openshift</a></li>
<li>Part13a: <a href="../../cloud/openshift-letsencrypt/">Use Letsencrypt on Openshift</a></li>
<li>Part13b: <a href="../../cloud/openshift-cert-manager/">Install cert-managger on Openshift</a></li>
<li>Part14: <a href="../../cloud/ansible-operator-overview/">Create Openshift operators</a></li>
<li>Part15: <a href="../../cloud/openshift-kompose/">Convert docker-compose file to Opeshift</a></li>
</ul>

<h3 id="environment">Environment</h3>

<pre><code>192.168.1.40    deployer
192.168.1.41    openshift01 # master node
192.168.1.42    openshift02 # infra node
192.168.1.43    openshift03 # worker node

# hardware requirement
4 CPU
16G RAM
</code></pre>

<h3 id="dns-config">DNS config</h3>

<pre><code>master.openshift     300 IN  A 192.168.1.41
openshift            300 IN  A 192.168.1.42
*.openshift            300 IN  A 192.168.1.42
</code></pre>

<h3 id="prerequirement">Prerequirement</h3>

<pre><code># deployer
yum install epel-release centos-release-openshift-origin311
yum --disablerepo=* --enablerepo=centos-ansible26 install ansible
yum install openshift-ansible nano

echo &quot;exclude=ansible&quot; &gt;&gt; /etc/yum.conf

nano ~/.ssh/config
Host openshift01
    Hostname openshift01.devopstales.intra
    User origin

Host openshift02
    Hostname openshift02.devopstales.intra
    User origin

Host openshift03
    Hostname openshift03.devopstales.intra
    User origin
</code></pre>

<pre><code># on all openshift hosts
hostnamectl set-hostname openshift01
yum -y update
yum -y install centos-release-openshift-origin311 epel-release docker git pyOpenSSL

useradd origin
passwd origin
echo -e 'Defaults:origin !requiretty\norigin ALL = (root) NOPASSWD:ALL' | tee /etc/sudoers.d/origin
chmod 440 /etc/sudoers.d/origin
reboot

# Disable swap permanently
nano /etc/fstab
#/dev/mapper/centos_openshift01-swap swap                    swap    defaults        0 0

sudo swapoff -a

sudo lvremove -Ay /dev/centos/swap
sudo lvextend -l +100%FREE centos/root
sudo xfs_growfs /

sudo nano /etc/default/grub
GRUB_TIMEOUT=5
GRUB_DEFAULT=saved
GRUB_DISABLE_SUBMENU=true
GRUB_TERMINAL_OUTPUT=&quot;console&quot;
# GRUB_CMDLINE_LINUX=&quot;rd.lvm.lv=centos/root rd.lvm.lv=centos/swap crashkernel=auto rhgb quiet&quot;
GRUB_CMDLINE_LINUX=&quot;rd.lvm.lv=centos/root crashkernel=auto rhgb quiet&quot;
GRUB_DISABLE_RECOVERY=&quot;true&quot;

dracut --regenerate-all -f
grub2-mkconfig -o /boot/grub2/grub.cfg
</code></pre>

<h3 id="configurate-installer">Configurate Installer</h3>

<pre><code># deployer

nano /etc/ansible/hosts
[OSEv3:children]
masters
nodes
etcd

[OSEv3:vars]
# admin user created in previous section
ansible_ssh_user=origin
ansible_become=true
openshift_deployment_type=origin
os_firewall_use_firewalld=True
openshift_clock_enabled=true

# use HTPasswd for authentication
openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'}]

# define default sub-domain for Master node
openshift_master_default_subdomain=openshift.devopstales.intra
osm_default_subdomain=openshift.devopstales.intra

# allow unencrypted connection within cluster
openshift_docker_insecure_registries=172.30.0.0/16

openshift_master_cluster_hostname=master.openshift.devopstales.intra
openshift_master_cluster_public_hostname=master.openshift.devopstales.intra
openshift_public_hostname=master.openshift.devopstales.intra

openshift_master_api_port=443
openshift_master_console_port=443

[masters]
openshift01 containerized=true openshift_public_hostname=master.openshift.devopstales.intra

[etcd]
openshift01 containerized=true

[nodes]
# defined values for [openshift_node_group_name] in the file below
# [/usr/share/ansible/openshift-ansible/roles/openshift_facts/defaults/main.yml]
openshift01 openshift_node_group_name='node-config-master'
openshift02 openshift_node_group_name='node-config-infra'
openshift03 openshift_node_group_name='node-config-compute'
</code></pre>

<h3 id="run-the-installer">Run the Installer</h3>

<pre><code># deployer
cd /usr/share/ansible/openshift-ansible/
sudo ansible-playbook playbooks/prerequisites.yml
sudo ansible-playbook playbooks/deploy_cluster.yml

# If installastion failed or went wrong, the following uninstallation script can be run, and running installation can be tried again:
sudo ansible-playbook playbooks/adhoc/uninstall.yml
</code></pre>

<h3 id="user-management">User management</h3>

<pre><code># on openshift master

cd /etc/origin/master/
# add user
htpasswd [/path/to/users.htpasswd] [user_name]
htpasswd htpasswd devopstales

# delete user
htpasswd -D [htpasswd/file/path/]  [user-name] [password]
htpasswd -D htpasswd devopstales Password1

# it will remove only the username from the htpasswd file by default it won’t remove user identity
oc delete  identity htpasswd_auth:user
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Use Ceph Block Device]]></title>
            <link href="https://devopstales.github.io/home/ceph-block-device/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/install-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ceph cluster" />
                <link href="https://devopstales.github.io/home/install-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ceph cluster" />
                <link href="https://devopstales.github.io/cloud/ceph-block-device/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph Block Device" />
                <link href="https://devopstales.github.io/home/ceph-cephfs/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph CephFS" />
                <link href="https://devopstales.github.io/cloud/ceph-cephfs/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph CephFS" />
            
                <id>https://devopstales.github.io/home/ceph-block-device/</id>
            
            
            <published>2019-03-12T00:00:00+00:00</published>
            <updated>2019-03-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Ceph block devices are thin-provisioned, resizable and store data striped over multiple OSDs in a Ceph cluster. Ceph block devices leverage RADOS capabilities such as snapshotting, replication and consistency. Ceph’s RADOS Block Devices (RBD) interact with OSDs using kernel modules or the librbd library.</p>

<h3 id="environment">Environment</h3>

<pre><code>192.168.1.31    ceph01
192.168.1.32    ceph02
192.168.1.33    ceph03
</code></pre>

<h3 id="install-client-cli">Install Client cli</h3>

<pre><code>ceph-deploy install client
ceph-deploy admin client

sudo chmod 644 /etc/ceph/ceph.client.admin.keyring
</code></pre>

<h3 id="basic-usage">Basic Usage</h3>

<pre><code># create pool
ceph osd pool create stack 64 64

# createdisk to pool
rbd create disk01 --size 10G --image-feature layering --pool stack --allow-shrink

# show list
rbd ls -l

# show info
rbd --image disk01 -p stack info

# resize
rbd resize --image disk01 -p stack --size 6G

# remove
rbd rm disk01 -p stack

# map the image to device
sudo rbd map disk01

# show mapping
rbd showmapped

# format with XFS
sudo mkfs.xfs /dev/rbd0

# mount device
sudo mount /dev/rbd0 /mnt
df -hT
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Use Ceph CephFS]]></title>
            <link href="https://devopstales.github.io/home/ceph-cephfs/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/install-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ceph cluster" />
                <link href="https://devopstales.github.io/cloud/install-ceph/?utm_source=atom_feed" rel="related" type="text/html" title="Install Ceph cluster" />
                <link href="https://devopstales.github.io/home/ceph-block-device/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph Block Device" />
                <link href="https://devopstales.github.io/cloud/ceph-block-device/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph Block Device" />
                <link href="https://devopstales.github.io/cloud/ceph-cephfs/?utm_source=atom_feed" rel="related" type="text/html" title="Use Ceph CephFS" />
            
                <id>https://devopstales.github.io/home/ceph-cephfs/</id>
            
            
            <published>2019-03-12T00:00:00+00:00</published>
            <updated>2019-03-12T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>The Ceph Filesystem (CephFS) is a POSIX-compliant filesystem that uses a Ceph Storage Cluster to store its data. The Ceph filesystem uses the same Ceph Storage Cluster system as Ceph Block Devices, Ceph Object Storage with its S3 and Swift APIs, or native bindings (librados).</p>

<h3 id="environment">Environment</h3>

<pre><code>192.168.1.31    ceph01
192.168.1.32    ceph02
192.168.1.33    ceph03
</code></pre>

<h3 id="prerequirements">Prerequirements</h3>

<pre><code># Create MDS (MetaData Server)
ceph-deploy --overwrite-conf mds create ceph02

# create pools
sudo ceph osd pool create cephfs_data 128
sudo ceph osd pool create cephfs_metadata 128

# enable pools
sudo ceph fs new cephfs cephfs_metadata cephfs_data

# show pools
sudo ceph osd lspools
sudo ceph fs ls
sudo ceph mds stat
</code></pre>

<h3 id="basic-usage">Basic Usage</h3>

<pre><code># Mount CephFS on a Client.

yum -y install ceph-fuse
ssh ceph@ceph01 &quot;sudo ceph-authtool -p /etc/ceph/ceph.client.admin.keyring&quot; &gt; admin.key
chmod 600 admin.key

mount -t ceph ceph01:6789:/ /mnt -o name=admin,secretfile=admin.key
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Configure kubectl for multiple clusters]]></title>
            <link href="https://devopstales.github.io/home/kubectl-multi-cluster-config/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/kubectl-multi-cluster-config/?utm_source=atom_feed" rel="related" type="text/html" title="Configure kubectl for multiple clusters" />
            
                <id>https://devopstales.github.io/home/kubectl-multi-cluster-config/</id>
            
            
            <published>2019-03-11T00:00:00+00:00</published>
            <updated>2019-03-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I use a multiple Kubernetes clusters on a daily basis, so keeping my configs organized is important to don’t confuse myself.</p>

<p>kubectl looks at an environment variable called KUBECONFIG to hold a colon-separated list of paths to configuration files, so I can use multiple cluster config files.</p>

<h3 id="download-kubernetes-config">Download kubernetes config</h3>

<pre><code>scp root@DEV_SERVER:/etc/kubernetes/admin.conf ~/.kube/dev-config
scp root@TST_SERVER:/etc/kubernetes/admin.conf ~/.kube/tst-config
scp root@UAT_SERVER:/etc/kubernetes/admin.conf ~/.kube/uat-config
scp root@PROD_SERVER:/etc/kubernetes/admin.conf ~/.kube/prod-config
</code></pre>

<h3 id="edit-config-files">Edit config files</h3>

<pre><code>nano ~/.kube/dev-config
...
- cluster:
    server: https://1.1.1.1:6443
  name: dev-config
...
contexts:
- context:
    cluster: dev-config
    user: dev-admin
  name: dev-config
...
users:
- name: dev-admin
...
</code></pre>

<h3 id="use-config-files-in-kubeconfig-variable">Use config files in KUBECONFIG variable</h3>

<pre><code>nano ~/.bashrc
export KUBECONFIG=$HOME/.kube/dev-config:$HOME/.kube/tst-config:$HOME/.kube/uat-config:$HOME/.kube/prod-config

echo $KUBECONFIG

/home/ME/.kube/dev-config:/home/ME/.kube/tst-config:/home/ME/.kube/uat-config:/home/ME/.kube/prod-config

source ~/.bashrc
</code></pre>

<h3 id="use-clusters-with-kubectl">Use clusters with kubectl</h3>

<pre><code># get the current context
kubectl config current-context

# use a different context
kubectl config use-context work-dev
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Proxmox backup with pve-zsync]]></title>
            <link href="https://devopstales.github.io/home/proxmox-backup-pve-zsync/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/proxmox-backup-pve-zsync/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with pve-zsync" />
                <link href="https://devopstales.github.io/cloud/proxmox-backup-sanoid/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with sanoid" />
                <link href="https://devopstales.github.io/home/proxmox-backup-sanoid/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with sanoid" />
                <link href="https://devopstales.github.io/cloud/proxmox-backup-znapzend/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with znapzend" />
                <link href="https://devopstales.github.io/home/proxmox-backup-znapzend/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with znapzend" />
            
                <id>https://devopstales.github.io/home/proxmox-backup-pve-zsync/</id>
            
            
            <published>2019-03-11T00:00:00+00:00</published>
            <updated>2019-03-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Backup your machines on Pfsense with pve-zsync tool.</p>

<h3 id="the-servers">The servers</h3>

<pre><code>proxmox-node1 192.168.10.20
omv-nas1 192.168.10.50
</code></pre>

<h3 id="install-pve-zsync-on-servers">Install pve-zsync on servers</h3>

<pre><code>apt-get install pve-zsync
</code></pre>

<h3 id="configure-pve-zsync">Configure pve-zsync</h3>

<pre><code>pve-zsync create --source 107 --dest 192.168.10.50:tank --verbose --maxsnap 14 --name Backup_ZFS_srv_107

# test the config
cat /etc/cron.d/pve-zsync
* 8 * * * root pve-zsync sync --source 107 --dest 192.168.10.50:tank --name Backup_ZFS_srv_107 --maxsnap 14 --method ssh

# send diff
pve-zsync sync --source 107 --dest 192.168.10.50:tank --verbose --maxsnap 14 --name Backup_ZFS_srv_107

# the tool send the vm config to the /var/lib/pve-zsync/
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Proxmox backup with sanoid]]></title>
            <link href="https://devopstales.github.io/home/proxmox-backup-sanoid/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/home/proxmox-backup-pve-zsync/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with pve-zsync" />
                <link href="https://devopstales.github.io/cloud/proxmox-backup-pve-zsync/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with pve-zsync" />
                <link href="https://devopstales.github.io/cloud/proxmox-backup-sanoid/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with sanoid" />
                <link href="https://devopstales.github.io/cloud/proxmox-backup-znapzend/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with znapzend" />
                <link href="https://devopstales.github.io/home/proxmox-backup-znapzend/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with znapzend" />
            
                <id>https://devopstales.github.io/home/proxmox-backup-sanoid/</id>
            
            
            <published>2019-03-11T00:00:00+00:00</published>
            <updated>2019-03-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Backup your machines on Pfsense with sanoid tool.</p>

<h3 id="the-servers">The servers</h3>

<pre><code>proxmox-node1 192.168.10.20
omv-nas1 192.168.10.50
</code></pre>

<h3 id="build-sanoid-on-servers">Build sanoid on servers</h3>

<pre><code>apt-get install libconfig-inifiles-perl git

cd /opt
git clone https://github.com/jimsalterjrs/sanoid

ln /opt/sanoid/sanoid /usr/sbin/
</code></pre>

<p>Or you can build deb package:</p>

<h3 id="build-and-install-sanoid-deb-package">Build and install sanoid deb package</h3>

<pre><code>sudo apt-get install devscripts debhelper dh-systemd
git clone https://github.com/jimsalterjrs/sanoid.git
cd sanoid
debuild -us -uc

cd ..
sudo apt-get install libconfig-inifiles-perl
sudo dpkg -i sanoid_2.0.1_all.deb
</code></pre>

<h3 id="configure-sanoid">Configure sanoid</h3>

<pre><code>mkdir -p /etc/sanoid
cp /opt/sanoid/sanoid.conf /etc/sanoid/sanoid.conf
cp /opt/sanoid/sanoid.defaults.conf /etc/sanoid/sanoid.defaults.conf

nano /etc/crontab
* 2 * * * root /usr/sbin/sanoid --cron
* 3 * * * root /usr/sbin/syncoid --recursive tank root@192.168.10.50:tank

</code></pre>

<pre><code>    ####################
    # sanoid.conf file #
    ####################
    [local-zfs]
            use_template = production
    #############################
    # templates below this line #
    #############################
    [template_production]
            # store hourly snapshots 36h
            # hourly = 36
            # store 14 days of daily snaps
            daily = 14
            # store back 6 months of monthly
            # monthly = 6
            # store back 3 yearly (remove manually if to large)
            # yearly = 3
            # create new snapshots
            autosnap = yes
            # clean old snapshot
            autoprune = yes
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Proxmox backup with znapzend]]></title>
            <link href="https://devopstales.github.io/home/proxmox-backup-znapzend/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/proxmox-backup-znapzend/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with znapzend" />
                <link href="https://devopstales.github.io/cloud/proxmox-backup-pve-zsync/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with pve-zsync" />
                <link href="https://devopstales.github.io/home/proxmox-backup-pve-zsync/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with pve-zsync" />
                <link href="https://devopstales.github.io/home/proxmox-backup-sanoid/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with sanoid" />
                <link href="https://devopstales.github.io/cloud/proxmox-backup-sanoid/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox backup with sanoid" />
            
                <id>https://devopstales.github.io/home/proxmox-backup-znapzend/</id>
            
            
            <published>2019-03-11T00:00:00+00:00</published>
            <updated>2019-03-11T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Backup your machines on Pfsense with znapzend tool.</p>

<h3 id="the-servers">The servers</h3>

<pre><code>proxmox-node1 192.168.10.20
omv-nas1 192.168.10.50
</code></pre>

<h3 id="create-sub-volume">Create sub volume</h3>

<p>Note that while recursive configurations are well supported to set up backup and retention policies for a whole dataset subtree under the dataset to which you have applied explicit configuration, at this time pruning of such trees (&ldquo;I want every dataset under var except var/tmp&rdquo;) is not supported.</p>

<pre><code>zfs create local-zfs/vm-data
pvesm add zfspool local-zfs --pool local-zfs/vm-data
</code></pre>

<h3 id="build-znapzend-on-servers">Build znapzend on servers</h3>

<pre><code>apt-get install perl unzip git mbuffer build-essential git

cd /root
git clone https://github.com/oetiker/znapzend
cd /root/znapzend
./configure --prefix=/opt/znapzend

make
make install

ln -s /opt/znapzend/bin/znapzend /usr/local/bin/znapzend
ln -s /opt/znapzend/bin/znapzendzetup /usr/local/bin/znapzendzetup
ln -s /opt/znapzend/bin/znapzendztatz /usr/local/bin/znapzendztatz

znapzend --version
</code></pre>

<p>Or you can download a the deb package from here:
<a href="https://github.com/devopstales/znapzend-debian/releases">https://github.com/devopstales/znapzend-debian/releases</a></p>

<h3 id="install-znapzend-on-servers">Install znapzend on servers</h3>

<pre><code>dpkg -i znapzend_0.19.1_amd64_stretch.deb
</code></pre>

<h3 id="configure-znapzend">Configure znapzend</h3>

<pre><code>znapzendzetup create --recursive\
--mbuffer=/usr/bin/mbuffer \
--mbuffersize=1G \
SRC '2d=&gt;1d' local-zfs/vmdata \
DST:a '14d=&gt;1d' root@192.168.10.50:tank

# test
znapzend --debug --noaction --runonce=local-zfs
znapzendzetup list
</code></pre>

<h3 id="create-znapzend-service">Create znapzend service</h3>

<pre><code>nano /etc/default/znapzend
ZNAPZENDOPTIONS=&quot;--logto=/var/log/znapzend.log&quot;

systemctl enable znapzend.service
systemctl restart znapzend.service
systemctl status znapzend.service
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Proxmox node removal]]></title>
            <link href="https://devopstales.github.io/home/proxmox-node-remove/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/cloud/proxmox-node-remove/?utm_source=atom_feed" rel="related" type="text/html" title="Proxmox node removal" />
            
                <id>https://devopstales.github.io/home/proxmox-node-remove/</id>
            
            
            <published>2019-03-07T00:00:00+00:00</published>
            <updated>2019-03-07T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>The correct way to remove nod from proxmox cluster.</p>

<h3 id="display-all-active-nodes">Display all active nodes</h3>

<pre><code>root@proxmox-node2:~# pvecm nodes
Membership information
----------------------
Nodeid Votes Name
1 1 proxmox-node1 (local)
2 1 proxmox-node2
3 1 proxmox-node3
4 1 proxmox-node4
</code></pre>

<h3 id="shutdown-node-and-remove">Shutdown node and remove</h3>

<pre><code>root@proxmox-node2:~# pvecm delnode proxmox-node3

root@proxmox-node2:~# ls -l /etc/pve/nodes/
proxmox-node1 proxmox-node2 proxmox-node3 proxmox-node4

root@proxmox-node2:~# rm -rf /etc/pve/nodes/proxmox-node3
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install telegraf on pfsense]]></title>
            <link href="https://devopstales.github.io/home/pfsense-telegraf/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/pfsense-telegraf/</id>
            
            
            <published>2019-03-06T00:00:00+00:00</published>
            <updated>2019-03-06T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>Install and configure telegraf on pfsense to provides system information to prometheus.</p>

<h3 id="install-telegraf">Install telegraf</h3>

<ul>
<li>At the System / Package menu install the telegraf service to the pfsense.</li>
<li>ssh to the pfsense server and open a shell</li>
</ul>

<p><img src="/img/include/pfsense-telegraf.png" alt="Example image" /></p>

<h3 id="install-nano">Install nano</h3>

<pre><code>pkg
pkg update
pkg install nano
</code></pre>

<h3 id="configure-telegraf">Configure telegraf</h3>

<pre><code>cd /usr/local/etc

nano telegraf.conf
[[outputs.prometheus_client]]
 listen = &quot;:9273&quot;

echo &quot;telegraf_enable=&quot;YES&quot;&quot; &gt;&gt; /etc/rc.conf

cd /usr/local/etc/rc.d
./telegraf restart
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Prometheus with Influxdb storage]]></title>
            <link href="https://devopstales.github.io/home/prometheus-influxdb/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/prometheus-influxdb/</id>
            
            
            <published>2019-02-02T00:00:00+00:00</published>
            <updated>2019-02-02T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Example article description</blockquote><p>Use Influxdb to as storage for Prometheus.</p>

<h3 id="install-infludxb">Install Infludxb</h3>

<pre><code>cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/influxdb.repo
[influxdb]
name = InfluxDB Repository - RHEL \$releasever
baseurl = https://repos.influxdata.com/rhel/\$releasever/\$basearch/stable
enabled = 1
gpgcheck = 1
gpgkey = https://repos.influxdata.com/influxdb.key
EOF

yum install influxdb -y
</code></pre>

<h3 id="configure-influxdb">Configure Influxdb</h3>

<pre><code>nano /etc/influxdb/influxdb.conf
[http]
   enabled = true
   bind-address = &quot;localhost:8086&quot;
   auth-enabled = false
</code></pre>

<h3 id="configure-prometheus">Configure Prometheus</h3>

<pre><code>nano /etc/prometheus/prometheus.yml
remote_write:
  - url: &quot;http://localhost:8086/api/v1/prom/write?db=prometheus&quot;

remote_read:
  - url: &quot;http://localhost:8086/api/v1/prom/read?db=prometheus&quot;

# with authentication
#remote_write:
#  - url: &quot;http://localhost:8086/api/v1/prom/write?db=prometheus&amp;u=username&amp;p=password&quot;

#remote_read:
#  - url: &quot;http://localhost:8086/api/v1/prom/read?db=prometheus&amp;u=username&amp;p=password&quot;
</code></pre>

<pre><code>systemctl start influxdb
systemctl enable influxdb

echo 'CREATE DATABASE &quot;prometheus&quot;' | influx

systemctl start prometheus
systemctl status prometheus

influx
USE prometheus
select * from /.*/ limit 1
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install PostgreSQL]]></title>
            <link href="https://devopstales.github.io/home/install-postgresql/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
                <link href="https://devopstales.github.io/linux/install-postgresql/?utm_source=atom_feed" rel="related" type="text/html" title="Install PostgreSQL" />
            
                <id>https://devopstales.github.io/home/install-postgresql/</id>
            
            
            <published>2019-01-10T00:00:00+00:00</published>
            <updated>2019-01-10T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this pos I will show you how to install Postgresql on difrent Linux distributions.</p>

<p>PostgreSQL, also known as Postgres, is a free and open-source relational database management system (RDBMS).</p>

<h3 id="install-postgresql-9-6-on-centos-7">Install PostgreSQL 9.6 on CentOS 7</h3>

<pre><code>yum install epel-release -y
yum update -y
rpm -Uvh https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-centos10-10-2.noarch.rpm

yum install -y postgresql96-server postgresql96 postgresql96-contrib unzip
/usr/pgsql-9.6/bin/postgresql96-setup initdb

nano /var/lib/pgsql/9.6/data/pg_hba.conf
local   all             all                                      trust

sudo systemctl start postgresql-9.6
sudo systemctl enable postgresql-9.6
</code></pre>

<h3 id="install-postgresql-10-on-centos-7">Install PostgreSQL 10 on CentOS 7</h3>

<pre><code>yum install epel-release -y
yum update -y
rpm -Uvh https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-centos10-10-2.noarch.rpm

yum -y install postgresql10-server postgresql10-contrib postgresql10
/usr/pgsql-10/bin/postgresql-10-setup initdb

nano /var/lib/pgsql/10/data/pg_hba.conf
host    all             all             127.0.0.1/32            md5
host    all             all             ::1/128                 md5
local   all             postgres                                peer

sudo systemctl start postgresql-10
sudo systemctl enable postgresql-10
</code></pre>

<h3 id="install-postgresql-10-on-centos-8">Install PostgreSQL 10 on CentOS 8</h3>

<pre><code>yum install postgresql postgresql-server postgresql-contrib -y

postgresql-setup --initdb

nano /var/lib/pgsql/data/pg_hba.conf
host    all             all             127.0.0.1/32            md5
host    all             all             ::1/128                 md5
local   all             postgres                                peer

systemctl start postgresql
systemctl enable postgresql
</code></pre>

<h3 id="install-postgresql-10-on-debian">Install PostgreSQL 10 on Debian</h3>

<pre><code>wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
sudo sh -c 'echo &quot;deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -sc)-pgdg main&quot; &gt; /etc/apt/sources.list.d/PostgreSQL.list'

apt update
apt-get install postgresql-10
apt-get install pgadmin4

systemctl enable postgresql.service
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Nemon with Influxdb storage]]></title>
            <link href="https://devopstales.github.io/home/naemon-influxdb/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/naemon-influxdb/</id>
            
            
            <published>2019-01-01T00:00:00+00:00</published>
            <updated>2019-01-01T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<p>I prefer to use Naemon (a fork of nagos) with Influxdb as a storage for graphical data.</p>

<h3 id="install-naemon">Install Naemon</h3>

<pre><code>yum install epel-release nano -y
yum install httpd php php-gd -y

rpm -Uvh &quot;https://labs.consol.de/repo/stable/rhel7/x86_64/labs-consol-stable.rhel7.noarch.rpm&quot;

yum install naemon* -y
yum install nagios-plugins nagios-plugins-all nagios-plugins-nrpe nrpe -y

nano /etc/php.ini
date.timezone = Europe/Budapest


systemctl enable httpd
systemctl enable naemon
systemctl start httpd
systemctl start naemon

htpasswd /etc/thruk/htpasswd thrukadmin
# http://SERVER-IP/naemon
</code></pre>

<h3 id="install-infludxb">Install Infludxb</h3>

<pre><code>cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/influxdb.repo
[influxdb]
name = InfluxDB Repository - RHEL \$releasever
baseurl = https://repos.influxdata.com/rhel/\$releasever/\$basearch/stable
enabled = 1
gpgcheck = 1
gpgkey = https://repos.influxdata.com/influxdb.key
EOF

yum install influxdb -y

nano /etc/influxdb/influxdb.conf
[http]
   enabled = true
   bind-address = &quot;localhost:8086&quot;
   auth-enabled = false

systemctl start influxdb
systemctl enable influxdb
</code></pre>

<h3 id="configurate-naemon">Configurate Naemon</h3>

<pre><code>sed -i &quot;s@^process_performance_data=0@#process_performance_data=0@&quot; /etc/naemon/naemon.cfg
# config nagios
nano /etc/naemon/module-conf.d/nagios_nagflux.cfg
process_performance_data=1

host_perfdata_file=/var/naemon/host-perfdata
host_perfdata_file_template=DATATYPE::HOSTPERFDATA\tTIMET::$TIMET$\tHOSTNAME::$HOSTNAME$\tHOSTPERFDATA::$HOSTPERFDATA$\tHOSTCHECKCOMMAND::$HOSTCHECKCOMMAND$
host_perfdata_file_mode=a
host_perfdata_file_processing_interval=15
host_perfdata_file_processing_command=process-host-perfdata-file-nagflux

service_perfdata_file=/var/naemon/service-perfdata
service_perfdata_file_template=DATATYPE::SERVICEPERFDATA\tTIMET::$TIMET$\tHOSTNAME::$HOSTNAME$\tSERVICEDESC::$SERVICEDESC$\tSERVICEPERFDATA::$SERVICEPERFDATA$\tSERVICECHECKCOMMAND::$SERVICECHECKCOMMAND$
service_perfdata_file_mode=a
service_perfdata_file_processing_interval=15
service_perfdata_file_processing_command=process-service-perfdata-file-nagflux

chown naemon:naemon /etc/naemon/module-conf.d/nagios_nagflux.cfg

nano /etc/naemon/conf.d/histou.cfg
define command {
    command_name    process-host-perfdata-file-nagflux
    command_line    /bin/mv /var/naemon/host-perfdata /var/nagflux/perfdata/$TIMET$.perfdata.host
    }

define command {
    command_name    process-service-perfdata-file-nagflux
    command_line    /bin/mv /var/naemon/service-perfdata /var/nagflux/perfdata/$TIMET$.perfdata.service
    }

define host {
   name       host-grafana
   action_url http://192.168.10.112/grafana/dashboard/script/histou.js?host=$HOSTNAME$&amp;theme=light&amp;annotations=true
   notes_url   http://192.168.10.112/dokuwiki/doku.php?id=inventory:$HOSTNAME$
   register   0
}

define service {
   name       service-grafana
   action_url http://192.168.10.112/grafana/dashboard/script/histou.js?host=$HOSTNAME$&amp;service=$SERVICEDESC$&amp;theme=light&amp;annotations=true
   register   0
}

mkdir /var/naemon/
chown -R naemon:naemon /var/naemon/

cd /etc/thruk/ssi/
cp extinfo-header.ssi.example extinfo-header.ssi
cp status-header.ssi.example status-header.ssi

systemctl restart naemon

ll /var/naemon/
ll /var/nagflux/perfdata/
</code></pre>

<h3 id="install-nagflux">Install nagflux</h3>

<pre><code>cd /usr/bin/
wget https://github.com/Griesbacher/nagflux/releases/download/v0.4.1/nagflux
chmod +x nagflux

mkdir -p /var/nagflux/perfdata
mkdir -p /var/nagflux/spool
chown -R naemon:apache /var/nagflux

mkdir /etc/nagflux
cat &lt;&lt;EOF | sudo tee /etc/nagflux/config.gcfg
[main]
NagiosSpoolfileFolder = &quot;/var/nagflux/perfdata&quot;
NagiosSpoolfileWorker = 1
InfluxWorker = 2
MaxInfluxWorker = 5
DumpFile = &quot;/var/log/nagflux/nagflux.dump&quot;
NagfluxSpoolfileFolder = &quot;/var/nagflux/spool&quot;
FieldSeparator = &quot;&amp;&quot;
BufferSize = 1000
FileBufferSize = 65536
DefaultTarget = &quot;Influxdb&quot;

[Log]
LogFile = &quot;/var/log/nagflux/nagflux.log&quot;
MinSeverity = &quot;INFO&quot;

[InfluxDBGlobal]
CreateDatabaseIfNotExists = true
NastyString = &quot;&quot;
NastyStringToReplace = &quot;&quot;
HostcheckAlias = &quot;hostcheck&quot;

[InfluxDB &quot;nagflux&quot;]
Enabled = true
Version = 1.0
Address = &quot;http://localhost:8086&quot;
Arguments = &quot;precision=ms&amp;db=nagflux&amp;u=admin&amp;p=Password1&quot;
StopPullingDataIfDown = true

[Livestatus]
#tcp or file
Type = &quot;file&quot;
#tcp: 127.0.0.1:6557 or file /var/run/live
Address = &quot;/var/cache/naemon/live&quot;
MinutesToWait = 3
Version = &quot;&quot;
EOF

mkdir /var/log/nagflux
mkdir /var/nagflux

cat &lt;&lt;EOF | sudo tee /etc/systemd/system/nagflux.service
[Unit]
Description=A connector which transforms performancedata from Nagios/Icinga(2)/Naemon to InfluxDB/Elasticsearch
Documentation=https://github.com/Griesbacher/nagflux
After=network-online.target

[Service]
User=root
Group=root
ExecStart=/usr/bin/nagflux -configPath /etc/nagflux/config.gcfg
Restart=on-failure

[Install]
WantedBy=multi-user.target
Alias=nagflux.service
EOF

systemctl daemon-reload
systemctl start nagflux
systemctl enable nagflux

tailf /var/log/nagflux/nagflux.log
</code></pre>

<h3 id="install-grafana">Install grafana</h3>

<pre><code>curl -s https://packagecloud.io/install/repositories/grafana/stable/script.rpm.sh | sudo bash
yum install grafana -y

cp /etc/grafana/grafana.ini /etc/grafana/grafana.ini.bak
echo &quot;&quot; &gt; /etc/grafana/grafana.ini
nano /etc/grafana/grafana.ini
[paths]
logs = /var/log/grafana

[log]
mode = file
[log.file]
level =  Info
daily_rotate = true

[server]
http_port = 3000
http_addr = 0.0.0.0
domain = localhost
root_url = %(protocol)s://%(domain)s/grafana/
enable_gzip = false

[snapshots]
external_enabled = false

[security]
disable_gravatar = true
# same username and password for thruk
admin_user = thrukadmin
admin_password = Password1

[users]
allow_sign_up = false
default_theme = light

[auth.basic]
enabled = false

[auth.proxy]
enabled = true
auto_sign_up = true

[alerting]
enabled = true
execute_alerts = true

nano /etc/httpd/conf.d/grafana.conf
&lt;IfModule !mod_proxy.c&gt;
    LoadModule proxy_module /usr/lib64/httpd/modules/mod_proxy.so
&lt;/IfModule&gt;
&lt;IfModule !mod_proxy_http.c&gt;
    LoadModule proxy_http_module /usr/lib64/httpd/modules/mod_proxy_http.so
&lt;/IfModule&gt;

&lt;Location /grafana&gt;
    ProxyPass http://127.0.0.1:3000 retry=0 disablereuse=On
    ProxyPassReverse http://127.0.0.1:3000/grafana
    RewriteEngine On
    RewriteRule .* - [E=PROXY_USER:%{LA-U:REMOTE_USER},NS]
    SetEnvIf Request_Protocol ^HTTPS.* IS_HTTPS=1
    SetEnvIf Authorization &quot;^.+$&quot; IS_BASIC_AUTH=1
    # without thruk cookie auth, use the proxy user from the rewrite rule above
    RequestHeader set X-WEBAUTH-USER &quot;%{PROXY_USER}s&quot;  env=IS_HTTPS
    RequestHeader set X-WEBAUTH-USER &quot;%{PROXY_USER}e&quot;  env=!IS_HTTPS
    # when thruk cookie auth is used, fallback to remote user directly
    RequestHeader set X-WEBAUTH-USER &quot;%{REMOTE_USER}e&quot; env=!IS_BASIC_AUTH
    RequestHeader unset Authorization
&lt;/Location&gt;

echo &quot;
apiVersion: 1

deleteDatasources:
  - name: nagflux

datasources:
- name: nagflux
  type: influxdb
  url: http://localhost:8086
  access: proxy
  database: nagflux
  isDefault: true
  version: 1
  editable: true
&quot; &gt; /etc/grafana/provisioning/datasources/nagflux.yaml

systemctl start grafana-server
systemctl enable grafana-server
systemctl restart httpd


# http://SERVER-IP:3000
# admin/admin

# datasource:
nagflux
influxdb
http://localhost:8086
</code></pre>

<h3 id="install-histou">Install histou</h3>

<pre><code>cd /tmp
wget -O histou.tar.gz https://github.com/Griesbacher/histou/archive/v0.4.3.tar.gz
mkdir -p /var/www/html/histou
cd /var/www/html/histou
tar xzf /tmp/histou.tar.gz --strip-components 1
cp histou.ini.example histou.ini
cp histou.js /usr/share/grafana/public/dashboards/

nano /usr/share/grafana/public/dashboards/histou.js
var url = 'http://192.168.10.112/histou/';

systemctl restart httpd
systemctl restart grafana-server

# http://192.168.10.112/histou/?host=localhost&amp;service=PING
# http://192.168.10.112:3000/dashboard/script/histou.js?host=localhost&amp;service=PING

# nagios config

sed -i '/name.*generic-host/a\        use                             host-grafana' /etc/naemon/conf.d/templates/hosts.cfg
sed -i '/name.*generic-service/a\        use                             service-grafana' /etc/naemon/conf.d/templates/services.cfg

systemctl restart naemon
</code></pre>

<h3 id="inpluxdb-commands">Inpluxdb commands</h3>

<pre><code>influx
create database nagflux;
CREATE USER &quot;admin&quot; WITH PASSWORD 'Password1' WITH ALL PRIVILEGES;
show DATABASES;
USE nagflux;
select * from /.*/ limit 1;
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Alertmanagger]]></title>
            <link href="https://devopstales.github.io/home/prometheus-alertmanagger/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/prometheus-alertmanagger/</id>
            
            
            <published>2018-10-18T00:00:00+00:00</published>
            <updated>2018-10-18T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Example article description</blockquote><p>The Alertmanager handles alerts sent by client applications such as the Prometheus server.</p>

<h3 id="download-alertmanager">Download Alertmanager</h3>

<pre><code>wget https://github.com/prometheus/alertmanager/releases/download/v0.15.0-rc.1/alertmanager-0.15.0-rc.1.linux-amd64.tar.gz
tar -xzf alertmanager-0.15.0-rc.1.linux-amd64.tar.gz
</code></pre>

<h3 id="install-binaris">Install binaris</h3>

<pre><code>useradd --no-create-home --shell /bin/false alertmanager

mkdir /etc/alertmanager
mkdir /etc/alertmanager/template
mkdir -p /var/lib/alertmanager/data
touch /etc/alertmanager/alertmanager.yml

chown -R alertmanager:alertmanager /etc/alertmanager
chown -R alertmanager:alertmanager /var/lib/alertmanager

cp alertmanager-*linux-amd64/alertmanager /usr/local/bin/
cp alertmanager-*linux-amd64/amtool /usr/local/bin/

chown alertmanager:alertmanager /usr/local/bin/alertmanager
chown alertmanager:alertmanager /usr/local/bin/amtool
</code></pre>

<h3 id="create-servis-for-alertmanager">Create servis for Alertmanager</h3>

<pre><code>nano /etc/systemd/system/alertmanager.service
[Unit]
Description=Prometheus Alertmanager Service
Wants=network-online.target
After=network.target

[Service]
User=alertmanager
Group=alertmanager
Type=simple
ExecStart=/usr/local/bin/alertmanager \
    --config.file /etc/alertmanager/alertmanager.yml \
    --storage.path /var/lib/alertmanager/data
Restart=always

[Install]
WantedBy=multi-user.target
</code></pre>

<h3 id="configure-alertmanager">Configure Alertmanager</h3>

<pre><code>nano /etc/alertmanager/alertmanager.yml
global:
  smtp_smarthost: 'localhost:25'
  smtp_from: 'alertmanager@devopstales.intra'
#  smtp_auth_username: 'alertmanager'
#  smtp_auth_password: 'password'

templates:
- '/etc/alertmanager/template/*.tmpl'

route:
  repeat_interval: 3h
  receiver: mails

receivers:
- name: 'mails'
  email_configs:
  - to: 'admin@devopstales.intra'
</code></pre>

<h3 id="configure-prometheus">Configure Prometheus</h3>

<pre><code>nano /etc/prometheus/prometheus.yml
global:
  scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.

alerting:
  alertmanagers:
  - static_configs:
    - targets:
       - localhost:9093
</code></pre>

<pre><code>sudo systemctl daemon-reload
sudo systemctl enable alertmanager
sudo systemctl start alertmanager
sudo systemctl ststus alertmanager
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install Node-exporter]]></title>
            <link href="https://devopstales.github.io/home/prometheus-node-exporter/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/prometheus-node-exporter/</id>
            
            
            <published>2018-10-17T00:00:00+00:00</published>
            <updated>2018-10-17T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Example article description</blockquote><p>Install node-exporter to provides detailed information about the system, including CPU, disk, and memory usage.</p>

<h3 id="download-node-exporter">Download Node-exporter</h3>

<pre><code>cd /tmp
wget https://github.com/prometheus/node_exporter/releases/download/v0.16.0/node_exporter-0.16.0.linux-amd64.tar.gz
tar -xzf node_exporter-0.16.0.linux-amd64.tar.gz
</code></pre>

<h3 id="install-binaris">Install binaris</h3>

<pre><code>sudo useradd -rs /bin/false node_exporter

sudo mv node_exporter*linux-amd64/node_exporter /usr/local/bin
mkdir -p /etc/node_exporter/data

chown -R node_exporter:node_exporter /etc/node_exporter

# host role based teg
cat &lt;&lt;EOF &gt; /etc/node_exporter/data/roles.prom
machine_role{role=&quot;postfix&quot;} 1
machine_role{role=&quot;apache&quot;} 1
EOF
</code></pre>

<h3 id="create-servis-for-node-exporter">Create servis for Node-exporter</h3>

<pre><code>cat &lt;&lt;EOF &gt; /etc/systemd/system/node_exporter.service
[Unit]
Description=Node Exporter
Wants=network-online.target
After=network-online.target

[Service]
User=node_exporter
Group=node_exporter
Type=simple
ExecStart=/usr/local/bin/node_exporter --collector.textfile.directory /etc/node_exporter/data/

[Install]
WantedBy=multi-user.target
EOF
</code></pre>

<pre><code>systemctl daemon-reload
systemctl enable node_exporter
systemctl start node_exporter
systemctl status node_exporter
</code></pre>

<h3 id="configure-prometheus">Configure Prometheus</h3>

<pre><code>nano /etc/prometheus/prometheus.yml
global:
  scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.

alerting:
  alertmanagers:
  - static_configs:
    - targets:
       - localhost:9093

rule_files:
  # - &quot;first_rules.yml&quot;
  # - &quot;second_rules.yml&quot;

scrape_configs:
  - job_name: 'prometheus_metrics'
    scrape_interval: 5s
    static_configs:
      - targets: ['prometheus01.devopstales.intra:9090']

  - job_name: 'node_exporter_metrics'
    scrape_interval: 5s
    static_configs:
      - targets: ['prometheus01.devopstales.intra:9100']
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Prometheus Install]]></title>
            <link href="https://devopstales.github.io/home/prometheus-install/?utm_source=atom_feed" rel="alternate" type="text/html" />
            
            
                <id>https://devopstales.github.io/home/prometheus-install/</id>
            
            
            <published>2018-08-21T00:00:00+00:00</published>
            <updated>2018-08-21T00:00:00+00:00</updated>
            
            
            <content type="html"><![CDATA[<blockquote>Example article description</blockquote><p>Prometheus is an open-source monitoring system with a built-in noSQL time-series database. It offers a multi-dimensional data model, a flexible query language, and diverse visualization possibilities. Prometheus collects metrics from http nedpoint. Most service dind&rsquo;t have this endpoint so you need optional programs that generate additional metrics cald exporters.</p>

<p>In this tutorial, you&rsquo;ll install, configure, and secure Prometheus and Node Exporter to generate metrics about your server&rsquo;s performance.</p>

<h3 id="download-prometheus">Download Prometheus</h3>

<pre><code>curl -LO &quot;https://github.com/prometheus/prometheus/releases/download/v2.2.1/prometheus-2.2.1.linux-amd64.tar.gz&quot;
tar -xzf prometheus-2.2.1.linux-amd64.tar.gz
</code></pre>

<h3 id="install-binaris">Install binaris</h3>

<pre><code>cp prometheus-*linux-amd64/prometheus /usr/local/bin/
cp prometheus-*linux-amd64/promtool /usr/local/bin/

useradd --no-create-home --shell /bin/false prometheus

mkdir /etc/prometheus
mkdir /var/lib/prometheus

chown prometheus:prometheus /var/lib/prometheus
chown prometheus:prometheus /usr/local/bin/prometheus
chown prometheus:prometheus /usr/local/bin/promtool

cp -r prometheus-*linux-amd64/consoles /etc/prometheus
cp -r prometheus-*linux-amd64/console_libraries /etc/prometheus

chown -R prometheus:prometheus /etc/prometheus/consoles
chown -R prometheus:prometheus /etc/prometheus/console_libraries

cp prometheus-*linux-amd64/prometheus.yml /etc/prometheus/
chown -R prometheus:prometheus /etc/prometheus/prometheus.yml
</code></pre>

<h3 id="create-servis-for-prometheus">Create servis for prometheus</h3>

<pre><code>nano /etc/systemd/system/prometheus.service
[Unit]
Description=Prometheus
Wants=network-online.target
After=network-online.target

[Service]
User=prometheus
Group=prometheus
Type=simple
ExecStart=/usr/local/bin/prometheus \
    --config.file /etc/prometheus/prometheus.yml \
    --storage.tsdb.path /var/lib/prometheus/ \
    --web.console.templates=/etc/prometheus/consoles \
    --web.console.libraries=/etc/prometheus/console_libraries

[Install]
WantedBy=multi-user.target
</code></pre>

<h3 id="configure-prometheus">Configure Prometheus</h3>

<pre><code>nano /etc/prometheus/prometheus.yml

global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'prometheus'
    scrape_interval: 5s
    static_configs:
      - targets: ['localhost:9090']
</code></pre>

<pre><code>systemctl daemon-reload
systemctl start prometheus
systemctl status prometheus
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
</feed>
